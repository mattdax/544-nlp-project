{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.6024096385542169,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006024096385542169,
      "grad_norm": 1.473740577697754,
      "learning_rate": 4.999246987951807e-06,
      "loss": 3.1348,
      "step": 1
    },
    {
      "epoch": 0.0012048192771084338,
      "grad_norm": 1.4747812747955322,
      "learning_rate": 4.998493975903615e-06,
      "loss": 3.0872,
      "step": 2
    },
    {
      "epoch": 0.0018072289156626507,
      "grad_norm": 1.4424713850021362,
      "learning_rate": 4.997740963855422e-06,
      "loss": 3.1008,
      "step": 3
    },
    {
      "epoch": 0.0024096385542168677,
      "grad_norm": 1.3801730871200562,
      "learning_rate": 4.99698795180723e-06,
      "loss": 2.919,
      "step": 4
    },
    {
      "epoch": 0.0030120481927710845,
      "grad_norm": 1.4852893352508545,
      "learning_rate": 4.996234939759037e-06,
      "loss": 3.081,
      "step": 5
    },
    {
      "epoch": 0.0036144578313253013,
      "grad_norm": 1.5137826204299927,
      "learning_rate": 4.995481927710844e-06,
      "loss": 3.0792,
      "step": 6
    },
    {
      "epoch": 0.004216867469879518,
      "grad_norm": 1.4194889068603516,
      "learning_rate": 4.9947289156626514e-06,
      "loss": 3.0358,
      "step": 7
    },
    {
      "epoch": 0.004819277108433735,
      "grad_norm": 1.4966731071472168,
      "learning_rate": 4.993975903614458e-06,
      "loss": 3.0494,
      "step": 8
    },
    {
      "epoch": 0.005421686746987952,
      "grad_norm": 1.4340115785598755,
      "learning_rate": 4.993222891566265e-06,
      "loss": 2.9788,
      "step": 9
    },
    {
      "epoch": 0.006024096385542169,
      "grad_norm": 1.4976229667663574,
      "learning_rate": 4.992469879518072e-06,
      "loss": 3.125,
      "step": 10
    },
    {
      "epoch": 0.006626506024096385,
      "grad_norm": 1.4902578592300415,
      "learning_rate": 4.99171686746988e-06,
      "loss": 3.0341,
      "step": 11
    },
    {
      "epoch": 0.007228915662650603,
      "grad_norm": 1.486533284187317,
      "learning_rate": 4.990963855421687e-06,
      "loss": 3.0247,
      "step": 12
    },
    {
      "epoch": 0.00783132530120482,
      "grad_norm": 1.412873387336731,
      "learning_rate": 4.990210843373494e-06,
      "loss": 2.9421,
      "step": 13
    },
    {
      "epoch": 0.008433734939759036,
      "grad_norm": 1.5661983489990234,
      "learning_rate": 4.989457831325302e-06,
      "loss": 3.0958,
      "step": 14
    },
    {
      "epoch": 0.009036144578313253,
      "grad_norm": 1.4612163305282593,
      "learning_rate": 4.9887048192771085e-06,
      "loss": 2.9777,
      "step": 15
    },
    {
      "epoch": 0.00963855421686747,
      "grad_norm": 1.5731759071350098,
      "learning_rate": 4.987951807228916e-06,
      "loss": 3.1209,
      "step": 16
    },
    {
      "epoch": 0.010240963855421687,
      "grad_norm": 1.754318356513977,
      "learning_rate": 4.987198795180723e-06,
      "loss": 2.9918,
      "step": 17
    },
    {
      "epoch": 0.010843373493975903,
      "grad_norm": 1.5088142156600952,
      "learning_rate": 4.986445783132531e-06,
      "loss": 2.9968,
      "step": 18
    },
    {
      "epoch": 0.01144578313253012,
      "grad_norm": 1.4607973098754883,
      "learning_rate": 4.985692771084338e-06,
      "loss": 2.942,
      "step": 19
    },
    {
      "epoch": 0.012048192771084338,
      "grad_norm": 1.540770173072815,
      "learning_rate": 4.984939759036145e-06,
      "loss": 2.9179,
      "step": 20
    },
    {
      "epoch": 0.012650602409638554,
      "grad_norm": 1.565718173980713,
      "learning_rate": 4.984186746987953e-06,
      "loss": 3.0175,
      "step": 21
    },
    {
      "epoch": 0.01325301204819277,
      "grad_norm": 1.6376641988754272,
      "learning_rate": 4.9834337349397595e-06,
      "loss": 3.0746,
      "step": 22
    },
    {
      "epoch": 0.013855421686746987,
      "grad_norm": 1.535663366317749,
      "learning_rate": 4.982680722891567e-06,
      "loss": 2.9844,
      "step": 23
    },
    {
      "epoch": 0.014457831325301205,
      "grad_norm": 1.6042248010635376,
      "learning_rate": 4.981927710843374e-06,
      "loss": 2.9934,
      "step": 24
    },
    {
      "epoch": 0.015060240963855422,
      "grad_norm": 1.704008936882019,
      "learning_rate": 4.981174698795181e-06,
      "loss": 2.9837,
      "step": 25
    },
    {
      "epoch": 0.01566265060240964,
      "grad_norm": 1.5935983657836914,
      "learning_rate": 4.980421686746988e-06,
      "loss": 2.9995,
      "step": 26
    },
    {
      "epoch": 0.016265060240963854,
      "grad_norm": 1.625032663345337,
      "learning_rate": 4.979668674698796e-06,
      "loss": 2.9215,
      "step": 27
    },
    {
      "epoch": 0.016867469879518072,
      "grad_norm": 1.6691863536834717,
      "learning_rate": 4.978915662650603e-06,
      "loss": 2.999,
      "step": 28
    },
    {
      "epoch": 0.01746987951807229,
      "grad_norm": 2.006518840789795,
      "learning_rate": 4.97816265060241e-06,
      "loss": 2.9524,
      "step": 29
    },
    {
      "epoch": 0.018072289156626505,
      "grad_norm": 1.969132423400879,
      "learning_rate": 4.9774096385542175e-06,
      "loss": 3.0297,
      "step": 30
    },
    {
      "epoch": 0.018674698795180723,
      "grad_norm": 1.7055975198745728,
      "learning_rate": 4.976656626506024e-06,
      "loss": 3.0185,
      "step": 31
    },
    {
      "epoch": 0.01927710843373494,
      "grad_norm": 1.650692105293274,
      "learning_rate": 4.975903614457831e-06,
      "loss": 2.9986,
      "step": 32
    },
    {
      "epoch": 0.019879518072289156,
      "grad_norm": 2.550161838531494,
      "learning_rate": 4.975150602409639e-06,
      "loss": 2.9903,
      "step": 33
    },
    {
      "epoch": 0.020481927710843374,
      "grad_norm": 1.6015815734863281,
      "learning_rate": 4.974397590361446e-06,
      "loss": 2.9555,
      "step": 34
    },
    {
      "epoch": 0.02108433734939759,
      "grad_norm": 1.7236346006393433,
      "learning_rate": 4.973644578313254e-06,
      "loss": 3.0156,
      "step": 35
    },
    {
      "epoch": 0.021686746987951807,
      "grad_norm": 1.6021451950073242,
      "learning_rate": 4.972891566265061e-06,
      "loss": 2.8815,
      "step": 36
    },
    {
      "epoch": 0.022289156626506025,
      "grad_norm": 1.7612340450286865,
      "learning_rate": 4.972138554216868e-06,
      "loss": 2.8865,
      "step": 37
    },
    {
      "epoch": 0.02289156626506024,
      "grad_norm": 1.6199023723602295,
      "learning_rate": 4.971385542168675e-06,
      "loss": 2.9298,
      "step": 38
    },
    {
      "epoch": 0.023493975903614458,
      "grad_norm": 1.7079846858978271,
      "learning_rate": 4.970632530120482e-06,
      "loss": 2.9558,
      "step": 39
    },
    {
      "epoch": 0.024096385542168676,
      "grad_norm": 1.7215967178344727,
      "learning_rate": 4.96987951807229e-06,
      "loss": 2.9865,
      "step": 40
    },
    {
      "epoch": 0.02469879518072289,
      "grad_norm": 1.9738972187042236,
      "learning_rate": 4.969126506024097e-06,
      "loss": 2.9504,
      "step": 41
    },
    {
      "epoch": 0.02530120481927711,
      "grad_norm": 1.7214441299438477,
      "learning_rate": 4.968373493975904e-06,
      "loss": 2.8726,
      "step": 42
    },
    {
      "epoch": 0.025903614457831327,
      "grad_norm": 1.6974633932113647,
      "learning_rate": 4.967620481927711e-06,
      "loss": 2.8846,
      "step": 43
    },
    {
      "epoch": 0.02650602409638554,
      "grad_norm": 1.5904995203018188,
      "learning_rate": 4.966867469879519e-06,
      "loss": 2.7622,
      "step": 44
    },
    {
      "epoch": 0.02710843373493976,
      "grad_norm": 1.7613439559936523,
      "learning_rate": 4.966114457831326e-06,
      "loss": 2.8873,
      "step": 45
    },
    {
      "epoch": 0.027710843373493974,
      "grad_norm": 1.636739730834961,
      "learning_rate": 4.9653614457831325e-06,
      "loss": 2.8282,
      "step": 46
    },
    {
      "epoch": 0.028313253012048192,
      "grad_norm": 1.7522393465042114,
      "learning_rate": 4.96460843373494e-06,
      "loss": 2.8216,
      "step": 47
    },
    {
      "epoch": 0.02891566265060241,
      "grad_norm": 1.779678463935852,
      "learning_rate": 4.963855421686747e-06,
      "loss": 2.8863,
      "step": 48
    },
    {
      "epoch": 0.029518072289156625,
      "grad_norm": 1.8429738283157349,
      "learning_rate": 4.963102409638554e-06,
      "loss": 2.8981,
      "step": 49
    },
    {
      "epoch": 0.030120481927710843,
      "grad_norm": 1.8048309087753296,
      "learning_rate": 4.962349397590362e-06,
      "loss": 2.8496,
      "step": 50
    },
    {
      "epoch": 0.03072289156626506,
      "grad_norm": 1.857170820236206,
      "learning_rate": 4.961596385542169e-06,
      "loss": 2.903,
      "step": 51
    },
    {
      "epoch": 0.03132530120481928,
      "grad_norm": 1.7957650423049927,
      "learning_rate": 4.960843373493977e-06,
      "loss": 2.8482,
      "step": 52
    },
    {
      "epoch": 0.031927710843373494,
      "grad_norm": 2.0291588306427,
      "learning_rate": 4.9600903614457835e-06,
      "loss": 2.8912,
      "step": 53
    },
    {
      "epoch": 0.03253012048192771,
      "grad_norm": 1.8417772054672241,
      "learning_rate": 4.959337349397591e-06,
      "loss": 2.8569,
      "step": 54
    },
    {
      "epoch": 0.03313253012048193,
      "grad_norm": 1.9789530038833618,
      "learning_rate": 4.958584337349398e-06,
      "loss": 2.9424,
      "step": 55
    },
    {
      "epoch": 0.033734939759036145,
      "grad_norm": 1.8783278465270996,
      "learning_rate": 4.957831325301205e-06,
      "loss": 2.8678,
      "step": 56
    },
    {
      "epoch": 0.03433734939759036,
      "grad_norm": 1.9034353494644165,
      "learning_rate": 4.957078313253013e-06,
      "loss": 2.9186,
      "step": 57
    },
    {
      "epoch": 0.03493975903614458,
      "grad_norm": 1.835208773612976,
      "learning_rate": 4.95632530120482e-06,
      "loss": 2.801,
      "step": 58
    },
    {
      "epoch": 0.035542168674698796,
      "grad_norm": 1.9433399438858032,
      "learning_rate": 4.955572289156627e-06,
      "loss": 2.9268,
      "step": 59
    },
    {
      "epoch": 0.03614457831325301,
      "grad_norm": 1.872772216796875,
      "learning_rate": 4.9548192771084345e-06,
      "loss": 2.8414,
      "step": 60
    },
    {
      "epoch": 0.03674698795180723,
      "grad_norm": 1.9383940696716309,
      "learning_rate": 4.9540662650602415e-06,
      "loss": 2.9157,
      "step": 61
    },
    {
      "epoch": 0.03734939759036145,
      "grad_norm": 1.8429044485092163,
      "learning_rate": 4.953313253012048e-06,
      "loss": 2.7457,
      "step": 62
    },
    {
      "epoch": 0.03795180722891566,
      "grad_norm": 1.8404359817504883,
      "learning_rate": 4.952560240963855e-06,
      "loss": 2.79,
      "step": 63
    },
    {
      "epoch": 0.03855421686746988,
      "grad_norm": 1.8242859840393066,
      "learning_rate": 4.951807228915663e-06,
      "loss": 2.8098,
      "step": 64
    },
    {
      "epoch": 0.0391566265060241,
      "grad_norm": 2.6848158836364746,
      "learning_rate": 4.95105421686747e-06,
      "loss": 2.8277,
      "step": 65
    },
    {
      "epoch": 0.03975903614457831,
      "grad_norm": 1.8670793771743774,
      "learning_rate": 4.950301204819278e-06,
      "loss": 2.7951,
      "step": 66
    },
    {
      "epoch": 0.04036144578313253,
      "grad_norm": 1.9470345973968506,
      "learning_rate": 4.949548192771085e-06,
      "loss": 2.7731,
      "step": 67
    },
    {
      "epoch": 0.04096385542168675,
      "grad_norm": 2.2263574600219727,
      "learning_rate": 4.948795180722892e-06,
      "loss": 2.7721,
      "step": 68
    },
    {
      "epoch": 0.04156626506024096,
      "grad_norm": 1.9051399230957031,
      "learning_rate": 4.948042168674699e-06,
      "loss": 2.7183,
      "step": 69
    },
    {
      "epoch": 0.04216867469879518,
      "grad_norm": 1.9903534650802612,
      "learning_rate": 4.947289156626506e-06,
      "loss": 2.801,
      "step": 70
    },
    {
      "epoch": 0.0427710843373494,
      "grad_norm": 1.7365846633911133,
      "learning_rate": 4.946536144578314e-06,
      "loss": 2.6404,
      "step": 71
    },
    {
      "epoch": 0.043373493975903614,
      "grad_norm": 1.9861185550689697,
      "learning_rate": 4.945783132530121e-06,
      "loss": 2.8174,
      "step": 72
    },
    {
      "epoch": 0.04397590361445783,
      "grad_norm": 1.8458343744277954,
      "learning_rate": 4.945030120481928e-06,
      "loss": 2.7475,
      "step": 73
    },
    {
      "epoch": 0.04457831325301205,
      "grad_norm": 1.9032632112503052,
      "learning_rate": 4.944277108433736e-06,
      "loss": 2.707,
      "step": 74
    },
    {
      "epoch": 0.045180722891566265,
      "grad_norm": 1.9599093198776245,
      "learning_rate": 4.943524096385543e-06,
      "loss": 2.6694,
      "step": 75
    },
    {
      "epoch": 0.04578313253012048,
      "grad_norm": 2.004028558731079,
      "learning_rate": 4.9427710843373496e-06,
      "loss": 2.7501,
      "step": 76
    },
    {
      "epoch": 0.0463855421686747,
      "grad_norm": 1.873877763748169,
      "learning_rate": 4.942018072289157e-06,
      "loss": 2.6985,
      "step": 77
    },
    {
      "epoch": 0.046987951807228916,
      "grad_norm": 2.001469373703003,
      "learning_rate": 4.941265060240964e-06,
      "loss": 2.6939,
      "step": 78
    },
    {
      "epoch": 0.04759036144578313,
      "grad_norm": 2.03161883354187,
      "learning_rate": 4.940512048192771e-06,
      "loss": 2.693,
      "step": 79
    },
    {
      "epoch": 0.04819277108433735,
      "grad_norm": 1.9754451513290405,
      "learning_rate": 4.939759036144578e-06,
      "loss": 2.699,
      "step": 80
    },
    {
      "epoch": 0.04879518072289157,
      "grad_norm": 2.01879620552063,
      "learning_rate": 4.939006024096386e-06,
      "loss": 2.6982,
      "step": 81
    },
    {
      "epoch": 0.04939759036144578,
      "grad_norm": 1.9582531452178955,
      "learning_rate": 4.938253012048193e-06,
      "loss": 2.6839,
      "step": 82
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.9701021909713745,
      "learning_rate": 4.937500000000001e-06,
      "loss": 2.7169,
      "step": 83
    },
    {
      "epoch": 0.05060240963855422,
      "grad_norm": 1.8917077779769897,
      "learning_rate": 4.9367469879518075e-06,
      "loss": 2.6248,
      "step": 84
    },
    {
      "epoch": 0.05120481927710843,
      "grad_norm": 1.9316285848617554,
      "learning_rate": 4.9359939759036144e-06,
      "loss": 2.6465,
      "step": 85
    },
    {
      "epoch": 0.051807228915662654,
      "grad_norm": 1.978715419769287,
      "learning_rate": 4.935240963855422e-06,
      "loss": 2.621,
      "step": 86
    },
    {
      "epoch": 0.05240963855421687,
      "grad_norm": 1.9492319822311401,
      "learning_rate": 4.934487951807229e-06,
      "loss": 2.5716,
      "step": 87
    },
    {
      "epoch": 0.05301204819277108,
      "grad_norm": 1.9757020473480225,
      "learning_rate": 4.933734939759037e-06,
      "loss": 2.6228,
      "step": 88
    },
    {
      "epoch": 0.053614457831325305,
      "grad_norm": 1.9418715238571167,
      "learning_rate": 4.932981927710844e-06,
      "loss": 2.6106,
      "step": 89
    },
    {
      "epoch": 0.05421686746987952,
      "grad_norm": 1.933706283569336,
      "learning_rate": 4.932228915662652e-06,
      "loss": 2.6018,
      "step": 90
    },
    {
      "epoch": 0.054819277108433734,
      "grad_norm": 1.9495391845703125,
      "learning_rate": 4.9314759036144585e-06,
      "loss": 2.6195,
      "step": 91
    },
    {
      "epoch": 0.05542168674698795,
      "grad_norm": 1.9512412548065186,
      "learning_rate": 4.9307228915662654e-06,
      "loss": 2.5641,
      "step": 92
    },
    {
      "epoch": 0.05602409638554217,
      "grad_norm": 1.9239850044250488,
      "learning_rate": 4.929969879518073e-06,
      "loss": 2.5893,
      "step": 93
    },
    {
      "epoch": 0.056626506024096385,
      "grad_norm": 1.8520278930664062,
      "learning_rate": 4.92921686746988e-06,
      "loss": 2.5251,
      "step": 94
    },
    {
      "epoch": 0.0572289156626506,
      "grad_norm": 2.0181539058685303,
      "learning_rate": 4.928463855421687e-06,
      "loss": 2.5568,
      "step": 95
    },
    {
      "epoch": 0.05783132530120482,
      "grad_norm": 1.9635249376296997,
      "learning_rate": 4.927710843373494e-06,
      "loss": 2.5604,
      "step": 96
    },
    {
      "epoch": 0.058433734939759036,
      "grad_norm": 1.9368311166763306,
      "learning_rate": 4.926957831325302e-06,
      "loss": 2.5741,
      "step": 97
    },
    {
      "epoch": 0.05903614457831325,
      "grad_norm": 1.9514737129211426,
      "learning_rate": 4.926204819277109e-06,
      "loss": 2.5503,
      "step": 98
    },
    {
      "epoch": 0.05963855421686747,
      "grad_norm": 1.944197654724121,
      "learning_rate": 4.925451807228916e-06,
      "loss": 2.5346,
      "step": 99
    },
    {
      "epoch": 0.060240963855421686,
      "grad_norm": 1.9432839155197144,
      "learning_rate": 4.924698795180723e-06,
      "loss": 2.545,
      "step": 100
    },
    {
      "epoch": 0.0608433734939759,
      "grad_norm": 2.4351277351379395,
      "learning_rate": 4.92394578313253e-06,
      "loss": 2.5633,
      "step": 101
    },
    {
      "epoch": 0.06144578313253012,
      "grad_norm": 1.9407269954681396,
      "learning_rate": 4.923192771084338e-06,
      "loss": 2.5454,
      "step": 102
    },
    {
      "epoch": 0.06204819277108434,
      "grad_norm": 1.9103487730026245,
      "learning_rate": 4.922439759036145e-06,
      "loss": 2.4921,
      "step": 103
    },
    {
      "epoch": 0.06265060240963856,
      "grad_norm": 2.0310845375061035,
      "learning_rate": 4.921686746987952e-06,
      "loss": 2.4952,
      "step": 104
    },
    {
      "epoch": 0.06325301204819277,
      "grad_norm": 1.9880471229553223,
      "learning_rate": 4.92093373493976e-06,
      "loss": 2.5261,
      "step": 105
    },
    {
      "epoch": 0.06385542168674699,
      "grad_norm": 1.9818381071090698,
      "learning_rate": 4.920180722891567e-06,
      "loss": 2.5571,
      "step": 106
    },
    {
      "epoch": 0.06445783132530121,
      "grad_norm": 1.92890202999115,
      "learning_rate": 4.919427710843374e-06,
      "loss": 2.461,
      "step": 107
    },
    {
      "epoch": 0.06506024096385542,
      "grad_norm": 1.8091654777526855,
      "learning_rate": 4.918674698795181e-06,
      "loss": 2.4532,
      "step": 108
    },
    {
      "epoch": 0.06566265060240964,
      "grad_norm": 2.071336030960083,
      "learning_rate": 4.917921686746988e-06,
      "loss": 2.4692,
      "step": 109
    },
    {
      "epoch": 0.06626506024096386,
      "grad_norm": 1.8897559642791748,
      "learning_rate": 4.917168674698796e-06,
      "loss": 2.4916,
      "step": 110
    },
    {
      "epoch": 0.06686746987951807,
      "grad_norm": 1.8858671188354492,
      "learning_rate": 4.916415662650603e-06,
      "loss": 2.4089,
      "step": 111
    },
    {
      "epoch": 0.06746987951807229,
      "grad_norm": 2.057424545288086,
      "learning_rate": 4.91566265060241e-06,
      "loss": 2.4985,
      "step": 112
    },
    {
      "epoch": 0.06807228915662651,
      "grad_norm": 1.9049299955368042,
      "learning_rate": 4.914909638554217e-06,
      "loss": 2.4241,
      "step": 113
    },
    {
      "epoch": 0.06867469879518072,
      "grad_norm": 1.963156819343567,
      "learning_rate": 4.9141566265060246e-06,
      "loss": 2.4726,
      "step": 114
    },
    {
      "epoch": 0.06927710843373494,
      "grad_norm": 1.8943158388137817,
      "learning_rate": 4.9134036144578315e-06,
      "loss": 2.3883,
      "step": 115
    },
    {
      "epoch": 0.06987951807228916,
      "grad_norm": 1.8814094066619873,
      "learning_rate": 4.912650602409638e-06,
      "loss": 2.4412,
      "step": 116
    },
    {
      "epoch": 0.07048192771084337,
      "grad_norm": 1.922216773033142,
      "learning_rate": 4.911897590361446e-06,
      "loss": 2.3972,
      "step": 117
    },
    {
      "epoch": 0.07108433734939759,
      "grad_norm": 2.0009446144104004,
      "learning_rate": 4.911144578313253e-06,
      "loss": 2.4212,
      "step": 118
    },
    {
      "epoch": 0.07168674698795181,
      "grad_norm": 1.8443005084991455,
      "learning_rate": 4.910391566265061e-06,
      "loss": 2.3541,
      "step": 119
    },
    {
      "epoch": 0.07228915662650602,
      "grad_norm": 1.954594612121582,
      "learning_rate": 4.909638554216868e-06,
      "loss": 2.3404,
      "step": 120
    },
    {
      "epoch": 0.07289156626506024,
      "grad_norm": 2.1541054248809814,
      "learning_rate": 4.908885542168675e-06,
      "loss": 2.3936,
      "step": 121
    },
    {
      "epoch": 0.07349397590361446,
      "grad_norm": 1.8508579730987549,
      "learning_rate": 4.9081325301204825e-06,
      "loss": 2.3971,
      "step": 122
    },
    {
      "epoch": 0.07409638554216867,
      "grad_norm": 1.808905839920044,
      "learning_rate": 4.9073795180722894e-06,
      "loss": 2.3069,
      "step": 123
    },
    {
      "epoch": 0.0746987951807229,
      "grad_norm": 2.0047147274017334,
      "learning_rate": 4.906626506024097e-06,
      "loss": 2.4361,
      "step": 124
    },
    {
      "epoch": 0.07530120481927711,
      "grad_norm": 2.1422369480133057,
      "learning_rate": 4.905873493975904e-06,
      "loss": 2.3731,
      "step": 125
    },
    {
      "epoch": 0.07590361445783132,
      "grad_norm": 1.795076847076416,
      "learning_rate": 4.905120481927712e-06,
      "loss": 2.3497,
      "step": 126
    },
    {
      "epoch": 0.07650602409638554,
      "grad_norm": 1.6871236562728882,
      "learning_rate": 4.904367469879519e-06,
      "loss": 2.2504,
      "step": 127
    },
    {
      "epoch": 0.07710843373493977,
      "grad_norm": 1.874390959739685,
      "learning_rate": 4.903614457831326e-06,
      "loss": 2.3499,
      "step": 128
    },
    {
      "epoch": 0.07771084337349397,
      "grad_norm": 1.7345837354660034,
      "learning_rate": 4.902861445783133e-06,
      "loss": 2.2352,
      "step": 129
    },
    {
      "epoch": 0.0783132530120482,
      "grad_norm": 1.6911994218826294,
      "learning_rate": 4.9021084337349405e-06,
      "loss": 2.1987,
      "step": 130
    },
    {
      "epoch": 0.0789156626506024,
      "grad_norm": 1.7713083028793335,
      "learning_rate": 4.901355421686747e-06,
      "loss": 2.2679,
      "step": 131
    },
    {
      "epoch": 0.07951807228915662,
      "grad_norm": 1.7190380096435547,
      "learning_rate": 4.900602409638554e-06,
      "loss": 2.263,
      "step": 132
    },
    {
      "epoch": 0.08012048192771085,
      "grad_norm": 1.7694402933120728,
      "learning_rate": 4.899849397590361e-06,
      "loss": 2.2921,
      "step": 133
    },
    {
      "epoch": 0.08072289156626505,
      "grad_norm": 1.7882152795791626,
      "learning_rate": 4.899096385542169e-06,
      "loss": 2.2819,
      "step": 134
    },
    {
      "epoch": 0.08132530120481928,
      "grad_norm": 1.794653296470642,
      "learning_rate": 4.898343373493976e-06,
      "loss": 2.277,
      "step": 135
    },
    {
      "epoch": 0.0819277108433735,
      "grad_norm": 1.7778522968292236,
      "learning_rate": 4.897590361445784e-06,
      "loss": 2.2266,
      "step": 136
    },
    {
      "epoch": 0.0825301204819277,
      "grad_norm": 1.694179892539978,
      "learning_rate": 4.896837349397591e-06,
      "loss": 2.214,
      "step": 137
    },
    {
      "epoch": 0.08313253012048193,
      "grad_norm": 1.7486155033111572,
      "learning_rate": 4.896084337349398e-06,
      "loss": 2.2603,
      "step": 138
    },
    {
      "epoch": 0.08373493975903615,
      "grad_norm": 1.7560853958129883,
      "learning_rate": 4.895331325301205e-06,
      "loss": 2.2442,
      "step": 139
    },
    {
      "epoch": 0.08433734939759036,
      "grad_norm": 1.6624503135681152,
      "learning_rate": 4.894578313253012e-06,
      "loss": 2.189,
      "step": 140
    },
    {
      "epoch": 0.08493975903614458,
      "grad_norm": 1.6971596479415894,
      "learning_rate": 4.89382530120482e-06,
      "loss": 2.1953,
      "step": 141
    },
    {
      "epoch": 0.0855421686746988,
      "grad_norm": 1.7242510318756104,
      "learning_rate": 4.893072289156627e-06,
      "loss": 2.2044,
      "step": 142
    },
    {
      "epoch": 0.086144578313253,
      "grad_norm": 1.6905677318572998,
      "learning_rate": 4.892319277108435e-06,
      "loss": 2.1698,
      "step": 143
    },
    {
      "epoch": 0.08674698795180723,
      "grad_norm": 1.6370210647583008,
      "learning_rate": 4.891566265060242e-06,
      "loss": 2.2041,
      "step": 144
    },
    {
      "epoch": 0.08734939759036145,
      "grad_norm": 1.7600444555282593,
      "learning_rate": 4.8908132530120486e-06,
      "loss": 2.1849,
      "step": 145
    },
    {
      "epoch": 0.08795180722891566,
      "grad_norm": 1.6222892999649048,
      "learning_rate": 4.8900602409638555e-06,
      "loss": 2.1211,
      "step": 146
    },
    {
      "epoch": 0.08855421686746988,
      "grad_norm": 1.724839448928833,
      "learning_rate": 4.889307228915663e-06,
      "loss": 2.1977,
      "step": 147
    },
    {
      "epoch": 0.0891566265060241,
      "grad_norm": 1.6800349950790405,
      "learning_rate": 4.88855421686747e-06,
      "loss": 2.2406,
      "step": 148
    },
    {
      "epoch": 0.08975903614457831,
      "grad_norm": 1.6775932312011719,
      "learning_rate": 4.887801204819277e-06,
      "loss": 2.1723,
      "step": 149
    },
    {
      "epoch": 0.09036144578313253,
      "grad_norm": 1.6793549060821533,
      "learning_rate": 4.887048192771085e-06,
      "loss": 2.2018,
      "step": 150
    },
    {
      "epoch": 0.09096385542168675,
      "grad_norm": 1.6732984781265259,
      "learning_rate": 4.886295180722892e-06,
      "loss": 2.1889,
      "step": 151
    },
    {
      "epoch": 0.09156626506024096,
      "grad_norm": 1.5455031394958496,
      "learning_rate": 4.885542168674699e-06,
      "loss": 2.1362,
      "step": 152
    },
    {
      "epoch": 0.09216867469879518,
      "grad_norm": 1.6271357536315918,
      "learning_rate": 4.8847891566265065e-06,
      "loss": 2.1267,
      "step": 153
    },
    {
      "epoch": 0.0927710843373494,
      "grad_norm": 1.6043084859848022,
      "learning_rate": 4.8840361445783134e-06,
      "loss": 2.1358,
      "step": 154
    },
    {
      "epoch": 0.09337349397590361,
      "grad_norm": 1.4855022430419922,
      "learning_rate": 4.883283132530121e-06,
      "loss": 2.0811,
      "step": 155
    },
    {
      "epoch": 0.09397590361445783,
      "grad_norm": 1.5488998889923096,
      "learning_rate": 4.882530120481928e-06,
      "loss": 2.1251,
      "step": 156
    },
    {
      "epoch": 0.09457831325301205,
      "grad_norm": 1.6124851703643799,
      "learning_rate": 4.881777108433735e-06,
      "loss": 2.1506,
      "step": 157
    },
    {
      "epoch": 0.09518072289156626,
      "grad_norm": 1.6023409366607666,
      "learning_rate": 4.881024096385543e-06,
      "loss": 2.1106,
      "step": 158
    },
    {
      "epoch": 0.09578313253012048,
      "grad_norm": 1.559129238128662,
      "learning_rate": 4.88027108433735e-06,
      "loss": 2.0869,
      "step": 159
    },
    {
      "epoch": 0.0963855421686747,
      "grad_norm": 1.491356611251831,
      "learning_rate": 4.8795180722891575e-06,
      "loss": 2.0678,
      "step": 160
    },
    {
      "epoch": 0.09698795180722891,
      "grad_norm": 1.5758033990859985,
      "learning_rate": 4.8787650602409644e-06,
      "loss": 2.1259,
      "step": 161
    },
    {
      "epoch": 0.09759036144578313,
      "grad_norm": 1.5489639043807983,
      "learning_rate": 4.878012048192771e-06,
      "loss": 2.1218,
      "step": 162
    },
    {
      "epoch": 0.09819277108433735,
      "grad_norm": 1.4820585250854492,
      "learning_rate": 4.877259036144579e-06,
      "loss": 2.065,
      "step": 163
    },
    {
      "epoch": 0.09879518072289156,
      "grad_norm": 1.474873661994934,
      "learning_rate": 4.876506024096386e-06,
      "loss": 2.0096,
      "step": 164
    },
    {
      "epoch": 0.09939759036144578,
      "grad_norm": 1.4372928142547607,
      "learning_rate": 4.875753012048193e-06,
      "loss": 2.0396,
      "step": 165
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.5288587808609009,
      "learning_rate": 4.875e-06,
      "loss": 2.075,
      "step": 166
    },
    {
      "epoch": 0.10060240963855421,
      "grad_norm": 1.4783271551132202,
      "learning_rate": 4.874246987951808e-06,
      "loss": 2.0369,
      "step": 167
    },
    {
      "epoch": 0.10120481927710843,
      "grad_norm": 1.5354979038238525,
      "learning_rate": 4.873493975903615e-06,
      "loss": 2.0599,
      "step": 168
    },
    {
      "epoch": 0.10180722891566266,
      "grad_norm": 1.4715347290039062,
      "learning_rate": 4.8727409638554215e-06,
      "loss": 2.0796,
      "step": 169
    },
    {
      "epoch": 0.10240963855421686,
      "grad_norm": 1.4246761798858643,
      "learning_rate": 4.871987951807229e-06,
      "loss": 1.9957,
      "step": 170
    },
    {
      "epoch": 0.10301204819277109,
      "grad_norm": 1.4608674049377441,
      "learning_rate": 4.871234939759036e-06,
      "loss": 2.0363,
      "step": 171
    },
    {
      "epoch": 0.10361445783132531,
      "grad_norm": 1.4859869480133057,
      "learning_rate": 4.870481927710844e-06,
      "loss": 2.0264,
      "step": 172
    },
    {
      "epoch": 0.10421686746987951,
      "grad_norm": 1.3803449869155884,
      "learning_rate": 4.869728915662651e-06,
      "loss": 1.9844,
      "step": 173
    },
    {
      "epoch": 0.10481927710843374,
      "grad_norm": 1.380644679069519,
      "learning_rate": 4.868975903614459e-06,
      "loss": 2.0533,
      "step": 174
    },
    {
      "epoch": 0.10542168674698796,
      "grad_norm": 1.3377189636230469,
      "learning_rate": 4.868222891566266e-06,
      "loss": 1.97,
      "step": 175
    },
    {
      "epoch": 0.10602409638554217,
      "grad_norm": 1.409239649772644,
      "learning_rate": 4.8674698795180725e-06,
      "loss": 1.9866,
      "step": 176
    },
    {
      "epoch": 0.10662650602409639,
      "grad_norm": 1.394515872001648,
      "learning_rate": 4.86671686746988e-06,
      "loss": 1.9856,
      "step": 177
    },
    {
      "epoch": 0.10722891566265061,
      "grad_norm": 1.2693562507629395,
      "learning_rate": 4.865963855421687e-06,
      "loss": 1.8959,
      "step": 178
    },
    {
      "epoch": 0.10783132530120482,
      "grad_norm": 1.4128996133804321,
      "learning_rate": 4.865210843373494e-06,
      "loss": 2.0283,
      "step": 179
    },
    {
      "epoch": 0.10843373493975904,
      "grad_norm": 1.3271598815917969,
      "learning_rate": 4.864457831325302e-06,
      "loss": 1.9375,
      "step": 180
    },
    {
      "epoch": 0.10903614457831326,
      "grad_norm": 1.2907016277313232,
      "learning_rate": 4.863704819277109e-06,
      "loss": 1.9336,
      "step": 181
    },
    {
      "epoch": 0.10963855421686747,
      "grad_norm": 1.3166767358779907,
      "learning_rate": 4.862951807228916e-06,
      "loss": 1.9641,
      "step": 182
    },
    {
      "epoch": 0.11024096385542169,
      "grad_norm": 1.5335756540298462,
      "learning_rate": 4.862198795180723e-06,
      "loss": 1.9287,
      "step": 183
    },
    {
      "epoch": 0.1108433734939759,
      "grad_norm": 1.2912899255752563,
      "learning_rate": 4.8614457831325305e-06,
      "loss": 1.9707,
      "step": 184
    },
    {
      "epoch": 0.11144578313253012,
      "grad_norm": 1.269958257675171,
      "learning_rate": 4.860692771084337e-06,
      "loss": 1.98,
      "step": 185
    },
    {
      "epoch": 0.11204819277108434,
      "grad_norm": 1.2407171726226807,
      "learning_rate": 4.859939759036145e-06,
      "loss": 1.8997,
      "step": 186
    },
    {
      "epoch": 0.11265060240963855,
      "grad_norm": 1.3027952909469604,
      "learning_rate": 4.859186746987952e-06,
      "loss": 1.9861,
      "step": 187
    },
    {
      "epoch": 0.11325301204819277,
      "grad_norm": 1.1954265832901,
      "learning_rate": 4.858433734939759e-06,
      "loss": 1.8946,
      "step": 188
    },
    {
      "epoch": 0.11385542168674699,
      "grad_norm": 1.2401341199874878,
      "learning_rate": 4.857680722891567e-06,
      "loss": 1.9352,
      "step": 189
    },
    {
      "epoch": 0.1144578313253012,
      "grad_norm": 1.2804239988327026,
      "learning_rate": 4.856927710843374e-06,
      "loss": 1.9664,
      "step": 190
    },
    {
      "epoch": 0.11506024096385542,
      "grad_norm": 1.665408968925476,
      "learning_rate": 4.8561746987951815e-06,
      "loss": 1.941,
      "step": 191
    },
    {
      "epoch": 0.11566265060240964,
      "grad_norm": 1.2245901823043823,
      "learning_rate": 4.8554216867469884e-06,
      "loss": 1.9214,
      "step": 192
    },
    {
      "epoch": 0.11626506024096385,
      "grad_norm": 1.13723623752594,
      "learning_rate": 4.854668674698795e-06,
      "loss": 1.91,
      "step": 193
    },
    {
      "epoch": 0.11686746987951807,
      "grad_norm": 1.1623296737670898,
      "learning_rate": 4.853915662650603e-06,
      "loss": 1.8801,
      "step": 194
    },
    {
      "epoch": 0.11746987951807229,
      "grad_norm": 1.1651372909545898,
      "learning_rate": 4.85316265060241e-06,
      "loss": 1.8965,
      "step": 195
    },
    {
      "epoch": 0.1180722891566265,
      "grad_norm": 1.1593002080917358,
      "learning_rate": 4.852409638554218e-06,
      "loss": 1.9139,
      "step": 196
    },
    {
      "epoch": 0.11867469879518072,
      "grad_norm": 1.0648528337478638,
      "learning_rate": 4.851656626506025e-06,
      "loss": 1.8374,
      "step": 197
    },
    {
      "epoch": 0.11927710843373494,
      "grad_norm": 1.2154555320739746,
      "learning_rate": 4.850903614457832e-06,
      "loss": 1.8943,
      "step": 198
    },
    {
      "epoch": 0.11987951807228915,
      "grad_norm": 1.0764330625534058,
      "learning_rate": 4.850150602409639e-06,
      "loss": 1.8192,
      "step": 199
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 1.0981032848358154,
      "learning_rate": 4.849397590361446e-06,
      "loss": 1.8603,
      "step": 200
    },
    {
      "epoch": 0.1210843373493976,
      "grad_norm": 1.1300582885742188,
      "learning_rate": 4.848644578313253e-06,
      "loss": 1.8514,
      "step": 201
    },
    {
      "epoch": 0.1216867469879518,
      "grad_norm": 1.2305117845535278,
      "learning_rate": 4.84789156626506e-06,
      "loss": 1.9025,
      "step": 202
    },
    {
      "epoch": 0.12228915662650602,
      "grad_norm": 1.0266355276107788,
      "learning_rate": 4.847138554216868e-06,
      "loss": 1.8178,
      "step": 203
    },
    {
      "epoch": 0.12289156626506025,
      "grad_norm": 1.0954228639602661,
      "learning_rate": 4.846385542168675e-06,
      "loss": 1.8633,
      "step": 204
    },
    {
      "epoch": 0.12349397590361445,
      "grad_norm": 0.9696114659309387,
      "learning_rate": 4.845632530120482e-06,
      "loss": 1.7962,
      "step": 205
    },
    {
      "epoch": 0.12409638554216867,
      "grad_norm": 0.9872890114784241,
      "learning_rate": 4.84487951807229e-06,
      "loss": 1.8206,
      "step": 206
    },
    {
      "epoch": 0.1246987951807229,
      "grad_norm": 0.9578582644462585,
      "learning_rate": 4.8441265060240965e-06,
      "loss": 1.7759,
      "step": 207
    },
    {
      "epoch": 0.12530120481927712,
      "grad_norm": 0.9917846322059631,
      "learning_rate": 4.843373493975904e-06,
      "loss": 1.8212,
      "step": 208
    },
    {
      "epoch": 0.12590361445783133,
      "grad_norm": 1.000888466835022,
      "learning_rate": 4.842620481927711e-06,
      "loss": 1.8485,
      "step": 209
    },
    {
      "epoch": 0.12650602409638553,
      "grad_norm": 0.986721932888031,
      "learning_rate": 4.841867469879519e-06,
      "loss": 1.836,
      "step": 210
    },
    {
      "epoch": 0.12710843373493977,
      "grad_norm": 1.020324468612671,
      "learning_rate": 4.841114457831326e-06,
      "loss": 1.8313,
      "step": 211
    },
    {
      "epoch": 0.12771084337349398,
      "grad_norm": 0.9209928512573242,
      "learning_rate": 4.840361445783133e-06,
      "loss": 1.768,
      "step": 212
    },
    {
      "epoch": 0.12831325301204818,
      "grad_norm": 1.0388987064361572,
      "learning_rate": 4.839608433734941e-06,
      "loss": 1.8012,
      "step": 213
    },
    {
      "epoch": 0.12891566265060242,
      "grad_norm": 0.9404631853103638,
      "learning_rate": 4.8388554216867476e-06,
      "loss": 1.7706,
      "step": 214
    },
    {
      "epoch": 0.12951807228915663,
      "grad_norm": 0.9837149977684021,
      "learning_rate": 4.8381024096385545e-06,
      "loss": 1.8186,
      "step": 215
    },
    {
      "epoch": 0.13012048192771083,
      "grad_norm": 0.9033177495002747,
      "learning_rate": 4.837349397590361e-06,
      "loss": 1.7516,
      "step": 216
    },
    {
      "epoch": 0.13072289156626507,
      "grad_norm": 0.893429696559906,
      "learning_rate": 4.836596385542169e-06,
      "loss": 1.8012,
      "step": 217
    },
    {
      "epoch": 0.13132530120481928,
      "grad_norm": 0.8881344795227051,
      "learning_rate": 4.835843373493976e-06,
      "loss": 1.7948,
      "step": 218
    },
    {
      "epoch": 0.13192771084337349,
      "grad_norm": 0.8772730827331543,
      "learning_rate": 4.835090361445783e-06,
      "loss": 1.8104,
      "step": 219
    },
    {
      "epoch": 0.13253012048192772,
      "grad_norm": 0.8172335624694824,
      "learning_rate": 4.834337349397591e-06,
      "loss": 1.717,
      "step": 220
    },
    {
      "epoch": 0.13313253012048193,
      "grad_norm": 0.8400413990020752,
      "learning_rate": 4.833584337349398e-06,
      "loss": 1.7807,
      "step": 221
    },
    {
      "epoch": 0.13373493975903614,
      "grad_norm": 0.8863494992256165,
      "learning_rate": 4.8328313253012055e-06,
      "loss": 1.7727,
      "step": 222
    },
    {
      "epoch": 0.13433734939759037,
      "grad_norm": 0.9909966588020325,
      "learning_rate": 4.832078313253012e-06,
      "loss": 1.7628,
      "step": 223
    },
    {
      "epoch": 0.13493975903614458,
      "grad_norm": 0.8525331616401672,
      "learning_rate": 4.831325301204819e-06,
      "loss": 1.7783,
      "step": 224
    },
    {
      "epoch": 0.1355421686746988,
      "grad_norm": 0.8585079908370972,
      "learning_rate": 4.830572289156627e-06,
      "loss": 1.7473,
      "step": 225
    },
    {
      "epoch": 0.13614457831325302,
      "grad_norm": 0.8243316411972046,
      "learning_rate": 4.829819277108434e-06,
      "loss": 1.7819,
      "step": 226
    },
    {
      "epoch": 0.13674698795180723,
      "grad_norm": 0.8228163719177246,
      "learning_rate": 4.829066265060242e-06,
      "loss": 1.731,
      "step": 227
    },
    {
      "epoch": 0.13734939759036144,
      "grad_norm": 0.8049022555351257,
      "learning_rate": 4.828313253012049e-06,
      "loss": 1.742,
      "step": 228
    },
    {
      "epoch": 0.13795180722891567,
      "grad_norm": 0.8398966789245605,
      "learning_rate": 4.827560240963856e-06,
      "loss": 1.7708,
      "step": 229
    },
    {
      "epoch": 0.13855421686746988,
      "grad_norm": 0.8050366640090942,
      "learning_rate": 4.8268072289156634e-06,
      "loss": 1.7537,
      "step": 230
    },
    {
      "epoch": 0.1391566265060241,
      "grad_norm": 0.7946657538414001,
      "learning_rate": 4.82605421686747e-06,
      "loss": 1.774,
      "step": 231
    },
    {
      "epoch": 0.13975903614457832,
      "grad_norm": 0.8327892422676086,
      "learning_rate": 4.825301204819277e-06,
      "loss": 1.7746,
      "step": 232
    },
    {
      "epoch": 0.14036144578313253,
      "grad_norm": 0.7673815488815308,
      "learning_rate": 4.824548192771085e-06,
      "loss": 1.7135,
      "step": 233
    },
    {
      "epoch": 0.14096385542168674,
      "grad_norm": 0.7602956891059875,
      "learning_rate": 4.823795180722892e-06,
      "loss": 1.7322,
      "step": 234
    },
    {
      "epoch": 0.14156626506024098,
      "grad_norm": 0.7782509326934814,
      "learning_rate": 4.823042168674699e-06,
      "loss": 1.7059,
      "step": 235
    },
    {
      "epoch": 0.14216867469879518,
      "grad_norm": 0.7631418704986572,
      "learning_rate": 4.822289156626506e-06,
      "loss": 1.7509,
      "step": 236
    },
    {
      "epoch": 0.1427710843373494,
      "grad_norm": 0.7750449180603027,
      "learning_rate": 4.821536144578314e-06,
      "loss": 1.7262,
      "step": 237
    },
    {
      "epoch": 0.14337349397590363,
      "grad_norm": 0.765663206577301,
      "learning_rate": 4.8207831325301205e-06,
      "loss": 1.7323,
      "step": 238
    },
    {
      "epoch": 0.14397590361445783,
      "grad_norm": 0.741476833820343,
      "learning_rate": 4.820030120481928e-06,
      "loss": 1.7568,
      "step": 239
    },
    {
      "epoch": 0.14457831325301204,
      "grad_norm": 0.7734804749488831,
      "learning_rate": 4.819277108433735e-06,
      "loss": 1.7452,
      "step": 240
    },
    {
      "epoch": 0.14518072289156628,
      "grad_norm": 0.7453473806381226,
      "learning_rate": 4.818524096385542e-06,
      "loss": 1.7193,
      "step": 241
    },
    {
      "epoch": 0.14578313253012049,
      "grad_norm": 0.7575656175613403,
      "learning_rate": 4.81777108433735e-06,
      "loss": 1.7355,
      "step": 242
    },
    {
      "epoch": 0.1463855421686747,
      "grad_norm": 0.7615200281143188,
      "learning_rate": 4.817018072289157e-06,
      "loss": 1.7268,
      "step": 243
    },
    {
      "epoch": 0.14698795180722893,
      "grad_norm": 0.7498297691345215,
      "learning_rate": 4.816265060240965e-06,
      "loss": 1.7343,
      "step": 244
    },
    {
      "epoch": 0.14759036144578314,
      "grad_norm": 0.7488420605659485,
      "learning_rate": 4.8155120481927715e-06,
      "loss": 1.7388,
      "step": 245
    },
    {
      "epoch": 0.14819277108433734,
      "grad_norm": 0.72989821434021,
      "learning_rate": 4.814759036144579e-06,
      "loss": 1.6905,
      "step": 246
    },
    {
      "epoch": 0.14879518072289158,
      "grad_norm": 0.7123001217842102,
      "learning_rate": 4.814006024096386e-06,
      "loss": 1.6943,
      "step": 247
    },
    {
      "epoch": 0.1493975903614458,
      "grad_norm": 0.7553150653839111,
      "learning_rate": 4.813253012048193e-06,
      "loss": 1.7053,
      "step": 248
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.713471531867981,
      "learning_rate": 4.8125e-06,
      "loss": 1.6567,
      "step": 249
    },
    {
      "epoch": 0.15060240963855423,
      "grad_norm": 0.7307718992233276,
      "learning_rate": 4.811746987951808e-06,
      "loss": 1.7007,
      "step": 250
    },
    {
      "epoch": 0.15120481927710844,
      "grad_norm": 0.7311794757843018,
      "learning_rate": 4.810993975903615e-06,
      "loss": 1.7056,
      "step": 251
    },
    {
      "epoch": 0.15180722891566265,
      "grad_norm": 0.7625172734260559,
      "learning_rate": 4.810240963855422e-06,
      "loss": 1.7039,
      "step": 252
    },
    {
      "epoch": 0.15240963855421688,
      "grad_norm": 0.7785393595695496,
      "learning_rate": 4.809487951807229e-06,
      "loss": 1.7381,
      "step": 253
    },
    {
      "epoch": 0.1530120481927711,
      "grad_norm": 0.7344098091125488,
      "learning_rate": 4.808734939759036e-06,
      "loss": 1.7311,
      "step": 254
    },
    {
      "epoch": 0.1536144578313253,
      "grad_norm": 0.7868589162826538,
      "learning_rate": 4.807981927710843e-06,
      "loss": 1.6812,
      "step": 255
    },
    {
      "epoch": 0.15421686746987953,
      "grad_norm": 0.6652498841285706,
      "learning_rate": 4.807228915662651e-06,
      "loss": 1.5736,
      "step": 256
    },
    {
      "epoch": 0.15481927710843374,
      "grad_norm": 0.7033169269561768,
      "learning_rate": 4.806475903614458e-06,
      "loss": 1.7236,
      "step": 257
    },
    {
      "epoch": 0.15542168674698795,
      "grad_norm": 0.7170470356941223,
      "learning_rate": 4.805722891566266e-06,
      "loss": 1.6599,
      "step": 258
    },
    {
      "epoch": 0.15602409638554218,
      "grad_norm": 0.6867480278015137,
      "learning_rate": 4.804969879518073e-06,
      "loss": 1.6448,
      "step": 259
    },
    {
      "epoch": 0.1566265060240964,
      "grad_norm": 0.6744500398635864,
      "learning_rate": 4.80421686746988e-06,
      "loss": 1.6445,
      "step": 260
    },
    {
      "epoch": 0.1572289156626506,
      "grad_norm": 0.7147858142852783,
      "learning_rate": 4.803463855421687e-06,
      "loss": 1.6474,
      "step": 261
    },
    {
      "epoch": 0.1578313253012048,
      "grad_norm": 0.7288215756416321,
      "learning_rate": 4.802710843373494e-06,
      "loss": 1.6891,
      "step": 262
    },
    {
      "epoch": 0.15843373493975904,
      "grad_norm": 0.7007225751876831,
      "learning_rate": 4.801957831325302e-06,
      "loss": 1.6811,
      "step": 263
    },
    {
      "epoch": 0.15903614457831325,
      "grad_norm": 0.7068501710891724,
      "learning_rate": 4.801204819277109e-06,
      "loss": 1.6696,
      "step": 264
    },
    {
      "epoch": 0.15963855421686746,
      "grad_norm": 0.6887246370315552,
      "learning_rate": 4.800451807228916e-06,
      "loss": 1.6249,
      "step": 265
    },
    {
      "epoch": 0.1602409638554217,
      "grad_norm": 0.6977202296257019,
      "learning_rate": 4.799698795180724e-06,
      "loss": 1.6655,
      "step": 266
    },
    {
      "epoch": 0.1608433734939759,
      "grad_norm": 0.6817418336868286,
      "learning_rate": 4.798945783132531e-06,
      "loss": 1.6714,
      "step": 267
    },
    {
      "epoch": 0.1614457831325301,
      "grad_norm": 0.686543345451355,
      "learning_rate": 4.798192771084338e-06,
      "loss": 1.6408,
      "step": 268
    },
    {
      "epoch": 0.16204819277108434,
      "grad_norm": 0.7135939598083496,
      "learning_rate": 4.7974397590361445e-06,
      "loss": 1.6654,
      "step": 269
    },
    {
      "epoch": 0.16265060240963855,
      "grad_norm": 0.7083780765533447,
      "learning_rate": 4.796686746987952e-06,
      "loss": 1.6782,
      "step": 270
    },
    {
      "epoch": 0.16325301204819276,
      "grad_norm": 0.7175024151802063,
      "learning_rate": 4.795933734939759e-06,
      "loss": 1.6318,
      "step": 271
    },
    {
      "epoch": 0.163855421686747,
      "grad_norm": 0.6701497435569763,
      "learning_rate": 4.795180722891566e-06,
      "loss": 1.6414,
      "step": 272
    },
    {
      "epoch": 0.1644578313253012,
      "grad_norm": 0.6988034844398499,
      "learning_rate": 4.794427710843374e-06,
      "loss": 1.6565,
      "step": 273
    },
    {
      "epoch": 0.1650602409638554,
      "grad_norm": 0.7012876868247986,
      "learning_rate": 4.793674698795181e-06,
      "loss": 1.644,
      "step": 274
    },
    {
      "epoch": 0.16566265060240964,
      "grad_norm": 0.6780220866203308,
      "learning_rate": 4.792921686746989e-06,
      "loss": 1.6402,
      "step": 275
    },
    {
      "epoch": 0.16626506024096385,
      "grad_norm": 0.7401126027107239,
      "learning_rate": 4.7921686746987955e-06,
      "loss": 1.6518,
      "step": 276
    },
    {
      "epoch": 0.16686746987951806,
      "grad_norm": 0.6798343658447266,
      "learning_rate": 4.7914156626506025e-06,
      "loss": 1.6596,
      "step": 277
    },
    {
      "epoch": 0.1674698795180723,
      "grad_norm": 0.6503322720527649,
      "learning_rate": 4.79066265060241e-06,
      "loss": 1.6066,
      "step": 278
    },
    {
      "epoch": 0.1680722891566265,
      "grad_norm": 0.6418917179107666,
      "learning_rate": 4.789909638554217e-06,
      "loss": 1.6208,
      "step": 279
    },
    {
      "epoch": 0.1686746987951807,
      "grad_norm": 0.6618983745574951,
      "learning_rate": 4.789156626506025e-06,
      "loss": 1.6256,
      "step": 280
    },
    {
      "epoch": 0.16927710843373495,
      "grad_norm": 0.6828663945198059,
      "learning_rate": 4.788403614457832e-06,
      "loss": 1.6004,
      "step": 281
    },
    {
      "epoch": 0.16987951807228915,
      "grad_norm": 0.6781529784202576,
      "learning_rate": 4.787650602409639e-06,
      "loss": 1.6317,
      "step": 282
    },
    {
      "epoch": 0.17048192771084336,
      "grad_norm": 0.6674323081970215,
      "learning_rate": 4.7868975903614465e-06,
      "loss": 1.619,
      "step": 283
    },
    {
      "epoch": 0.1710843373493976,
      "grad_norm": 0.651633620262146,
      "learning_rate": 4.7861445783132535e-06,
      "loss": 1.6327,
      "step": 284
    },
    {
      "epoch": 0.1716867469879518,
      "grad_norm": 0.6679467558860779,
      "learning_rate": 4.78539156626506e-06,
      "loss": 1.6277,
      "step": 285
    },
    {
      "epoch": 0.172289156626506,
      "grad_norm": 0.6723544001579285,
      "learning_rate": 4.784638554216867e-06,
      "loss": 1.6486,
      "step": 286
    },
    {
      "epoch": 0.17289156626506025,
      "grad_norm": 0.6982719898223877,
      "learning_rate": 4.783885542168675e-06,
      "loss": 1.6473,
      "step": 287
    },
    {
      "epoch": 0.17349397590361446,
      "grad_norm": 0.6932685375213623,
      "learning_rate": 4.783132530120482e-06,
      "loss": 1.6154,
      "step": 288
    },
    {
      "epoch": 0.17409638554216866,
      "grad_norm": 0.6223523020744324,
      "learning_rate": 4.782379518072289e-06,
      "loss": 1.5463,
      "step": 289
    },
    {
      "epoch": 0.1746987951807229,
      "grad_norm": 0.6461967825889587,
      "learning_rate": 4.781626506024097e-06,
      "loss": 1.6105,
      "step": 290
    },
    {
      "epoch": 0.1753012048192771,
      "grad_norm": 0.6662501096725464,
      "learning_rate": 4.780873493975904e-06,
      "loss": 1.5946,
      "step": 291
    },
    {
      "epoch": 0.17590361445783131,
      "grad_norm": 0.6728113293647766,
      "learning_rate": 4.780120481927711e-06,
      "loss": 1.6325,
      "step": 292
    },
    {
      "epoch": 0.17650602409638555,
      "grad_norm": 0.6636098623275757,
      "learning_rate": 4.779367469879518e-06,
      "loss": 1.6081,
      "step": 293
    },
    {
      "epoch": 0.17710843373493976,
      "grad_norm": 0.6523126363754272,
      "learning_rate": 4.778614457831326e-06,
      "loss": 1.5923,
      "step": 294
    },
    {
      "epoch": 0.17771084337349397,
      "grad_norm": 0.6664504408836365,
      "learning_rate": 4.777861445783133e-06,
      "loss": 1.6155,
      "step": 295
    },
    {
      "epoch": 0.1783132530120482,
      "grad_norm": 0.6312705874443054,
      "learning_rate": 4.77710843373494e-06,
      "loss": 1.5547,
      "step": 296
    },
    {
      "epoch": 0.1789156626506024,
      "grad_norm": 0.6612650752067566,
      "learning_rate": 4.776355421686748e-06,
      "loss": 1.6198,
      "step": 297
    },
    {
      "epoch": 0.17951807228915662,
      "grad_norm": 0.659209668636322,
      "learning_rate": 4.775602409638555e-06,
      "loss": 1.6074,
      "step": 298
    },
    {
      "epoch": 0.18012048192771085,
      "grad_norm": 0.6165990829467773,
      "learning_rate": 4.7748493975903624e-06,
      "loss": 1.5485,
      "step": 299
    },
    {
      "epoch": 0.18072289156626506,
      "grad_norm": 0.6521856784820557,
      "learning_rate": 4.774096385542169e-06,
      "loss": 1.5813,
      "step": 300
    },
    {
      "epoch": 0.18132530120481927,
      "grad_norm": 0.6347154974937439,
      "learning_rate": 4.773343373493976e-06,
      "loss": 1.5918,
      "step": 301
    },
    {
      "epoch": 0.1819277108433735,
      "grad_norm": 0.6403963565826416,
      "learning_rate": 4.772590361445783e-06,
      "loss": 1.5772,
      "step": 302
    },
    {
      "epoch": 0.1825301204819277,
      "grad_norm": 0.6578680276870728,
      "learning_rate": 4.771837349397591e-06,
      "loss": 1.5872,
      "step": 303
    },
    {
      "epoch": 0.18313253012048192,
      "grad_norm": 0.6100031733512878,
      "learning_rate": 4.771084337349398e-06,
      "loss": 1.5595,
      "step": 304
    },
    {
      "epoch": 0.18373493975903615,
      "grad_norm": 0.6260285973548889,
      "learning_rate": 4.770331325301205e-06,
      "loss": 1.5757,
      "step": 305
    },
    {
      "epoch": 0.18433734939759036,
      "grad_norm": 0.6456859707832336,
      "learning_rate": 4.769578313253013e-06,
      "loss": 1.5878,
      "step": 306
    },
    {
      "epoch": 0.18493975903614457,
      "grad_norm": 0.6364273428916931,
      "learning_rate": 4.7688253012048195e-06,
      "loss": 1.5537,
      "step": 307
    },
    {
      "epoch": 0.1855421686746988,
      "grad_norm": 0.6614553928375244,
      "learning_rate": 4.7680722891566264e-06,
      "loss": 1.5906,
      "step": 308
    },
    {
      "epoch": 0.186144578313253,
      "grad_norm": 0.680141031742096,
      "learning_rate": 4.767319277108434e-06,
      "loss": 1.5662,
      "step": 309
    },
    {
      "epoch": 0.18674698795180722,
      "grad_norm": 0.6465786695480347,
      "learning_rate": 4.766566265060241e-06,
      "loss": 1.5959,
      "step": 310
    },
    {
      "epoch": 0.18734939759036146,
      "grad_norm": 0.6303747296333313,
      "learning_rate": 4.765813253012049e-06,
      "loss": 1.5397,
      "step": 311
    },
    {
      "epoch": 0.18795180722891566,
      "grad_norm": 0.6537888050079346,
      "learning_rate": 4.765060240963856e-06,
      "loss": 1.5636,
      "step": 312
    },
    {
      "epoch": 0.18855421686746987,
      "grad_norm": 0.6520653367042542,
      "learning_rate": 4.764307228915663e-06,
      "loss": 1.5704,
      "step": 313
    },
    {
      "epoch": 0.1891566265060241,
      "grad_norm": 0.6491609811782837,
      "learning_rate": 4.7635542168674705e-06,
      "loss": 1.5712,
      "step": 314
    },
    {
      "epoch": 0.1897590361445783,
      "grad_norm": 0.6224614977836609,
      "learning_rate": 4.7628012048192775e-06,
      "loss": 1.5351,
      "step": 315
    },
    {
      "epoch": 0.19036144578313252,
      "grad_norm": 0.6187222003936768,
      "learning_rate": 4.762048192771085e-06,
      "loss": 1.5486,
      "step": 316
    },
    {
      "epoch": 0.19096385542168676,
      "grad_norm": 0.6230939030647278,
      "learning_rate": 4.761295180722892e-06,
      "loss": 1.5315,
      "step": 317
    },
    {
      "epoch": 0.19156626506024096,
      "grad_norm": 0.6235955357551575,
      "learning_rate": 4.760542168674699e-06,
      "loss": 1.5575,
      "step": 318
    },
    {
      "epoch": 0.19216867469879517,
      "grad_norm": 0.6413629055023193,
      "learning_rate": 4.759789156626506e-06,
      "loss": 1.5581,
      "step": 319
    },
    {
      "epoch": 0.1927710843373494,
      "grad_norm": 0.653306245803833,
      "learning_rate": 4.759036144578314e-06,
      "loss": 1.5846,
      "step": 320
    },
    {
      "epoch": 0.19337349397590362,
      "grad_norm": 0.6563746333122253,
      "learning_rate": 4.758283132530121e-06,
      "loss": 1.5782,
      "step": 321
    },
    {
      "epoch": 0.19397590361445782,
      "grad_norm": 0.6224275231361389,
      "learning_rate": 4.757530120481928e-06,
      "loss": 1.4901,
      "step": 322
    },
    {
      "epoch": 0.19457831325301206,
      "grad_norm": 0.6679585576057434,
      "learning_rate": 4.756777108433735e-06,
      "loss": 1.5426,
      "step": 323
    },
    {
      "epoch": 0.19518072289156627,
      "grad_norm": 0.6160849928855896,
      "learning_rate": 4.756024096385542e-06,
      "loss": 1.5484,
      "step": 324
    },
    {
      "epoch": 0.19578313253012047,
      "grad_norm": 0.825554370880127,
      "learning_rate": 4.755271084337349e-06,
      "loss": 1.5753,
      "step": 325
    },
    {
      "epoch": 0.1963855421686747,
      "grad_norm": 0.6330503821372986,
      "learning_rate": 4.754518072289157e-06,
      "loss": 1.5308,
      "step": 326
    },
    {
      "epoch": 0.19698795180722892,
      "grad_norm": 0.6340656280517578,
      "learning_rate": 4.753765060240964e-06,
      "loss": 1.5084,
      "step": 327
    },
    {
      "epoch": 0.19759036144578312,
      "grad_norm": 0.6422789096832275,
      "learning_rate": 4.753012048192772e-06,
      "loss": 1.533,
      "step": 328
    },
    {
      "epoch": 0.19819277108433736,
      "grad_norm": 0.6656094193458557,
      "learning_rate": 4.752259036144579e-06,
      "loss": 1.5321,
      "step": 329
    },
    {
      "epoch": 0.19879518072289157,
      "grad_norm": 0.6251810193061829,
      "learning_rate": 4.751506024096386e-06,
      "loss": 1.5206,
      "step": 330
    },
    {
      "epoch": 0.19939759036144578,
      "grad_norm": 0.6450276374816895,
      "learning_rate": 4.750753012048193e-06,
      "loss": 1.5724,
      "step": 331
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.6403321623802185,
      "learning_rate": 4.75e-06,
      "loss": 1.5496,
      "step": 332
    },
    {
      "epoch": 0.20060240963855422,
      "grad_norm": 0.6244162321090698,
      "learning_rate": 4.749246987951808e-06,
      "loss": 1.5279,
      "step": 333
    },
    {
      "epoch": 0.20120481927710843,
      "grad_norm": 0.6277642846107483,
      "learning_rate": 4.748493975903615e-06,
      "loss": 1.5126,
      "step": 334
    },
    {
      "epoch": 0.20180722891566266,
      "grad_norm": 0.603257954120636,
      "learning_rate": 4.747740963855422e-06,
      "loss": 1.5214,
      "step": 335
    },
    {
      "epoch": 0.20240963855421687,
      "grad_norm": 0.6357108354568481,
      "learning_rate": 4.74698795180723e-06,
      "loss": 1.5195,
      "step": 336
    },
    {
      "epoch": 0.20301204819277108,
      "grad_norm": 0.6353623867034912,
      "learning_rate": 4.746234939759037e-06,
      "loss": 1.5074,
      "step": 337
    },
    {
      "epoch": 0.2036144578313253,
      "grad_norm": 0.6504127383232117,
      "learning_rate": 4.7454819277108435e-06,
      "loss": 1.5226,
      "step": 338
    },
    {
      "epoch": 0.20421686746987952,
      "grad_norm": 0.6555492877960205,
      "learning_rate": 4.7447289156626504e-06,
      "loss": 1.5477,
      "step": 339
    },
    {
      "epoch": 0.20481927710843373,
      "grad_norm": 0.6293894648551941,
      "learning_rate": 4.743975903614458e-06,
      "loss": 1.5194,
      "step": 340
    },
    {
      "epoch": 0.20542168674698796,
      "grad_norm": 0.6484460234642029,
      "learning_rate": 4.743222891566265e-06,
      "loss": 1.4976,
      "step": 341
    },
    {
      "epoch": 0.20602409638554217,
      "grad_norm": 0.6341803669929504,
      "learning_rate": 4.742469879518073e-06,
      "loss": 1.508,
      "step": 342
    },
    {
      "epoch": 0.20662650602409638,
      "grad_norm": 0.641331672668457,
      "learning_rate": 4.74171686746988e-06,
      "loss": 1.4985,
      "step": 343
    },
    {
      "epoch": 0.20722891566265061,
      "grad_norm": 0.6322843432426453,
      "learning_rate": 4.740963855421687e-06,
      "loss": 1.5147,
      "step": 344
    },
    {
      "epoch": 0.20783132530120482,
      "grad_norm": 0.6105960011482239,
      "learning_rate": 4.7402108433734945e-06,
      "loss": 1.4782,
      "step": 345
    },
    {
      "epoch": 0.20843373493975903,
      "grad_norm": 0.602277398109436,
      "learning_rate": 4.7394578313253014e-06,
      "loss": 1.4727,
      "step": 346
    },
    {
      "epoch": 0.20903614457831327,
      "grad_norm": 0.6308878064155579,
      "learning_rate": 4.738704819277109e-06,
      "loss": 1.5354,
      "step": 347
    },
    {
      "epoch": 0.20963855421686747,
      "grad_norm": 0.6364786028862,
      "learning_rate": 4.737951807228916e-06,
      "loss": 1.5163,
      "step": 348
    },
    {
      "epoch": 0.21024096385542168,
      "grad_norm": 0.6155357360839844,
      "learning_rate": 4.737198795180723e-06,
      "loss": 1.4466,
      "step": 349
    },
    {
      "epoch": 0.21084337349397592,
      "grad_norm": 0.642551839351654,
      "learning_rate": 4.736445783132531e-06,
      "loss": 1.4871,
      "step": 350
    },
    {
      "epoch": 0.21144578313253012,
      "grad_norm": 0.6344416737556458,
      "learning_rate": 4.735692771084338e-06,
      "loss": 1.5245,
      "step": 351
    },
    {
      "epoch": 0.21204819277108433,
      "grad_norm": 0.6066713333129883,
      "learning_rate": 4.734939759036145e-06,
      "loss": 1.4559,
      "step": 352
    },
    {
      "epoch": 0.21265060240963857,
      "grad_norm": 0.5916743874549866,
      "learning_rate": 4.7341867469879525e-06,
      "loss": 1.4746,
      "step": 353
    },
    {
      "epoch": 0.21325301204819277,
      "grad_norm": 0.6337381601333618,
      "learning_rate": 4.733433734939759e-06,
      "loss": 1.4603,
      "step": 354
    },
    {
      "epoch": 0.21385542168674698,
      "grad_norm": 0.6421207785606384,
      "learning_rate": 4.732680722891566e-06,
      "loss": 1.4652,
      "step": 355
    },
    {
      "epoch": 0.21445783132530122,
      "grad_norm": 0.6392531394958496,
      "learning_rate": 4.731927710843373e-06,
      "loss": 1.4521,
      "step": 356
    },
    {
      "epoch": 0.21506024096385543,
      "grad_norm": 0.6268361806869507,
      "learning_rate": 4.731174698795181e-06,
      "loss": 1.4586,
      "step": 357
    },
    {
      "epoch": 0.21566265060240963,
      "grad_norm": 0.612407386302948,
      "learning_rate": 4.730421686746988e-06,
      "loss": 1.4588,
      "step": 358
    },
    {
      "epoch": 0.21626506024096387,
      "grad_norm": 0.6424269080162048,
      "learning_rate": 4.729668674698796e-06,
      "loss": 1.4456,
      "step": 359
    },
    {
      "epoch": 0.21686746987951808,
      "grad_norm": 0.7704424262046814,
      "learning_rate": 4.728915662650603e-06,
      "loss": 1.4981,
      "step": 360
    },
    {
      "epoch": 0.21746987951807228,
      "grad_norm": 0.7144001126289368,
      "learning_rate": 4.7281626506024096e-06,
      "loss": 1.509,
      "step": 361
    },
    {
      "epoch": 0.21807228915662652,
      "grad_norm": 0.6408788561820984,
      "learning_rate": 4.727409638554217e-06,
      "loss": 1.4523,
      "step": 362
    },
    {
      "epoch": 0.21867469879518073,
      "grad_norm": 0.6352127194404602,
      "learning_rate": 4.726656626506024e-06,
      "loss": 1.463,
      "step": 363
    },
    {
      "epoch": 0.21927710843373494,
      "grad_norm": 0.6153895854949951,
      "learning_rate": 4.725903614457832e-06,
      "loss": 1.4792,
      "step": 364
    },
    {
      "epoch": 0.21987951807228914,
      "grad_norm": 0.6368789076805115,
      "learning_rate": 4.725150602409639e-06,
      "loss": 1.4591,
      "step": 365
    },
    {
      "epoch": 0.22048192771084338,
      "grad_norm": 0.6379714012145996,
      "learning_rate": 4.724397590361447e-06,
      "loss": 1.4643,
      "step": 366
    },
    {
      "epoch": 0.22108433734939759,
      "grad_norm": 0.6261733770370483,
      "learning_rate": 4.723644578313254e-06,
      "loss": 1.4392,
      "step": 367
    },
    {
      "epoch": 0.2216867469879518,
      "grad_norm": 0.6274462938308716,
      "learning_rate": 4.7228915662650606e-06,
      "loss": 1.4477,
      "step": 368
    },
    {
      "epoch": 0.22228915662650603,
      "grad_norm": 0.6333312392234802,
      "learning_rate": 4.722138554216868e-06,
      "loss": 1.46,
      "step": 369
    },
    {
      "epoch": 0.22289156626506024,
      "grad_norm": 0.7191333770751953,
      "learning_rate": 4.721385542168675e-06,
      "loss": 1.482,
      "step": 370
    },
    {
      "epoch": 0.22349397590361444,
      "grad_norm": 0.5841360092163086,
      "learning_rate": 4.720632530120482e-06,
      "loss": 1.4342,
      "step": 371
    },
    {
      "epoch": 0.22409638554216868,
      "grad_norm": 0.6095926761627197,
      "learning_rate": 4.719879518072289e-06,
      "loss": 1.4366,
      "step": 372
    },
    {
      "epoch": 0.2246987951807229,
      "grad_norm": 0.6386562585830688,
      "learning_rate": 4.719126506024097e-06,
      "loss": 1.4597,
      "step": 373
    },
    {
      "epoch": 0.2253012048192771,
      "grad_norm": 0.6346952319145203,
      "learning_rate": 4.718373493975904e-06,
      "loss": 1.4462,
      "step": 374
    },
    {
      "epoch": 0.22590361445783133,
      "grad_norm": 0.6672772169113159,
      "learning_rate": 4.717620481927711e-06,
      "loss": 1.4568,
      "step": 375
    },
    {
      "epoch": 0.22650602409638554,
      "grad_norm": 0.6520745158195496,
      "learning_rate": 4.7168674698795185e-06,
      "loss": 1.4446,
      "step": 376
    },
    {
      "epoch": 0.22710843373493975,
      "grad_norm": 0.6350225806236267,
      "learning_rate": 4.7161144578313254e-06,
      "loss": 1.4537,
      "step": 377
    },
    {
      "epoch": 0.22771084337349398,
      "grad_norm": 0.6168689131736755,
      "learning_rate": 4.715361445783133e-06,
      "loss": 1.4375,
      "step": 378
    },
    {
      "epoch": 0.2283132530120482,
      "grad_norm": 0.644926905632019,
      "learning_rate": 4.71460843373494e-06,
      "loss": 1.4585,
      "step": 379
    },
    {
      "epoch": 0.2289156626506024,
      "grad_norm": 0.6660984754562378,
      "learning_rate": 4.713855421686747e-06,
      "loss": 1.4602,
      "step": 380
    },
    {
      "epoch": 0.22951807228915663,
      "grad_norm": 0.6645481586456299,
      "learning_rate": 4.713102409638555e-06,
      "loss": 1.4553,
      "step": 381
    },
    {
      "epoch": 0.23012048192771084,
      "grad_norm": 0.6541915535926819,
      "learning_rate": 4.712349397590362e-06,
      "loss": 1.4359,
      "step": 382
    },
    {
      "epoch": 0.23072289156626505,
      "grad_norm": 0.6309393644332886,
      "learning_rate": 4.7115963855421695e-06,
      "loss": 1.4236,
      "step": 383
    },
    {
      "epoch": 0.23132530120481928,
      "grad_norm": 0.6497915387153625,
      "learning_rate": 4.7108433734939764e-06,
      "loss": 1.418,
      "step": 384
    },
    {
      "epoch": 0.2319277108433735,
      "grad_norm": 0.600816011428833,
      "learning_rate": 4.710090361445783e-06,
      "loss": 1.4152,
      "step": 385
    },
    {
      "epoch": 0.2325301204819277,
      "grad_norm": 0.6516527533531189,
      "learning_rate": 4.709337349397591e-06,
      "loss": 1.4609,
      "step": 386
    },
    {
      "epoch": 0.23313253012048193,
      "grad_norm": 0.6584140062332153,
      "learning_rate": 4.708584337349398e-06,
      "loss": 1.4314,
      "step": 387
    },
    {
      "epoch": 0.23373493975903614,
      "grad_norm": 0.6199894547462463,
      "learning_rate": 4.707831325301205e-06,
      "loss": 1.4178,
      "step": 388
    },
    {
      "epoch": 0.23433734939759035,
      "grad_norm": 0.6462453007698059,
      "learning_rate": 4.707078313253013e-06,
      "loss": 1.4194,
      "step": 389
    },
    {
      "epoch": 0.23493975903614459,
      "grad_norm": 0.6618443131446838,
      "learning_rate": 4.70632530120482e-06,
      "loss": 1.4279,
      "step": 390
    },
    {
      "epoch": 0.2355421686746988,
      "grad_norm": 0.6546756029129028,
      "learning_rate": 4.705572289156627e-06,
      "loss": 1.4072,
      "step": 391
    },
    {
      "epoch": 0.236144578313253,
      "grad_norm": 0.6192677617073059,
      "learning_rate": 4.7048192771084335e-06,
      "loss": 1.3794,
      "step": 392
    },
    {
      "epoch": 0.23674698795180724,
      "grad_norm": 0.6528082489967346,
      "learning_rate": 4.704066265060241e-06,
      "loss": 1.3875,
      "step": 393
    },
    {
      "epoch": 0.23734939759036144,
      "grad_norm": 0.65110844373703,
      "learning_rate": 4.703313253012048e-06,
      "loss": 1.4078,
      "step": 394
    },
    {
      "epoch": 0.23795180722891565,
      "grad_norm": 0.6746704578399658,
      "learning_rate": 4.702560240963856e-06,
      "loss": 1.4339,
      "step": 395
    },
    {
      "epoch": 0.2385542168674699,
      "grad_norm": 0.6184611916542053,
      "learning_rate": 4.701807228915663e-06,
      "loss": 1.3897,
      "step": 396
    },
    {
      "epoch": 0.2391566265060241,
      "grad_norm": 0.7287032008171082,
      "learning_rate": 4.70105421686747e-06,
      "loss": 1.4431,
      "step": 397
    },
    {
      "epoch": 0.2397590361445783,
      "grad_norm": 0.642220675945282,
      "learning_rate": 4.700301204819278e-06,
      "loss": 1.4244,
      "step": 398
    },
    {
      "epoch": 0.24036144578313254,
      "grad_norm": 0.6729488372802734,
      "learning_rate": 4.6995481927710846e-06,
      "loss": 1.425,
      "step": 399
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 0.6432154178619385,
      "learning_rate": 4.698795180722892e-06,
      "loss": 1.4029,
      "step": 400
    },
    {
      "epoch": 0.24156626506024095,
      "grad_norm": 0.6297951340675354,
      "learning_rate": 4.698042168674699e-06,
      "loss": 1.393,
      "step": 401
    },
    {
      "epoch": 0.2421686746987952,
      "grad_norm": 0.6074402332305908,
      "learning_rate": 4.697289156626507e-06,
      "loss": 1.3294,
      "step": 402
    },
    {
      "epoch": 0.2427710843373494,
      "grad_norm": 0.6134657263755798,
      "learning_rate": 4.696536144578314e-06,
      "loss": 1.3417,
      "step": 403
    },
    {
      "epoch": 0.2433734939759036,
      "grad_norm": 0.685746431350708,
      "learning_rate": 4.695783132530121e-06,
      "loss": 1.3849,
      "step": 404
    },
    {
      "epoch": 0.24397590361445784,
      "grad_norm": 0.6510821580886841,
      "learning_rate": 4.695030120481928e-06,
      "loss": 1.3835,
      "step": 405
    },
    {
      "epoch": 0.24457831325301205,
      "grad_norm": 0.6784501075744629,
      "learning_rate": 4.6942771084337356e-06,
      "loss": 1.4062,
      "step": 406
    },
    {
      "epoch": 0.24518072289156626,
      "grad_norm": 0.6459832191467285,
      "learning_rate": 4.6935240963855425e-06,
      "loss": 1.3758,
      "step": 407
    },
    {
      "epoch": 0.2457831325301205,
      "grad_norm": 0.6256124377250671,
      "learning_rate": 4.692771084337349e-06,
      "loss": 1.3566,
      "step": 408
    },
    {
      "epoch": 0.2463855421686747,
      "grad_norm": 0.6524090766906738,
      "learning_rate": 4.692018072289156e-06,
      "loss": 1.3736,
      "step": 409
    },
    {
      "epoch": 0.2469879518072289,
      "grad_norm": 0.7491938471794128,
      "learning_rate": 4.691265060240964e-06,
      "loss": 1.3792,
      "step": 410
    },
    {
      "epoch": 0.24759036144578314,
      "grad_norm": 0.6368998885154724,
      "learning_rate": 4.690512048192771e-06,
      "loss": 1.3586,
      "step": 411
    },
    {
      "epoch": 0.24819277108433735,
      "grad_norm": 0.6606225371360779,
      "learning_rate": 4.689759036144579e-06,
      "loss": 1.386,
      "step": 412
    },
    {
      "epoch": 0.24879518072289156,
      "grad_norm": 0.6357008814811707,
      "learning_rate": 4.689006024096386e-06,
      "loss": 1.3646,
      "step": 413
    },
    {
      "epoch": 0.2493975903614458,
      "grad_norm": 0.6497612595558167,
      "learning_rate": 4.6882530120481935e-06,
      "loss": 1.3868,
      "step": 414
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.6428272724151611,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 1.3506,
      "step": 415
    },
    {
      "epoch": 0.25060240963855424,
      "grad_norm": 0.6612029671669006,
      "learning_rate": 4.686746987951807e-06,
      "loss": 1.375,
      "step": 416
    },
    {
      "epoch": 0.2512048192771084,
      "grad_norm": 0.6559531092643738,
      "learning_rate": 4.685993975903615e-06,
      "loss": 1.3618,
      "step": 417
    },
    {
      "epoch": 0.25180722891566265,
      "grad_norm": 0.6473674178123474,
      "learning_rate": 4.685240963855422e-06,
      "loss": 1.3507,
      "step": 418
    },
    {
      "epoch": 0.2524096385542169,
      "grad_norm": 0.6254408955574036,
      "learning_rate": 4.68448795180723e-06,
      "loss": 1.3406,
      "step": 419
    },
    {
      "epoch": 0.25301204819277107,
      "grad_norm": 0.639141857624054,
      "learning_rate": 4.683734939759037e-06,
      "loss": 1.3338,
      "step": 420
    },
    {
      "epoch": 0.2536144578313253,
      "grad_norm": 0.6584200859069824,
      "learning_rate": 4.682981927710844e-06,
      "loss": 1.3334,
      "step": 421
    },
    {
      "epoch": 0.25421686746987954,
      "grad_norm": 0.7064493298530579,
      "learning_rate": 4.6822289156626515e-06,
      "loss": 1.3876,
      "step": 422
    },
    {
      "epoch": 0.2548192771084337,
      "grad_norm": 0.6526504158973694,
      "learning_rate": 4.681475903614458e-06,
      "loss": 1.322,
      "step": 423
    },
    {
      "epoch": 0.25542168674698795,
      "grad_norm": 0.6836284399032593,
      "learning_rate": 4.680722891566265e-06,
      "loss": 1.3641,
      "step": 424
    },
    {
      "epoch": 0.2560240963855422,
      "grad_norm": 0.6590929627418518,
      "learning_rate": 4.679969879518072e-06,
      "loss": 1.3604,
      "step": 425
    },
    {
      "epoch": 0.25662650602409637,
      "grad_norm": 0.6832489967346191,
      "learning_rate": 4.67921686746988e-06,
      "loss": 1.3551,
      "step": 426
    },
    {
      "epoch": 0.2572289156626506,
      "grad_norm": 0.6621153950691223,
      "learning_rate": 4.678463855421687e-06,
      "loss": 1.3815,
      "step": 427
    },
    {
      "epoch": 0.25783132530120484,
      "grad_norm": 0.6535840630531311,
      "learning_rate": 4.677710843373494e-06,
      "loss": 1.3493,
      "step": 428
    },
    {
      "epoch": 0.258433734939759,
      "grad_norm": 0.645634114742279,
      "learning_rate": 4.676957831325302e-06,
      "loss": 1.3203,
      "step": 429
    },
    {
      "epoch": 0.25903614457831325,
      "grad_norm": 0.6622574925422668,
      "learning_rate": 4.6762048192771085e-06,
      "loss": 1.3138,
      "step": 430
    },
    {
      "epoch": 0.2596385542168675,
      "grad_norm": 0.6293851733207703,
      "learning_rate": 4.675451807228916e-06,
      "loss": 1.3363,
      "step": 431
    },
    {
      "epoch": 0.26024096385542167,
      "grad_norm": 0.6446045637130737,
      "learning_rate": 4.674698795180723e-06,
      "loss": 1.3654,
      "step": 432
    },
    {
      "epoch": 0.2608433734939759,
      "grad_norm": 0.6908028721809387,
      "learning_rate": 4.673945783132531e-06,
      "loss": 1.3169,
      "step": 433
    },
    {
      "epoch": 0.26144578313253014,
      "grad_norm": 0.9223051071166992,
      "learning_rate": 4.673192771084338e-06,
      "loss": 1.3294,
      "step": 434
    },
    {
      "epoch": 0.2620481927710843,
      "grad_norm": 0.6753144264221191,
      "learning_rate": 4.672439759036145e-06,
      "loss": 1.327,
      "step": 435
    },
    {
      "epoch": 0.26265060240963856,
      "grad_norm": 0.6677996516227722,
      "learning_rate": 4.671686746987953e-06,
      "loss": 1.3128,
      "step": 436
    },
    {
      "epoch": 0.2632530120481928,
      "grad_norm": 0.6614099740982056,
      "learning_rate": 4.6709337349397596e-06,
      "loss": 1.3485,
      "step": 437
    },
    {
      "epoch": 0.26385542168674697,
      "grad_norm": 0.6674147248268127,
      "learning_rate": 4.6701807228915665e-06,
      "loss": 1.32,
      "step": 438
    },
    {
      "epoch": 0.2644578313253012,
      "grad_norm": 0.7128778100013733,
      "learning_rate": 4.669427710843374e-06,
      "loss": 1.2987,
      "step": 439
    },
    {
      "epoch": 0.26506024096385544,
      "grad_norm": 0.710616946220398,
      "learning_rate": 4.668674698795181e-06,
      "loss": 1.3444,
      "step": 440
    },
    {
      "epoch": 0.2656626506024096,
      "grad_norm": 0.6815958023071289,
      "learning_rate": 4.667921686746988e-06,
      "loss": 1.308,
      "step": 441
    },
    {
      "epoch": 0.26626506024096386,
      "grad_norm": 0.6912776231765747,
      "learning_rate": 4.667168674698795e-06,
      "loss": 1.3332,
      "step": 442
    },
    {
      "epoch": 0.2668674698795181,
      "grad_norm": 0.6830790638923645,
      "learning_rate": 4.666415662650603e-06,
      "loss": 1.3041,
      "step": 443
    },
    {
      "epoch": 0.2674698795180723,
      "grad_norm": 0.6750925183296204,
      "learning_rate": 4.66566265060241e-06,
      "loss": 1.3343,
      "step": 444
    },
    {
      "epoch": 0.2680722891566265,
      "grad_norm": 0.686276376247406,
      "learning_rate": 4.6649096385542175e-06,
      "loss": 1.2842,
      "step": 445
    },
    {
      "epoch": 0.26867469879518074,
      "grad_norm": 0.6884699463844299,
      "learning_rate": 4.6641566265060244e-06,
      "loss": 1.3217,
      "step": 446
    },
    {
      "epoch": 0.2692771084337349,
      "grad_norm": 0.6877424120903015,
      "learning_rate": 4.663403614457831e-06,
      "loss": 1.3073,
      "step": 447
    },
    {
      "epoch": 0.26987951807228916,
      "grad_norm": 0.7200512290000916,
      "learning_rate": 4.662650602409639e-06,
      "loss": 1.3249,
      "step": 448
    },
    {
      "epoch": 0.2704819277108434,
      "grad_norm": 0.6656315922737122,
      "learning_rate": 4.661897590361446e-06,
      "loss": 1.2966,
      "step": 449
    },
    {
      "epoch": 0.2710843373493976,
      "grad_norm": 0.7276847958564758,
      "learning_rate": 4.661144578313254e-06,
      "loss": 1.2998,
      "step": 450
    },
    {
      "epoch": 0.2716867469879518,
      "grad_norm": 0.7223863005638123,
      "learning_rate": 4.660391566265061e-06,
      "loss": 1.3132,
      "step": 451
    },
    {
      "epoch": 0.27228915662650605,
      "grad_norm": 0.7315244078636169,
      "learning_rate": 4.659638554216868e-06,
      "loss": 1.3155,
      "step": 452
    },
    {
      "epoch": 0.2728915662650602,
      "grad_norm": 0.7454656362533569,
      "learning_rate": 4.6588855421686754e-06,
      "loss": 1.3018,
      "step": 453
    },
    {
      "epoch": 0.27349397590361446,
      "grad_norm": 0.706108033657074,
      "learning_rate": 4.658132530120482e-06,
      "loss": 1.2744,
      "step": 454
    },
    {
      "epoch": 0.2740963855421687,
      "grad_norm": 0.6452051401138306,
      "learning_rate": 4.65737951807229e-06,
      "loss": 1.2746,
      "step": 455
    },
    {
      "epoch": 0.2746987951807229,
      "grad_norm": 0.7260199785232544,
      "learning_rate": 4.656626506024097e-06,
      "loss": 1.3104,
      "step": 456
    },
    {
      "epoch": 0.2753012048192771,
      "grad_norm": 0.6723315715789795,
      "learning_rate": 4.655873493975904e-06,
      "loss": 1.2757,
      "step": 457
    },
    {
      "epoch": 0.27590361445783135,
      "grad_norm": 0.6911063194274902,
      "learning_rate": 4.655120481927711e-06,
      "loss": 1.2763,
      "step": 458
    },
    {
      "epoch": 0.2765060240963855,
      "grad_norm": 0.672784686088562,
      "learning_rate": 4.654367469879519e-06,
      "loss": 1.2659,
      "step": 459
    },
    {
      "epoch": 0.27710843373493976,
      "grad_norm": 0.7133693099021912,
      "learning_rate": 4.653614457831326e-06,
      "loss": 1.2889,
      "step": 460
    },
    {
      "epoch": 0.277710843373494,
      "grad_norm": 0.7149325609207153,
      "learning_rate": 4.6528614457831325e-06,
      "loss": 1.3006,
      "step": 461
    },
    {
      "epoch": 0.2783132530120482,
      "grad_norm": 0.6761752367019653,
      "learning_rate": 4.65210843373494e-06,
      "loss": 1.2691,
      "step": 462
    },
    {
      "epoch": 0.2789156626506024,
      "grad_norm": 0.6940613985061646,
      "learning_rate": 4.651355421686747e-06,
      "loss": 1.2956,
      "step": 463
    },
    {
      "epoch": 0.27951807228915665,
      "grad_norm": 0.678714394569397,
      "learning_rate": 4.650602409638554e-06,
      "loss": 1.2569,
      "step": 464
    },
    {
      "epoch": 0.28012048192771083,
      "grad_norm": 0.6937147378921509,
      "learning_rate": 4.649849397590362e-06,
      "loss": 1.2866,
      "step": 465
    },
    {
      "epoch": 0.28072289156626506,
      "grad_norm": 0.7353542447090149,
      "learning_rate": 4.649096385542169e-06,
      "loss": 1.2889,
      "step": 466
    },
    {
      "epoch": 0.2813253012048193,
      "grad_norm": 0.7408084869384766,
      "learning_rate": 4.648343373493977e-06,
      "loss": 1.2801,
      "step": 467
    },
    {
      "epoch": 0.2819277108433735,
      "grad_norm": 0.7216913104057312,
      "learning_rate": 4.6475903614457835e-06,
      "loss": 1.2812,
      "step": 468
    },
    {
      "epoch": 0.2825301204819277,
      "grad_norm": 0.7195177674293518,
      "learning_rate": 4.646837349397591e-06,
      "loss": 1.2937,
      "step": 469
    },
    {
      "epoch": 0.28313253012048195,
      "grad_norm": 0.7052258253097534,
      "learning_rate": 4.646084337349398e-06,
      "loss": 1.2559,
      "step": 470
    },
    {
      "epoch": 0.28373493975903613,
      "grad_norm": 0.771229088306427,
      "learning_rate": 4.645331325301205e-06,
      "loss": 1.273,
      "step": 471
    },
    {
      "epoch": 0.28433734939759037,
      "grad_norm": 0.7329566478729248,
      "learning_rate": 4.644578313253013e-06,
      "loss": 1.2604,
      "step": 472
    },
    {
      "epoch": 0.2849397590361446,
      "grad_norm": 0.8567209839820862,
      "learning_rate": 4.64382530120482e-06,
      "loss": 1.2558,
      "step": 473
    },
    {
      "epoch": 0.2855421686746988,
      "grad_norm": 0.7164388298988342,
      "learning_rate": 4.643072289156627e-06,
      "loss": 1.2661,
      "step": 474
    },
    {
      "epoch": 0.286144578313253,
      "grad_norm": 0.7328698039054871,
      "learning_rate": 4.642319277108434e-06,
      "loss": 1.2473,
      "step": 475
    },
    {
      "epoch": 0.28674698795180725,
      "grad_norm": 0.7389503121376038,
      "learning_rate": 4.6415662650602415e-06,
      "loss": 1.2489,
      "step": 476
    },
    {
      "epoch": 0.28734939759036143,
      "grad_norm": 0.7524943947792053,
      "learning_rate": 4.640813253012048e-06,
      "loss": 1.2256,
      "step": 477
    },
    {
      "epoch": 0.28795180722891567,
      "grad_norm": 0.719539225101471,
      "learning_rate": 4.640060240963855e-06,
      "loss": 1.2234,
      "step": 478
    },
    {
      "epoch": 0.2885542168674699,
      "grad_norm": 0.7186351418495178,
      "learning_rate": 4.639307228915663e-06,
      "loss": 1.2438,
      "step": 479
    },
    {
      "epoch": 0.2891566265060241,
      "grad_norm": 0.7489399313926697,
      "learning_rate": 4.63855421686747e-06,
      "loss": 1.2658,
      "step": 480
    },
    {
      "epoch": 0.2897590361445783,
      "grad_norm": 0.7756184935569763,
      "learning_rate": 4.637801204819278e-06,
      "loss": 1.2213,
      "step": 481
    },
    {
      "epoch": 0.29036144578313255,
      "grad_norm": 0.7671425342559814,
      "learning_rate": 4.637048192771085e-06,
      "loss": 1.221,
      "step": 482
    },
    {
      "epoch": 0.29096385542168673,
      "grad_norm": 0.7466318607330322,
      "learning_rate": 4.636295180722892e-06,
      "loss": 1.2424,
      "step": 483
    },
    {
      "epoch": 0.29156626506024097,
      "grad_norm": 0.7561440467834473,
      "learning_rate": 4.6355421686746994e-06,
      "loss": 1.2081,
      "step": 484
    },
    {
      "epoch": 0.2921686746987952,
      "grad_norm": 0.7540988326072693,
      "learning_rate": 4.634789156626506e-06,
      "loss": 1.2498,
      "step": 485
    },
    {
      "epoch": 0.2927710843373494,
      "grad_norm": 0.8176242113113403,
      "learning_rate": 4.634036144578314e-06,
      "loss": 1.2283,
      "step": 486
    },
    {
      "epoch": 0.2933734939759036,
      "grad_norm": 0.7305704355239868,
      "learning_rate": 4.633283132530121e-06,
      "loss": 1.2148,
      "step": 487
    },
    {
      "epoch": 0.29397590361445786,
      "grad_norm": 0.7556591033935547,
      "learning_rate": 4.632530120481928e-06,
      "loss": 1.2099,
      "step": 488
    },
    {
      "epoch": 0.29457831325301204,
      "grad_norm": 0.7583631873130798,
      "learning_rate": 4.631777108433736e-06,
      "loss": 1.1997,
      "step": 489
    },
    {
      "epoch": 0.29518072289156627,
      "grad_norm": 0.7480889558792114,
      "learning_rate": 4.631024096385543e-06,
      "loss": 1.2139,
      "step": 490
    },
    {
      "epoch": 0.2957831325301205,
      "grad_norm": 0.7059411406517029,
      "learning_rate": 4.63027108433735e-06,
      "loss": 1.1888,
      "step": 491
    },
    {
      "epoch": 0.2963855421686747,
      "grad_norm": 0.7075276374816895,
      "learning_rate": 4.629518072289157e-06,
      "loss": 1.1933,
      "step": 492
    },
    {
      "epoch": 0.2969879518072289,
      "grad_norm": 0.7904917001724243,
      "learning_rate": 4.628765060240964e-06,
      "loss": 1.1979,
      "step": 493
    },
    {
      "epoch": 0.29759036144578316,
      "grad_norm": 0.7569703459739685,
      "learning_rate": 4.628012048192771e-06,
      "loss": 1.2082,
      "step": 494
    },
    {
      "epoch": 0.29819277108433734,
      "grad_norm": 0.8090049028396606,
      "learning_rate": 4.627259036144578e-06,
      "loss": 1.2189,
      "step": 495
    },
    {
      "epoch": 0.2987951807228916,
      "grad_norm": 0.8234100937843323,
      "learning_rate": 4.626506024096386e-06,
      "loss": 1.1795,
      "step": 496
    },
    {
      "epoch": 0.2993975903614458,
      "grad_norm": 0.8175089359283447,
      "learning_rate": 4.625753012048193e-06,
      "loss": 1.1992,
      "step": 497
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.7967278361320496,
      "learning_rate": 4.625000000000001e-06,
      "loss": 1.2157,
      "step": 498
    },
    {
      "epoch": 0.3006024096385542,
      "grad_norm": 0.7595409154891968,
      "learning_rate": 4.6242469879518075e-06,
      "loss": 1.1583,
      "step": 499
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 0.8082883954048157,
      "learning_rate": 4.6234939759036145e-06,
      "loss": 1.1759,
      "step": 500
    },
    {
      "epoch": 0.30180722891566264,
      "grad_norm": 0.7454391121864319,
      "learning_rate": 4.622740963855422e-06,
      "loss": 1.18,
      "step": 501
    },
    {
      "epoch": 0.3024096385542169,
      "grad_norm": 0.7492027878761292,
      "learning_rate": 4.621987951807229e-06,
      "loss": 1.1795,
      "step": 502
    },
    {
      "epoch": 0.3030120481927711,
      "grad_norm": 0.8003729581832886,
      "learning_rate": 4.621234939759037e-06,
      "loss": 1.1923,
      "step": 503
    },
    {
      "epoch": 0.3036144578313253,
      "grad_norm": 0.8372066020965576,
      "learning_rate": 4.620481927710844e-06,
      "loss": 1.2009,
      "step": 504
    },
    {
      "epoch": 0.3042168674698795,
      "grad_norm": 0.790095329284668,
      "learning_rate": 4.619728915662652e-06,
      "loss": 1.1585,
      "step": 505
    },
    {
      "epoch": 0.30481927710843376,
      "grad_norm": 0.7633000612258911,
      "learning_rate": 4.6189759036144586e-06,
      "loss": 1.155,
      "step": 506
    },
    {
      "epoch": 0.30542168674698794,
      "grad_norm": 0.7380251288414001,
      "learning_rate": 4.6182228915662655e-06,
      "loss": 1.179,
      "step": 507
    },
    {
      "epoch": 0.3060240963855422,
      "grad_norm": 0.7651327848434448,
      "learning_rate": 4.617469879518072e-06,
      "loss": 1.1754,
      "step": 508
    },
    {
      "epoch": 0.3066265060240964,
      "grad_norm": 0.8397035598754883,
      "learning_rate": 4.61671686746988e-06,
      "loss": 1.1261,
      "step": 509
    },
    {
      "epoch": 0.3072289156626506,
      "grad_norm": 0.752021849155426,
      "learning_rate": 4.615963855421687e-06,
      "loss": 1.1378,
      "step": 510
    },
    {
      "epoch": 0.30783132530120483,
      "grad_norm": 0.7667669057846069,
      "learning_rate": 4.615210843373494e-06,
      "loss": 1.169,
      "step": 511
    },
    {
      "epoch": 0.30843373493975906,
      "grad_norm": 0.7769167423248291,
      "learning_rate": 4.614457831325301e-06,
      "loss": 1.168,
      "step": 512
    },
    {
      "epoch": 0.30903614457831324,
      "grad_norm": 0.7425531148910522,
      "learning_rate": 4.613704819277109e-06,
      "loss": 1.143,
      "step": 513
    },
    {
      "epoch": 0.3096385542168675,
      "grad_norm": 0.8274052739143372,
      "learning_rate": 4.612951807228916e-06,
      "loss": 1.1465,
      "step": 514
    },
    {
      "epoch": 0.3102409638554217,
      "grad_norm": 1.6008063554763794,
      "learning_rate": 4.612198795180723e-06,
      "loss": 1.1318,
      "step": 515
    },
    {
      "epoch": 0.3108433734939759,
      "grad_norm": 0.7985171675682068,
      "learning_rate": 4.61144578313253e-06,
      "loss": 1.152,
      "step": 516
    },
    {
      "epoch": 0.31144578313253013,
      "grad_norm": 0.8232523202896118,
      "learning_rate": 4.610692771084338e-06,
      "loss": 1.1435,
      "step": 517
    },
    {
      "epoch": 0.31204819277108437,
      "grad_norm": 0.7856816053390503,
      "learning_rate": 4.609939759036145e-06,
      "loss": 1.1818,
      "step": 518
    },
    {
      "epoch": 0.31265060240963854,
      "grad_norm": 1.1465102434158325,
      "learning_rate": 4.609186746987952e-06,
      "loss": 1.0955,
      "step": 519
    },
    {
      "epoch": 0.3132530120481928,
      "grad_norm": 0.8099993467330933,
      "learning_rate": 4.60843373493976e-06,
      "loss": 1.1333,
      "step": 520
    },
    {
      "epoch": 0.31385542168674696,
      "grad_norm": 0.7175696492195129,
      "learning_rate": 4.607680722891567e-06,
      "loss": 1.1243,
      "step": 521
    },
    {
      "epoch": 0.3144578313253012,
      "grad_norm": 0.8137258887290955,
      "learning_rate": 4.6069277108433744e-06,
      "loss": 1.1597,
      "step": 522
    },
    {
      "epoch": 0.31506024096385543,
      "grad_norm": 0.8002985715866089,
      "learning_rate": 4.606174698795181e-06,
      "loss": 1.1363,
      "step": 523
    },
    {
      "epoch": 0.3156626506024096,
      "grad_norm": 0.8019974827766418,
      "learning_rate": 4.605421686746988e-06,
      "loss": 1.1337,
      "step": 524
    },
    {
      "epoch": 0.31626506024096385,
      "grad_norm": 0.8031078577041626,
      "learning_rate": 4.604668674698796e-06,
      "loss": 1.1039,
      "step": 525
    },
    {
      "epoch": 0.3168674698795181,
      "grad_norm": 0.7890410423278809,
      "learning_rate": 4.603915662650603e-06,
      "loss": 1.1279,
      "step": 526
    },
    {
      "epoch": 0.31746987951807226,
      "grad_norm": 0.7995169162750244,
      "learning_rate": 4.60316265060241e-06,
      "loss": 1.1005,
      "step": 527
    },
    {
      "epoch": 0.3180722891566265,
      "grad_norm": 0.8057255148887634,
      "learning_rate": 4.602409638554217e-06,
      "loss": 1.1538,
      "step": 528
    },
    {
      "epoch": 0.31867469879518073,
      "grad_norm": 0.8614428639411926,
      "learning_rate": 4.601656626506025e-06,
      "loss": 1.0864,
      "step": 529
    },
    {
      "epoch": 0.3192771084337349,
      "grad_norm": 0.7864918112754822,
      "learning_rate": 4.6009036144578315e-06,
      "loss": 1.0939,
      "step": 530
    },
    {
      "epoch": 0.31987951807228915,
      "grad_norm": 0.8994051218032837,
      "learning_rate": 4.6001506024096384e-06,
      "loss": 1.118,
      "step": 531
    },
    {
      "epoch": 0.3204819277108434,
      "grad_norm": 0.8140515685081482,
      "learning_rate": 4.599397590361446e-06,
      "loss": 1.1104,
      "step": 532
    },
    {
      "epoch": 0.32108433734939756,
      "grad_norm": 0.8136924505233765,
      "learning_rate": 4.598644578313253e-06,
      "loss": 1.0804,
      "step": 533
    },
    {
      "epoch": 0.3216867469879518,
      "grad_norm": 0.8099343180656433,
      "learning_rate": 4.597891566265061e-06,
      "loss": 1.1287,
      "step": 534
    },
    {
      "epoch": 0.32228915662650603,
      "grad_norm": 0.7928326725959778,
      "learning_rate": 4.597138554216868e-06,
      "loss": 1.1328,
      "step": 535
    },
    {
      "epoch": 0.3228915662650602,
      "grad_norm": 0.8063804507255554,
      "learning_rate": 4.596385542168675e-06,
      "loss": 1.0906,
      "step": 536
    },
    {
      "epoch": 0.32349397590361445,
      "grad_norm": 0.8306028842926025,
      "learning_rate": 4.5956325301204825e-06,
      "loss": 1.0974,
      "step": 537
    },
    {
      "epoch": 0.3240963855421687,
      "grad_norm": 0.8332862257957458,
      "learning_rate": 4.5948795180722895e-06,
      "loss": 1.1073,
      "step": 538
    },
    {
      "epoch": 0.32469879518072287,
      "grad_norm": 0.813503086566925,
      "learning_rate": 4.594126506024097e-06,
      "loss": 1.093,
      "step": 539
    },
    {
      "epoch": 0.3253012048192771,
      "grad_norm": 0.8339914679527283,
      "learning_rate": 4.593373493975904e-06,
      "loss": 1.1028,
      "step": 540
    },
    {
      "epoch": 0.32590361445783134,
      "grad_norm": 0.9993857145309448,
      "learning_rate": 4.592620481927711e-06,
      "loss": 1.1268,
      "step": 541
    },
    {
      "epoch": 0.3265060240963855,
      "grad_norm": 0.8917161822319031,
      "learning_rate": 4.591867469879519e-06,
      "loss": 1.0719,
      "step": 542
    },
    {
      "epoch": 0.32710843373493975,
      "grad_norm": 0.8462717533111572,
      "learning_rate": 4.591114457831326e-06,
      "loss": 1.0674,
      "step": 543
    },
    {
      "epoch": 0.327710843373494,
      "grad_norm": 1.0687875747680664,
      "learning_rate": 4.590361445783133e-06,
      "loss": 1.0583,
      "step": 544
    },
    {
      "epoch": 0.32831325301204817,
      "grad_norm": 0.8134668469429016,
      "learning_rate": 4.58960843373494e-06,
      "loss": 1.0574,
      "step": 545
    },
    {
      "epoch": 0.3289156626506024,
      "grad_norm": 0.8262046575546265,
      "learning_rate": 4.588855421686747e-06,
      "loss": 1.0749,
      "step": 546
    },
    {
      "epoch": 0.32951807228915664,
      "grad_norm": 0.9147099852561951,
      "learning_rate": 4.588102409638554e-06,
      "loss": 1.0797,
      "step": 547
    },
    {
      "epoch": 0.3301204819277108,
      "grad_norm": 0.8510633111000061,
      "learning_rate": 4.587349397590361e-06,
      "loss": 1.0754,
      "step": 548
    },
    {
      "epoch": 0.33072289156626505,
      "grad_norm": 0.7794294953346252,
      "learning_rate": 4.586596385542169e-06,
      "loss": 1.0579,
      "step": 549
    },
    {
      "epoch": 0.3313253012048193,
      "grad_norm": 0.7738134264945984,
      "learning_rate": 4.585843373493976e-06,
      "loss": 1.0469,
      "step": 550
    },
    {
      "epoch": 0.33192771084337347,
      "grad_norm": 0.8296244740486145,
      "learning_rate": 4.585090361445784e-06,
      "loss": 1.0869,
      "step": 551
    },
    {
      "epoch": 0.3325301204819277,
      "grad_norm": 0.8794206380844116,
      "learning_rate": 4.584337349397591e-06,
      "loss": 1.0154,
      "step": 552
    },
    {
      "epoch": 0.33313253012048194,
      "grad_norm": 0.8910216689109802,
      "learning_rate": 4.583584337349398e-06,
      "loss": 1.0379,
      "step": 553
    },
    {
      "epoch": 0.3337349397590361,
      "grad_norm": 0.8590061068534851,
      "learning_rate": 4.582831325301205e-06,
      "loss": 1.0799,
      "step": 554
    },
    {
      "epoch": 0.33433734939759036,
      "grad_norm": 0.8788748979568481,
      "learning_rate": 4.582078313253012e-06,
      "loss": 1.0272,
      "step": 555
    },
    {
      "epoch": 0.3349397590361446,
      "grad_norm": 0.8865940570831299,
      "learning_rate": 4.58132530120482e-06,
      "loss": 1.067,
      "step": 556
    },
    {
      "epoch": 0.33554216867469877,
      "grad_norm": 0.8577322363853455,
      "learning_rate": 4.580572289156627e-06,
      "loss": 1.0143,
      "step": 557
    },
    {
      "epoch": 0.336144578313253,
      "grad_norm": 0.9324508309364319,
      "learning_rate": 4.579819277108435e-06,
      "loss": 1.0183,
      "step": 558
    },
    {
      "epoch": 0.33674698795180724,
      "grad_norm": 0.9491841793060303,
      "learning_rate": 4.579066265060242e-06,
      "loss": 1.0274,
      "step": 559
    },
    {
      "epoch": 0.3373493975903614,
      "grad_norm": 0.9048909544944763,
      "learning_rate": 4.578313253012049e-06,
      "loss": 1.0178,
      "step": 560
    },
    {
      "epoch": 0.33795180722891566,
      "grad_norm": 0.8817954659461975,
      "learning_rate": 4.5775602409638555e-06,
      "loss": 1.011,
      "step": 561
    },
    {
      "epoch": 0.3385542168674699,
      "grad_norm": 0.9615432024002075,
      "learning_rate": 4.576807228915663e-06,
      "loss": 1.0369,
      "step": 562
    },
    {
      "epoch": 0.3391566265060241,
      "grad_norm": 0.8535183668136597,
      "learning_rate": 4.57605421686747e-06,
      "loss": 1.0178,
      "step": 563
    },
    {
      "epoch": 0.3397590361445783,
      "grad_norm": 0.9814767837524414,
      "learning_rate": 4.575301204819277e-06,
      "loss": 1.006,
      "step": 564
    },
    {
      "epoch": 0.34036144578313254,
      "grad_norm": 0.882534921169281,
      "learning_rate": 4.574548192771085e-06,
      "loss": 0.9888,
      "step": 565
    },
    {
      "epoch": 0.3409638554216867,
      "grad_norm": 0.8681419491767883,
      "learning_rate": 4.573795180722892e-06,
      "loss": 0.9919,
      "step": 566
    },
    {
      "epoch": 0.34156626506024096,
      "grad_norm": 0.9057894945144653,
      "learning_rate": 4.573042168674699e-06,
      "loss": 0.9934,
      "step": 567
    },
    {
      "epoch": 0.3421686746987952,
      "grad_norm": 0.8694475889205933,
      "learning_rate": 4.5722891566265065e-06,
      "loss": 1.0,
      "step": 568
    },
    {
      "epoch": 0.3427710843373494,
      "grad_norm": 0.9099330902099609,
      "learning_rate": 4.5715361445783135e-06,
      "loss": 0.9799,
      "step": 569
    },
    {
      "epoch": 0.3433734939759036,
      "grad_norm": 0.8328333497047424,
      "learning_rate": 4.570783132530121e-06,
      "loss": 1.0405,
      "step": 570
    },
    {
      "epoch": 0.34397590361445785,
      "grad_norm": 0.9287550449371338,
      "learning_rate": 4.570030120481928e-06,
      "loss": 0.9911,
      "step": 571
    },
    {
      "epoch": 0.344578313253012,
      "grad_norm": 0.862338125705719,
      "learning_rate": 4.569277108433735e-06,
      "loss": 0.9903,
      "step": 572
    },
    {
      "epoch": 0.34518072289156626,
      "grad_norm": 0.9282112121582031,
      "learning_rate": 4.568524096385543e-06,
      "loss": 0.9574,
      "step": 573
    },
    {
      "epoch": 0.3457831325301205,
      "grad_norm": 0.8905463218688965,
      "learning_rate": 4.56777108433735e-06,
      "loss": 0.9967,
      "step": 574
    },
    {
      "epoch": 0.3463855421686747,
      "grad_norm": 0.8808298110961914,
      "learning_rate": 4.5670180722891575e-06,
      "loss": 0.968,
      "step": 575
    },
    {
      "epoch": 0.3469879518072289,
      "grad_norm": 0.8901328444480896,
      "learning_rate": 4.5662650602409645e-06,
      "loss": 0.9936,
      "step": 576
    },
    {
      "epoch": 0.34759036144578315,
      "grad_norm": 0.9009279012680054,
      "learning_rate": 4.565512048192771e-06,
      "loss": 0.9656,
      "step": 577
    },
    {
      "epoch": 0.3481927710843373,
      "grad_norm": 0.8769810199737549,
      "learning_rate": 4.564759036144578e-06,
      "loss": 0.9642,
      "step": 578
    },
    {
      "epoch": 0.34879518072289156,
      "grad_norm": 0.9731817245483398,
      "learning_rate": 4.564006024096386e-06,
      "loss": 1.0062,
      "step": 579
    },
    {
      "epoch": 0.3493975903614458,
      "grad_norm": 0.9255688786506653,
      "learning_rate": 4.563253012048193e-06,
      "loss": 0.9733,
      "step": 580
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.9204980134963989,
      "learning_rate": 4.5625e-06,
      "loss": 0.9265,
      "step": 581
    },
    {
      "epoch": 0.3506024096385542,
      "grad_norm": 0.9825111627578735,
      "learning_rate": 4.561746987951808e-06,
      "loss": 0.9838,
      "step": 582
    },
    {
      "epoch": 0.35120481927710845,
      "grad_norm": 0.9375790357589722,
      "learning_rate": 4.560993975903615e-06,
      "loss": 0.9304,
      "step": 583
    },
    {
      "epoch": 0.35180722891566263,
      "grad_norm": 0.9558883309364319,
      "learning_rate": 4.5602409638554216e-06,
      "loss": 0.9341,
      "step": 584
    },
    {
      "epoch": 0.35240963855421686,
      "grad_norm": 0.9476211071014404,
      "learning_rate": 4.559487951807229e-06,
      "loss": 0.9479,
      "step": 585
    },
    {
      "epoch": 0.3530120481927711,
      "grad_norm": 0.9789583683013916,
      "learning_rate": 4.558734939759036e-06,
      "loss": 0.9392,
      "step": 586
    },
    {
      "epoch": 0.3536144578313253,
      "grad_norm": 1.0323759317398071,
      "learning_rate": 4.557981927710844e-06,
      "loss": 0.9519,
      "step": 587
    },
    {
      "epoch": 0.3542168674698795,
      "grad_norm": 0.9491678476333618,
      "learning_rate": 4.557228915662651e-06,
      "loss": 0.9806,
      "step": 588
    },
    {
      "epoch": 0.35481927710843375,
      "grad_norm": 0.8710739612579346,
      "learning_rate": 4.556475903614459e-06,
      "loss": 0.9402,
      "step": 589
    },
    {
      "epoch": 0.35542168674698793,
      "grad_norm": 0.909956693649292,
      "learning_rate": 4.555722891566266e-06,
      "loss": 0.9217,
      "step": 590
    },
    {
      "epoch": 0.35602409638554217,
      "grad_norm": 0.8869108557701111,
      "learning_rate": 4.5549698795180726e-06,
      "loss": 0.954,
      "step": 591
    },
    {
      "epoch": 0.3566265060240964,
      "grad_norm": 0.9693204760551453,
      "learning_rate": 4.55421686746988e-06,
      "loss": 0.9179,
      "step": 592
    },
    {
      "epoch": 0.3572289156626506,
      "grad_norm": 0.9338054060935974,
      "learning_rate": 4.553463855421687e-06,
      "loss": 0.9577,
      "step": 593
    },
    {
      "epoch": 0.3578313253012048,
      "grad_norm": 0.9699175357818604,
      "learning_rate": 4.552710843373494e-06,
      "loss": 0.9308,
      "step": 594
    },
    {
      "epoch": 0.35843373493975905,
      "grad_norm": 1.003976583480835,
      "learning_rate": 4.551957831325302e-06,
      "loss": 0.8975,
      "step": 595
    },
    {
      "epoch": 0.35903614457831323,
      "grad_norm": 0.9842025637626648,
      "learning_rate": 4.551204819277109e-06,
      "loss": 0.9054,
      "step": 596
    },
    {
      "epoch": 0.35963855421686747,
      "grad_norm": 0.9319078922271729,
      "learning_rate": 4.550451807228916e-06,
      "loss": 0.9272,
      "step": 597
    },
    {
      "epoch": 0.3602409638554217,
      "grad_norm": 1.0650275945663452,
      "learning_rate": 4.549698795180723e-06,
      "loss": 0.9075,
      "step": 598
    },
    {
      "epoch": 0.3608433734939759,
      "grad_norm": 0.9960916042327881,
      "learning_rate": 4.5489457831325305e-06,
      "loss": 0.8718,
      "step": 599
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 1.001706838607788,
      "learning_rate": 4.5481927710843374e-06,
      "loss": 0.8936,
      "step": 600
    },
    {
      "epoch": 0.36204819277108435,
      "grad_norm": 0.9815955758094788,
      "learning_rate": 4.547439759036145e-06,
      "loss": 0.9019,
      "step": 601
    },
    {
      "epoch": 0.36265060240963853,
      "grad_norm": 0.9800592064857483,
      "learning_rate": 4.546686746987952e-06,
      "loss": 0.9255,
      "step": 602
    },
    {
      "epoch": 0.36325301204819277,
      "grad_norm": 0.988575279712677,
      "learning_rate": 4.545933734939759e-06,
      "loss": 0.9027,
      "step": 603
    },
    {
      "epoch": 0.363855421686747,
      "grad_norm": 0.985105574131012,
      "learning_rate": 4.545180722891567e-06,
      "loss": 0.8928,
      "step": 604
    },
    {
      "epoch": 0.3644578313253012,
      "grad_norm": 0.908420205116272,
      "learning_rate": 4.544427710843374e-06,
      "loss": 0.9175,
      "step": 605
    },
    {
      "epoch": 0.3650602409638554,
      "grad_norm": 0.9181346297264099,
      "learning_rate": 4.5436746987951815e-06,
      "loss": 0.9098,
      "step": 606
    },
    {
      "epoch": 0.36566265060240966,
      "grad_norm": 1.002541184425354,
      "learning_rate": 4.5429216867469885e-06,
      "loss": 0.9019,
      "step": 607
    },
    {
      "epoch": 0.36626506024096384,
      "grad_norm": 0.9376067519187927,
      "learning_rate": 4.542168674698795e-06,
      "loss": 0.8461,
      "step": 608
    },
    {
      "epoch": 0.36686746987951807,
      "grad_norm": 0.9519748687744141,
      "learning_rate": 4.541415662650603e-06,
      "loss": 0.8432,
      "step": 609
    },
    {
      "epoch": 0.3674698795180723,
      "grad_norm": 1.136351227760315,
      "learning_rate": 4.54066265060241e-06,
      "loss": 0.8102,
      "step": 610
    },
    {
      "epoch": 0.3680722891566265,
      "grad_norm": 0.9343248605728149,
      "learning_rate": 4.539909638554217e-06,
      "loss": 0.8452,
      "step": 611
    },
    {
      "epoch": 0.3686746987951807,
      "grad_norm": 0.9426173567771912,
      "learning_rate": 4.539156626506025e-06,
      "loss": 0.8373,
      "step": 612
    },
    {
      "epoch": 0.36927710843373496,
      "grad_norm": 0.9152857661247253,
      "learning_rate": 4.538403614457832e-06,
      "loss": 0.8594,
      "step": 613
    },
    {
      "epoch": 0.36987951807228914,
      "grad_norm": 0.9090049266815186,
      "learning_rate": 4.537650602409639e-06,
      "loss": 0.8805,
      "step": 614
    },
    {
      "epoch": 0.3704819277108434,
      "grad_norm": 0.9620448350906372,
      "learning_rate": 4.5368975903614455e-06,
      "loss": 0.866,
      "step": 615
    },
    {
      "epoch": 0.3710843373493976,
      "grad_norm": 0.9901975989341736,
      "learning_rate": 4.536144578313253e-06,
      "loss": 0.8637,
      "step": 616
    },
    {
      "epoch": 0.3716867469879518,
      "grad_norm": 0.9469422101974487,
      "learning_rate": 4.53539156626506e-06,
      "loss": 0.8289,
      "step": 617
    },
    {
      "epoch": 0.372289156626506,
      "grad_norm": 1.0357321500778198,
      "learning_rate": 4.534638554216868e-06,
      "loss": 0.8376,
      "step": 618
    },
    {
      "epoch": 0.37289156626506026,
      "grad_norm": 0.999031126499176,
      "learning_rate": 4.533885542168675e-06,
      "loss": 0.8445,
      "step": 619
    },
    {
      "epoch": 0.37349397590361444,
      "grad_norm": 0.9613181352615356,
      "learning_rate": 4.533132530120482e-06,
      "loss": 0.8255,
      "step": 620
    },
    {
      "epoch": 0.3740963855421687,
      "grad_norm": 0.9610326290130615,
      "learning_rate": 4.53237951807229e-06,
      "loss": 0.8342,
      "step": 621
    },
    {
      "epoch": 0.3746987951807229,
      "grad_norm": 1.0821201801300049,
      "learning_rate": 4.5316265060240966e-06,
      "loss": 0.8356,
      "step": 622
    },
    {
      "epoch": 0.3753012048192771,
      "grad_norm": 1.047034740447998,
      "learning_rate": 4.530873493975904e-06,
      "loss": 0.817,
      "step": 623
    },
    {
      "epoch": 0.3759036144578313,
      "grad_norm": 0.9263137578964233,
      "learning_rate": 4.530120481927711e-06,
      "loss": 0.7894,
      "step": 624
    },
    {
      "epoch": 0.37650602409638556,
      "grad_norm": 0.9721913933753967,
      "learning_rate": 4.529367469879519e-06,
      "loss": 0.8198,
      "step": 625
    },
    {
      "epoch": 0.37710843373493974,
      "grad_norm": 0.9413060545921326,
      "learning_rate": 4.528614457831326e-06,
      "loss": 0.7946,
      "step": 626
    },
    {
      "epoch": 0.377710843373494,
      "grad_norm": 0.954142689704895,
      "learning_rate": 4.527861445783133e-06,
      "loss": 0.8136,
      "step": 627
    },
    {
      "epoch": 0.3783132530120482,
      "grad_norm": 1.1040890216827393,
      "learning_rate": 4.527108433734941e-06,
      "loss": 0.8331,
      "step": 628
    },
    {
      "epoch": 0.3789156626506024,
      "grad_norm": 1.0288211107254028,
      "learning_rate": 4.526355421686748e-06,
      "loss": 0.8124,
      "step": 629
    },
    {
      "epoch": 0.3795180722891566,
      "grad_norm": 0.9828630089759827,
      "learning_rate": 4.5256024096385545e-06,
      "loss": 0.8077,
      "step": 630
    },
    {
      "epoch": 0.38012048192771086,
      "grad_norm": 0.964911699295044,
      "learning_rate": 4.5248493975903614e-06,
      "loss": 0.8012,
      "step": 631
    },
    {
      "epoch": 0.38072289156626504,
      "grad_norm": 0.9970154166221619,
      "learning_rate": 4.524096385542169e-06,
      "loss": 0.7621,
      "step": 632
    },
    {
      "epoch": 0.3813253012048193,
      "grad_norm": 1.0088462829589844,
      "learning_rate": 4.523343373493976e-06,
      "loss": 0.7747,
      "step": 633
    },
    {
      "epoch": 0.3819277108433735,
      "grad_norm": 0.9924386143684387,
      "learning_rate": 4.522590361445783e-06,
      "loss": 0.7601,
      "step": 634
    },
    {
      "epoch": 0.3825301204819277,
      "grad_norm": 1.0044641494750977,
      "learning_rate": 4.521837349397591e-06,
      "loss": 0.7776,
      "step": 635
    },
    {
      "epoch": 0.38313253012048193,
      "grad_norm": 0.9805490374565125,
      "learning_rate": 4.521084337349398e-06,
      "loss": 0.7965,
      "step": 636
    },
    {
      "epoch": 0.38373493975903616,
      "grad_norm": 0.966906726360321,
      "learning_rate": 4.5203313253012055e-06,
      "loss": 0.7631,
      "step": 637
    },
    {
      "epoch": 0.38433734939759034,
      "grad_norm": 1.1182326078414917,
      "learning_rate": 4.5195783132530124e-06,
      "loss": 0.7531,
      "step": 638
    },
    {
      "epoch": 0.3849397590361446,
      "grad_norm": 1.0099408626556396,
      "learning_rate": 4.518825301204819e-06,
      "loss": 0.7624,
      "step": 639
    },
    {
      "epoch": 0.3855421686746988,
      "grad_norm": 0.9878208637237549,
      "learning_rate": 4.518072289156627e-06,
      "loss": 0.7601,
      "step": 640
    },
    {
      "epoch": 0.386144578313253,
      "grad_norm": 1.125728726387024,
      "learning_rate": 4.517319277108434e-06,
      "loss": 0.7822,
      "step": 641
    },
    {
      "epoch": 0.38674698795180723,
      "grad_norm": 1.0349582433700562,
      "learning_rate": 4.516566265060242e-06,
      "loss": 0.7747,
      "step": 642
    },
    {
      "epoch": 0.38734939759036147,
      "grad_norm": 1.0631352663040161,
      "learning_rate": 4.515813253012049e-06,
      "loss": 0.7723,
      "step": 643
    },
    {
      "epoch": 0.38795180722891565,
      "grad_norm": 1.0718674659729004,
      "learning_rate": 4.515060240963856e-06,
      "loss": 0.7715,
      "step": 644
    },
    {
      "epoch": 0.3885542168674699,
      "grad_norm": 1.0990155935287476,
      "learning_rate": 4.5143072289156635e-06,
      "loss": 0.8042,
      "step": 645
    },
    {
      "epoch": 0.3891566265060241,
      "grad_norm": 1.0174449682235718,
      "learning_rate": 4.51355421686747e-06,
      "loss": 0.7595,
      "step": 646
    },
    {
      "epoch": 0.3897590361445783,
      "grad_norm": 1.1837811470031738,
      "learning_rate": 4.512801204819277e-06,
      "loss": 0.7464,
      "step": 647
    },
    {
      "epoch": 0.39036144578313253,
      "grad_norm": 1.1599847078323364,
      "learning_rate": 4.512048192771084e-06,
      "loss": 0.7587,
      "step": 648
    },
    {
      "epoch": 0.39096385542168677,
      "grad_norm": 1.0774868726730347,
      "learning_rate": 4.511295180722892e-06,
      "loss": 0.725,
      "step": 649
    },
    {
      "epoch": 0.39156626506024095,
      "grad_norm": 0.9660741090774536,
      "learning_rate": 4.510542168674699e-06,
      "loss": 0.7875,
      "step": 650
    },
    {
      "epoch": 0.3921686746987952,
      "grad_norm": 1.0221377611160278,
      "learning_rate": 4.509789156626506e-06,
      "loss": 0.7498,
      "step": 651
    },
    {
      "epoch": 0.3927710843373494,
      "grad_norm": 1.0415996313095093,
      "learning_rate": 4.509036144578314e-06,
      "loss": 0.702,
      "step": 652
    },
    {
      "epoch": 0.3933734939759036,
      "grad_norm": 1.0113518238067627,
      "learning_rate": 4.5082831325301206e-06,
      "loss": 0.737,
      "step": 653
    },
    {
      "epoch": 0.39397590361445783,
      "grad_norm": 1.1010925769805908,
      "learning_rate": 4.507530120481928e-06,
      "loss": 0.752,
      "step": 654
    },
    {
      "epoch": 0.39457831325301207,
      "grad_norm": 1.0029491186141968,
      "learning_rate": 4.506777108433735e-06,
      "loss": 0.7445,
      "step": 655
    },
    {
      "epoch": 0.39518072289156625,
      "grad_norm": 1.1449438333511353,
      "learning_rate": 4.506024096385542e-06,
      "loss": 0.7078,
      "step": 656
    },
    {
      "epoch": 0.3957831325301205,
      "grad_norm": 1.0588957071304321,
      "learning_rate": 4.50527108433735e-06,
      "loss": 0.7135,
      "step": 657
    },
    {
      "epoch": 0.3963855421686747,
      "grad_norm": 1.0264630317687988,
      "learning_rate": 4.504518072289157e-06,
      "loss": 0.7432,
      "step": 658
    },
    {
      "epoch": 0.3969879518072289,
      "grad_norm": 0.9456748366355896,
      "learning_rate": 4.503765060240965e-06,
      "loss": 0.7136,
      "step": 659
    },
    {
      "epoch": 0.39759036144578314,
      "grad_norm": 1.0326026678085327,
      "learning_rate": 4.5030120481927716e-06,
      "loss": 0.7882,
      "step": 660
    },
    {
      "epoch": 0.39819277108433737,
      "grad_norm": 1.026642084121704,
      "learning_rate": 4.502259036144579e-06,
      "loss": 0.7063,
      "step": 661
    },
    {
      "epoch": 0.39879518072289155,
      "grad_norm": 1.1484055519104004,
      "learning_rate": 4.501506024096386e-06,
      "loss": 0.7575,
      "step": 662
    },
    {
      "epoch": 0.3993975903614458,
      "grad_norm": 0.9679239392280579,
      "learning_rate": 4.500753012048193e-06,
      "loss": 0.752,
      "step": 663
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.031883716583252,
      "learning_rate": 4.5e-06,
      "loss": 0.7625,
      "step": 664
    },
    {
      "epoch": 0.4006024096385542,
      "grad_norm": 0.9326608777046204,
      "learning_rate": 4.499246987951808e-06,
      "loss": 0.7539,
      "step": 665
    },
    {
      "epoch": 0.40120481927710844,
      "grad_norm": 0.9804477691650391,
      "learning_rate": 4.498493975903615e-06,
      "loss": 0.7797,
      "step": 666
    },
    {
      "epoch": 0.4018072289156627,
      "grad_norm": 0.9202876091003418,
      "learning_rate": 4.497740963855422e-06,
      "loss": 0.7471,
      "step": 667
    },
    {
      "epoch": 0.40240963855421685,
      "grad_norm": 1.0226390361785889,
      "learning_rate": 4.496987951807229e-06,
      "loss": 0.7713,
      "step": 668
    },
    {
      "epoch": 0.4030120481927711,
      "grad_norm": 1.0094815492630005,
      "learning_rate": 4.4962349397590364e-06,
      "loss": 0.7396,
      "step": 669
    },
    {
      "epoch": 0.4036144578313253,
      "grad_norm": 0.8498848080635071,
      "learning_rate": 4.495481927710843e-06,
      "loss": 0.7481,
      "step": 670
    },
    {
      "epoch": 0.4042168674698795,
      "grad_norm": 0.8926054239273071,
      "learning_rate": 4.494728915662651e-06,
      "loss": 0.7034,
      "step": 671
    },
    {
      "epoch": 0.40481927710843374,
      "grad_norm": 1.01282799243927,
      "learning_rate": 4.493975903614458e-06,
      "loss": 0.6883,
      "step": 672
    },
    {
      "epoch": 0.405421686746988,
      "grad_norm": 0.9260305762290955,
      "learning_rate": 4.493222891566266e-06,
      "loss": 0.7298,
      "step": 673
    },
    {
      "epoch": 0.40602409638554215,
      "grad_norm": 0.9207394123077393,
      "learning_rate": 4.492469879518073e-06,
      "loss": 0.7108,
      "step": 674
    },
    {
      "epoch": 0.4066265060240964,
      "grad_norm": 0.9619960784912109,
      "learning_rate": 4.49171686746988e-06,
      "loss": 0.6785,
      "step": 675
    },
    {
      "epoch": 0.4072289156626506,
      "grad_norm": 0.9400011897087097,
      "learning_rate": 4.4909638554216874e-06,
      "loss": 0.7147,
      "step": 676
    },
    {
      "epoch": 0.4078313253012048,
      "grad_norm": 0.911755383014679,
      "learning_rate": 4.490210843373494e-06,
      "loss": 0.7024,
      "step": 677
    },
    {
      "epoch": 0.40843373493975904,
      "grad_norm": 0.8969728350639343,
      "learning_rate": 4.489457831325302e-06,
      "loss": 0.6864,
      "step": 678
    },
    {
      "epoch": 0.4090361445783133,
      "grad_norm": 1.1272653341293335,
      "learning_rate": 4.488704819277109e-06,
      "loss": 0.6898,
      "step": 679
    },
    {
      "epoch": 0.40963855421686746,
      "grad_norm": 0.9585755467414856,
      "learning_rate": 4.487951807228916e-06,
      "loss": 0.6776,
      "step": 680
    },
    {
      "epoch": 0.4102409638554217,
      "grad_norm": 1.0780611038208008,
      "learning_rate": 4.487198795180723e-06,
      "loss": 0.7071,
      "step": 681
    },
    {
      "epoch": 0.4108433734939759,
      "grad_norm": 1.102491855621338,
      "learning_rate": 4.486445783132531e-06,
      "loss": 0.6778,
      "step": 682
    },
    {
      "epoch": 0.4114457831325301,
      "grad_norm": 0.9309822916984558,
      "learning_rate": 4.485692771084338e-06,
      "loss": 0.6902,
      "step": 683
    },
    {
      "epoch": 0.41204819277108434,
      "grad_norm": 0.9775459170341492,
      "learning_rate": 4.4849397590361445e-06,
      "loss": 0.6976,
      "step": 684
    },
    {
      "epoch": 0.4126506024096386,
      "grad_norm": 0.9899704456329346,
      "learning_rate": 4.484186746987952e-06,
      "loss": 0.666,
      "step": 685
    },
    {
      "epoch": 0.41325301204819276,
      "grad_norm": 1.0712119340896606,
      "learning_rate": 4.483433734939759e-06,
      "loss": 0.7253,
      "step": 686
    },
    {
      "epoch": 0.413855421686747,
      "grad_norm": 0.9476547241210938,
      "learning_rate": 4.482680722891566e-06,
      "loss": 0.6993,
      "step": 687
    },
    {
      "epoch": 0.41445783132530123,
      "grad_norm": 0.9115734696388245,
      "learning_rate": 4.481927710843374e-06,
      "loss": 0.6932,
      "step": 688
    },
    {
      "epoch": 0.4150602409638554,
      "grad_norm": 1.7077184915542603,
      "learning_rate": 4.481174698795181e-06,
      "loss": 0.695,
      "step": 689
    },
    {
      "epoch": 0.41566265060240964,
      "grad_norm": 1.043157696723938,
      "learning_rate": 4.480421686746989e-06,
      "loss": 0.702,
      "step": 690
    },
    {
      "epoch": 0.4162650602409639,
      "grad_norm": 1.0398238897323608,
      "learning_rate": 4.4796686746987956e-06,
      "loss": 0.6591,
      "step": 691
    },
    {
      "epoch": 0.41686746987951806,
      "grad_norm": 0.9030842781066895,
      "learning_rate": 4.4789156626506025e-06,
      "loss": 0.666,
      "step": 692
    },
    {
      "epoch": 0.4174698795180723,
      "grad_norm": 1.0230324268341064,
      "learning_rate": 4.47816265060241e-06,
      "loss": 0.6817,
      "step": 693
    },
    {
      "epoch": 0.41807228915662653,
      "grad_norm": 0.9749019742012024,
      "learning_rate": 4.477409638554217e-06,
      "loss": 0.6899,
      "step": 694
    },
    {
      "epoch": 0.4186746987951807,
      "grad_norm": 1.0223050117492676,
      "learning_rate": 4.476656626506025e-06,
      "loss": 0.6755,
      "step": 695
    },
    {
      "epoch": 0.41927710843373495,
      "grad_norm": 1.1057755947113037,
      "learning_rate": 4.475903614457832e-06,
      "loss": 0.6335,
      "step": 696
    },
    {
      "epoch": 0.4198795180722892,
      "grad_norm": 1.0513943433761597,
      "learning_rate": 4.475150602409639e-06,
      "loss": 0.6901,
      "step": 697
    },
    {
      "epoch": 0.42048192771084336,
      "grad_norm": 0.9192396402359009,
      "learning_rate": 4.4743975903614466e-06,
      "loss": 0.6568,
      "step": 698
    },
    {
      "epoch": 0.4210843373493976,
      "grad_norm": 1.0096818208694458,
      "learning_rate": 4.4736445783132535e-06,
      "loss": 0.6635,
      "step": 699
    },
    {
      "epoch": 0.42168674698795183,
      "grad_norm": 0.9889012575149536,
      "learning_rate": 4.47289156626506e-06,
      "loss": 0.6482,
      "step": 700
    },
    {
      "epoch": 0.422289156626506,
      "grad_norm": 0.9985100626945496,
      "learning_rate": 4.472138554216867e-06,
      "loss": 0.6881,
      "step": 701
    },
    {
      "epoch": 0.42289156626506025,
      "grad_norm": 1.0344470739364624,
      "learning_rate": 4.471385542168675e-06,
      "loss": 0.6334,
      "step": 702
    },
    {
      "epoch": 0.4234939759036145,
      "grad_norm": 0.9842323064804077,
      "learning_rate": 4.470632530120482e-06,
      "loss": 0.6661,
      "step": 703
    },
    {
      "epoch": 0.42409638554216866,
      "grad_norm": 0.9014646410942078,
      "learning_rate": 4.469879518072289e-06,
      "loss": 0.6623,
      "step": 704
    },
    {
      "epoch": 0.4246987951807229,
      "grad_norm": 0.9486470222473145,
      "learning_rate": 4.469126506024097e-06,
      "loss": 0.6664,
      "step": 705
    },
    {
      "epoch": 0.42530120481927713,
      "grad_norm": 1.0533274412155151,
      "learning_rate": 4.468373493975904e-06,
      "loss": 0.6377,
      "step": 706
    },
    {
      "epoch": 0.4259036144578313,
      "grad_norm": 1.0267809629440308,
      "learning_rate": 4.4676204819277114e-06,
      "loss": 0.6852,
      "step": 707
    },
    {
      "epoch": 0.42650602409638555,
      "grad_norm": 1.0292917490005493,
      "learning_rate": 4.466867469879518e-06,
      "loss": 0.6255,
      "step": 708
    },
    {
      "epoch": 0.4271084337349398,
      "grad_norm": 0.9228330850601196,
      "learning_rate": 4.466114457831326e-06,
      "loss": 0.6267,
      "step": 709
    },
    {
      "epoch": 0.42771084337349397,
      "grad_norm": 1.1929038763046265,
      "learning_rate": 4.465361445783133e-06,
      "loss": 0.6742,
      "step": 710
    },
    {
      "epoch": 0.4283132530120482,
      "grad_norm": 0.8751698732376099,
      "learning_rate": 4.46460843373494e-06,
      "loss": 0.6371,
      "step": 711
    },
    {
      "epoch": 0.42891566265060244,
      "grad_norm": 0.9583964347839355,
      "learning_rate": 4.463855421686748e-06,
      "loss": 0.6595,
      "step": 712
    },
    {
      "epoch": 0.4295180722891566,
      "grad_norm": 0.8990480899810791,
      "learning_rate": 4.463102409638555e-06,
      "loss": 0.6355,
      "step": 713
    },
    {
      "epoch": 0.43012048192771085,
      "grad_norm": 0.9760796427726746,
      "learning_rate": 4.462349397590362e-06,
      "loss": 0.6294,
      "step": 714
    },
    {
      "epoch": 0.4307228915662651,
      "grad_norm": 0.8755533695220947,
      "learning_rate": 4.461596385542169e-06,
      "loss": 0.6425,
      "step": 715
    },
    {
      "epoch": 0.43132530120481927,
      "grad_norm": 0.9539238810539246,
      "learning_rate": 4.460843373493976e-06,
      "loss": 0.6155,
      "step": 716
    },
    {
      "epoch": 0.4319277108433735,
      "grad_norm": 0.9495978355407715,
      "learning_rate": 4.460090361445783e-06,
      "loss": 0.6143,
      "step": 717
    },
    {
      "epoch": 0.43253012048192774,
      "grad_norm": 0.910720705986023,
      "learning_rate": 4.45933734939759e-06,
      "loss": 0.5568,
      "step": 718
    },
    {
      "epoch": 0.4331325301204819,
      "grad_norm": 0.9646309614181519,
      "learning_rate": 4.458584337349398e-06,
      "loss": 0.6044,
      "step": 719
    },
    {
      "epoch": 0.43373493975903615,
      "grad_norm": 1.2737126350402832,
      "learning_rate": 4.457831325301205e-06,
      "loss": 0.6685,
      "step": 720
    },
    {
      "epoch": 0.4343373493975904,
      "grad_norm": 0.9361264109611511,
      "learning_rate": 4.457078313253013e-06,
      "loss": 0.6171,
      "step": 721
    },
    {
      "epoch": 0.43493975903614457,
      "grad_norm": 0.8227005004882812,
      "learning_rate": 4.4563253012048195e-06,
      "loss": 0.6427,
      "step": 722
    },
    {
      "epoch": 0.4355421686746988,
      "grad_norm": 0.994440495967865,
      "learning_rate": 4.4555722891566265e-06,
      "loss": 0.588,
      "step": 723
    },
    {
      "epoch": 0.43614457831325304,
      "grad_norm": 0.9769623875617981,
      "learning_rate": 4.454819277108434e-06,
      "loss": 0.6412,
      "step": 724
    },
    {
      "epoch": 0.4367469879518072,
      "grad_norm": 1.0083932876586914,
      "learning_rate": 4.454066265060241e-06,
      "loss": 0.5943,
      "step": 725
    },
    {
      "epoch": 0.43734939759036146,
      "grad_norm": 0.9585238695144653,
      "learning_rate": 4.453313253012049e-06,
      "loss": 0.607,
      "step": 726
    },
    {
      "epoch": 0.43795180722891563,
      "grad_norm": 0.9229143857955933,
      "learning_rate": 4.452560240963856e-06,
      "loss": 0.6423,
      "step": 727
    },
    {
      "epoch": 0.43855421686746987,
      "grad_norm": 1.02794349193573,
      "learning_rate": 4.451807228915663e-06,
      "loss": 0.6156,
      "step": 728
    },
    {
      "epoch": 0.4391566265060241,
      "grad_norm": 0.9746854901313782,
      "learning_rate": 4.4510542168674706e-06,
      "loss": 0.5547,
      "step": 729
    },
    {
      "epoch": 0.4397590361445783,
      "grad_norm": 1.0333980321884155,
      "learning_rate": 4.4503012048192775e-06,
      "loss": 0.5804,
      "step": 730
    },
    {
      "epoch": 0.4403614457831325,
      "grad_norm": 0.9143245220184326,
      "learning_rate": 4.449548192771085e-06,
      "loss": 0.6191,
      "step": 731
    },
    {
      "epoch": 0.44096385542168676,
      "grad_norm": 0.9375326037406921,
      "learning_rate": 4.448795180722892e-06,
      "loss": 0.5873,
      "step": 732
    },
    {
      "epoch": 0.44156626506024094,
      "grad_norm": 0.9180657863616943,
      "learning_rate": 4.448042168674699e-06,
      "loss": 0.6351,
      "step": 733
    },
    {
      "epoch": 0.44216867469879517,
      "grad_norm": 0.9116321206092834,
      "learning_rate": 4.447289156626506e-06,
      "loss": 0.5777,
      "step": 734
    },
    {
      "epoch": 0.4427710843373494,
      "grad_norm": 0.94403076171875,
      "learning_rate": 4.446536144578314e-06,
      "loss": 0.5907,
      "step": 735
    },
    {
      "epoch": 0.4433734939759036,
      "grad_norm": 1.0800135135650635,
      "learning_rate": 4.445783132530121e-06,
      "loss": 0.5928,
      "step": 736
    },
    {
      "epoch": 0.4439759036144578,
      "grad_norm": 0.9354470372200012,
      "learning_rate": 4.445030120481928e-06,
      "loss": 0.6598,
      "step": 737
    },
    {
      "epoch": 0.44457831325301206,
      "grad_norm": 0.8613961338996887,
      "learning_rate": 4.4442771084337354e-06,
      "loss": 0.6041,
      "step": 738
    },
    {
      "epoch": 0.44518072289156624,
      "grad_norm": 0.9568660855293274,
      "learning_rate": 4.443524096385542e-06,
      "loss": 0.6016,
      "step": 739
    },
    {
      "epoch": 0.4457831325301205,
      "grad_norm": 0.94954913854599,
      "learning_rate": 4.442771084337349e-06,
      "loss": 0.5819,
      "step": 740
    },
    {
      "epoch": 0.4463855421686747,
      "grad_norm": 0.9143937230110168,
      "learning_rate": 4.442018072289157e-06,
      "loss": 0.5314,
      "step": 741
    },
    {
      "epoch": 0.4469879518072289,
      "grad_norm": 0.8926975131034851,
      "learning_rate": 4.441265060240964e-06,
      "loss": 0.5564,
      "step": 742
    },
    {
      "epoch": 0.4475903614457831,
      "grad_norm": 0.9002459645271301,
      "learning_rate": 4.440512048192772e-06,
      "loss": 0.5585,
      "step": 743
    },
    {
      "epoch": 0.44819277108433736,
      "grad_norm": 0.9211053848266602,
      "learning_rate": 4.439759036144579e-06,
      "loss": 0.5697,
      "step": 744
    },
    {
      "epoch": 0.44879518072289154,
      "grad_norm": 0.862062394618988,
      "learning_rate": 4.4390060240963864e-06,
      "loss": 0.5598,
      "step": 745
    },
    {
      "epoch": 0.4493975903614458,
      "grad_norm": 0.9581020474433899,
      "learning_rate": 4.438253012048193e-06,
      "loss": 0.571,
      "step": 746
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.9433762431144714,
      "learning_rate": 4.4375e-06,
      "loss": 0.5991,
      "step": 747
    },
    {
      "epoch": 0.4506024096385542,
      "grad_norm": 0.8838898539543152,
      "learning_rate": 4.436746987951808e-06,
      "loss": 0.5814,
      "step": 748
    },
    {
      "epoch": 0.4512048192771084,
      "grad_norm": 0.8996318578720093,
      "learning_rate": 4.435993975903615e-06,
      "loss": 0.5696,
      "step": 749
    },
    {
      "epoch": 0.45180722891566266,
      "grad_norm": 0.931672215461731,
      "learning_rate": 4.435240963855422e-06,
      "loss": 0.5305,
      "step": 750
    },
    {
      "epoch": 0.45240963855421684,
      "grad_norm": 0.936844527721405,
      "learning_rate": 4.434487951807229e-06,
      "loss": 0.5667,
      "step": 751
    },
    {
      "epoch": 0.4530120481927711,
      "grad_norm": 1.020194411277771,
      "learning_rate": 4.433734939759037e-06,
      "loss": 0.5895,
      "step": 752
    },
    {
      "epoch": 0.4536144578313253,
      "grad_norm": 0.9936975836753845,
      "learning_rate": 4.4329819277108435e-06,
      "loss": 0.5716,
      "step": 753
    },
    {
      "epoch": 0.4542168674698795,
      "grad_norm": 0.9872843623161316,
      "learning_rate": 4.4322289156626505e-06,
      "loss": 0.53,
      "step": 754
    },
    {
      "epoch": 0.45481927710843373,
      "grad_norm": 0.9138345718383789,
      "learning_rate": 4.431475903614458e-06,
      "loss": 0.6431,
      "step": 755
    },
    {
      "epoch": 0.45542168674698796,
      "grad_norm": 1.04710853099823,
      "learning_rate": 4.430722891566265e-06,
      "loss": 0.5417,
      "step": 756
    },
    {
      "epoch": 0.45602409638554214,
      "grad_norm": 0.8972265124320984,
      "learning_rate": 4.429969879518073e-06,
      "loss": 0.5924,
      "step": 757
    },
    {
      "epoch": 0.4566265060240964,
      "grad_norm": 0.9210885167121887,
      "learning_rate": 4.42921686746988e-06,
      "loss": 0.5361,
      "step": 758
    },
    {
      "epoch": 0.4572289156626506,
      "grad_norm": 0.9498475790023804,
      "learning_rate": 4.428463855421687e-06,
      "loss": 0.5412,
      "step": 759
    },
    {
      "epoch": 0.4578313253012048,
      "grad_norm": 0.8670477271080017,
      "learning_rate": 4.4277108433734945e-06,
      "loss": 0.5808,
      "step": 760
    },
    {
      "epoch": 0.45843373493975903,
      "grad_norm": 0.8222115635871887,
      "learning_rate": 4.4269578313253015e-06,
      "loss": 0.5537,
      "step": 761
    },
    {
      "epoch": 0.45903614457831327,
      "grad_norm": 0.8547554016113281,
      "learning_rate": 4.426204819277109e-06,
      "loss": 0.5573,
      "step": 762
    },
    {
      "epoch": 0.45963855421686745,
      "grad_norm": 0.9777780771255493,
      "learning_rate": 4.425451807228916e-06,
      "loss": 0.5392,
      "step": 763
    },
    {
      "epoch": 0.4602409638554217,
      "grad_norm": 0.971968412399292,
      "learning_rate": 4.424698795180723e-06,
      "loss": 0.584,
      "step": 764
    },
    {
      "epoch": 0.4608433734939759,
      "grad_norm": 0.8879720568656921,
      "learning_rate": 4.423945783132531e-06,
      "loss": 0.5118,
      "step": 765
    },
    {
      "epoch": 0.4614457831325301,
      "grad_norm": 0.905521810054779,
      "learning_rate": 4.423192771084338e-06,
      "loss": 0.5674,
      "step": 766
    },
    {
      "epoch": 0.46204819277108433,
      "grad_norm": 0.8956527709960938,
      "learning_rate": 4.422439759036145e-06,
      "loss": 0.5312,
      "step": 767
    },
    {
      "epoch": 0.46265060240963857,
      "grad_norm": 0.9621406197547913,
      "learning_rate": 4.4216867469879525e-06,
      "loss": 0.5749,
      "step": 768
    },
    {
      "epoch": 0.46325301204819275,
      "grad_norm": 0.9208889007568359,
      "learning_rate": 4.420933734939759e-06,
      "loss": 0.5331,
      "step": 769
    },
    {
      "epoch": 0.463855421686747,
      "grad_norm": 0.8904247283935547,
      "learning_rate": 4.420180722891566e-06,
      "loss": 0.5575,
      "step": 770
    },
    {
      "epoch": 0.4644578313253012,
      "grad_norm": 1.0006369352340698,
      "learning_rate": 4.419427710843373e-06,
      "loss": 0.551,
      "step": 771
    },
    {
      "epoch": 0.4650602409638554,
      "grad_norm": 1.020616888999939,
      "learning_rate": 4.418674698795181e-06,
      "loss": 0.5519,
      "step": 772
    },
    {
      "epoch": 0.46566265060240963,
      "grad_norm": 0.859223484992981,
      "learning_rate": 4.417921686746988e-06,
      "loss": 0.5139,
      "step": 773
    },
    {
      "epoch": 0.46626506024096387,
      "grad_norm": 0.8742741346359253,
      "learning_rate": 4.417168674698796e-06,
      "loss": 0.4962,
      "step": 774
    },
    {
      "epoch": 0.46686746987951805,
      "grad_norm": 0.9250572919845581,
      "learning_rate": 4.416415662650603e-06,
      "loss": 0.4829,
      "step": 775
    },
    {
      "epoch": 0.4674698795180723,
      "grad_norm": 0.826255202293396,
      "learning_rate": 4.41566265060241e-06,
      "loss": 0.5095,
      "step": 776
    },
    {
      "epoch": 0.4680722891566265,
      "grad_norm": 0.8889195919036865,
      "learning_rate": 4.414909638554217e-06,
      "loss": 0.5342,
      "step": 777
    },
    {
      "epoch": 0.4686746987951807,
      "grad_norm": 0.8370757699012756,
      "learning_rate": 4.414156626506024e-06,
      "loss": 0.5147,
      "step": 778
    },
    {
      "epoch": 0.46927710843373494,
      "grad_norm": 0.8637650609016418,
      "learning_rate": 4.413403614457832e-06,
      "loss": 0.4943,
      "step": 779
    },
    {
      "epoch": 0.46987951807228917,
      "grad_norm": 0.981637179851532,
      "learning_rate": 4.412650602409639e-06,
      "loss": 0.5673,
      "step": 780
    },
    {
      "epoch": 0.47048192771084335,
      "grad_norm": 1.0625932216644287,
      "learning_rate": 4.411897590361447e-06,
      "loss": 0.5243,
      "step": 781
    },
    {
      "epoch": 0.4710843373493976,
      "grad_norm": 0.8619560599327087,
      "learning_rate": 4.411144578313254e-06,
      "loss": 0.5054,
      "step": 782
    },
    {
      "epoch": 0.4716867469879518,
      "grad_norm": 0.9234750270843506,
      "learning_rate": 4.410391566265061e-06,
      "loss": 0.5116,
      "step": 783
    },
    {
      "epoch": 0.472289156626506,
      "grad_norm": 0.8960304260253906,
      "learning_rate": 4.4096385542168675e-06,
      "loss": 0.515,
      "step": 784
    },
    {
      "epoch": 0.47289156626506024,
      "grad_norm": 0.9217971563339233,
      "learning_rate": 4.408885542168675e-06,
      "loss": 0.5015,
      "step": 785
    },
    {
      "epoch": 0.4734939759036145,
      "grad_norm": 0.8694425225257874,
      "learning_rate": 4.408132530120482e-06,
      "loss": 0.5079,
      "step": 786
    },
    {
      "epoch": 0.47409638554216865,
      "grad_norm": 0.8410780429840088,
      "learning_rate": 4.407379518072289e-06,
      "loss": 0.4958,
      "step": 787
    },
    {
      "epoch": 0.4746987951807229,
      "grad_norm": 0.9259112477302551,
      "learning_rate": 4.406626506024096e-06,
      "loss": 0.5332,
      "step": 788
    },
    {
      "epoch": 0.4753012048192771,
      "grad_norm": 0.8892222046852112,
      "learning_rate": 4.405873493975904e-06,
      "loss": 0.5011,
      "step": 789
    },
    {
      "epoch": 0.4759036144578313,
      "grad_norm": 0.9058607220649719,
      "learning_rate": 4.405120481927711e-06,
      "loss": 0.4766,
      "step": 790
    },
    {
      "epoch": 0.47650602409638554,
      "grad_norm": 0.8970216512680054,
      "learning_rate": 4.4043674698795185e-06,
      "loss": 0.5247,
      "step": 791
    },
    {
      "epoch": 0.4771084337349398,
      "grad_norm": 0.9919549226760864,
      "learning_rate": 4.4036144578313255e-06,
      "loss": 0.5378,
      "step": 792
    },
    {
      "epoch": 0.47771084337349395,
      "grad_norm": 0.8798623085021973,
      "learning_rate": 4.402861445783133e-06,
      "loss": 0.4803,
      "step": 793
    },
    {
      "epoch": 0.4783132530120482,
      "grad_norm": 0.9225683808326721,
      "learning_rate": 4.40210843373494e-06,
      "loss": 0.5229,
      "step": 794
    },
    {
      "epoch": 0.4789156626506024,
      "grad_norm": 0.9390084743499756,
      "learning_rate": 4.401355421686747e-06,
      "loss": 0.4413,
      "step": 795
    },
    {
      "epoch": 0.4795180722891566,
      "grad_norm": 0.8461403846740723,
      "learning_rate": 4.400602409638555e-06,
      "loss": 0.5403,
      "step": 796
    },
    {
      "epoch": 0.48012048192771084,
      "grad_norm": 1.0186222791671753,
      "learning_rate": 4.399849397590362e-06,
      "loss": 0.473,
      "step": 797
    },
    {
      "epoch": 0.4807228915662651,
      "grad_norm": 1.6233899593353271,
      "learning_rate": 4.3990963855421696e-06,
      "loss": 0.5583,
      "step": 798
    },
    {
      "epoch": 0.48132530120481926,
      "grad_norm": 1.0425585508346558,
      "learning_rate": 4.3983433734939765e-06,
      "loss": 0.5211,
      "step": 799
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 0.8595830202102661,
      "learning_rate": 4.397590361445783e-06,
      "loss": 0.4674,
      "step": 800
    },
    {
      "epoch": 0.4825301204819277,
      "grad_norm": 0.8353645205497742,
      "learning_rate": 4.396837349397591e-06,
      "loss": 0.534,
      "step": 801
    },
    {
      "epoch": 0.4831325301204819,
      "grad_norm": 0.8940737247467041,
      "learning_rate": 4.396084337349398e-06,
      "loss": 0.4624,
      "step": 802
    },
    {
      "epoch": 0.48373493975903614,
      "grad_norm": 0.8384044170379639,
      "learning_rate": 4.395331325301205e-06,
      "loss": 0.4594,
      "step": 803
    },
    {
      "epoch": 0.4843373493975904,
      "grad_norm": 0.9220396876335144,
      "learning_rate": 4.394578313253012e-06,
      "loss": 0.492,
      "step": 804
    },
    {
      "epoch": 0.48493975903614456,
      "grad_norm": 0.877571165561676,
      "learning_rate": 4.39382530120482e-06,
      "loss": 0.468,
      "step": 805
    },
    {
      "epoch": 0.4855421686746988,
      "grad_norm": 0.8074085116386414,
      "learning_rate": 4.393072289156627e-06,
      "loss": 0.5347,
      "step": 806
    },
    {
      "epoch": 0.48614457831325303,
      "grad_norm": 0.9848712086677551,
      "learning_rate": 4.3923192771084336e-06,
      "loss": 0.5242,
      "step": 807
    },
    {
      "epoch": 0.4867469879518072,
      "grad_norm": 0.8245373964309692,
      "learning_rate": 4.391566265060241e-06,
      "loss": 0.4322,
      "step": 808
    },
    {
      "epoch": 0.48734939759036144,
      "grad_norm": 0.8395320773124695,
      "learning_rate": 4.390813253012048e-06,
      "loss": 0.5317,
      "step": 809
    },
    {
      "epoch": 0.4879518072289157,
      "grad_norm": 0.9135757684707642,
      "learning_rate": 4.390060240963856e-06,
      "loss": 0.4538,
      "step": 810
    },
    {
      "epoch": 0.48855421686746986,
      "grad_norm": 0.9059491753578186,
      "learning_rate": 4.389307228915663e-06,
      "loss": 0.4714,
      "step": 811
    },
    {
      "epoch": 0.4891566265060241,
      "grad_norm": 0.8512129187583923,
      "learning_rate": 4.38855421686747e-06,
      "loss": 0.4708,
      "step": 812
    },
    {
      "epoch": 0.48975903614457833,
      "grad_norm": 1.007010817527771,
      "learning_rate": 4.387801204819278e-06,
      "loss": 0.502,
      "step": 813
    },
    {
      "epoch": 0.4903614457831325,
      "grad_norm": 0.8526752591133118,
      "learning_rate": 4.387048192771085e-06,
      "loss": 0.4346,
      "step": 814
    },
    {
      "epoch": 0.49096385542168675,
      "grad_norm": 0.8140800595283508,
      "learning_rate": 4.386295180722892e-06,
      "loss": 0.464,
      "step": 815
    },
    {
      "epoch": 0.491566265060241,
      "grad_norm": 1.0066548585891724,
      "learning_rate": 4.385542168674699e-06,
      "loss": 0.5057,
      "step": 816
    },
    {
      "epoch": 0.49216867469879516,
      "grad_norm": 0.9289240837097168,
      "learning_rate": 4.384789156626506e-06,
      "loss": 0.4885,
      "step": 817
    },
    {
      "epoch": 0.4927710843373494,
      "grad_norm": 0.8303787112236023,
      "learning_rate": 4.384036144578314e-06,
      "loss": 0.5219,
      "step": 818
    },
    {
      "epoch": 0.49337349397590363,
      "grad_norm": 0.9355288147926331,
      "learning_rate": 4.383283132530121e-06,
      "loss": 0.4875,
      "step": 819
    },
    {
      "epoch": 0.4939759036144578,
      "grad_norm": 0.8053423166275024,
      "learning_rate": 4.382530120481928e-06,
      "loss": 0.4594,
      "step": 820
    },
    {
      "epoch": 0.49457831325301205,
      "grad_norm": 0.8500730395317078,
      "learning_rate": 4.381777108433735e-06,
      "loss": 0.489,
      "step": 821
    },
    {
      "epoch": 0.4951807228915663,
      "grad_norm": 0.8741005063056946,
      "learning_rate": 4.3810240963855425e-06,
      "loss": 0.5445,
      "step": 822
    },
    {
      "epoch": 0.49578313253012046,
      "grad_norm": 0.8961488604545593,
      "learning_rate": 4.3802710843373494e-06,
      "loss": 0.4539,
      "step": 823
    },
    {
      "epoch": 0.4963855421686747,
      "grad_norm": 0.8381823897361755,
      "learning_rate": 4.379518072289156e-06,
      "loss": 0.4419,
      "step": 824
    },
    {
      "epoch": 0.49698795180722893,
      "grad_norm": 0.8166192173957825,
      "learning_rate": 4.378765060240964e-06,
      "loss": 0.4892,
      "step": 825
    },
    {
      "epoch": 0.4975903614457831,
      "grad_norm": 0.8568537831306458,
      "learning_rate": 4.378012048192771e-06,
      "loss": 0.5137,
      "step": 826
    },
    {
      "epoch": 0.49819277108433735,
      "grad_norm": 0.8609552979469299,
      "learning_rate": 4.377259036144579e-06,
      "loss": 0.4581,
      "step": 827
    },
    {
      "epoch": 0.4987951807228916,
      "grad_norm": 0.8258835077285767,
      "learning_rate": 4.376506024096386e-06,
      "loss": 0.4359,
      "step": 828
    },
    {
      "epoch": 0.49939759036144576,
      "grad_norm": 0.8820326328277588,
      "learning_rate": 4.3757530120481935e-06,
      "loss": 0.4537,
      "step": 829
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.9478237628936768,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 0.4491,
      "step": 830
    },
    {
      "epoch": 0.5006024096385542,
      "grad_norm": 0.8308257460594177,
      "learning_rate": 4.374246987951807e-06,
      "loss": 0.4293,
      "step": 831
    },
    {
      "epoch": 0.5012048192771085,
      "grad_norm": 0.8911095261573792,
      "learning_rate": 4.373493975903615e-06,
      "loss": 0.439,
      "step": 832
    },
    {
      "epoch": 0.5018072289156627,
      "grad_norm": 0.7860403656959534,
      "learning_rate": 4.372740963855422e-06,
      "loss": 0.4266,
      "step": 833
    },
    {
      "epoch": 0.5024096385542168,
      "grad_norm": 0.9084836840629578,
      "learning_rate": 4.37198795180723e-06,
      "loss": 0.4599,
      "step": 834
    },
    {
      "epoch": 0.5030120481927711,
      "grad_norm": 0.9454955458641052,
      "learning_rate": 4.371234939759037e-06,
      "loss": 0.458,
      "step": 835
    },
    {
      "epoch": 0.5036144578313253,
      "grad_norm": 0.8724521994590759,
      "learning_rate": 4.370481927710844e-06,
      "loss": 0.451,
      "step": 836
    },
    {
      "epoch": 0.5042168674698795,
      "grad_norm": 0.7946798205375671,
      "learning_rate": 4.369728915662651e-06,
      "loss": 0.4188,
      "step": 837
    },
    {
      "epoch": 0.5048192771084338,
      "grad_norm": 0.8337303996086121,
      "learning_rate": 4.368975903614458e-06,
      "loss": 0.4835,
      "step": 838
    },
    {
      "epoch": 0.505421686746988,
      "grad_norm": 0.7714263796806335,
      "learning_rate": 4.368222891566265e-06,
      "loss": 0.4424,
      "step": 839
    },
    {
      "epoch": 0.5060240963855421,
      "grad_norm": 0.7719920873641968,
      "learning_rate": 4.367469879518072e-06,
      "loss": 0.4212,
      "step": 840
    },
    {
      "epoch": 0.5066265060240964,
      "grad_norm": 0.8134554028511047,
      "learning_rate": 4.36671686746988e-06,
      "loss": 0.436,
      "step": 841
    },
    {
      "epoch": 0.5072289156626506,
      "grad_norm": 0.8611939549446106,
      "learning_rate": 4.365963855421687e-06,
      "loss": 0.4246,
      "step": 842
    },
    {
      "epoch": 0.5078313253012048,
      "grad_norm": 0.8500586748123169,
      "learning_rate": 4.365210843373494e-06,
      "loss": 0.4098,
      "step": 843
    },
    {
      "epoch": 0.5084337349397591,
      "grad_norm": 0.9387107491493225,
      "learning_rate": 4.364457831325302e-06,
      "loss": 0.4306,
      "step": 844
    },
    {
      "epoch": 0.5090361445783133,
      "grad_norm": 0.8165612816810608,
      "learning_rate": 4.3637048192771086e-06,
      "loss": 0.4512,
      "step": 845
    },
    {
      "epoch": 0.5096385542168674,
      "grad_norm": 0.8268747925758362,
      "learning_rate": 4.362951807228916e-06,
      "loss": 0.4428,
      "step": 846
    },
    {
      "epoch": 0.5102409638554217,
      "grad_norm": 0.8284243941307068,
      "learning_rate": 4.362198795180723e-06,
      "loss": 0.4489,
      "step": 847
    },
    {
      "epoch": 0.5108433734939759,
      "grad_norm": 0.819239616394043,
      "learning_rate": 4.361445783132531e-06,
      "loss": 0.4763,
      "step": 848
    },
    {
      "epoch": 0.5114457831325301,
      "grad_norm": 0.7279148697853088,
      "learning_rate": 4.360692771084338e-06,
      "loss": 0.4479,
      "step": 849
    },
    {
      "epoch": 0.5120481927710844,
      "grad_norm": 0.7785976529121399,
      "learning_rate": 4.359939759036145e-06,
      "loss": 0.432,
      "step": 850
    },
    {
      "epoch": 0.5126506024096386,
      "grad_norm": 0.8366201519966125,
      "learning_rate": 4.359186746987953e-06,
      "loss": 0.4526,
      "step": 851
    },
    {
      "epoch": 0.5132530120481927,
      "grad_norm": 0.8758049607276917,
      "learning_rate": 4.35843373493976e-06,
      "loss": 0.4321,
      "step": 852
    },
    {
      "epoch": 0.513855421686747,
      "grad_norm": 0.7876024842262268,
      "learning_rate": 4.3576807228915665e-06,
      "loss": 0.4226,
      "step": 853
    },
    {
      "epoch": 0.5144578313253012,
      "grad_norm": 0.8127527236938477,
      "learning_rate": 4.3569277108433734e-06,
      "loss": 0.4649,
      "step": 854
    },
    {
      "epoch": 0.5150602409638554,
      "grad_norm": 0.8468538522720337,
      "learning_rate": 4.356174698795181e-06,
      "loss": 0.4254,
      "step": 855
    },
    {
      "epoch": 0.5156626506024097,
      "grad_norm": 0.7570654153823853,
      "learning_rate": 4.355421686746988e-06,
      "loss": 0.4349,
      "step": 856
    },
    {
      "epoch": 0.5162650602409639,
      "grad_norm": 0.7648956775665283,
      "learning_rate": 4.354668674698795e-06,
      "loss": 0.3949,
      "step": 857
    },
    {
      "epoch": 0.516867469879518,
      "grad_norm": 0.8621246218681335,
      "learning_rate": 4.353915662650603e-06,
      "loss": 0.4488,
      "step": 858
    },
    {
      "epoch": 0.5174698795180723,
      "grad_norm": 0.9242040514945984,
      "learning_rate": 4.35316265060241e-06,
      "loss": 0.4539,
      "step": 859
    },
    {
      "epoch": 0.5180722891566265,
      "grad_norm": 0.8289987444877625,
      "learning_rate": 4.3524096385542175e-06,
      "loss": 0.4111,
      "step": 860
    },
    {
      "epoch": 0.5186746987951807,
      "grad_norm": 0.8043241500854492,
      "learning_rate": 4.3516566265060245e-06,
      "loss": 0.402,
      "step": 861
    },
    {
      "epoch": 0.519277108433735,
      "grad_norm": 0.8070041537284851,
      "learning_rate": 4.350903614457831e-06,
      "loss": 0.4106,
      "step": 862
    },
    {
      "epoch": 0.5198795180722892,
      "grad_norm": 0.8589888215065002,
      "learning_rate": 4.350150602409639e-06,
      "loss": 0.4195,
      "step": 863
    },
    {
      "epoch": 0.5204819277108433,
      "grad_norm": 0.8025203347206116,
      "learning_rate": 4.349397590361446e-06,
      "loss": 0.4575,
      "step": 864
    },
    {
      "epoch": 0.5210843373493976,
      "grad_norm": 0.8255767226219177,
      "learning_rate": 4.348644578313254e-06,
      "loss": 0.4026,
      "step": 865
    },
    {
      "epoch": 0.5216867469879518,
      "grad_norm": 0.7913536429405212,
      "learning_rate": 4.347891566265061e-06,
      "loss": 0.423,
      "step": 866
    },
    {
      "epoch": 0.522289156626506,
      "grad_norm": 0.8950340747833252,
      "learning_rate": 4.347138554216868e-06,
      "loss": 0.4959,
      "step": 867
    },
    {
      "epoch": 0.5228915662650603,
      "grad_norm": 0.7893226742744446,
      "learning_rate": 4.3463855421686755e-06,
      "loss": 0.4419,
      "step": 868
    },
    {
      "epoch": 0.5234939759036145,
      "grad_norm": 0.7645740509033203,
      "learning_rate": 4.345632530120482e-06,
      "loss": 0.4853,
      "step": 869
    },
    {
      "epoch": 0.5240963855421686,
      "grad_norm": 0.8077744841575623,
      "learning_rate": 4.344879518072289e-06,
      "loss": 0.3858,
      "step": 870
    },
    {
      "epoch": 0.5246987951807229,
      "grad_norm": 0.889937698841095,
      "learning_rate": 4.344126506024097e-06,
      "loss": 0.4389,
      "step": 871
    },
    {
      "epoch": 0.5253012048192771,
      "grad_norm": 0.854813277721405,
      "learning_rate": 4.343373493975904e-06,
      "loss": 0.4476,
      "step": 872
    },
    {
      "epoch": 0.5259036144578313,
      "grad_norm": 0.8685604333877563,
      "learning_rate": 4.342620481927711e-06,
      "loss": 0.4235,
      "step": 873
    },
    {
      "epoch": 0.5265060240963856,
      "grad_norm": 0.8481271862983704,
      "learning_rate": 4.341867469879518e-06,
      "loss": 0.4086,
      "step": 874
    },
    {
      "epoch": 0.5271084337349398,
      "grad_norm": 0.7809633016586304,
      "learning_rate": 4.341114457831326e-06,
      "loss": 0.4718,
      "step": 875
    },
    {
      "epoch": 0.5277108433734939,
      "grad_norm": 0.7809087634086609,
      "learning_rate": 4.3403614457831326e-06,
      "loss": 0.4478,
      "step": 876
    },
    {
      "epoch": 0.5283132530120482,
      "grad_norm": 0.7725089192390442,
      "learning_rate": 4.33960843373494e-06,
      "loss": 0.3602,
      "step": 877
    },
    {
      "epoch": 0.5289156626506024,
      "grad_norm": 0.7976964116096497,
      "learning_rate": 4.338855421686747e-06,
      "loss": 0.4,
      "step": 878
    },
    {
      "epoch": 0.5295180722891566,
      "grad_norm": 0.7837836742401123,
      "learning_rate": 4.338102409638554e-06,
      "loss": 0.4511,
      "step": 879
    },
    {
      "epoch": 0.5301204819277109,
      "grad_norm": 0.8589077591896057,
      "learning_rate": 4.337349397590362e-06,
      "loss": 0.4395,
      "step": 880
    },
    {
      "epoch": 0.5307228915662651,
      "grad_norm": 0.7532952427864075,
      "learning_rate": 4.336596385542169e-06,
      "loss": 0.4354,
      "step": 881
    },
    {
      "epoch": 0.5313253012048192,
      "grad_norm": 0.7978301048278809,
      "learning_rate": 4.335843373493977e-06,
      "loss": 0.4127,
      "step": 882
    },
    {
      "epoch": 0.5319277108433735,
      "grad_norm": 0.791192889213562,
      "learning_rate": 4.3350903614457836e-06,
      "loss": 0.4687,
      "step": 883
    },
    {
      "epoch": 0.5325301204819277,
      "grad_norm": 0.7697460055351257,
      "learning_rate": 4.334337349397591e-06,
      "loss": 0.4466,
      "step": 884
    },
    {
      "epoch": 0.5331325301204819,
      "grad_norm": 0.747159481048584,
      "learning_rate": 4.333584337349398e-06,
      "loss": 0.3768,
      "step": 885
    },
    {
      "epoch": 0.5337349397590362,
      "grad_norm": 0.8109819293022156,
      "learning_rate": 4.332831325301205e-06,
      "loss": 0.4189,
      "step": 886
    },
    {
      "epoch": 0.5343373493975904,
      "grad_norm": 0.801983654499054,
      "learning_rate": 4.332078313253012e-06,
      "loss": 0.3661,
      "step": 887
    },
    {
      "epoch": 0.5349397590361445,
      "grad_norm": 0.7872856259346008,
      "learning_rate": 4.33132530120482e-06,
      "loss": 0.4264,
      "step": 888
    },
    {
      "epoch": 0.5355421686746988,
      "grad_norm": 0.7103592157363892,
      "learning_rate": 4.330572289156627e-06,
      "loss": 0.4024,
      "step": 889
    },
    {
      "epoch": 0.536144578313253,
      "grad_norm": 0.9163087606430054,
      "learning_rate": 4.329819277108434e-06,
      "loss": 0.4329,
      "step": 890
    },
    {
      "epoch": 0.5367469879518072,
      "grad_norm": 0.8233628273010254,
      "learning_rate": 4.329066265060241e-06,
      "loss": 0.4438,
      "step": 891
    },
    {
      "epoch": 0.5373493975903615,
      "grad_norm": 0.8702658414840698,
      "learning_rate": 4.3283132530120484e-06,
      "loss": 0.3972,
      "step": 892
    },
    {
      "epoch": 0.5379518072289157,
      "grad_norm": 0.7793692350387573,
      "learning_rate": 4.327560240963855e-06,
      "loss": 0.4229,
      "step": 893
    },
    {
      "epoch": 0.5385542168674698,
      "grad_norm": 0.898735761642456,
      "learning_rate": 4.326807228915663e-06,
      "loss": 0.4242,
      "step": 894
    },
    {
      "epoch": 0.5391566265060241,
      "grad_norm": 0.7567571997642517,
      "learning_rate": 4.32605421686747e-06,
      "loss": 0.4399,
      "step": 895
    },
    {
      "epoch": 0.5397590361445783,
      "grad_norm": 0.728022575378418,
      "learning_rate": 4.325301204819278e-06,
      "loss": 0.4058,
      "step": 896
    },
    {
      "epoch": 0.5403614457831325,
      "grad_norm": 0.7490084171295166,
      "learning_rate": 4.324548192771085e-06,
      "loss": 0.3913,
      "step": 897
    },
    {
      "epoch": 0.5409638554216868,
      "grad_norm": 0.756706714630127,
      "learning_rate": 4.323795180722892e-06,
      "loss": 0.4104,
      "step": 898
    },
    {
      "epoch": 0.541566265060241,
      "grad_norm": 0.7040330767631531,
      "learning_rate": 4.3230421686746995e-06,
      "loss": 0.427,
      "step": 899
    },
    {
      "epoch": 0.5421686746987951,
      "grad_norm": 0.7740331888198853,
      "learning_rate": 4.322289156626506e-06,
      "loss": 0.4522,
      "step": 900
    },
    {
      "epoch": 0.5427710843373494,
      "grad_norm": 0.77347731590271,
      "learning_rate": 4.321536144578314e-06,
      "loss": 0.4177,
      "step": 901
    },
    {
      "epoch": 0.5433734939759036,
      "grad_norm": 0.7620518803596497,
      "learning_rate": 4.320783132530121e-06,
      "loss": 0.3814,
      "step": 902
    },
    {
      "epoch": 0.5439759036144578,
      "grad_norm": 0.8443256616592407,
      "learning_rate": 4.320030120481928e-06,
      "loss": 0.4178,
      "step": 903
    },
    {
      "epoch": 0.5445783132530121,
      "grad_norm": 0.7521129250526428,
      "learning_rate": 4.319277108433736e-06,
      "loss": 0.4463,
      "step": 904
    },
    {
      "epoch": 0.5451807228915663,
      "grad_norm": 0.7458223700523376,
      "learning_rate": 4.318524096385543e-06,
      "loss": 0.4256,
      "step": 905
    },
    {
      "epoch": 0.5457831325301205,
      "grad_norm": 0.7762418985366821,
      "learning_rate": 4.31777108433735e-06,
      "loss": 0.4391,
      "step": 906
    },
    {
      "epoch": 0.5463855421686747,
      "grad_norm": 0.7956472039222717,
      "learning_rate": 4.3170180722891565e-06,
      "loss": 0.4249,
      "step": 907
    },
    {
      "epoch": 0.5469879518072289,
      "grad_norm": 0.7703011631965637,
      "learning_rate": 4.316265060240964e-06,
      "loss": 0.366,
      "step": 908
    },
    {
      "epoch": 0.5475903614457831,
      "grad_norm": 2.76875901222229,
      "learning_rate": 4.315512048192771e-06,
      "loss": 0.5203,
      "step": 909
    },
    {
      "epoch": 0.5481927710843374,
      "grad_norm": 0.7585433721542358,
      "learning_rate": 4.314759036144578e-06,
      "loss": 0.3967,
      "step": 910
    },
    {
      "epoch": 0.5487951807228916,
      "grad_norm": 0.7242310047149658,
      "learning_rate": 4.314006024096386e-06,
      "loss": 0.3916,
      "step": 911
    },
    {
      "epoch": 0.5493975903614458,
      "grad_norm": 0.6951138377189636,
      "learning_rate": 4.313253012048193e-06,
      "loss": 0.405,
      "step": 912
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.7247406244277954,
      "learning_rate": 4.312500000000001e-06,
      "loss": 0.4109,
      "step": 913
    },
    {
      "epoch": 0.5506024096385542,
      "grad_norm": 1.5602473020553589,
      "learning_rate": 4.3117469879518076e-06,
      "loss": 0.4983,
      "step": 914
    },
    {
      "epoch": 0.5512048192771084,
      "grad_norm": 0.7763420343399048,
      "learning_rate": 4.3109939759036145e-06,
      "loss": 0.4389,
      "step": 915
    },
    {
      "epoch": 0.5518072289156627,
      "grad_norm": 0.8975186347961426,
      "learning_rate": 4.310240963855422e-06,
      "loss": 0.3928,
      "step": 916
    },
    {
      "epoch": 0.5524096385542169,
      "grad_norm": 0.7616184949874878,
      "learning_rate": 4.309487951807229e-06,
      "loss": 0.4923,
      "step": 917
    },
    {
      "epoch": 0.553012048192771,
      "grad_norm": 0.7527912259101868,
      "learning_rate": 4.308734939759037e-06,
      "loss": 0.386,
      "step": 918
    },
    {
      "epoch": 0.5536144578313253,
      "grad_norm": 0.7431317567825317,
      "learning_rate": 4.307981927710844e-06,
      "loss": 0.3909,
      "step": 919
    },
    {
      "epoch": 0.5542168674698795,
      "grad_norm": 0.8612467646598816,
      "learning_rate": 4.307228915662651e-06,
      "loss": 0.4059,
      "step": 920
    },
    {
      "epoch": 0.5548192771084337,
      "grad_norm": 0.6779335737228394,
      "learning_rate": 4.306475903614459e-06,
      "loss": 0.3985,
      "step": 921
    },
    {
      "epoch": 0.555421686746988,
      "grad_norm": 0.7587804794311523,
      "learning_rate": 4.3057228915662655e-06,
      "loss": 0.4342,
      "step": 922
    },
    {
      "epoch": 0.5560240963855422,
      "grad_norm": 0.7712708115577698,
      "learning_rate": 4.3049698795180724e-06,
      "loss": 0.3735,
      "step": 923
    },
    {
      "epoch": 0.5566265060240964,
      "grad_norm": 0.7755638360977173,
      "learning_rate": 4.304216867469879e-06,
      "loss": 0.389,
      "step": 924
    },
    {
      "epoch": 0.5572289156626506,
      "grad_norm": 0.725884199142456,
      "learning_rate": 4.303463855421687e-06,
      "loss": 0.3927,
      "step": 925
    },
    {
      "epoch": 0.5578313253012048,
      "grad_norm": 0.7515182495117188,
      "learning_rate": 4.302710843373494e-06,
      "loss": 0.3719,
      "step": 926
    },
    {
      "epoch": 0.558433734939759,
      "grad_norm": 0.7528185248374939,
      "learning_rate": 4.301957831325301e-06,
      "loss": 0.3635,
      "step": 927
    },
    {
      "epoch": 0.5590361445783133,
      "grad_norm": 0.7803112864494324,
      "learning_rate": 4.301204819277109e-06,
      "loss": 0.3991,
      "step": 928
    },
    {
      "epoch": 0.5596385542168675,
      "grad_norm": 0.757688581943512,
      "learning_rate": 4.300451807228916e-06,
      "loss": 0.3801,
      "step": 929
    },
    {
      "epoch": 0.5602409638554217,
      "grad_norm": 0.7228885293006897,
      "learning_rate": 4.2996987951807234e-06,
      "loss": 0.3589,
      "step": 930
    },
    {
      "epoch": 0.560843373493976,
      "grad_norm": 0.7398354411125183,
      "learning_rate": 4.29894578313253e-06,
      "loss": 0.3457,
      "step": 931
    },
    {
      "epoch": 0.5614457831325301,
      "grad_norm": 0.7415940165519714,
      "learning_rate": 4.298192771084338e-06,
      "loss": 0.389,
      "step": 932
    },
    {
      "epoch": 0.5620481927710843,
      "grad_norm": 0.67811518907547,
      "learning_rate": 4.297439759036145e-06,
      "loss": 0.3889,
      "step": 933
    },
    {
      "epoch": 0.5626506024096386,
      "grad_norm": 0.7798686623573303,
      "learning_rate": 4.296686746987952e-06,
      "loss": 0.3597,
      "step": 934
    },
    {
      "epoch": 0.5632530120481928,
      "grad_norm": 0.7076371908187866,
      "learning_rate": 4.29593373493976e-06,
      "loss": 0.3569,
      "step": 935
    },
    {
      "epoch": 0.563855421686747,
      "grad_norm": 0.7034572958946228,
      "learning_rate": 4.295180722891567e-06,
      "loss": 0.3328,
      "step": 936
    },
    {
      "epoch": 0.5644578313253013,
      "grad_norm": 0.8550699353218079,
      "learning_rate": 4.2944277108433745e-06,
      "loss": 0.3542,
      "step": 937
    },
    {
      "epoch": 0.5650602409638554,
      "grad_norm": 0.7882698774337769,
      "learning_rate": 4.293674698795181e-06,
      "loss": 0.3566,
      "step": 938
    },
    {
      "epoch": 0.5656626506024096,
      "grad_norm": 0.763667643070221,
      "learning_rate": 4.292921686746988e-06,
      "loss": 0.3586,
      "step": 939
    },
    {
      "epoch": 0.5662650602409639,
      "grad_norm": 0.7616035342216492,
      "learning_rate": 4.292168674698795e-06,
      "loss": 0.4185,
      "step": 940
    },
    {
      "epoch": 0.5668674698795181,
      "grad_norm": 0.6692790985107422,
      "learning_rate": 4.291415662650603e-06,
      "loss": 0.3948,
      "step": 941
    },
    {
      "epoch": 0.5674698795180723,
      "grad_norm": 1.2474620342254639,
      "learning_rate": 4.29066265060241e-06,
      "loss": 0.381,
      "step": 942
    },
    {
      "epoch": 0.5680722891566266,
      "grad_norm": 0.7150376439094543,
      "learning_rate": 4.289909638554217e-06,
      "loss": 0.3878,
      "step": 943
    },
    {
      "epoch": 0.5686746987951807,
      "grad_norm": 0.7788494825363159,
      "learning_rate": 4.289156626506025e-06,
      "loss": 0.3953,
      "step": 944
    },
    {
      "epoch": 0.5692771084337349,
      "grad_norm": 0.7402669191360474,
      "learning_rate": 4.2884036144578316e-06,
      "loss": 0.3589,
      "step": 945
    },
    {
      "epoch": 0.5698795180722892,
      "grad_norm": 0.6882504224777222,
      "learning_rate": 4.2876506024096385e-06,
      "loss": 0.4167,
      "step": 946
    },
    {
      "epoch": 0.5704819277108434,
      "grad_norm": 0.743391215801239,
      "learning_rate": 4.286897590361446e-06,
      "loss": 0.4167,
      "step": 947
    },
    {
      "epoch": 0.5710843373493976,
      "grad_norm": 0.7819777131080627,
      "learning_rate": 4.286144578313253e-06,
      "loss": 0.3491,
      "step": 948
    },
    {
      "epoch": 0.5716867469879519,
      "grad_norm": 0.7761412262916565,
      "learning_rate": 4.285391566265061e-06,
      "loss": 0.4384,
      "step": 949
    },
    {
      "epoch": 0.572289156626506,
      "grad_norm": 0.713735818862915,
      "learning_rate": 4.284638554216868e-06,
      "loss": 0.3701,
      "step": 950
    },
    {
      "epoch": 0.5728915662650602,
      "grad_norm": 0.6958031058311462,
      "learning_rate": 4.283885542168675e-06,
      "loss": 0.3777,
      "step": 951
    },
    {
      "epoch": 0.5734939759036145,
      "grad_norm": 0.8020367622375488,
      "learning_rate": 4.2831325301204826e-06,
      "loss": 0.3512,
      "step": 952
    },
    {
      "epoch": 0.5740963855421687,
      "grad_norm": 0.7627357840538025,
      "learning_rate": 4.2823795180722895e-06,
      "loss": 0.3557,
      "step": 953
    },
    {
      "epoch": 0.5746987951807229,
      "grad_norm": 0.7070096135139465,
      "learning_rate": 4.281626506024097e-06,
      "loss": 0.3931,
      "step": 954
    },
    {
      "epoch": 0.5753012048192772,
      "grad_norm": 0.7142391204833984,
      "learning_rate": 4.280873493975904e-06,
      "loss": 0.3697,
      "step": 955
    },
    {
      "epoch": 0.5759036144578313,
      "grad_norm": 0.6912004947662354,
      "learning_rate": 4.280120481927711e-06,
      "loss": 0.3334,
      "step": 956
    },
    {
      "epoch": 0.5765060240963855,
      "grad_norm": 0.7116250991821289,
      "learning_rate": 4.279367469879518e-06,
      "loss": 0.3408,
      "step": 957
    },
    {
      "epoch": 0.5771084337349398,
      "grad_norm": 0.7619776725769043,
      "learning_rate": 4.278614457831326e-06,
      "loss": 0.3914,
      "step": 958
    },
    {
      "epoch": 0.577710843373494,
      "grad_norm": 0.6691693663597107,
      "learning_rate": 4.277861445783133e-06,
      "loss": 0.338,
      "step": 959
    },
    {
      "epoch": 0.5783132530120482,
      "grad_norm": 0.6661312580108643,
      "learning_rate": 4.27710843373494e-06,
      "loss": 0.3869,
      "step": 960
    },
    {
      "epoch": 0.5789156626506025,
      "grad_norm": 0.8463139533996582,
      "learning_rate": 4.2763554216867474e-06,
      "loss": 0.3804,
      "step": 961
    },
    {
      "epoch": 0.5795180722891566,
      "grad_norm": 0.723332405090332,
      "learning_rate": 4.275602409638554e-06,
      "loss": 0.3958,
      "step": 962
    },
    {
      "epoch": 0.5801204819277108,
      "grad_norm": 0.7827841639518738,
      "learning_rate": 4.274849397590361e-06,
      "loss": 0.3641,
      "step": 963
    },
    {
      "epoch": 0.5807228915662651,
      "grad_norm": 0.7277905941009521,
      "learning_rate": 4.274096385542169e-06,
      "loss": 0.379,
      "step": 964
    },
    {
      "epoch": 0.5813253012048193,
      "grad_norm": 0.6435388326644897,
      "learning_rate": 4.273343373493976e-06,
      "loss": 0.3635,
      "step": 965
    },
    {
      "epoch": 0.5819277108433735,
      "grad_norm": 0.7749106287956238,
      "learning_rate": 4.272590361445784e-06,
      "loss": 0.3712,
      "step": 966
    },
    {
      "epoch": 0.5825301204819278,
      "grad_norm": 0.6583210825920105,
      "learning_rate": 4.271837349397591e-06,
      "loss": 0.3929,
      "step": 967
    },
    {
      "epoch": 0.5831325301204819,
      "grad_norm": 0.7435518503189087,
      "learning_rate": 4.2710843373493984e-06,
      "loss": 0.348,
      "step": 968
    },
    {
      "epoch": 0.5837349397590361,
      "grad_norm": 0.7098272442817688,
      "learning_rate": 4.270331325301205e-06,
      "loss": 0.4122,
      "step": 969
    },
    {
      "epoch": 0.5843373493975904,
      "grad_norm": 0.6944602727890015,
      "learning_rate": 4.269578313253012e-06,
      "loss": 0.3763,
      "step": 970
    },
    {
      "epoch": 0.5849397590361446,
      "grad_norm": 0.6551894545555115,
      "learning_rate": 4.26882530120482e-06,
      "loss": 0.3786,
      "step": 971
    },
    {
      "epoch": 0.5855421686746988,
      "grad_norm": 0.7569825053215027,
      "learning_rate": 4.268072289156627e-06,
      "loss": 0.4192,
      "step": 972
    },
    {
      "epoch": 0.5861445783132531,
      "grad_norm": 0.6757923364639282,
      "learning_rate": 4.267319277108434e-06,
      "loss": 0.3641,
      "step": 973
    },
    {
      "epoch": 0.5867469879518072,
      "grad_norm": 1.0137137174606323,
      "learning_rate": 4.266566265060242e-06,
      "loss": 0.3892,
      "step": 974
    },
    {
      "epoch": 0.5873493975903614,
      "grad_norm": 0.7723792195320129,
      "learning_rate": 4.265813253012049e-06,
      "loss": 0.401,
      "step": 975
    },
    {
      "epoch": 0.5879518072289157,
      "grad_norm": 0.7240537405014038,
      "learning_rate": 4.2650602409638555e-06,
      "loss": 0.4193,
      "step": 976
    },
    {
      "epoch": 0.5885542168674699,
      "grad_norm": 0.7204244136810303,
      "learning_rate": 4.2643072289156625e-06,
      "loss": 0.3705,
      "step": 977
    },
    {
      "epoch": 0.5891566265060241,
      "grad_norm": 0.7468684911727905,
      "learning_rate": 4.26355421686747e-06,
      "loss": 0.3856,
      "step": 978
    },
    {
      "epoch": 0.5897590361445784,
      "grad_norm": 0.6780726909637451,
      "learning_rate": 4.262801204819277e-06,
      "loss": 0.3313,
      "step": 979
    },
    {
      "epoch": 0.5903614457831325,
      "grad_norm": 0.6781066656112671,
      "learning_rate": 4.262048192771085e-06,
      "loss": 0.3791,
      "step": 980
    },
    {
      "epoch": 0.5909638554216867,
      "grad_norm": 0.7417112588882446,
      "learning_rate": 4.261295180722892e-06,
      "loss": 0.3991,
      "step": 981
    },
    {
      "epoch": 0.591566265060241,
      "grad_norm": 0.7315050959587097,
      "learning_rate": 4.260542168674699e-06,
      "loss": 0.3673,
      "step": 982
    },
    {
      "epoch": 0.5921686746987952,
      "grad_norm": 0.7675021886825562,
      "learning_rate": 4.2597891566265066e-06,
      "loss": 0.3505,
      "step": 983
    },
    {
      "epoch": 0.5927710843373494,
      "grad_norm": 0.6921820044517517,
      "learning_rate": 4.2590361445783135e-06,
      "loss": 0.3889,
      "step": 984
    },
    {
      "epoch": 0.5933734939759037,
      "grad_norm": 0.6872013211250305,
      "learning_rate": 4.258283132530121e-06,
      "loss": 0.3612,
      "step": 985
    },
    {
      "epoch": 0.5939759036144578,
      "grad_norm": 0.7211378812789917,
      "learning_rate": 4.257530120481928e-06,
      "loss": 0.3799,
      "step": 986
    },
    {
      "epoch": 0.594578313253012,
      "grad_norm": 0.7020621299743652,
      "learning_rate": 4.256777108433735e-06,
      "loss": 0.3768,
      "step": 987
    },
    {
      "epoch": 0.5951807228915663,
      "grad_norm": 0.7114576101303101,
      "learning_rate": 4.256024096385543e-06,
      "loss": 0.3643,
      "step": 988
    },
    {
      "epoch": 0.5957831325301205,
      "grad_norm": 0.6991768479347229,
      "learning_rate": 4.25527108433735e-06,
      "loss": 0.3706,
      "step": 989
    },
    {
      "epoch": 0.5963855421686747,
      "grad_norm": 0.6980689764022827,
      "learning_rate": 4.254518072289157e-06,
      "loss": 0.3595,
      "step": 990
    },
    {
      "epoch": 0.596987951807229,
      "grad_norm": 0.6639752984046936,
      "learning_rate": 4.2537650602409645e-06,
      "loss": 0.3735,
      "step": 991
    },
    {
      "epoch": 0.5975903614457831,
      "grad_norm": 0.6763629913330078,
      "learning_rate": 4.253012048192771e-06,
      "loss": 0.3475,
      "step": 992
    },
    {
      "epoch": 0.5981927710843373,
      "grad_norm": 0.6860172748565674,
      "learning_rate": 4.252259036144578e-06,
      "loss": 0.4058,
      "step": 993
    },
    {
      "epoch": 0.5987951807228916,
      "grad_norm": 0.6260606646537781,
      "learning_rate": 4.251506024096385e-06,
      "loss": 0.3593,
      "step": 994
    },
    {
      "epoch": 0.5993975903614458,
      "grad_norm": 0.6889044642448425,
      "learning_rate": 4.250753012048193e-06,
      "loss": 0.3526,
      "step": 995
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.7136476039886475,
      "learning_rate": 4.25e-06,
      "loss": 0.3472,
      "step": 996
    },
    {
      "epoch": 0.6006024096385543,
      "grad_norm": 0.7861083745956421,
      "learning_rate": 4.249246987951808e-06,
      "loss": 0.3958,
      "step": 997
    },
    {
      "epoch": 0.6012048192771084,
      "grad_norm": 0.687587559223175,
      "learning_rate": 4.248493975903615e-06,
      "loss": 0.3779,
      "step": 998
    },
    {
      "epoch": 0.6018072289156626,
      "grad_norm": 0.7171517014503479,
      "learning_rate": 4.247740963855422e-06,
      "loss": 0.3611,
      "step": 999
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 0.688416063785553,
      "learning_rate": 4.246987951807229e-06,
      "loss": 0.3297,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 6640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.97767617888256e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
