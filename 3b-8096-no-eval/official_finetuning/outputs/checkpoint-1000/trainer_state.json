{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.5174506828528074,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015174506828528073,
      "grad_norm": 2.63923716545105,
      "learning_rate": 4.998103186646434e-06,
      "loss": 2.484,
      "step": 1
    },
    {
      "epoch": 0.0030349013657056147,
      "grad_norm": 1.6273452043533325,
      "learning_rate": 4.996206373292868e-06,
      "loss": 2.3948,
      "step": 2
    },
    {
      "epoch": 0.004552352048558422,
      "grad_norm": 1.7469837665557861,
      "learning_rate": 4.994309559939302e-06,
      "loss": 2.614,
      "step": 3
    },
    {
      "epoch": 0.006069802731411229,
      "grad_norm": 2.0129024982452393,
      "learning_rate": 4.992412746585736e-06,
      "loss": 2.9288,
      "step": 4
    },
    {
      "epoch": 0.007587253414264037,
      "grad_norm": 1.8048086166381836,
      "learning_rate": 4.99051593323217e-06,
      "loss": 2.6545,
      "step": 5
    },
    {
      "epoch": 0.009104704097116844,
      "grad_norm": 1.8853697776794434,
      "learning_rate": 4.988619119878604e-06,
      "loss": 2.7474,
      "step": 6
    },
    {
      "epoch": 0.010622154779969651,
      "grad_norm": 3.2002928256988525,
      "learning_rate": 4.986722306525038e-06,
      "loss": 2.6091,
      "step": 7
    },
    {
      "epoch": 0.012139605462822459,
      "grad_norm": 1.922617793083191,
      "learning_rate": 4.984825493171473e-06,
      "loss": 2.8413,
      "step": 8
    },
    {
      "epoch": 0.013657056145675266,
      "grad_norm": 1.8113216161727905,
      "learning_rate": 4.982928679817906e-06,
      "loss": 2.667,
      "step": 9
    },
    {
      "epoch": 0.015174506828528073,
      "grad_norm": 1.8500645160675049,
      "learning_rate": 4.9810318664643405e-06,
      "loss": 2.6608,
      "step": 10
    },
    {
      "epoch": 0.01669195751138088,
      "grad_norm": 1.6712608337402344,
      "learning_rate": 4.9791350531107744e-06,
      "loss": 2.5167,
      "step": 11
    },
    {
      "epoch": 0.018209408194233688,
      "grad_norm": 1.9611780643463135,
      "learning_rate": 4.977238239757208e-06,
      "loss": 2.8051,
      "step": 12
    },
    {
      "epoch": 0.019726858877086494,
      "grad_norm": 2.0223429203033447,
      "learning_rate": 4.975341426403642e-06,
      "loss": 2.7847,
      "step": 13
    },
    {
      "epoch": 0.021244309559939303,
      "grad_norm": 1.771559715270996,
      "learning_rate": 4.973444613050076e-06,
      "loss": 2.5207,
      "step": 14
    },
    {
      "epoch": 0.02276176024279211,
      "grad_norm": 1.8467152118682861,
      "learning_rate": 4.97154779969651e-06,
      "loss": 2.5761,
      "step": 15
    },
    {
      "epoch": 0.024279210925644917,
      "grad_norm": 1.9992544651031494,
      "learning_rate": 4.969650986342944e-06,
      "loss": 2.7381,
      "step": 16
    },
    {
      "epoch": 0.025796661608497723,
      "grad_norm": 2.0231010913848877,
      "learning_rate": 4.967754172989378e-06,
      "loss": 2.8551,
      "step": 17
    },
    {
      "epoch": 0.027314112291350532,
      "grad_norm": NaN,
      "learning_rate": 4.967754172989378e-06,
      "loss": 2.8648,
      "step": 18
    },
    {
      "epoch": 0.028831562974203338,
      "grad_norm": 1.6868470907211304,
      "learning_rate": 4.965857359635812e-06,
      "loss": 2.4799,
      "step": 19
    },
    {
      "epoch": 0.030349013657056147,
      "grad_norm": 2.122006416320801,
      "learning_rate": 4.963960546282246e-06,
      "loss": 2.9161,
      "step": 20
    },
    {
      "epoch": 0.03186646433990895,
      "grad_norm": 1.9273914098739624,
      "learning_rate": 4.962063732928681e-06,
      "loss": 2.6345,
      "step": 21
    },
    {
      "epoch": 0.03338391502276176,
      "grad_norm": 2.1196916103363037,
      "learning_rate": 4.960166919575114e-06,
      "loss": 2.889,
      "step": 22
    },
    {
      "epoch": 0.03490136570561457,
      "grad_norm": 1.9797147512435913,
      "learning_rate": 4.9582701062215485e-06,
      "loss": 2.7373,
      "step": 23
    },
    {
      "epoch": 0.036418816388467376,
      "grad_norm": 1.7539207935333252,
      "learning_rate": 4.956373292867982e-06,
      "loss": 2.3926,
      "step": 24
    },
    {
      "epoch": 0.03793626707132018,
      "grad_norm": 2.027322769165039,
      "learning_rate": 4.954476479514416e-06,
      "loss": 2.8521,
      "step": 25
    },
    {
      "epoch": 0.03945371775417299,
      "grad_norm": 2.0555593967437744,
      "learning_rate": 4.95257966616085e-06,
      "loss": 2.7984,
      "step": 26
    },
    {
      "epoch": 0.0409711684370258,
      "grad_norm": 1.9005848169326782,
      "learning_rate": 4.950682852807284e-06,
      "loss": 2.5891,
      "step": 27
    },
    {
      "epoch": 0.042488619119878605,
      "grad_norm": 1.8821803331375122,
      "learning_rate": 4.948786039453718e-06,
      "loss": 2.6322,
      "step": 28
    },
    {
      "epoch": 0.04400606980273141,
      "grad_norm": 1.9137921333312988,
      "learning_rate": 4.946889226100152e-06,
      "loss": 2.669,
      "step": 29
    },
    {
      "epoch": 0.04552352048558422,
      "grad_norm": 1.8307174444198608,
      "learning_rate": 4.944992412746586e-06,
      "loss": 2.5301,
      "step": 30
    },
    {
      "epoch": 0.04704097116843703,
      "grad_norm": 1.8602086305618286,
      "learning_rate": 4.94309559939302e-06,
      "loss": 2.5499,
      "step": 31
    },
    {
      "epoch": 0.048558421851289835,
      "grad_norm": 2.136868715286255,
      "learning_rate": 4.941198786039454e-06,
      "loss": 2.837,
      "step": 32
    },
    {
      "epoch": 0.05007587253414264,
      "grad_norm": 1.8418922424316406,
      "learning_rate": 4.9393019726858886e-06,
      "loss": 2.4038,
      "step": 33
    },
    {
      "epoch": 0.051593323216995446,
      "grad_norm": 2.2106001377105713,
      "learning_rate": 4.937405159332322e-06,
      "loss": 2.8256,
      "step": 34
    },
    {
      "epoch": 0.05311077389984825,
      "grad_norm": 2.163856029510498,
      "learning_rate": 4.935508345978756e-06,
      "loss": 2.7883,
      "step": 35
    },
    {
      "epoch": 0.054628224582701064,
      "grad_norm": 2.509023666381836,
      "learning_rate": 4.9336115326251895e-06,
      "loss": 2.6859,
      "step": 36
    },
    {
      "epoch": 0.05614567526555387,
      "grad_norm": 1.8113304376602173,
      "learning_rate": 4.931714719271624e-06,
      "loss": 2.4338,
      "step": 37
    },
    {
      "epoch": 0.057663125948406675,
      "grad_norm": 1.8090928792953491,
      "learning_rate": 4.929817905918058e-06,
      "loss": 2.3921,
      "step": 38
    },
    {
      "epoch": 0.05918057663125948,
      "grad_norm": 2.1836321353912354,
      "learning_rate": 4.927921092564492e-06,
      "loss": 2.729,
      "step": 39
    },
    {
      "epoch": 0.06069802731411229,
      "grad_norm": 1.8560848236083984,
      "learning_rate": 4.926024279210926e-06,
      "loss": 2.3526,
      "step": 40
    },
    {
      "epoch": 0.0622154779969651,
      "grad_norm": 2.0140435695648193,
      "learning_rate": 4.92412746585736e-06,
      "loss": 2.585,
      "step": 41
    },
    {
      "epoch": 0.0637329286798179,
      "grad_norm": 1.994552493095398,
      "learning_rate": 4.922230652503794e-06,
      "loss": 2.5889,
      "step": 42
    },
    {
      "epoch": 0.06525037936267071,
      "grad_norm": 2.128871202468872,
      "learning_rate": 4.920333839150228e-06,
      "loss": 2.66,
      "step": 43
    },
    {
      "epoch": 0.06676783004552352,
      "grad_norm": 2.198652744293213,
      "learning_rate": 4.918437025796662e-06,
      "loss": 2.6668,
      "step": 44
    },
    {
      "epoch": 0.06828528072837632,
      "grad_norm": 2.0352489948272705,
      "learning_rate": 4.916540212443096e-06,
      "loss": 2.458,
      "step": 45
    },
    {
      "epoch": 0.06980273141122914,
      "grad_norm": 2.0870494842529297,
      "learning_rate": 4.91464339908953e-06,
      "loss": 2.573,
      "step": 46
    },
    {
      "epoch": 0.07132018209408195,
      "grad_norm": 1.9691811800003052,
      "learning_rate": 4.912746585735964e-06,
      "loss": 2.4957,
      "step": 47
    },
    {
      "epoch": 0.07283763277693475,
      "grad_norm": 2.1792538166046143,
      "learning_rate": 4.9108497723823974e-06,
      "loss": 2.6554,
      "step": 48
    },
    {
      "epoch": 0.07435508345978756,
      "grad_norm": 2.055633306503296,
      "learning_rate": 4.908952959028832e-06,
      "loss": 2.647,
      "step": 49
    },
    {
      "epoch": 0.07587253414264036,
      "grad_norm": 2.07570743560791,
      "learning_rate": 4.907056145675266e-06,
      "loss": 2.5963,
      "step": 50
    },
    {
      "epoch": 0.07738998482549317,
      "grad_norm": 2.084028720855713,
      "learning_rate": 4.9051593323217e-06,
      "loss": 2.7668,
      "step": 51
    },
    {
      "epoch": 0.07890743550834597,
      "grad_norm": 1.8007159233093262,
      "learning_rate": 4.903262518968134e-06,
      "loss": 2.2654,
      "step": 52
    },
    {
      "epoch": 0.08042488619119878,
      "grad_norm": 1.995640754699707,
      "learning_rate": 4.901365705614568e-06,
      "loss": 2.4512,
      "step": 53
    },
    {
      "epoch": 0.0819423368740516,
      "grad_norm": 2.092635154724121,
      "learning_rate": 4.899468892261002e-06,
      "loss": 2.5414,
      "step": 54
    },
    {
      "epoch": 0.0834597875569044,
      "grad_norm": 1.990322232246399,
      "learning_rate": 4.897572078907436e-06,
      "loss": 2.3962,
      "step": 55
    },
    {
      "epoch": 0.08497723823975721,
      "grad_norm": 2.0550074577331543,
      "learning_rate": 4.89567526555387e-06,
      "loss": 2.522,
      "step": 56
    },
    {
      "epoch": 0.08649468892261002,
      "grad_norm": 2.290249824523926,
      "learning_rate": 4.893778452200304e-06,
      "loss": 2.6671,
      "step": 57
    },
    {
      "epoch": 0.08801213960546282,
      "grad_norm": 2.1940150260925293,
      "learning_rate": 4.8918816388467376e-06,
      "loss": 2.691,
      "step": 58
    },
    {
      "epoch": 0.08952959028831563,
      "grad_norm": 2.2603113651275635,
      "learning_rate": 4.889984825493172e-06,
      "loss": 2.7381,
      "step": 59
    },
    {
      "epoch": 0.09104704097116843,
      "grad_norm": 2.172110080718994,
      "learning_rate": 4.888088012139605e-06,
      "loss": 2.5728,
      "step": 60
    },
    {
      "epoch": 0.09256449165402124,
      "grad_norm": 2.0453908443450928,
      "learning_rate": 4.88619119878604e-06,
      "loss": 2.5076,
      "step": 61
    },
    {
      "epoch": 0.09408194233687406,
      "grad_norm": 1.900107502937317,
      "learning_rate": 4.884294385432474e-06,
      "loss": 2.3235,
      "step": 62
    },
    {
      "epoch": 0.09559939301972686,
      "grad_norm": 2.2727878093719482,
      "learning_rate": 4.882397572078908e-06,
      "loss": 2.671,
      "step": 63
    },
    {
      "epoch": 0.09711684370257967,
      "grad_norm": 2.090489149093628,
      "learning_rate": 4.880500758725342e-06,
      "loss": 2.4763,
      "step": 64
    },
    {
      "epoch": 0.09863429438543247,
      "grad_norm": 2.3440065383911133,
      "learning_rate": 4.878603945371776e-06,
      "loss": 2.6812,
      "step": 65
    },
    {
      "epoch": 0.10015174506828528,
      "grad_norm": 2.290780782699585,
      "learning_rate": 4.87670713201821e-06,
      "loss": 2.5922,
      "step": 66
    },
    {
      "epoch": 0.10166919575113809,
      "grad_norm": 2.286217451095581,
      "learning_rate": 4.874810318664644e-06,
      "loss": 2.6747,
      "step": 67
    },
    {
      "epoch": 0.10318664643399089,
      "grad_norm": 2.151233434677124,
      "learning_rate": 4.872913505311078e-06,
      "loss": 2.5189,
      "step": 68
    },
    {
      "epoch": 0.1047040971168437,
      "grad_norm": 2.1663994789123535,
      "learning_rate": 4.871016691957512e-06,
      "loss": 2.4654,
      "step": 69
    },
    {
      "epoch": 0.1062215477996965,
      "grad_norm": 1.9851864576339722,
      "learning_rate": 4.8691198786039455e-06,
      "loss": 2.3618,
      "step": 70
    },
    {
      "epoch": 0.10773899848254932,
      "grad_norm": 2.002471446990967,
      "learning_rate": 4.86722306525038e-06,
      "loss": 2.4097,
      "step": 71
    },
    {
      "epoch": 0.10925644916540213,
      "grad_norm": 2.293764114379883,
      "learning_rate": 4.865326251896813e-06,
      "loss": 2.6815,
      "step": 72
    },
    {
      "epoch": 0.11077389984825493,
      "grad_norm": 2.2215654850006104,
      "learning_rate": 4.863429438543248e-06,
      "loss": 2.565,
      "step": 73
    },
    {
      "epoch": 0.11229135053110774,
      "grad_norm": 2.3223812580108643,
      "learning_rate": 4.861532625189681e-06,
      "loss": 2.6274,
      "step": 74
    },
    {
      "epoch": 0.11380880121396054,
      "grad_norm": 2.189030885696411,
      "learning_rate": 4.859635811836116e-06,
      "loss": 2.5123,
      "step": 75
    },
    {
      "epoch": 0.11532625189681335,
      "grad_norm": 2.0655641555786133,
      "learning_rate": 4.85773899848255e-06,
      "loss": 2.4818,
      "step": 76
    },
    {
      "epoch": 0.11684370257966616,
      "grad_norm": 2.106187582015991,
      "learning_rate": 4.855842185128984e-06,
      "loss": 2.3792,
      "step": 77
    },
    {
      "epoch": 0.11836115326251896,
      "grad_norm": 2.1273066997528076,
      "learning_rate": 4.853945371775418e-06,
      "loss": 2.4956,
      "step": 78
    },
    {
      "epoch": 0.11987860394537178,
      "grad_norm": 2.0351805686950684,
      "learning_rate": 4.852048558421852e-06,
      "loss": 2.4225,
      "step": 79
    },
    {
      "epoch": 0.12139605462822459,
      "grad_norm": 2.151801347732544,
      "learning_rate": 4.850151745068286e-06,
      "loss": 2.5412,
      "step": 80
    },
    {
      "epoch": 0.12291350531107739,
      "grad_norm": 2.12904691696167,
      "learning_rate": 4.8482549317147195e-06,
      "loss": 2.4693,
      "step": 81
    },
    {
      "epoch": 0.1244309559939302,
      "grad_norm": 2.111043930053711,
      "learning_rate": 4.8463581183611535e-06,
      "loss": 2.5101,
      "step": 82
    },
    {
      "epoch": 0.125948406676783,
      "grad_norm": 2.0726983547210693,
      "learning_rate": 4.844461305007588e-06,
      "loss": 2.3682,
      "step": 83
    },
    {
      "epoch": 0.1274658573596358,
      "grad_norm": 2.1633191108703613,
      "learning_rate": 4.842564491654021e-06,
      "loss": 2.4703,
      "step": 84
    },
    {
      "epoch": 0.12898330804248861,
      "grad_norm": 2.0191047191619873,
      "learning_rate": 4.840667678300456e-06,
      "loss": 2.3973,
      "step": 85
    },
    {
      "epoch": 0.13050075872534142,
      "grad_norm": 2.2715060710906982,
      "learning_rate": 4.838770864946889e-06,
      "loss": 2.634,
      "step": 86
    },
    {
      "epoch": 0.13201820940819423,
      "grad_norm": 2.255190372467041,
      "learning_rate": 4.836874051593324e-06,
      "loss": 2.6052,
      "step": 87
    },
    {
      "epoch": 0.13353566009104703,
      "grad_norm": 2.1927037239074707,
      "learning_rate": 4.834977238239758e-06,
      "loss": 2.4252,
      "step": 88
    },
    {
      "epoch": 0.13505311077389984,
      "grad_norm": 2.1862878799438477,
      "learning_rate": 4.833080424886192e-06,
      "loss": 2.4563,
      "step": 89
    },
    {
      "epoch": 0.13657056145675264,
      "grad_norm": 2.235668420791626,
      "learning_rate": 4.831183611532626e-06,
      "loss": 2.5296,
      "step": 90
    },
    {
      "epoch": 0.13808801213960548,
      "grad_norm": 1.8104183673858643,
      "learning_rate": 4.82928679817906e-06,
      "loss": 2.1196,
      "step": 91
    },
    {
      "epoch": 0.13960546282245828,
      "grad_norm": 1.9396520853042603,
      "learning_rate": 4.8273899848254936e-06,
      "loss": 2.2485,
      "step": 92
    },
    {
      "epoch": 0.1411229135053111,
      "grad_norm": 2.0394287109375,
      "learning_rate": 4.8254931714719275e-06,
      "loss": 2.3236,
      "step": 93
    },
    {
      "epoch": 0.1426403641881639,
      "grad_norm": 1.8569812774658203,
      "learning_rate": 4.823596358118361e-06,
      "loss": 2.0872,
      "step": 94
    },
    {
      "epoch": 0.1441578148710167,
      "grad_norm": 2.096402645111084,
      "learning_rate": 4.821699544764795e-06,
      "loss": 2.4306,
      "step": 95
    },
    {
      "epoch": 0.1456752655538695,
      "grad_norm": 2.1314823627471924,
      "learning_rate": 4.819802731411229e-06,
      "loss": 2.3815,
      "step": 96
    },
    {
      "epoch": 0.1471927162367223,
      "grad_norm": 2.1128504276275635,
      "learning_rate": 4.817905918057664e-06,
      "loss": 2.4459,
      "step": 97
    },
    {
      "epoch": 0.14871016691957512,
      "grad_norm": 2.144059181213379,
      "learning_rate": 4.816009104704097e-06,
      "loss": 2.395,
      "step": 98
    },
    {
      "epoch": 0.15022761760242792,
      "grad_norm": 2.3131015300750732,
      "learning_rate": 4.814112291350532e-06,
      "loss": 2.5363,
      "step": 99
    },
    {
      "epoch": 0.15174506828528073,
      "grad_norm": 2.143098831176758,
      "learning_rate": 4.812215477996966e-06,
      "loss": 2.3425,
      "step": 100
    },
    {
      "epoch": 0.15326251896813353,
      "grad_norm": 2.2773549556732178,
      "learning_rate": 4.8103186646434e-06,
      "loss": 2.5358,
      "step": 101
    },
    {
      "epoch": 0.15477996965098634,
      "grad_norm": 2.0721702575683594,
      "learning_rate": 4.808421851289834e-06,
      "loss": 2.2631,
      "step": 102
    },
    {
      "epoch": 0.15629742033383914,
      "grad_norm": 2.123516082763672,
      "learning_rate": 4.806525037936268e-06,
      "loss": 2.4111,
      "step": 103
    },
    {
      "epoch": 0.15781487101669195,
      "grad_norm": 2.011852979660034,
      "learning_rate": 4.8046282245827015e-06,
      "loss": 2.3027,
      "step": 104
    },
    {
      "epoch": 0.15933232169954475,
      "grad_norm": 2.110259532928467,
      "learning_rate": 4.8027314112291354e-06,
      "loss": 2.3936,
      "step": 105
    },
    {
      "epoch": 0.16084977238239756,
      "grad_norm": 2.0878117084503174,
      "learning_rate": 4.800834597875569e-06,
      "loss": 2.4215,
      "step": 106
    },
    {
      "epoch": 0.16236722306525037,
      "grad_norm": 2.000732421875,
      "learning_rate": 4.798937784522003e-06,
      "loss": 2.315,
      "step": 107
    },
    {
      "epoch": 0.1638846737481032,
      "grad_norm": 1.9328895807266235,
      "learning_rate": 4.797040971168437e-06,
      "loss": 2.2777,
      "step": 108
    },
    {
      "epoch": 0.165402124430956,
      "grad_norm": 1.8945529460906982,
      "learning_rate": 4.795144157814872e-06,
      "loss": 2.2259,
      "step": 109
    },
    {
      "epoch": 0.1669195751138088,
      "grad_norm": 1.7818320989608765,
      "learning_rate": 4.793247344461305e-06,
      "loss": 2.0824,
      "step": 110
    },
    {
      "epoch": 0.16843702579666162,
      "grad_norm": 2.062708616256714,
      "learning_rate": 4.79135053110774e-06,
      "loss": 2.4503,
      "step": 111
    },
    {
      "epoch": 0.16995447647951442,
      "grad_norm": 1.6058317422866821,
      "learning_rate": 4.789453717754173e-06,
      "loss": 1.8973,
      "step": 112
    },
    {
      "epoch": 0.17147192716236723,
      "grad_norm": 2.1267478466033936,
      "learning_rate": 4.787556904400608e-06,
      "loss": 2.3533,
      "step": 113
    },
    {
      "epoch": 0.17298937784522003,
      "grad_norm": 1.909965991973877,
      "learning_rate": 4.785660091047042e-06,
      "loss": 2.2686,
      "step": 114
    },
    {
      "epoch": 0.17450682852807284,
      "grad_norm": 1.9346671104431152,
      "learning_rate": 4.7837632776934755e-06,
      "loss": 2.2794,
      "step": 115
    },
    {
      "epoch": 0.17602427921092564,
      "grad_norm": 1.988786220550537,
      "learning_rate": 4.7818664643399095e-06,
      "loss": 2.1182,
      "step": 116
    },
    {
      "epoch": 0.17754172989377845,
      "grad_norm": 2.0355496406555176,
      "learning_rate": 4.779969650986343e-06,
      "loss": 2.158,
      "step": 117
    },
    {
      "epoch": 0.17905918057663125,
      "grad_norm": 1.8247604370117188,
      "learning_rate": 4.778072837632777e-06,
      "loss": 2.0327,
      "step": 118
    },
    {
      "epoch": 0.18057663125948406,
      "grad_norm": 1.852206826210022,
      "learning_rate": 4.776176024279211e-06,
      "loss": 2.1969,
      "step": 119
    },
    {
      "epoch": 0.18209408194233687,
      "grad_norm": 1.8666757345199585,
      "learning_rate": 4.774279210925645e-06,
      "loss": 2.1716,
      "step": 120
    },
    {
      "epoch": 0.18361153262518967,
      "grad_norm": 1.7100365161895752,
      "learning_rate": 4.77238239757208e-06,
      "loss": 2.0859,
      "step": 121
    },
    {
      "epoch": 0.18512898330804248,
      "grad_norm": 2.0169107913970947,
      "learning_rate": 4.770485584218513e-06,
      "loss": 2.4459,
      "step": 122
    },
    {
      "epoch": 0.18664643399089528,
      "grad_norm": 2.0655996799468994,
      "learning_rate": 4.768588770864948e-06,
      "loss": 2.3117,
      "step": 123
    },
    {
      "epoch": 0.18816388467374812,
      "grad_norm": 1.6711820363998413,
      "learning_rate": 4.766691957511381e-06,
      "loss": 1.9535,
      "step": 124
    },
    {
      "epoch": 0.18968133535660092,
      "grad_norm": 1.9324477910995483,
      "learning_rate": 4.764795144157816e-06,
      "loss": 2.299,
      "step": 125
    },
    {
      "epoch": 0.19119878603945373,
      "grad_norm": 1.739269733428955,
      "learning_rate": 4.7628983308042496e-06,
      "loss": 2.1227,
      "step": 126
    },
    {
      "epoch": 0.19271623672230653,
      "grad_norm": 1.7694181203842163,
      "learning_rate": 4.7610015174506835e-06,
      "loss": 2.0844,
      "step": 127
    },
    {
      "epoch": 0.19423368740515934,
      "grad_norm": 2.0194592475891113,
      "learning_rate": 4.759104704097117e-06,
      "loss": 2.3054,
      "step": 128
    },
    {
      "epoch": 0.19575113808801214,
      "grad_norm": 1.9378927946090698,
      "learning_rate": 4.757207890743551e-06,
      "loss": 2.268,
      "step": 129
    },
    {
      "epoch": 0.19726858877086495,
      "grad_norm": 1.7570464611053467,
      "learning_rate": 4.755311077389985e-06,
      "loss": 2.1775,
      "step": 130
    },
    {
      "epoch": 0.19878603945371776,
      "grad_norm": 1.4144189357757568,
      "learning_rate": 4.753414264036419e-06,
      "loss": 1.8212,
      "step": 131
    },
    {
      "epoch": 0.20030349013657056,
      "grad_norm": 1.8524763584136963,
      "learning_rate": 4.751517450682853e-06,
      "loss": 2.1872,
      "step": 132
    },
    {
      "epoch": 0.20182094081942337,
      "grad_norm": 1.9369138479232788,
      "learning_rate": 4.749620637329287e-06,
      "loss": 2.2867,
      "step": 133
    },
    {
      "epoch": 0.20333839150227617,
      "grad_norm": 1.8201218843460083,
      "learning_rate": 4.747723823975721e-06,
      "loss": 2.1956,
      "step": 134
    },
    {
      "epoch": 0.20485584218512898,
      "grad_norm": 1.339734435081482,
      "learning_rate": 4.745827010622155e-06,
      "loss": 1.7054,
      "step": 135
    },
    {
      "epoch": 0.20637329286798178,
      "grad_norm": 2.010845899581909,
      "learning_rate": 4.743930197268589e-06,
      "loss": 2.3046,
      "step": 136
    },
    {
      "epoch": 0.2078907435508346,
      "grad_norm": 1.870203971862793,
      "learning_rate": 4.742033383915023e-06,
      "loss": 2.2461,
      "step": 137
    },
    {
      "epoch": 0.2094081942336874,
      "grad_norm": 1.5797744989395142,
      "learning_rate": 4.7401365705614575e-06,
      "loss": 1.9805,
      "step": 138
    },
    {
      "epoch": 0.2109256449165402,
      "grad_norm": 1.8298470973968506,
      "learning_rate": 4.738239757207891e-06,
      "loss": 2.1946,
      "step": 139
    },
    {
      "epoch": 0.212443095599393,
      "grad_norm": 1.937387466430664,
      "learning_rate": 4.736342943854325e-06,
      "loss": 2.223,
      "step": 140
    },
    {
      "epoch": 0.21396054628224584,
      "grad_norm": 1.399312138557434,
      "learning_rate": 4.7344461305007585e-06,
      "loss": 1.844,
      "step": 141
    },
    {
      "epoch": 0.21547799696509864,
      "grad_norm": 1.749246597290039,
      "learning_rate": 4.732549317147193e-06,
      "loss": 2.2617,
      "step": 142
    },
    {
      "epoch": 0.21699544764795145,
      "grad_norm": 1.5941622257232666,
      "learning_rate": 4.730652503793627e-06,
      "loss": 1.9959,
      "step": 143
    },
    {
      "epoch": 0.21851289833080426,
      "grad_norm": 1.7424471378326416,
      "learning_rate": 4.728755690440061e-06,
      "loss": 2.1661,
      "step": 144
    },
    {
      "epoch": 0.22003034901365706,
      "grad_norm": 1.5631738901138306,
      "learning_rate": 4.726858877086495e-06,
      "loss": 2.0362,
      "step": 145
    },
    {
      "epoch": 0.22154779969650987,
      "grad_norm": 1.6330149173736572,
      "learning_rate": 4.724962063732929e-06,
      "loss": 2.0702,
      "step": 146
    },
    {
      "epoch": 0.22306525037936267,
      "grad_norm": 1.9025793075561523,
      "learning_rate": 4.723065250379363e-06,
      "loss": 2.3072,
      "step": 147
    },
    {
      "epoch": 0.22458270106221548,
      "grad_norm": 1.4846453666687012,
      "learning_rate": 4.721168437025797e-06,
      "loss": 1.9421,
      "step": 148
    },
    {
      "epoch": 0.22610015174506828,
      "grad_norm": 1.5631577968597412,
      "learning_rate": 4.719271623672231e-06,
      "loss": 2.1208,
      "step": 149
    },
    {
      "epoch": 0.2276176024279211,
      "grad_norm": 1.6431032419204712,
      "learning_rate": 4.7173748103186655e-06,
      "loss": 2.2193,
      "step": 150
    },
    {
      "epoch": 0.2291350531107739,
      "grad_norm": 3.527414083480835,
      "learning_rate": 4.7154779969650986e-06,
      "loss": 2.2175,
      "step": 151
    },
    {
      "epoch": 0.2306525037936267,
      "grad_norm": 1.5680243968963623,
      "learning_rate": 4.713581183611533e-06,
      "loss": 2.0781,
      "step": 152
    },
    {
      "epoch": 0.2321699544764795,
      "grad_norm": 1.5294932126998901,
      "learning_rate": 4.711684370257966e-06,
      "loss": 1.9989,
      "step": 153
    },
    {
      "epoch": 0.2336874051593323,
      "grad_norm": 1.555995225906372,
      "learning_rate": 4.709787556904401e-06,
      "loss": 2.0813,
      "step": 154
    },
    {
      "epoch": 0.23520485584218512,
      "grad_norm": 1.3353482484817505,
      "learning_rate": 4.707890743550835e-06,
      "loss": 1.6925,
      "step": 155
    },
    {
      "epoch": 0.23672230652503792,
      "grad_norm": 2.4012539386749268,
      "learning_rate": 4.705993930197269e-06,
      "loss": 1.9668,
      "step": 156
    },
    {
      "epoch": 0.23823975720789076,
      "grad_norm": 1.7187604904174805,
      "learning_rate": 4.704097116843703e-06,
      "loss": 2.1909,
      "step": 157
    },
    {
      "epoch": 0.23975720789074356,
      "grad_norm": 1.6220577955245972,
      "learning_rate": 4.702200303490137e-06,
      "loss": 2.1242,
      "step": 158
    },
    {
      "epoch": 0.24127465857359637,
      "grad_norm": 1.4794807434082031,
      "learning_rate": 4.700303490136571e-06,
      "loss": 1.8935,
      "step": 159
    },
    {
      "epoch": 0.24279210925644917,
      "grad_norm": 1.5458165407180786,
      "learning_rate": 4.698406676783005e-06,
      "loss": 2.0691,
      "step": 160
    },
    {
      "epoch": 0.24430955993930198,
      "grad_norm": 1.4653401374816895,
      "learning_rate": 4.696509863429439e-06,
      "loss": 1.8284,
      "step": 161
    },
    {
      "epoch": 0.24582701062215478,
      "grad_norm": 1.4459104537963867,
      "learning_rate": 4.694613050075873e-06,
      "loss": 1.8775,
      "step": 162
    },
    {
      "epoch": 0.2473444613050076,
      "grad_norm": 1.4679832458496094,
      "learning_rate": 4.6927162367223065e-06,
      "loss": 1.9973,
      "step": 163
    },
    {
      "epoch": 0.2488619119878604,
      "grad_norm": 1.524503469467163,
      "learning_rate": 4.690819423368741e-06,
      "loss": 2.0529,
      "step": 164
    },
    {
      "epoch": 0.2503793626707132,
      "grad_norm": 1.4276930093765259,
      "learning_rate": 4.688922610015174e-06,
      "loss": 1.8981,
      "step": 165
    },
    {
      "epoch": 0.251896813353566,
      "grad_norm": 1.4873645305633545,
      "learning_rate": 4.687025796661609e-06,
      "loss": 2.0076,
      "step": 166
    },
    {
      "epoch": 0.2534142640364188,
      "grad_norm": 1.4560511112213135,
      "learning_rate": 4.685128983308043e-06,
      "loss": 1.9497,
      "step": 167
    },
    {
      "epoch": 0.2549317147192716,
      "grad_norm": 1.5240691900253296,
      "learning_rate": 4.683232169954477e-06,
      "loss": 2.0608,
      "step": 168
    },
    {
      "epoch": 0.2564491654021244,
      "grad_norm": 1.2688676118850708,
      "learning_rate": 4.681335356600911e-06,
      "loss": 1.8347,
      "step": 169
    },
    {
      "epoch": 0.25796661608497723,
      "grad_norm": 1.44892418384552,
      "learning_rate": 4.679438543247345e-06,
      "loss": 1.9601,
      "step": 170
    },
    {
      "epoch": 0.25948406676783003,
      "grad_norm": 1.4545681476593018,
      "learning_rate": 4.677541729893779e-06,
      "loss": 2.0096,
      "step": 171
    },
    {
      "epoch": 0.26100151745068284,
      "grad_norm": 1.305098056793213,
      "learning_rate": 4.675644916540213e-06,
      "loss": 1.9301,
      "step": 172
    },
    {
      "epoch": 0.26251896813353565,
      "grad_norm": 1.440585970878601,
      "learning_rate": 4.673748103186647e-06,
      "loss": 1.9391,
      "step": 173
    },
    {
      "epoch": 0.26403641881638845,
      "grad_norm": 1.2744163274765015,
      "learning_rate": 4.6718512898330805e-06,
      "loss": 1.7357,
      "step": 174
    },
    {
      "epoch": 0.26555386949924126,
      "grad_norm": 1.4672338962554932,
      "learning_rate": 4.6699544764795145e-06,
      "loss": 1.9782,
      "step": 175
    },
    {
      "epoch": 0.26707132018209406,
      "grad_norm": 1.3258421421051025,
      "learning_rate": 4.668057663125949e-06,
      "loss": 1.8719,
      "step": 176
    },
    {
      "epoch": 0.26858877086494687,
      "grad_norm": 1.2565325498580933,
      "learning_rate": 4.666160849772382e-06,
      "loss": 1.7892,
      "step": 177
    },
    {
      "epoch": 0.2701062215477997,
      "grad_norm": 1.4629207849502563,
      "learning_rate": 4.664264036418817e-06,
      "loss": 2.046,
      "step": 178
    },
    {
      "epoch": 0.2716236722306525,
      "grad_norm": 1.3818567991256714,
      "learning_rate": 4.66236722306525e-06,
      "loss": 1.9305,
      "step": 179
    },
    {
      "epoch": 0.2731411229135053,
      "grad_norm": 1.5568277835845947,
      "learning_rate": 4.660470409711685e-06,
      "loss": 1.988,
      "step": 180
    },
    {
      "epoch": 0.2746585735963581,
      "grad_norm": 1.4730825424194336,
      "learning_rate": 4.658573596358119e-06,
      "loss": 1.9988,
      "step": 181
    },
    {
      "epoch": 0.27617602427921095,
      "grad_norm": 0.9968308210372925,
      "learning_rate": 4.656676783004553e-06,
      "loss": 1.6262,
      "step": 182
    },
    {
      "epoch": 0.27769347496206376,
      "grad_norm": 1.3853973150253296,
      "learning_rate": 4.654779969650987e-06,
      "loss": 2.0433,
      "step": 183
    },
    {
      "epoch": 0.27921092564491656,
      "grad_norm": 1.4565132856369019,
      "learning_rate": 4.652883156297421e-06,
      "loss": 2.0715,
      "step": 184
    },
    {
      "epoch": 0.28072837632776937,
      "grad_norm": 1.1644779443740845,
      "learning_rate": 4.6509863429438546e-06,
      "loss": 1.7322,
      "step": 185
    },
    {
      "epoch": 0.2822458270106222,
      "grad_norm": 1.2778548002243042,
      "learning_rate": 4.6490895295902885e-06,
      "loss": 1.9474,
      "step": 186
    },
    {
      "epoch": 0.283763277693475,
      "grad_norm": 1.2050098180770874,
      "learning_rate": 4.647192716236722e-06,
      "loss": 1.8067,
      "step": 187
    },
    {
      "epoch": 0.2852807283763278,
      "grad_norm": 1.3183622360229492,
      "learning_rate": 4.645295902883157e-06,
      "loss": 1.9728,
      "step": 188
    },
    {
      "epoch": 0.2867981790591806,
      "grad_norm": 1.2808395624160767,
      "learning_rate": 4.64339908952959e-06,
      "loss": 1.9142,
      "step": 189
    },
    {
      "epoch": 0.2883156297420334,
      "grad_norm": 1.3666157722473145,
      "learning_rate": 4.641502276176025e-06,
      "loss": 2.0131,
      "step": 190
    },
    {
      "epoch": 0.2898330804248862,
      "grad_norm": 1.2419134378433228,
      "learning_rate": 4.639605462822458e-06,
      "loss": 1.8308,
      "step": 191
    },
    {
      "epoch": 0.291350531107739,
      "grad_norm": 1.2077723741531372,
      "learning_rate": 4.637708649468893e-06,
      "loss": 1.8981,
      "step": 192
    },
    {
      "epoch": 0.2928679817905918,
      "grad_norm": 1.2003333568572998,
      "learning_rate": 4.635811836115327e-06,
      "loss": 1.8741,
      "step": 193
    },
    {
      "epoch": 0.2943854324734446,
      "grad_norm": 1.3200513124465942,
      "learning_rate": 4.633915022761761e-06,
      "loss": 1.9087,
      "step": 194
    },
    {
      "epoch": 0.2959028831562974,
      "grad_norm": 1.254772424697876,
      "learning_rate": 4.632018209408195e-06,
      "loss": 1.92,
      "step": 195
    },
    {
      "epoch": 0.29742033383915023,
      "grad_norm": 1.2774337530136108,
      "learning_rate": 4.630121396054629e-06,
      "loss": 1.9808,
      "step": 196
    },
    {
      "epoch": 0.29893778452200304,
      "grad_norm": 1.2523548603057861,
      "learning_rate": 4.6282245827010625e-06,
      "loss": 1.9593,
      "step": 197
    },
    {
      "epoch": 0.30045523520485584,
      "grad_norm": 1.1916251182556152,
      "learning_rate": 4.6263277693474964e-06,
      "loss": 1.9484,
      "step": 198
    },
    {
      "epoch": 0.30197268588770865,
      "grad_norm": 1.2164472341537476,
      "learning_rate": 4.62443095599393e-06,
      "loss": 1.8794,
      "step": 199
    },
    {
      "epoch": 0.30349013657056145,
      "grad_norm": 1.1980657577514648,
      "learning_rate": 4.622534142640364e-06,
      "loss": 1.8773,
      "step": 200
    },
    {
      "epoch": 0.30500758725341426,
      "grad_norm": 1.171488642692566,
      "learning_rate": 4.620637329286798e-06,
      "loss": 1.8603,
      "step": 201
    },
    {
      "epoch": 0.30652503793626706,
      "grad_norm": 1.092696189880371,
      "learning_rate": 4.618740515933233e-06,
      "loss": 1.7397,
      "step": 202
    },
    {
      "epoch": 0.30804248861911987,
      "grad_norm": 1.0775257349014282,
      "learning_rate": 4.616843702579666e-06,
      "loss": 1.7131,
      "step": 203
    },
    {
      "epoch": 0.3095599393019727,
      "grad_norm": 1.0368870496749878,
      "learning_rate": 4.614946889226101e-06,
      "loss": 1.6935,
      "step": 204
    },
    {
      "epoch": 0.3110773899848255,
      "grad_norm": 1.1989238262176514,
      "learning_rate": 4.613050075872535e-06,
      "loss": 1.8863,
      "step": 205
    },
    {
      "epoch": 0.3125948406676783,
      "grad_norm": 1.1287822723388672,
      "learning_rate": 4.611153262518969e-06,
      "loss": 1.8135,
      "step": 206
    },
    {
      "epoch": 0.3141122913505311,
      "grad_norm": 1.181717872619629,
      "learning_rate": 4.609256449165403e-06,
      "loss": 1.9418,
      "step": 207
    },
    {
      "epoch": 0.3156297420333839,
      "grad_norm": 1.218904972076416,
      "learning_rate": 4.6073596358118365e-06,
      "loss": 1.9189,
      "step": 208
    },
    {
      "epoch": 0.3171471927162367,
      "grad_norm": 1.1242988109588623,
      "learning_rate": 4.6054628224582705e-06,
      "loss": 1.8643,
      "step": 209
    },
    {
      "epoch": 0.3186646433990895,
      "grad_norm": 1.1326851844787598,
      "learning_rate": 4.603566009104704e-06,
      "loss": 1.8656,
      "step": 210
    },
    {
      "epoch": 0.3201820940819423,
      "grad_norm": 0.9841458201408386,
      "learning_rate": 4.601669195751138e-06,
      "loss": 1.7562,
      "step": 211
    },
    {
      "epoch": 0.3216995447647951,
      "grad_norm": 0.9461800456047058,
      "learning_rate": 4.599772382397572e-06,
      "loss": 1.5694,
      "step": 212
    },
    {
      "epoch": 0.3232169954476479,
      "grad_norm": 1.0814789533615112,
      "learning_rate": 4.597875569044006e-06,
      "loss": 1.8371,
      "step": 213
    },
    {
      "epoch": 0.32473444613050073,
      "grad_norm": 1.1916582584381104,
      "learning_rate": 4.595978755690441e-06,
      "loss": 1.853,
      "step": 214
    },
    {
      "epoch": 0.3262518968133536,
      "grad_norm": 1.1136468648910522,
      "learning_rate": 4.594081942336874e-06,
      "loss": 1.8776,
      "step": 215
    },
    {
      "epoch": 0.3277693474962064,
      "grad_norm": 1.1315356492996216,
      "learning_rate": 4.592185128983309e-06,
      "loss": 1.8402,
      "step": 216
    },
    {
      "epoch": 0.3292867981790592,
      "grad_norm": 1.1523298025131226,
      "learning_rate": 4.590288315629743e-06,
      "loss": 1.9045,
      "step": 217
    },
    {
      "epoch": 0.330804248861912,
      "grad_norm": 1.162235975265503,
      "learning_rate": 4.588391502276177e-06,
      "loss": 1.9387,
      "step": 218
    },
    {
      "epoch": 0.3323216995447648,
      "grad_norm": 1.0681637525558472,
      "learning_rate": 4.5864946889226106e-06,
      "loss": 1.8237,
      "step": 219
    },
    {
      "epoch": 0.3338391502276176,
      "grad_norm": 1.1120003461837769,
      "learning_rate": 4.5845978755690445e-06,
      "loss": 1.918,
      "step": 220
    },
    {
      "epoch": 0.3353566009104704,
      "grad_norm": 1.0437029600143433,
      "learning_rate": 4.582701062215478e-06,
      "loss": 1.8622,
      "step": 221
    },
    {
      "epoch": 0.33687405159332323,
      "grad_norm": 0.9651450514793396,
      "learning_rate": 4.580804248861912e-06,
      "loss": 1.7755,
      "step": 222
    },
    {
      "epoch": 0.33839150227617604,
      "grad_norm": 0.948331892490387,
      "learning_rate": 4.578907435508346e-06,
      "loss": 1.6918,
      "step": 223
    },
    {
      "epoch": 0.33990895295902884,
      "grad_norm": 0.7561855912208557,
      "learning_rate": 4.57701062215478e-06,
      "loss": 1.5214,
      "step": 224
    },
    {
      "epoch": 0.34142640364188165,
      "grad_norm": 0.9600418210029602,
      "learning_rate": 4.575113808801214e-06,
      "loss": 1.7339,
      "step": 225
    },
    {
      "epoch": 0.34294385432473445,
      "grad_norm": 0.9488735198974609,
      "learning_rate": 4.573216995447649e-06,
      "loss": 1.7612,
      "step": 226
    },
    {
      "epoch": 0.34446130500758726,
      "grad_norm": 1.0073925256729126,
      "learning_rate": 4.571320182094082e-06,
      "loss": 1.7827,
      "step": 227
    },
    {
      "epoch": 0.34597875569044007,
      "grad_norm": 1.0173628330230713,
      "learning_rate": 4.569423368740517e-06,
      "loss": 1.7724,
      "step": 228
    },
    {
      "epoch": 0.34749620637329287,
      "grad_norm": 0.9984621405601501,
      "learning_rate": 4.56752655538695e-06,
      "loss": 1.7751,
      "step": 229
    },
    {
      "epoch": 0.3490136570561457,
      "grad_norm": 1.0084197521209717,
      "learning_rate": 4.565629742033385e-06,
      "loss": 1.7858,
      "step": 230
    },
    {
      "epoch": 0.3505311077389985,
      "grad_norm": 1.0055415630340576,
      "learning_rate": 4.5637329286798185e-06,
      "loss": 1.7778,
      "step": 231
    },
    {
      "epoch": 0.3520485584218513,
      "grad_norm": 0.9449430108070374,
      "learning_rate": 4.5618361153262524e-06,
      "loss": 1.8166,
      "step": 232
    },
    {
      "epoch": 0.3535660091047041,
      "grad_norm": 1.227531909942627,
      "learning_rate": 4.559939301972686e-06,
      "loss": 1.7791,
      "step": 233
    },
    {
      "epoch": 0.3550834597875569,
      "grad_norm": 0.97216796875,
      "learning_rate": 4.55804248861912e-06,
      "loss": 1.7935,
      "step": 234
    },
    {
      "epoch": 0.3566009104704097,
      "grad_norm": 1.1072120666503906,
      "learning_rate": 4.556145675265554e-06,
      "loss": 1.8442,
      "step": 235
    },
    {
      "epoch": 0.3581183611532625,
      "grad_norm": 0.7488012313842773,
      "learning_rate": 4.554248861911988e-06,
      "loss": 1.452,
      "step": 236
    },
    {
      "epoch": 0.3596358118361153,
      "grad_norm": 0.8499703407287598,
      "learning_rate": 4.552352048558422e-06,
      "loss": 1.5722,
      "step": 237
    },
    {
      "epoch": 0.3611532625189681,
      "grad_norm": 0.8547839522361755,
      "learning_rate": 4.550455235204857e-06,
      "loss": 1.6615,
      "step": 238
    },
    {
      "epoch": 0.3626707132018209,
      "grad_norm": 0.7242099642753601,
      "learning_rate": 4.54855842185129e-06,
      "loss": 1.4602,
      "step": 239
    },
    {
      "epoch": 0.36418816388467373,
      "grad_norm": 0.9448665976524353,
      "learning_rate": 4.546661608497725e-06,
      "loss": 1.7398,
      "step": 240
    },
    {
      "epoch": 0.36570561456752654,
      "grad_norm": 0.840931236743927,
      "learning_rate": 4.544764795144158e-06,
      "loss": 1.6483,
      "step": 241
    },
    {
      "epoch": 0.36722306525037934,
      "grad_norm": 0.8998667001724243,
      "learning_rate": 4.5428679817905926e-06,
      "loss": 1.7604,
      "step": 242
    },
    {
      "epoch": 0.36874051593323215,
      "grad_norm": 0.9764036536216736,
      "learning_rate": 4.5409711684370265e-06,
      "loss": 1.9183,
      "step": 243
    },
    {
      "epoch": 0.37025796661608495,
      "grad_norm": 0.9747097492218018,
      "learning_rate": 4.53907435508346e-06,
      "loss": 1.8585,
      "step": 244
    },
    {
      "epoch": 0.37177541729893776,
      "grad_norm": 0.858219563961029,
      "learning_rate": 4.537177541729894e-06,
      "loss": 1.7236,
      "step": 245
    },
    {
      "epoch": 0.37329286798179057,
      "grad_norm": 0.9880526065826416,
      "learning_rate": 4.535280728376328e-06,
      "loss": 1.7835,
      "step": 246
    },
    {
      "epoch": 0.37481031866464337,
      "grad_norm": 0.9022077918052673,
      "learning_rate": 4.533383915022762e-06,
      "loss": 1.7905,
      "step": 247
    },
    {
      "epoch": 0.37632776934749623,
      "grad_norm": 0.8826465606689453,
      "learning_rate": 4.531487101669196e-06,
      "loss": 1.7209,
      "step": 248
    },
    {
      "epoch": 0.37784522003034904,
      "grad_norm": 0.7954358458518982,
      "learning_rate": 4.52959028831563e-06,
      "loss": 1.6644,
      "step": 249
    },
    {
      "epoch": 0.37936267071320184,
      "grad_norm": 0.6864622235298157,
      "learning_rate": 4.527693474962064e-06,
      "loss": 1.5122,
      "step": 250
    },
    {
      "epoch": 0.38088012139605465,
      "grad_norm": 0.9847657680511475,
      "learning_rate": 4.525796661608498e-06,
      "loss": 1.8707,
      "step": 251
    },
    {
      "epoch": 0.38239757207890746,
      "grad_norm": 0.8702391982078552,
      "learning_rate": 4.523899848254933e-06,
      "loss": 1.8026,
      "step": 252
    },
    {
      "epoch": 0.38391502276176026,
      "grad_norm": 0.8775818347930908,
      "learning_rate": 4.522003034901366e-06,
      "loss": 1.7399,
      "step": 253
    },
    {
      "epoch": 0.38543247344461307,
      "grad_norm": 0.7890204191207886,
      "learning_rate": 4.5201062215478005e-06,
      "loss": 1.6387,
      "step": 254
    },
    {
      "epoch": 0.38694992412746587,
      "grad_norm": 0.821952760219574,
      "learning_rate": 4.5182094081942344e-06,
      "loss": 1.5933,
      "step": 255
    },
    {
      "epoch": 0.3884673748103187,
      "grad_norm": 0.8222979307174683,
      "learning_rate": 4.516312594840668e-06,
      "loss": 1.6854,
      "step": 256
    },
    {
      "epoch": 0.3899848254931715,
      "grad_norm": 0.811018705368042,
      "learning_rate": 4.514415781487102e-06,
      "loss": 1.7065,
      "step": 257
    },
    {
      "epoch": 0.3915022761760243,
      "grad_norm": 0.8324394226074219,
      "learning_rate": 4.512518968133536e-06,
      "loss": 1.735,
      "step": 258
    },
    {
      "epoch": 0.3930197268588771,
      "grad_norm": 0.8543226718902588,
      "learning_rate": 4.51062215477997e-06,
      "loss": 1.823,
      "step": 259
    },
    {
      "epoch": 0.3945371775417299,
      "grad_norm": 0.7927592992782593,
      "learning_rate": 4.508725341426404e-06,
      "loss": 1.6373,
      "step": 260
    },
    {
      "epoch": 0.3960546282245827,
      "grad_norm": 0.8078227639198303,
      "learning_rate": 4.506828528072838e-06,
      "loss": 1.6394,
      "step": 261
    },
    {
      "epoch": 0.3975720789074355,
      "grad_norm": 0.8117296099662781,
      "learning_rate": 4.504931714719272e-06,
      "loss": 1.7041,
      "step": 262
    },
    {
      "epoch": 0.3990895295902883,
      "grad_norm": 0.7474256157875061,
      "learning_rate": 4.503034901365706e-06,
      "loss": 1.6159,
      "step": 263
    },
    {
      "epoch": 0.4006069802731411,
      "grad_norm": 0.8151485919952393,
      "learning_rate": 4.501138088012141e-06,
      "loss": 1.7357,
      "step": 264
    },
    {
      "epoch": 0.40212443095599393,
      "grad_norm": 0.870727002620697,
      "learning_rate": 4.499241274658574e-06,
      "loss": 1.7752,
      "step": 265
    },
    {
      "epoch": 0.40364188163884673,
      "grad_norm": 0.8018729090690613,
      "learning_rate": 4.497344461305008e-06,
      "loss": 1.8297,
      "step": 266
    },
    {
      "epoch": 0.40515933232169954,
      "grad_norm": 0.7744446396827698,
      "learning_rate": 4.4954476479514415e-06,
      "loss": 1.6496,
      "step": 267
    },
    {
      "epoch": 0.40667678300455234,
      "grad_norm": 0.8398352861404419,
      "learning_rate": 4.4935508345978755e-06,
      "loss": 1.794,
      "step": 268
    },
    {
      "epoch": 0.40819423368740515,
      "grad_norm": 0.744754433631897,
      "learning_rate": 4.49165402124431e-06,
      "loss": 1.6439,
      "step": 269
    },
    {
      "epoch": 0.40971168437025796,
      "grad_norm": 0.8295300006866455,
      "learning_rate": 4.489757207890743e-06,
      "loss": 1.7289,
      "step": 270
    },
    {
      "epoch": 0.41122913505311076,
      "grad_norm": 0.7875678539276123,
      "learning_rate": 4.487860394537178e-06,
      "loss": 1.7453,
      "step": 271
    },
    {
      "epoch": 0.41274658573596357,
      "grad_norm": 0.884168803691864,
      "learning_rate": 4.485963581183612e-06,
      "loss": 1.8217,
      "step": 272
    },
    {
      "epoch": 0.4142640364188164,
      "grad_norm": 0.7136833667755127,
      "learning_rate": 4.484066767830046e-06,
      "loss": 1.6783,
      "step": 273
    },
    {
      "epoch": 0.4157814871016692,
      "grad_norm": 0.7708815336227417,
      "learning_rate": 4.48216995447648e-06,
      "loss": 1.7801,
      "step": 274
    },
    {
      "epoch": 0.417298937784522,
      "grad_norm": 0.8022453784942627,
      "learning_rate": 4.480273141122914e-06,
      "loss": 1.767,
      "step": 275
    },
    {
      "epoch": 0.4188163884673748,
      "grad_norm": 0.7267943620681763,
      "learning_rate": 4.478376327769348e-06,
      "loss": 1.6868,
      "step": 276
    },
    {
      "epoch": 0.4203338391502276,
      "grad_norm": 0.7397265434265137,
      "learning_rate": 4.476479514415782e-06,
      "loss": 1.5568,
      "step": 277
    },
    {
      "epoch": 0.4218512898330804,
      "grad_norm": 0.7897565364837646,
      "learning_rate": 4.4745827010622156e-06,
      "loss": 1.7746,
      "step": 278
    },
    {
      "epoch": 0.4233687405159332,
      "grad_norm": 0.7903605699539185,
      "learning_rate": 4.4726858877086495e-06,
      "loss": 1.7811,
      "step": 279
    },
    {
      "epoch": 0.424886191198786,
      "grad_norm": 0.6914036870002747,
      "learning_rate": 4.470789074355083e-06,
      "loss": 1.5955,
      "step": 280
    },
    {
      "epoch": 0.4264036418816389,
      "grad_norm": 0.7199018001556396,
      "learning_rate": 4.468892261001518e-06,
      "loss": 1.6723,
      "step": 281
    },
    {
      "epoch": 0.4279210925644917,
      "grad_norm": 0.8356529474258423,
      "learning_rate": 4.466995447647951e-06,
      "loss": 1.8057,
      "step": 282
    },
    {
      "epoch": 0.4294385432473445,
      "grad_norm": 0.7651631236076355,
      "learning_rate": 4.465098634294386e-06,
      "loss": 1.652,
      "step": 283
    },
    {
      "epoch": 0.4309559939301973,
      "grad_norm": 0.7723724842071533,
      "learning_rate": 4.46320182094082e-06,
      "loss": 1.754,
      "step": 284
    },
    {
      "epoch": 0.4324734446130501,
      "grad_norm": 0.7088655233383179,
      "learning_rate": 4.461305007587254e-06,
      "loss": 1.5989,
      "step": 285
    },
    {
      "epoch": 0.4339908952959029,
      "grad_norm": 0.7297683358192444,
      "learning_rate": 4.459408194233688e-06,
      "loss": 1.7439,
      "step": 286
    },
    {
      "epoch": 0.4355083459787557,
      "grad_norm": 0.8626397252082825,
      "learning_rate": 4.457511380880122e-06,
      "loss": 1.6548,
      "step": 287
    },
    {
      "epoch": 0.4370257966616085,
      "grad_norm": 0.7018701434135437,
      "learning_rate": 4.455614567526556e-06,
      "loss": 1.6422,
      "step": 288
    },
    {
      "epoch": 0.4385432473444613,
      "grad_norm": 0.6812629699707031,
      "learning_rate": 4.45371775417299e-06,
      "loss": 1.5898,
      "step": 289
    },
    {
      "epoch": 0.4400606980273141,
      "grad_norm": 0.7080680131912231,
      "learning_rate": 4.4518209408194235e-06,
      "loss": 1.6557,
      "step": 290
    },
    {
      "epoch": 0.44157814871016693,
      "grad_norm": 0.7304114699363708,
      "learning_rate": 4.4499241274658574e-06,
      "loss": 1.6892,
      "step": 291
    },
    {
      "epoch": 0.44309559939301973,
      "grad_norm": 0.6933820843696594,
      "learning_rate": 4.448027314112291e-06,
      "loss": 1.6784,
      "step": 292
    },
    {
      "epoch": 0.44461305007587254,
      "grad_norm": 0.6562873721122742,
      "learning_rate": 4.446130500758726e-06,
      "loss": 1.6188,
      "step": 293
    },
    {
      "epoch": 0.44613050075872535,
      "grad_norm": 0.6981267929077148,
      "learning_rate": 4.444233687405159e-06,
      "loss": 1.6579,
      "step": 294
    },
    {
      "epoch": 0.44764795144157815,
      "grad_norm": 0.6800389885902405,
      "learning_rate": 4.442336874051594e-06,
      "loss": 1.6889,
      "step": 295
    },
    {
      "epoch": 0.44916540212443096,
      "grad_norm": 0.7080093026161194,
      "learning_rate": 4.440440060698027e-06,
      "loss": 1.67,
      "step": 296
    },
    {
      "epoch": 0.45068285280728376,
      "grad_norm": 0.7705059051513672,
      "learning_rate": 4.438543247344462e-06,
      "loss": 1.7558,
      "step": 297
    },
    {
      "epoch": 0.45220030349013657,
      "grad_norm": 0.7393538951873779,
      "learning_rate": 4.436646433990896e-06,
      "loss": 1.6189,
      "step": 298
    },
    {
      "epoch": 0.4537177541729894,
      "grad_norm": 0.7073180079460144,
      "learning_rate": 4.43474962063733e-06,
      "loss": 1.6943,
      "step": 299
    },
    {
      "epoch": 0.4552352048558422,
      "grad_norm": 0.6316116452217102,
      "learning_rate": 4.432852807283764e-06,
      "loss": 1.5186,
      "step": 300
    },
    {
      "epoch": 0.456752655538695,
      "grad_norm": 0.7153854370117188,
      "learning_rate": 4.4309559939301975e-06,
      "loss": 1.6583,
      "step": 301
    },
    {
      "epoch": 0.4582701062215478,
      "grad_norm": 0.807453989982605,
      "learning_rate": 4.4290591805766315e-06,
      "loss": 1.692,
      "step": 302
    },
    {
      "epoch": 0.4597875569044006,
      "grad_norm": 0.704749584197998,
      "learning_rate": 4.427162367223065e-06,
      "loss": 1.7069,
      "step": 303
    },
    {
      "epoch": 0.4613050075872534,
      "grad_norm": 0.7624502182006836,
      "learning_rate": 4.425265553869499e-06,
      "loss": 1.725,
      "step": 304
    },
    {
      "epoch": 0.4628224582701062,
      "grad_norm": 0.6851209402084351,
      "learning_rate": 4.423368740515934e-06,
      "loss": 1.6175,
      "step": 305
    },
    {
      "epoch": 0.464339908952959,
      "grad_norm": 0.6620073914527893,
      "learning_rate": 4.421471927162367e-06,
      "loss": 1.5988,
      "step": 306
    },
    {
      "epoch": 0.4658573596358118,
      "grad_norm": 0.6067116856575012,
      "learning_rate": 4.419575113808802e-06,
      "loss": 1.4355,
      "step": 307
    },
    {
      "epoch": 0.4673748103186646,
      "grad_norm": 0.6834487915039062,
      "learning_rate": 4.417678300455235e-06,
      "loss": 1.4996,
      "step": 308
    },
    {
      "epoch": 0.46889226100151743,
      "grad_norm": 0.7118691205978394,
      "learning_rate": 4.41578148710167e-06,
      "loss": 1.7142,
      "step": 309
    },
    {
      "epoch": 0.47040971168437024,
      "grad_norm": 0.6076904535293579,
      "learning_rate": 4.413884673748104e-06,
      "loss": 1.5254,
      "step": 310
    },
    {
      "epoch": 0.47192716236722304,
      "grad_norm": 0.6854212284088135,
      "learning_rate": 4.411987860394538e-06,
      "loss": 1.6632,
      "step": 311
    },
    {
      "epoch": 0.47344461305007585,
      "grad_norm": 0.7077570557594299,
      "learning_rate": 4.4100910470409716e-06,
      "loss": 1.6856,
      "step": 312
    },
    {
      "epoch": 0.47496206373292865,
      "grad_norm": 0.6942095160484314,
      "learning_rate": 4.4081942336874055e-06,
      "loss": 1.7283,
      "step": 313
    },
    {
      "epoch": 0.4764795144157815,
      "grad_norm": 0.6328182816505432,
      "learning_rate": 4.406297420333839e-06,
      "loss": 1.5632,
      "step": 314
    },
    {
      "epoch": 0.4779969650986343,
      "grad_norm": 0.6597891449928284,
      "learning_rate": 4.404400606980273e-06,
      "loss": 1.5546,
      "step": 315
    },
    {
      "epoch": 0.4795144157814871,
      "grad_norm": 0.7185386419296265,
      "learning_rate": 4.402503793626707e-06,
      "loss": 1.6606,
      "step": 316
    },
    {
      "epoch": 0.48103186646433993,
      "grad_norm": 0.6410588026046753,
      "learning_rate": 4.400606980273141e-06,
      "loss": 1.5881,
      "step": 317
    },
    {
      "epoch": 0.48254931714719274,
      "grad_norm": 0.5984237790107727,
      "learning_rate": 4.398710166919575e-06,
      "loss": 1.5017,
      "step": 318
    },
    {
      "epoch": 0.48406676783004554,
      "grad_norm": 0.6295478343963623,
      "learning_rate": 4.39681335356601e-06,
      "loss": 1.6077,
      "step": 319
    },
    {
      "epoch": 0.48558421851289835,
      "grad_norm": 0.629698634147644,
      "learning_rate": 4.394916540212443e-06,
      "loss": 1.5417,
      "step": 320
    },
    {
      "epoch": 0.48710166919575115,
      "grad_norm": 0.6477785706520081,
      "learning_rate": 4.393019726858878e-06,
      "loss": 1.5859,
      "step": 321
    },
    {
      "epoch": 0.48861911987860396,
      "grad_norm": 0.6353811025619507,
      "learning_rate": 4.391122913505312e-06,
      "loss": 1.6313,
      "step": 322
    },
    {
      "epoch": 0.49013657056145676,
      "grad_norm": 0.5887274146080017,
      "learning_rate": 4.389226100151746e-06,
      "loss": 1.551,
      "step": 323
    },
    {
      "epoch": 0.49165402124430957,
      "grad_norm": 0.5703217387199402,
      "learning_rate": 4.3873292867981795e-06,
      "loss": 1.3588,
      "step": 324
    },
    {
      "epoch": 0.4931714719271624,
      "grad_norm": 0.6745344996452332,
      "learning_rate": 4.3854324734446134e-06,
      "loss": 1.6607,
      "step": 325
    },
    {
      "epoch": 0.4946889226100152,
      "grad_norm": 0.5992046594619751,
      "learning_rate": 4.383535660091047e-06,
      "loss": 1.5091,
      "step": 326
    },
    {
      "epoch": 0.496206373292868,
      "grad_norm": 0.64537113904953,
      "learning_rate": 4.381638846737481e-06,
      "loss": 1.6319,
      "step": 327
    },
    {
      "epoch": 0.4977238239757208,
      "grad_norm": 0.707757294178009,
      "learning_rate": 4.379742033383915e-06,
      "loss": 1.7288,
      "step": 328
    },
    {
      "epoch": 0.4992412746585736,
      "grad_norm": 0.5737532377243042,
      "learning_rate": 4.377845220030349e-06,
      "loss": 1.5343,
      "step": 329
    },
    {
      "epoch": 0.5007587253414264,
      "grad_norm": 0.6571551561355591,
      "learning_rate": 4.375948406676783e-06,
      "loss": 1.706,
      "step": 330
    },
    {
      "epoch": 0.5022761760242792,
      "grad_norm": 0.4812064468860626,
      "learning_rate": 4.374051593323218e-06,
      "loss": 1.3085,
      "step": 331
    },
    {
      "epoch": 0.503793626707132,
      "grad_norm": 0.627266526222229,
      "learning_rate": 4.372154779969651e-06,
      "loss": 1.5075,
      "step": 332
    },
    {
      "epoch": 0.5053110773899848,
      "grad_norm": 0.6521584987640381,
      "learning_rate": 4.370257966616086e-06,
      "loss": 1.6372,
      "step": 333
    },
    {
      "epoch": 0.5068285280728376,
      "grad_norm": 0.6178029775619507,
      "learning_rate": 4.368361153262519e-06,
      "loss": 1.6059,
      "step": 334
    },
    {
      "epoch": 0.5083459787556904,
      "grad_norm": 0.6504966020584106,
      "learning_rate": 4.3664643399089536e-06,
      "loss": 1.5308,
      "step": 335
    },
    {
      "epoch": 0.5098634294385432,
      "grad_norm": 0.6660060882568359,
      "learning_rate": 4.3645675265553875e-06,
      "loss": 1.6841,
      "step": 336
    },
    {
      "epoch": 0.511380880121396,
      "grad_norm": 0.6003990173339844,
      "learning_rate": 4.362670713201821e-06,
      "loss": 1.5143,
      "step": 337
    },
    {
      "epoch": 0.5128983308042488,
      "grad_norm": 0.8385249376296997,
      "learning_rate": 4.360773899848255e-06,
      "loss": 1.7286,
      "step": 338
    },
    {
      "epoch": 0.5144157814871017,
      "grad_norm": 0.7597273588180542,
      "learning_rate": 4.358877086494689e-06,
      "loss": 1.7612,
      "step": 339
    },
    {
      "epoch": 0.5159332321699545,
      "grad_norm": 0.6570375561714172,
      "learning_rate": 4.356980273141123e-06,
      "loss": 1.6884,
      "step": 340
    },
    {
      "epoch": 0.5174506828528073,
      "grad_norm": 0.6296700239181519,
      "learning_rate": 4.355083459787557e-06,
      "loss": 1.6118,
      "step": 341
    },
    {
      "epoch": 0.5189681335356601,
      "grad_norm": 0.6293483972549438,
      "learning_rate": 4.353186646433991e-06,
      "loss": 1.6183,
      "step": 342
    },
    {
      "epoch": 0.5204855842185129,
      "grad_norm": 0.6618730425834656,
      "learning_rate": 4.351289833080426e-06,
      "loss": 1.6569,
      "step": 343
    },
    {
      "epoch": 0.5220030349013657,
      "grad_norm": 0.8984569907188416,
      "learning_rate": 4.349393019726859e-06,
      "loss": 1.6418,
      "step": 344
    },
    {
      "epoch": 0.5235204855842185,
      "grad_norm": 0.6080475449562073,
      "learning_rate": 4.347496206373294e-06,
      "loss": 1.564,
      "step": 345
    },
    {
      "epoch": 0.5250379362670713,
      "grad_norm": 0.7053048014640808,
      "learning_rate": 4.345599393019727e-06,
      "loss": 1.6668,
      "step": 346
    },
    {
      "epoch": 0.5265553869499241,
      "grad_norm": 0.5829393863677979,
      "learning_rate": 4.3437025796661615e-06,
      "loss": 1.5245,
      "step": 347
    },
    {
      "epoch": 0.5280728376327769,
      "grad_norm": 0.659785807132721,
      "learning_rate": 4.3418057663125954e-06,
      "loss": 1.6782,
      "step": 348
    },
    {
      "epoch": 0.5295902883156297,
      "grad_norm": 0.6494567394256592,
      "learning_rate": 4.339908952959029e-06,
      "loss": 1.557,
      "step": 349
    },
    {
      "epoch": 0.5311077389984825,
      "grad_norm": 0.6029826998710632,
      "learning_rate": 4.338012139605463e-06,
      "loss": 1.5985,
      "step": 350
    },
    {
      "epoch": 0.5326251896813353,
      "grad_norm": 0.6589860320091248,
      "learning_rate": 4.336115326251897e-06,
      "loss": 1.6312,
      "step": 351
    },
    {
      "epoch": 0.5341426403641881,
      "grad_norm": 0.6462012529373169,
      "learning_rate": 4.334218512898331e-06,
      "loss": 1.703,
      "step": 352
    },
    {
      "epoch": 0.5356600910470409,
      "grad_norm": 0.5754967331886292,
      "learning_rate": 4.332321699544765e-06,
      "loss": 1.4815,
      "step": 353
    },
    {
      "epoch": 0.5371775417298937,
      "grad_norm": 0.6358936429023743,
      "learning_rate": 4.330424886191199e-06,
      "loss": 1.6312,
      "step": 354
    },
    {
      "epoch": 0.5386949924127465,
      "grad_norm": 0.6988011598587036,
      "learning_rate": 4.328528072837633e-06,
      "loss": 1.6651,
      "step": 355
    },
    {
      "epoch": 0.5402124430955993,
      "grad_norm": 0.7649612426757812,
      "learning_rate": 4.326631259484067e-06,
      "loss": 1.5895,
      "step": 356
    },
    {
      "epoch": 0.5417298937784522,
      "grad_norm": 0.6667112112045288,
      "learning_rate": 4.324734446130502e-06,
      "loss": 1.6441,
      "step": 357
    },
    {
      "epoch": 0.543247344461305,
      "grad_norm": 0.6389675140380859,
      "learning_rate": 4.322837632776935e-06,
      "loss": 1.5773,
      "step": 358
    },
    {
      "epoch": 0.5447647951441578,
      "grad_norm": 0.5892399549484253,
      "learning_rate": 4.3209408194233695e-06,
      "loss": 1.5016,
      "step": 359
    },
    {
      "epoch": 0.5462822458270106,
      "grad_norm": 0.6620190739631653,
      "learning_rate": 4.319044006069803e-06,
      "loss": 1.7004,
      "step": 360
    },
    {
      "epoch": 0.5477996965098634,
      "grad_norm": 0.5675914883613586,
      "learning_rate": 4.317147192716237e-06,
      "loss": 1.4344,
      "step": 361
    },
    {
      "epoch": 0.5493171471927162,
      "grad_norm": 0.6111488938331604,
      "learning_rate": 4.315250379362671e-06,
      "loss": 1.5464,
      "step": 362
    },
    {
      "epoch": 0.5508345978755691,
      "grad_norm": 0.6048813462257385,
      "learning_rate": 4.313353566009105e-06,
      "loss": 1.4615,
      "step": 363
    },
    {
      "epoch": 0.5523520485584219,
      "grad_norm": 0.5753006935119629,
      "learning_rate": 4.311456752655539e-06,
      "loss": 1.3993,
      "step": 364
    },
    {
      "epoch": 0.5538694992412747,
      "grad_norm": 0.5622251629829407,
      "learning_rate": 4.309559939301973e-06,
      "loss": 1.4188,
      "step": 365
    },
    {
      "epoch": 0.5553869499241275,
      "grad_norm": 0.6520721316337585,
      "learning_rate": 4.307663125948407e-06,
      "loss": 1.6291,
      "step": 366
    },
    {
      "epoch": 0.5569044006069803,
      "grad_norm": 0.6782432198524475,
      "learning_rate": 4.305766312594841e-06,
      "loss": 1.6267,
      "step": 367
    },
    {
      "epoch": 0.5584218512898331,
      "grad_norm": 0.5875648260116577,
      "learning_rate": 4.303869499241275e-06,
      "loss": 1.5924,
      "step": 368
    },
    {
      "epoch": 0.5599393019726859,
      "grad_norm": 0.5952248573303223,
      "learning_rate": 4.3019726858877096e-06,
      "loss": 1.4957,
      "step": 369
    },
    {
      "epoch": 0.5614567526555387,
      "grad_norm": 0.6013773083686829,
      "learning_rate": 4.300075872534143e-06,
      "loss": 1.5182,
      "step": 370
    },
    {
      "epoch": 0.5629742033383915,
      "grad_norm": 0.6489008665084839,
      "learning_rate": 4.298179059180577e-06,
      "loss": 1.5891,
      "step": 371
    },
    {
      "epoch": 0.5644916540212443,
      "grad_norm": 0.6077738404273987,
      "learning_rate": 4.296282245827011e-06,
      "loss": 1.5018,
      "step": 372
    },
    {
      "epoch": 0.5660091047040972,
      "grad_norm": 0.5997204780578613,
      "learning_rate": 4.294385432473445e-06,
      "loss": 1.6221,
      "step": 373
    },
    {
      "epoch": 0.56752655538695,
      "grad_norm": 0.6385869383811951,
      "learning_rate": 4.292488619119879e-06,
      "loss": 1.6492,
      "step": 374
    },
    {
      "epoch": 0.5690440060698028,
      "grad_norm": 0.674286425113678,
      "learning_rate": 4.290591805766313e-06,
      "loss": 1.6759,
      "step": 375
    },
    {
      "epoch": 0.5705614567526556,
      "grad_norm": 0.6796429753303528,
      "learning_rate": 4.288694992412747e-06,
      "loss": 1.6528,
      "step": 376
    },
    {
      "epoch": 0.5720789074355084,
      "grad_norm": 0.5818497538566589,
      "learning_rate": 4.286798179059181e-06,
      "loss": 1.4726,
      "step": 377
    },
    {
      "epoch": 0.5735963581183612,
      "grad_norm": 0.5799040198326111,
      "learning_rate": 4.284901365705615e-06,
      "loss": 1.5087,
      "step": 378
    },
    {
      "epoch": 0.575113808801214,
      "grad_norm": 0.5773376226425171,
      "learning_rate": 4.283004552352049e-06,
      "loss": 1.4544,
      "step": 379
    },
    {
      "epoch": 0.5766312594840668,
      "grad_norm": 0.7010246515274048,
      "learning_rate": 4.281107738998483e-06,
      "loss": 1.7205,
      "step": 380
    },
    {
      "epoch": 0.5781487101669196,
      "grad_norm": 0.5501715540885925,
      "learning_rate": 4.2792109256449175e-06,
      "loss": 1.3953,
      "step": 381
    },
    {
      "epoch": 0.5796661608497724,
      "grad_norm": 0.6272751092910767,
      "learning_rate": 4.277314112291351e-06,
      "loss": 1.5798,
      "step": 382
    },
    {
      "epoch": 0.5811836115326252,
      "grad_norm": 0.6273860931396484,
      "learning_rate": 4.275417298937785e-06,
      "loss": 1.5526,
      "step": 383
    },
    {
      "epoch": 0.582701062215478,
      "grad_norm": 0.6368005275726318,
      "learning_rate": 4.2735204855842184e-06,
      "loss": 1.5621,
      "step": 384
    },
    {
      "epoch": 0.5842185128983308,
      "grad_norm": 0.7168992161750793,
      "learning_rate": 4.271623672230653e-06,
      "loss": 1.5933,
      "step": 385
    },
    {
      "epoch": 0.5857359635811836,
      "grad_norm": 0.5898253917694092,
      "learning_rate": 4.269726858877087e-06,
      "loss": 1.5936,
      "step": 386
    },
    {
      "epoch": 0.5872534142640364,
      "grad_norm": 0.6682179570198059,
      "learning_rate": 4.267830045523521e-06,
      "loss": 1.6403,
      "step": 387
    },
    {
      "epoch": 0.5887708649468892,
      "grad_norm": 0.6522286534309387,
      "learning_rate": 4.265933232169955e-06,
      "loss": 1.6288,
      "step": 388
    },
    {
      "epoch": 0.590288315629742,
      "grad_norm": 0.5876317024230957,
      "learning_rate": 4.264036418816389e-06,
      "loss": 1.5196,
      "step": 389
    },
    {
      "epoch": 0.5918057663125948,
      "grad_norm": 0.5987347960472107,
      "learning_rate": 4.262139605462823e-06,
      "loss": 1.4369,
      "step": 390
    },
    {
      "epoch": 0.5933232169954477,
      "grad_norm": 0.5804250836372375,
      "learning_rate": 4.260242792109257e-06,
      "loss": 1.512,
      "step": 391
    },
    {
      "epoch": 0.5948406676783005,
      "grad_norm": 0.5873432755470276,
      "learning_rate": 4.258345978755691e-06,
      "loss": 1.5183,
      "step": 392
    },
    {
      "epoch": 0.5963581183611533,
      "grad_norm": 0.519212007522583,
      "learning_rate": 4.2564491654021255e-06,
      "loss": 1.3264,
      "step": 393
    },
    {
      "epoch": 0.5978755690440061,
      "grad_norm": 0.6431684494018555,
      "learning_rate": 4.2545523520485585e-06,
      "loss": 1.5916,
      "step": 394
    },
    {
      "epoch": 0.5993930197268589,
      "grad_norm": 0.5589029788970947,
      "learning_rate": 4.252655538694993e-06,
      "loss": 1.486,
      "step": 395
    },
    {
      "epoch": 0.6009104704097117,
      "grad_norm": 0.6102592945098877,
      "learning_rate": 4.250758725341426e-06,
      "loss": 1.445,
      "step": 396
    },
    {
      "epoch": 0.6024279210925645,
      "grad_norm": 0.6401807069778442,
      "learning_rate": 4.24886191198786e-06,
      "loss": 1.5819,
      "step": 397
    },
    {
      "epoch": 0.6039453717754173,
      "grad_norm": 0.5536525249481201,
      "learning_rate": 4.246965098634295e-06,
      "loss": 1.3862,
      "step": 398
    },
    {
      "epoch": 0.6054628224582701,
      "grad_norm": 0.6206418871879578,
      "learning_rate": 4.245068285280728e-06,
      "loss": 1.6309,
      "step": 399
    },
    {
      "epoch": 0.6069802731411229,
      "grad_norm": 0.5591133236885071,
      "learning_rate": 4.243171471927163e-06,
      "loss": 1.4256,
      "step": 400
    },
    {
      "epoch": 0.6084977238239757,
      "grad_norm": 0.5957683324813843,
      "learning_rate": 4.241274658573596e-06,
      "loss": 1.5531,
      "step": 401
    },
    {
      "epoch": 0.6100151745068285,
      "grad_norm": 0.6135489344596863,
      "learning_rate": 4.239377845220031e-06,
      "loss": 1.5542,
      "step": 402
    },
    {
      "epoch": 0.6115326251896813,
      "grad_norm": 0.5628994703292847,
      "learning_rate": 4.237481031866465e-06,
      "loss": 1.256,
      "step": 403
    },
    {
      "epoch": 0.6130500758725341,
      "grad_norm": 0.6794199347496033,
      "learning_rate": 4.235584218512899e-06,
      "loss": 1.6253,
      "step": 404
    },
    {
      "epoch": 0.6145675265553869,
      "grad_norm": 0.6059722304344177,
      "learning_rate": 4.2336874051593326e-06,
      "loss": 1.5526,
      "step": 405
    },
    {
      "epoch": 0.6160849772382397,
      "grad_norm": 0.5813862085342407,
      "learning_rate": 4.2317905918057665e-06,
      "loss": 1.4667,
      "step": 406
    },
    {
      "epoch": 0.6176024279210925,
      "grad_norm": 0.5336403846740723,
      "learning_rate": 4.2298937784522004e-06,
      "loss": 1.3902,
      "step": 407
    },
    {
      "epoch": 0.6191198786039454,
      "grad_norm": 0.6165726780891418,
      "learning_rate": 4.227996965098634e-06,
      "loss": 1.5416,
      "step": 408
    },
    {
      "epoch": 0.6206373292867982,
      "grad_norm": 0.6131974458694458,
      "learning_rate": 4.226100151745068e-06,
      "loss": 1.5848,
      "step": 409
    },
    {
      "epoch": 0.622154779969651,
      "grad_norm": 0.6421288847923279,
      "learning_rate": 4.224203338391503e-06,
      "loss": 1.5381,
      "step": 410
    },
    {
      "epoch": 0.6236722306525038,
      "grad_norm": 0.6111606955528259,
      "learning_rate": 4.222306525037936e-06,
      "loss": 1.4852,
      "step": 411
    },
    {
      "epoch": 0.6251896813353566,
      "grad_norm": 0.6244936585426331,
      "learning_rate": 4.220409711684371e-06,
      "loss": 1.6041,
      "step": 412
    },
    {
      "epoch": 0.6267071320182094,
      "grad_norm": 0.5795992612838745,
      "learning_rate": 4.218512898330804e-06,
      "loss": 1.4083,
      "step": 413
    },
    {
      "epoch": 0.6282245827010622,
      "grad_norm": 0.5329789519309998,
      "learning_rate": 4.216616084977239e-06,
      "loss": 1.4516,
      "step": 414
    },
    {
      "epoch": 0.629742033383915,
      "grad_norm": 0.6105883121490479,
      "learning_rate": 4.214719271623673e-06,
      "loss": 1.5549,
      "step": 415
    },
    {
      "epoch": 0.6312594840667678,
      "grad_norm": 0.6784496903419495,
      "learning_rate": 4.212822458270107e-06,
      "loss": 1.498,
      "step": 416
    },
    {
      "epoch": 0.6327769347496206,
      "grad_norm": 0.6270522475242615,
      "learning_rate": 4.2109256449165405e-06,
      "loss": 1.5814,
      "step": 417
    },
    {
      "epoch": 0.6342943854324734,
      "grad_norm": 0.7026311755180359,
      "learning_rate": 4.2090288315629745e-06,
      "loss": 1.4227,
      "step": 418
    },
    {
      "epoch": 0.6358118361153262,
      "grad_norm": 0.6067810654640198,
      "learning_rate": 4.207132018209408e-06,
      "loss": 1.5249,
      "step": 419
    },
    {
      "epoch": 0.637329286798179,
      "grad_norm": 0.6042413115501404,
      "learning_rate": 4.205235204855842e-06,
      "loss": 1.5206,
      "step": 420
    },
    {
      "epoch": 0.6388467374810318,
      "grad_norm": 0.5449179410934448,
      "learning_rate": 4.203338391502276e-06,
      "loss": 1.3315,
      "step": 421
    },
    {
      "epoch": 0.6403641881638846,
      "grad_norm": 0.5952794551849365,
      "learning_rate": 4.20144157814871e-06,
      "loss": 1.5029,
      "step": 422
    },
    {
      "epoch": 0.6418816388467374,
      "grad_norm": 0.8663390278816223,
      "learning_rate": 4.199544764795144e-06,
      "loss": 1.4875,
      "step": 423
    },
    {
      "epoch": 0.6433990895295902,
      "grad_norm": 0.5865616202354431,
      "learning_rate": 4.197647951441579e-06,
      "loss": 1.5374,
      "step": 424
    },
    {
      "epoch": 0.644916540212443,
      "grad_norm": 0.691513180732727,
      "learning_rate": 4.195751138088012e-06,
      "loss": 1.5339,
      "step": 425
    },
    {
      "epoch": 0.6464339908952959,
      "grad_norm": 0.6584438681602478,
      "learning_rate": 4.193854324734447e-06,
      "loss": 1.6151,
      "step": 426
    },
    {
      "epoch": 0.6479514415781487,
      "grad_norm": 0.6268851161003113,
      "learning_rate": 4.191957511380881e-06,
      "loss": 1.5454,
      "step": 427
    },
    {
      "epoch": 0.6494688922610015,
      "grad_norm": 0.6054689884185791,
      "learning_rate": 4.1900606980273146e-06,
      "loss": 1.4949,
      "step": 428
    },
    {
      "epoch": 0.6509863429438544,
      "grad_norm": 0.6113342046737671,
      "learning_rate": 4.1881638846737485e-06,
      "loss": 1.4944,
      "step": 429
    },
    {
      "epoch": 0.6525037936267072,
      "grad_norm": 0.5686102509498596,
      "learning_rate": 4.186267071320182e-06,
      "loss": 1.4647,
      "step": 430
    },
    {
      "epoch": 0.65402124430956,
      "grad_norm": 0.6794170141220093,
      "learning_rate": 4.184370257966616e-06,
      "loss": 1.599,
      "step": 431
    },
    {
      "epoch": 0.6555386949924128,
      "grad_norm": 0.5625924468040466,
      "learning_rate": 4.18247344461305e-06,
      "loss": 1.4239,
      "step": 432
    },
    {
      "epoch": 0.6570561456752656,
      "grad_norm": 0.6523870229721069,
      "learning_rate": 4.180576631259484e-06,
      "loss": 1.4846,
      "step": 433
    },
    {
      "epoch": 0.6585735963581184,
      "grad_norm": 0.6677373051643372,
      "learning_rate": 4.178679817905918e-06,
      "loss": 1.589,
      "step": 434
    },
    {
      "epoch": 0.6600910470409712,
      "grad_norm": 0.6911047697067261,
      "learning_rate": 4.176783004552352e-06,
      "loss": 1.3712,
      "step": 435
    },
    {
      "epoch": 0.661608497723824,
      "grad_norm": 0.5621192455291748,
      "learning_rate": 4.174886191198787e-06,
      "loss": 1.4633,
      "step": 436
    },
    {
      "epoch": 0.6631259484066768,
      "grad_norm": 0.6397700309753418,
      "learning_rate": 4.17298937784522e-06,
      "loss": 1.5918,
      "step": 437
    },
    {
      "epoch": 0.6646433990895296,
      "grad_norm": 0.6388602256774902,
      "learning_rate": 4.171092564491655e-06,
      "loss": 1.6033,
      "step": 438
    },
    {
      "epoch": 0.6661608497723824,
      "grad_norm": 0.5333991050720215,
      "learning_rate": 4.169195751138089e-06,
      "loss": 1.3228,
      "step": 439
    },
    {
      "epoch": 0.6676783004552352,
      "grad_norm": 0.6580480933189392,
      "learning_rate": 4.1672989377845225e-06,
      "loss": 1.5677,
      "step": 440
    },
    {
      "epoch": 0.669195751138088,
      "grad_norm": 0.569733738899231,
      "learning_rate": 4.1654021244309564e-06,
      "loss": 1.3876,
      "step": 441
    },
    {
      "epoch": 0.6707132018209409,
      "grad_norm": 0.7972626686096191,
      "learning_rate": 4.16350531107739e-06,
      "loss": 1.4154,
      "step": 442
    },
    {
      "epoch": 0.6722306525037937,
      "grad_norm": 0.5728926658630371,
      "learning_rate": 4.161608497723824e-06,
      "loss": 1.4789,
      "step": 443
    },
    {
      "epoch": 0.6737481031866465,
      "grad_norm": 0.6520633697509766,
      "learning_rate": 4.159711684370258e-06,
      "loss": 1.5944,
      "step": 444
    },
    {
      "epoch": 0.6752655538694993,
      "grad_norm": 0.5996243953704834,
      "learning_rate": 4.157814871016692e-06,
      "loss": 1.4678,
      "step": 445
    },
    {
      "epoch": 0.6767830045523521,
      "grad_norm": 0.6323056221008301,
      "learning_rate": 4.155918057663126e-06,
      "loss": 1.5861,
      "step": 446
    },
    {
      "epoch": 0.6783004552352049,
      "grad_norm": 0.624593198299408,
      "learning_rate": 4.15402124430956e-06,
      "loss": 1.5041,
      "step": 447
    },
    {
      "epoch": 0.6798179059180577,
      "grad_norm": 0.6001309752464294,
      "learning_rate": 4.152124430955995e-06,
      "loss": 1.4915,
      "step": 448
    },
    {
      "epoch": 0.6813353566009105,
      "grad_norm": 1.016117811203003,
      "learning_rate": 4.150227617602428e-06,
      "loss": 1.4582,
      "step": 449
    },
    {
      "epoch": 0.6828528072837633,
      "grad_norm": 0.5604355335235596,
      "learning_rate": 4.148330804248863e-06,
      "loss": 1.368,
      "step": 450
    },
    {
      "epoch": 0.6843702579666161,
      "grad_norm": 0.6250974535942078,
      "learning_rate": 4.146433990895296e-06,
      "loss": 1.4982,
      "step": 451
    },
    {
      "epoch": 0.6858877086494689,
      "grad_norm": 0.5715218782424927,
      "learning_rate": 4.1445371775417305e-06,
      "loss": 1.4279,
      "step": 452
    },
    {
      "epoch": 0.6874051593323217,
      "grad_norm": 0.5855033993721008,
      "learning_rate": 4.142640364188164e-06,
      "loss": 1.4185,
      "step": 453
    },
    {
      "epoch": 0.6889226100151745,
      "grad_norm": 0.5902236104011536,
      "learning_rate": 4.140743550834598e-06,
      "loss": 1.3486,
      "step": 454
    },
    {
      "epoch": 0.6904400606980273,
      "grad_norm": 0.6512507200241089,
      "learning_rate": 4.138846737481032e-06,
      "loss": 1.4809,
      "step": 455
    },
    {
      "epoch": 0.6919575113808801,
      "grad_norm": 0.6085068583488464,
      "learning_rate": 4.136949924127466e-06,
      "loss": 1.4716,
      "step": 456
    },
    {
      "epoch": 0.6934749620637329,
      "grad_norm": 0.6189948320388794,
      "learning_rate": 4.1350531107739e-06,
      "loss": 1.5206,
      "step": 457
    },
    {
      "epoch": 0.6949924127465857,
      "grad_norm": 0.5384073257446289,
      "learning_rate": 4.133156297420334e-06,
      "loss": 1.2781,
      "step": 458
    },
    {
      "epoch": 0.6965098634294385,
      "grad_norm": 0.5965737104415894,
      "learning_rate": 4.131259484066768e-06,
      "loss": 1.4368,
      "step": 459
    },
    {
      "epoch": 0.6980273141122914,
      "grad_norm": 0.6021190881729126,
      "learning_rate": 4.129362670713203e-06,
      "loss": 1.4737,
      "step": 460
    },
    {
      "epoch": 0.6995447647951442,
      "grad_norm": 0.5780550241470337,
      "learning_rate": 4.127465857359636e-06,
      "loss": 1.4247,
      "step": 461
    },
    {
      "epoch": 0.701062215477997,
      "grad_norm": 0.6187347769737244,
      "learning_rate": 4.1255690440060706e-06,
      "loss": 1.4389,
      "step": 462
    },
    {
      "epoch": 0.7025796661608498,
      "grad_norm": 0.5627894401550293,
      "learning_rate": 4.123672230652504e-06,
      "loss": 1.398,
      "step": 463
    },
    {
      "epoch": 0.7040971168437026,
      "grad_norm": 0.6145464181900024,
      "learning_rate": 4.121775417298938e-06,
      "loss": 1.4237,
      "step": 464
    },
    {
      "epoch": 0.7056145675265554,
      "grad_norm": 0.549309492111206,
      "learning_rate": 4.119878603945372e-06,
      "loss": 1.4242,
      "step": 465
    },
    {
      "epoch": 0.7071320182094082,
      "grad_norm": 0.664556622505188,
      "learning_rate": 4.117981790591806e-06,
      "loss": 1.5676,
      "step": 466
    },
    {
      "epoch": 0.708649468892261,
      "grad_norm": 0.6190958023071289,
      "learning_rate": 4.11608497723824e-06,
      "loss": 1.5005,
      "step": 467
    },
    {
      "epoch": 0.7101669195751138,
      "grad_norm": 0.6045020818710327,
      "learning_rate": 4.114188163884674e-06,
      "loss": 1.3927,
      "step": 468
    },
    {
      "epoch": 0.7116843702579666,
      "grad_norm": 0.6042170524597168,
      "learning_rate": 4.112291350531108e-06,
      "loss": 1.5092,
      "step": 469
    },
    {
      "epoch": 0.7132018209408194,
      "grad_norm": 0.576060950756073,
      "learning_rate": 4.110394537177542e-06,
      "loss": 1.4272,
      "step": 470
    },
    {
      "epoch": 0.7147192716236722,
      "grad_norm": 0.6165503263473511,
      "learning_rate": 4.108497723823976e-06,
      "loss": 1.3731,
      "step": 471
    },
    {
      "epoch": 0.716236722306525,
      "grad_norm": 0.6037637591362,
      "learning_rate": 4.10660091047041e-06,
      "loss": 1.4771,
      "step": 472
    },
    {
      "epoch": 0.7177541729893778,
      "grad_norm": 0.6072092652320862,
      "learning_rate": 4.104704097116844e-06,
      "loss": 1.4237,
      "step": 473
    },
    {
      "epoch": 0.7192716236722306,
      "grad_norm": 0.5861555933952332,
      "learning_rate": 4.1028072837632785e-06,
      "loss": 1.4567,
      "step": 474
    },
    {
      "epoch": 0.7207890743550834,
      "grad_norm": 0.5880296230316162,
      "learning_rate": 4.100910470409712e-06,
      "loss": 1.4405,
      "step": 475
    },
    {
      "epoch": 0.7223065250379362,
      "grad_norm": 0.6067490577697754,
      "learning_rate": 4.099013657056146e-06,
      "loss": 1.3684,
      "step": 476
    },
    {
      "epoch": 0.723823975720789,
      "grad_norm": 0.502601146697998,
      "learning_rate": 4.09711684370258e-06,
      "loss": 1.2469,
      "step": 477
    },
    {
      "epoch": 0.7253414264036419,
      "grad_norm": 0.6065641045570374,
      "learning_rate": 4.095220030349014e-06,
      "loss": 1.4532,
      "step": 478
    },
    {
      "epoch": 0.7268588770864947,
      "grad_norm": 0.615712583065033,
      "learning_rate": 4.093323216995448e-06,
      "loss": 1.4856,
      "step": 479
    },
    {
      "epoch": 0.7283763277693475,
      "grad_norm": 0.5769491195678711,
      "learning_rate": 4.091426403641882e-06,
      "loss": 1.3765,
      "step": 480
    },
    {
      "epoch": 0.7298937784522003,
      "grad_norm": 0.6346235871315002,
      "learning_rate": 4.089529590288316e-06,
      "loss": 1.5439,
      "step": 481
    },
    {
      "epoch": 0.7314112291350531,
      "grad_norm": 0.6479841470718384,
      "learning_rate": 4.08763277693475e-06,
      "loss": 1.5848,
      "step": 482
    },
    {
      "epoch": 0.7329286798179059,
      "grad_norm": 0.6207441091537476,
      "learning_rate": 4.085735963581184e-06,
      "loss": 1.5022,
      "step": 483
    },
    {
      "epoch": 0.7344461305007587,
      "grad_norm": 0.8650970458984375,
      "learning_rate": 4.083839150227618e-06,
      "loss": 1.518,
      "step": 484
    },
    {
      "epoch": 0.7359635811836115,
      "grad_norm": 0.6647722125053406,
      "learning_rate": 4.081942336874052e-06,
      "loss": 1.5028,
      "step": 485
    },
    {
      "epoch": 0.7374810318664643,
      "grad_norm": 0.6126411557197571,
      "learning_rate": 4.0800455235204865e-06,
      "loss": 1.4048,
      "step": 486
    },
    {
      "epoch": 0.7389984825493171,
      "grad_norm": 0.5503905415534973,
      "learning_rate": 4.0781487101669195e-06,
      "loss": 1.3522,
      "step": 487
    },
    {
      "epoch": 0.7405159332321699,
      "grad_norm": 0.5794267654418945,
      "learning_rate": 4.076251896813354e-06,
      "loss": 1.4158,
      "step": 488
    },
    {
      "epoch": 0.7420333839150227,
      "grad_norm": 0.5572088956832886,
      "learning_rate": 4.074355083459787e-06,
      "loss": 1.3702,
      "step": 489
    },
    {
      "epoch": 0.7435508345978755,
      "grad_norm": 0.5304098129272461,
      "learning_rate": 4.072458270106222e-06,
      "loss": 1.2865,
      "step": 490
    },
    {
      "epoch": 0.7450682852807283,
      "grad_norm": 0.6229831576347351,
      "learning_rate": 4.070561456752656e-06,
      "loss": 1.4667,
      "step": 491
    },
    {
      "epoch": 0.7465857359635811,
      "grad_norm": 0.6230302453041077,
      "learning_rate": 4.06866464339909e-06,
      "loss": 1.3795,
      "step": 492
    },
    {
      "epoch": 0.7481031866464339,
      "grad_norm": 0.615913987159729,
      "learning_rate": 4.066767830045524e-06,
      "loss": 1.4757,
      "step": 493
    },
    {
      "epoch": 0.7496206373292867,
      "grad_norm": 0.558756947517395,
      "learning_rate": 4.064871016691958e-06,
      "loss": 1.4081,
      "step": 494
    },
    {
      "epoch": 0.7511380880121397,
      "grad_norm": 0.5744472146034241,
      "learning_rate": 4.062974203338392e-06,
      "loss": 1.3198,
      "step": 495
    },
    {
      "epoch": 0.7526555386949925,
      "grad_norm": 0.5285035967826843,
      "learning_rate": 4.061077389984826e-06,
      "loss": 1.2591,
      "step": 496
    },
    {
      "epoch": 0.7541729893778453,
      "grad_norm": 0.6275262832641602,
      "learning_rate": 4.05918057663126e-06,
      "loss": 1.3969,
      "step": 497
    },
    {
      "epoch": 0.7556904400606981,
      "grad_norm": 0.6195974946022034,
      "learning_rate": 4.057283763277694e-06,
      "loss": 1.4356,
      "step": 498
    },
    {
      "epoch": 0.7572078907435509,
      "grad_norm": 0.5683231353759766,
      "learning_rate": 4.0553869499241275e-06,
      "loss": 1.3526,
      "step": 499
    },
    {
      "epoch": 0.7587253414264037,
      "grad_norm": 0.5529468655586243,
      "learning_rate": 4.053490136570562e-06,
      "loss": 1.2037,
      "step": 500
    },
    {
      "epoch": 0.7602427921092565,
      "grad_norm": 0.618276834487915,
      "learning_rate": 4.051593323216995e-06,
      "loss": 1.4638,
      "step": 501
    },
    {
      "epoch": 0.7617602427921093,
      "grad_norm": 0.8534848690032959,
      "learning_rate": 4.04969650986343e-06,
      "loss": 1.4245,
      "step": 502
    },
    {
      "epoch": 0.7632776934749621,
      "grad_norm": 0.5818397402763367,
      "learning_rate": 4.047799696509864e-06,
      "loss": 1.3874,
      "step": 503
    },
    {
      "epoch": 0.7647951441578149,
      "grad_norm": 0.6175028085708618,
      "learning_rate": 4.045902883156298e-06,
      "loss": 1.3843,
      "step": 504
    },
    {
      "epoch": 0.7663125948406677,
      "grad_norm": 0.5756270885467529,
      "learning_rate": 4.044006069802732e-06,
      "loss": 1.3891,
      "step": 505
    },
    {
      "epoch": 0.7678300455235205,
      "grad_norm": 0.6346330642700195,
      "learning_rate": 4.042109256449166e-06,
      "loss": 1.4675,
      "step": 506
    },
    {
      "epoch": 0.7693474962063733,
      "grad_norm": 0.7040528059005737,
      "learning_rate": 4.0402124430956e-06,
      "loss": 1.4573,
      "step": 507
    },
    {
      "epoch": 0.7708649468892261,
      "grad_norm": 0.495614230632782,
      "learning_rate": 4.038315629742034e-06,
      "loss": 1.2473,
      "step": 508
    },
    {
      "epoch": 0.7723823975720789,
      "grad_norm": 0.7733967900276184,
      "learning_rate": 4.036418816388468e-06,
      "loss": 1.4963,
      "step": 509
    },
    {
      "epoch": 0.7738998482549317,
      "grad_norm": 0.5844035148620605,
      "learning_rate": 4.0345220030349015e-06,
      "loss": 1.4291,
      "step": 510
    },
    {
      "epoch": 0.7754172989377845,
      "grad_norm": 0.569818377494812,
      "learning_rate": 4.0326251896813355e-06,
      "loss": 1.3059,
      "step": 511
    },
    {
      "epoch": 0.7769347496206374,
      "grad_norm": 0.6232802271842957,
      "learning_rate": 4.03072837632777e-06,
      "loss": 1.3766,
      "step": 512
    },
    {
      "epoch": 0.7784522003034902,
      "grad_norm": 0.6503587365150452,
      "learning_rate": 4.028831562974203e-06,
      "loss": 1.4921,
      "step": 513
    },
    {
      "epoch": 0.779969650986343,
      "grad_norm": 0.5812511444091797,
      "learning_rate": 4.026934749620638e-06,
      "loss": 1.408,
      "step": 514
    },
    {
      "epoch": 0.7814871016691958,
      "grad_norm": 0.6303573846817017,
      "learning_rate": 4.025037936267072e-06,
      "loss": 1.4471,
      "step": 515
    },
    {
      "epoch": 0.7830045523520486,
      "grad_norm": 0.6138178706169128,
      "learning_rate": 4.023141122913506e-06,
      "loss": 1.4152,
      "step": 516
    },
    {
      "epoch": 0.7845220030349014,
      "grad_norm": 0.641509473323822,
      "learning_rate": 4.02124430955994e-06,
      "loss": 1.4499,
      "step": 517
    },
    {
      "epoch": 0.7860394537177542,
      "grad_norm": 0.6031009554862976,
      "learning_rate": 4.019347496206374e-06,
      "loss": 1.4435,
      "step": 518
    },
    {
      "epoch": 0.787556904400607,
      "grad_norm": 0.5652389526367188,
      "learning_rate": 4.017450682852808e-06,
      "loss": 1.3292,
      "step": 519
    },
    {
      "epoch": 0.7890743550834598,
      "grad_norm": 0.6178549528121948,
      "learning_rate": 4.015553869499242e-06,
      "loss": 1.3684,
      "step": 520
    },
    {
      "epoch": 0.7905918057663126,
      "grad_norm": 0.6409093141555786,
      "learning_rate": 4.0136570561456756e-06,
      "loss": 1.4263,
      "step": 521
    },
    {
      "epoch": 0.7921092564491654,
      "grad_norm": 0.6351940035820007,
      "learning_rate": 4.0117602427921095e-06,
      "loss": 1.4216,
      "step": 522
    },
    {
      "epoch": 0.7936267071320182,
      "grad_norm": 0.6440988779067993,
      "learning_rate": 4.009863429438543e-06,
      "loss": 1.5016,
      "step": 523
    },
    {
      "epoch": 0.795144157814871,
      "grad_norm": 0.5990493297576904,
      "learning_rate": 4.007966616084978e-06,
      "loss": 1.4458,
      "step": 524
    },
    {
      "epoch": 0.7966616084977238,
      "grad_norm": 0.6305130124092102,
      "learning_rate": 4.006069802731411e-06,
      "loss": 1.4271,
      "step": 525
    },
    {
      "epoch": 0.7981790591805766,
      "grad_norm": 0.6293860077857971,
      "learning_rate": 4.004172989377846e-06,
      "loss": 1.476,
      "step": 526
    },
    {
      "epoch": 0.7996965098634294,
      "grad_norm": 0.6680485606193542,
      "learning_rate": 4.00227617602428e-06,
      "loss": 1.5056,
      "step": 527
    },
    {
      "epoch": 0.8012139605462822,
      "grad_norm": 0.5935494899749756,
      "learning_rate": 4.000379362670714e-06,
      "loss": 1.3039,
      "step": 528
    },
    {
      "epoch": 0.802731411229135,
      "grad_norm": 0.4867812693119049,
      "learning_rate": 3.998482549317148e-06,
      "loss": 1.215,
      "step": 529
    },
    {
      "epoch": 0.8042488619119879,
      "grad_norm": 0.6818399429321289,
      "learning_rate": 3.996585735963581e-06,
      "loss": 1.4988,
      "step": 530
    },
    {
      "epoch": 0.8057663125948407,
      "grad_norm": 0.5831763744354248,
      "learning_rate": 3.994688922610016e-06,
      "loss": 1.3724,
      "step": 531
    },
    {
      "epoch": 0.8072837632776935,
      "grad_norm": 0.7987034320831299,
      "learning_rate": 3.99279210925645e-06,
      "loss": 1.4467,
      "step": 532
    },
    {
      "epoch": 0.8088012139605463,
      "grad_norm": 0.6285800337791443,
      "learning_rate": 3.9908952959028835e-06,
      "loss": 1.4248,
      "step": 533
    },
    {
      "epoch": 0.8103186646433991,
      "grad_norm": 0.5647488236427307,
      "learning_rate": 3.9889984825493174e-06,
      "loss": 1.335,
      "step": 534
    },
    {
      "epoch": 0.8118361153262519,
      "grad_norm": 0.5904282927513123,
      "learning_rate": 3.987101669195751e-06,
      "loss": 1.3873,
      "step": 535
    },
    {
      "epoch": 0.8133535660091047,
      "grad_norm": 0.6176469326019287,
      "learning_rate": 3.985204855842185e-06,
      "loss": 1.3568,
      "step": 536
    },
    {
      "epoch": 0.8148710166919575,
      "grad_norm": 0.5715978741645813,
      "learning_rate": 3.983308042488619e-06,
      "loss": 1.3058,
      "step": 537
    },
    {
      "epoch": 0.8163884673748103,
      "grad_norm": 0.6868320107460022,
      "learning_rate": 3.981411229135053e-06,
      "loss": 1.4874,
      "step": 538
    },
    {
      "epoch": 0.8179059180576631,
      "grad_norm": 0.6232314109802246,
      "learning_rate": 3.979514415781487e-06,
      "loss": 1.469,
      "step": 539
    },
    {
      "epoch": 0.8194233687405159,
      "grad_norm": 0.6806364059448242,
      "learning_rate": 3.977617602427921e-06,
      "loss": 1.4819,
      "step": 540
    },
    {
      "epoch": 0.8209408194233687,
      "grad_norm": 0.6513505578041077,
      "learning_rate": 3.975720789074356e-06,
      "loss": 1.3962,
      "step": 541
    },
    {
      "epoch": 0.8224582701062215,
      "grad_norm": 0.6395048499107361,
      "learning_rate": 3.973823975720789e-06,
      "loss": 1.4295,
      "step": 542
    },
    {
      "epoch": 0.8239757207890743,
      "grad_norm": 0.6198773980140686,
      "learning_rate": 3.971927162367224e-06,
      "loss": 1.4551,
      "step": 543
    },
    {
      "epoch": 0.8254931714719271,
      "grad_norm": 0.6365298628807068,
      "learning_rate": 3.9700303490136575e-06,
      "loss": 1.4254,
      "step": 544
    },
    {
      "epoch": 0.8270106221547799,
      "grad_norm": 0.5582196116447449,
      "learning_rate": 3.9681335356600915e-06,
      "loss": 1.3482,
      "step": 545
    },
    {
      "epoch": 0.8285280728376327,
      "grad_norm": 0.6099219918251038,
      "learning_rate": 3.966236722306525e-06,
      "loss": 1.4507,
      "step": 546
    },
    {
      "epoch": 0.8300455235204856,
      "grad_norm": 0.6254249811172485,
      "learning_rate": 3.964339908952959e-06,
      "loss": 1.3338,
      "step": 547
    },
    {
      "epoch": 0.8315629742033384,
      "grad_norm": 0.5973380208015442,
      "learning_rate": 3.962443095599393e-06,
      "loss": 1.2515,
      "step": 548
    },
    {
      "epoch": 0.8330804248861912,
      "grad_norm": 0.6490448713302612,
      "learning_rate": 3.960546282245827e-06,
      "loss": 1.3491,
      "step": 549
    },
    {
      "epoch": 0.834597875569044,
      "grad_norm": 0.6257760524749756,
      "learning_rate": 3.958649468892261e-06,
      "loss": 1.321,
      "step": 550
    },
    {
      "epoch": 0.8361153262518968,
      "grad_norm": 0.5839082598686218,
      "learning_rate": 3.956752655538695e-06,
      "loss": 1.3021,
      "step": 551
    },
    {
      "epoch": 0.8376327769347496,
      "grad_norm": 0.6365636587142944,
      "learning_rate": 3.954855842185129e-06,
      "loss": 1.4417,
      "step": 552
    },
    {
      "epoch": 0.8391502276176024,
      "grad_norm": 0.6202276945114136,
      "learning_rate": 3.952959028831564e-06,
      "loss": 1.3949,
      "step": 553
    },
    {
      "epoch": 0.8406676783004552,
      "grad_norm": 0.6429421305656433,
      "learning_rate": 3.951062215477997e-06,
      "loss": 1.3491,
      "step": 554
    },
    {
      "epoch": 0.842185128983308,
      "grad_norm": 0.751859188079834,
      "learning_rate": 3.9491654021244316e-06,
      "loss": 1.4926,
      "step": 555
    },
    {
      "epoch": 0.8437025796661608,
      "grad_norm": 0.596811056137085,
      "learning_rate": 3.947268588770865e-06,
      "loss": 1.3483,
      "step": 556
    },
    {
      "epoch": 0.8452200303490136,
      "grad_norm": 0.6021826863288879,
      "learning_rate": 3.945371775417299e-06,
      "loss": 1.3812,
      "step": 557
    },
    {
      "epoch": 0.8467374810318664,
      "grad_norm": 0.5573948621749878,
      "learning_rate": 3.943474962063733e-06,
      "loss": 1.2867,
      "step": 558
    },
    {
      "epoch": 0.8482549317147192,
      "grad_norm": 0.5277321338653564,
      "learning_rate": 3.941578148710167e-06,
      "loss": 1.293,
      "step": 559
    },
    {
      "epoch": 0.849772382397572,
      "grad_norm": 0.6406909823417664,
      "learning_rate": 3.939681335356601e-06,
      "loss": 1.4151,
      "step": 560
    },
    {
      "epoch": 0.8512898330804249,
      "grad_norm": 0.6672632694244385,
      "learning_rate": 3.937784522003035e-06,
      "loss": 1.4212,
      "step": 561
    },
    {
      "epoch": 0.8528072837632777,
      "grad_norm": 0.6269914507865906,
      "learning_rate": 3.935887708649469e-06,
      "loss": 1.4476,
      "step": 562
    },
    {
      "epoch": 0.8543247344461306,
      "grad_norm": 0.6455856561660767,
      "learning_rate": 3.933990895295903e-06,
      "loss": 1.4116,
      "step": 563
    },
    {
      "epoch": 0.8558421851289834,
      "grad_norm": 0.6107638478279114,
      "learning_rate": 3.932094081942337e-06,
      "loss": 1.3495,
      "step": 564
    },
    {
      "epoch": 0.8573596358118362,
      "grad_norm": 0.5747246146202087,
      "learning_rate": 3.930197268588772e-06,
      "loss": 1.3429,
      "step": 565
    },
    {
      "epoch": 0.858877086494689,
      "grad_norm": 0.6526166200637817,
      "learning_rate": 3.928300455235205e-06,
      "loss": 1.3956,
      "step": 566
    },
    {
      "epoch": 0.8603945371775418,
      "grad_norm": 0.6660044193267822,
      "learning_rate": 3.9264036418816395e-06,
      "loss": 1.3814,
      "step": 567
    },
    {
      "epoch": 0.8619119878603946,
      "grad_norm": 0.6197123527526855,
      "learning_rate": 3.924506828528073e-06,
      "loss": 1.3228,
      "step": 568
    },
    {
      "epoch": 0.8634294385432474,
      "grad_norm": 0.674644410610199,
      "learning_rate": 3.922610015174507e-06,
      "loss": 1.3911,
      "step": 569
    },
    {
      "epoch": 0.8649468892261002,
      "grad_norm": 0.5645696520805359,
      "learning_rate": 3.920713201820941e-06,
      "loss": 1.2936,
      "step": 570
    },
    {
      "epoch": 0.866464339908953,
      "grad_norm": 0.5964365601539612,
      "learning_rate": 3.918816388467375e-06,
      "loss": 1.2325,
      "step": 571
    },
    {
      "epoch": 0.8679817905918058,
      "grad_norm": 0.6039345264434814,
      "learning_rate": 3.916919575113809e-06,
      "loss": 1.3159,
      "step": 572
    },
    {
      "epoch": 0.8694992412746586,
      "grad_norm": 0.6536744236946106,
      "learning_rate": 3.915022761760243e-06,
      "loss": 1.3732,
      "step": 573
    },
    {
      "epoch": 0.8710166919575114,
      "grad_norm": 0.5987400412559509,
      "learning_rate": 3.913125948406677e-06,
      "loss": 1.3266,
      "step": 574
    },
    {
      "epoch": 0.8725341426403642,
      "grad_norm": 0.667483925819397,
      "learning_rate": 3.911229135053111e-06,
      "loss": 1.4169,
      "step": 575
    },
    {
      "epoch": 0.874051593323217,
      "grad_norm": 0.6307308673858643,
      "learning_rate": 3.909332321699545e-06,
      "loss": 1.3141,
      "step": 576
    },
    {
      "epoch": 0.8755690440060698,
      "grad_norm": 0.6388614177703857,
      "learning_rate": 3.907435508345979e-06,
      "loss": 1.3481,
      "step": 577
    },
    {
      "epoch": 0.8770864946889226,
      "grad_norm": 0.6315230131149292,
      "learning_rate": 3.905538694992413e-06,
      "loss": 1.312,
      "step": 578
    },
    {
      "epoch": 0.8786039453717754,
      "grad_norm": 0.6739354133605957,
      "learning_rate": 3.9036418816388475e-06,
      "loss": 1.4257,
      "step": 579
    },
    {
      "epoch": 0.8801213960546282,
      "grad_norm": 0.5204105377197266,
      "learning_rate": 3.9017450682852805e-06,
      "loss": 1.1494,
      "step": 580
    },
    {
      "epoch": 0.881638846737481,
      "grad_norm": 0.6523438692092896,
      "learning_rate": 3.899848254931715e-06,
      "loss": 1.3847,
      "step": 581
    },
    {
      "epoch": 0.8831562974203339,
      "grad_norm": 0.5951887965202332,
      "learning_rate": 3.897951441578149e-06,
      "loss": 1.2102,
      "step": 582
    },
    {
      "epoch": 0.8846737481031867,
      "grad_norm": 0.5549905300140381,
      "learning_rate": 3.896054628224583e-06,
      "loss": 1.201,
      "step": 583
    },
    {
      "epoch": 0.8861911987860395,
      "grad_norm": 0.5926841497421265,
      "learning_rate": 3.894157814871017e-06,
      "loss": 1.345,
      "step": 584
    },
    {
      "epoch": 0.8877086494688923,
      "grad_norm": 0.5181722044944763,
      "learning_rate": 3.892261001517451e-06,
      "loss": 1.2058,
      "step": 585
    },
    {
      "epoch": 0.8892261001517451,
      "grad_norm": 0.5920351147651672,
      "learning_rate": 3.890364188163885e-06,
      "loss": 1.2738,
      "step": 586
    },
    {
      "epoch": 0.8907435508345979,
      "grad_norm": 0.6103830337524414,
      "learning_rate": 3.888467374810319e-06,
      "loss": 1.3205,
      "step": 587
    },
    {
      "epoch": 0.8922610015174507,
      "grad_norm": 0.6151667833328247,
      "learning_rate": 3.886570561456753e-06,
      "loss": 1.3101,
      "step": 588
    },
    {
      "epoch": 0.8937784522003035,
      "grad_norm": 0.6015356183052063,
      "learning_rate": 3.884673748103187e-06,
      "loss": 1.2838,
      "step": 589
    },
    {
      "epoch": 0.8952959028831563,
      "grad_norm": 0.7068501710891724,
      "learning_rate": 3.882776934749621e-06,
      "loss": 1.4536,
      "step": 590
    },
    {
      "epoch": 0.8968133535660091,
      "grad_norm": 0.812031626701355,
      "learning_rate": 3.880880121396055e-06,
      "loss": 1.4406,
      "step": 591
    },
    {
      "epoch": 0.8983308042488619,
      "grad_norm": 0.6442255973815918,
      "learning_rate": 3.8789833080424885e-06,
      "loss": 1.3435,
      "step": 592
    },
    {
      "epoch": 0.8998482549317147,
      "grad_norm": 0.7269890308380127,
      "learning_rate": 3.877086494688923e-06,
      "loss": 1.3977,
      "step": 593
    },
    {
      "epoch": 0.9013657056145675,
      "grad_norm": 0.6264085173606873,
      "learning_rate": 3.875189681335357e-06,
      "loss": 1.3736,
      "step": 594
    },
    {
      "epoch": 0.9028831562974203,
      "grad_norm": 0.6346254348754883,
      "learning_rate": 3.873292867981791e-06,
      "loss": 1.3738,
      "step": 595
    },
    {
      "epoch": 0.9044006069802731,
      "grad_norm": 0.6701223850250244,
      "learning_rate": 3.871396054628225e-06,
      "loss": 1.3736,
      "step": 596
    },
    {
      "epoch": 0.9059180576631259,
      "grad_norm": 0.649634838104248,
      "learning_rate": 3.869499241274659e-06,
      "loss": 1.3243,
      "step": 597
    },
    {
      "epoch": 0.9074355083459787,
      "grad_norm": 0.6022031307220459,
      "learning_rate": 3.867602427921093e-06,
      "loss": 1.2971,
      "step": 598
    },
    {
      "epoch": 0.9089529590288316,
      "grad_norm": 0.656579315662384,
      "learning_rate": 3.865705614567527e-06,
      "loss": 1.3968,
      "step": 599
    },
    {
      "epoch": 0.9104704097116844,
      "grad_norm": 0.6894131302833557,
      "learning_rate": 3.863808801213961e-06,
      "loss": 1.3791,
      "step": 600
    },
    {
      "epoch": 0.9119878603945372,
      "grad_norm": 0.6044033169746399,
      "learning_rate": 3.861911987860395e-06,
      "loss": 1.2534,
      "step": 601
    },
    {
      "epoch": 0.91350531107739,
      "grad_norm": 0.6499491333961487,
      "learning_rate": 3.860015174506829e-06,
      "loss": 1.2643,
      "step": 602
    },
    {
      "epoch": 0.9150227617602428,
      "grad_norm": 0.6297010779380798,
      "learning_rate": 3.858118361153263e-06,
      "loss": 1.2873,
      "step": 603
    },
    {
      "epoch": 0.9165402124430956,
      "grad_norm": 0.5303326845169067,
      "learning_rate": 3.8562215477996965e-06,
      "loss": 1.1415,
      "step": 604
    },
    {
      "epoch": 0.9180576631259484,
      "grad_norm": NaN,
      "learning_rate": 3.8562215477996965e-06,
      "loss": 1.4932,
      "step": 605
    },
    {
      "epoch": 0.9195751138088012,
      "grad_norm": 0.6612173914909363,
      "learning_rate": 3.854324734446131e-06,
      "loss": 1.3262,
      "step": 606
    },
    {
      "epoch": 0.921092564491654,
      "grad_norm": 0.49130645394325256,
      "learning_rate": 3.852427921092564e-06,
      "loss": 1.132,
      "step": 607
    },
    {
      "epoch": 0.9226100151745068,
      "grad_norm": 0.6650311350822449,
      "learning_rate": 3.850531107738999e-06,
      "loss": 1.3305,
      "step": 608
    },
    {
      "epoch": 0.9241274658573596,
      "grad_norm": 0.6545456647872925,
      "learning_rate": 3.848634294385433e-06,
      "loss": 1.3373,
      "step": 609
    },
    {
      "epoch": 0.9256449165402124,
      "grad_norm": 0.6846172213554382,
      "learning_rate": 3.846737481031867e-06,
      "loss": 1.3746,
      "step": 610
    },
    {
      "epoch": 0.9271623672230652,
      "grad_norm": 0.6039798259735107,
      "learning_rate": 3.844840667678301e-06,
      "loss": 1.2711,
      "step": 611
    },
    {
      "epoch": 0.928679817905918,
      "grad_norm": 0.6218371391296387,
      "learning_rate": 3.842943854324735e-06,
      "loss": 1.2404,
      "step": 612
    },
    {
      "epoch": 0.9301972685887708,
      "grad_norm": 0.6878196597099304,
      "learning_rate": 3.841047040971169e-06,
      "loss": 1.2852,
      "step": 613
    },
    {
      "epoch": 0.9317147192716236,
      "grad_norm": 0.6313140392303467,
      "learning_rate": 3.839150227617603e-06,
      "loss": 1.3082,
      "step": 614
    },
    {
      "epoch": 0.9332321699544764,
      "grad_norm": 0.7005747556686401,
      "learning_rate": 3.8372534142640366e-06,
      "loss": 1.387,
      "step": 615
    },
    {
      "epoch": 0.9347496206373292,
      "grad_norm": 0.6920257210731506,
      "learning_rate": 3.835356600910471e-06,
      "loss": 1.3443,
      "step": 616
    },
    {
      "epoch": 0.936267071320182,
      "grad_norm": 0.6604538559913635,
      "learning_rate": 3.833459787556904e-06,
      "loss": 1.3303,
      "step": 617
    },
    {
      "epoch": 0.9377845220030349,
      "grad_norm": 0.6003352403640747,
      "learning_rate": 3.831562974203339e-06,
      "loss": 1.2691,
      "step": 618
    },
    {
      "epoch": 0.9393019726858877,
      "grad_norm": 0.6682949066162109,
      "learning_rate": 3.829666160849772e-06,
      "loss": 1.3364,
      "step": 619
    },
    {
      "epoch": 0.9408194233687405,
      "grad_norm": 0.6774202585220337,
      "learning_rate": 3.827769347496207e-06,
      "loss": 1.3624,
      "step": 620
    },
    {
      "epoch": 0.9423368740515933,
      "grad_norm": 0.6771272420883179,
      "learning_rate": 3.825872534142641e-06,
      "loss": 1.2984,
      "step": 621
    },
    {
      "epoch": 0.9438543247344461,
      "grad_norm": 0.7217581272125244,
      "learning_rate": 3.823975720789075e-06,
      "loss": 1.2977,
      "step": 622
    },
    {
      "epoch": 0.9453717754172989,
      "grad_norm": 0.6390746235847473,
      "learning_rate": 3.822078907435509e-06,
      "loss": 1.294,
      "step": 623
    },
    {
      "epoch": 0.9468892261001517,
      "grad_norm": 0.6669262051582336,
      "learning_rate": 3.820182094081943e-06,
      "loss": 1.2962,
      "step": 624
    },
    {
      "epoch": 0.9484066767830045,
      "grad_norm": 0.6752104163169861,
      "learning_rate": 3.818285280728377e-06,
      "loss": 1.376,
      "step": 625
    },
    {
      "epoch": 0.9499241274658573,
      "grad_norm": 0.6424705982208252,
      "learning_rate": 3.816388467374811e-06,
      "loss": 1.2794,
      "step": 626
    },
    {
      "epoch": 0.9514415781487102,
      "grad_norm": 0.7425332069396973,
      "learning_rate": 3.8144916540212445e-06,
      "loss": 1.3782,
      "step": 627
    },
    {
      "epoch": 0.952959028831563,
      "grad_norm": 0.6374263763427734,
      "learning_rate": 3.812594840667679e-06,
      "loss": 1.2474,
      "step": 628
    },
    {
      "epoch": 0.9544764795144158,
      "grad_norm": 0.704616129398346,
      "learning_rate": 3.8106980273141124e-06,
      "loss": 1.3282,
      "step": 629
    },
    {
      "epoch": 0.9559939301972686,
      "grad_norm": 0.6759070754051208,
      "learning_rate": 3.8088012139605467e-06,
      "loss": 1.3351,
      "step": 630
    },
    {
      "epoch": 0.9575113808801214,
      "grad_norm": 0.7037423849105835,
      "learning_rate": 3.8069044006069806e-06,
      "loss": 1.3299,
      "step": 631
    },
    {
      "epoch": 0.9590288315629742,
      "grad_norm": 0.6006524562835693,
      "learning_rate": 3.805007587253415e-06,
      "loss": 1.215,
      "step": 632
    },
    {
      "epoch": 0.960546282245827,
      "grad_norm": 0.6937094330787659,
      "learning_rate": 3.8031107738998485e-06,
      "loss": 1.3671,
      "step": 633
    },
    {
      "epoch": 0.9620637329286799,
      "grad_norm": 0.6160791516304016,
      "learning_rate": 3.801213960546283e-06,
      "loss": 1.2423,
      "step": 634
    },
    {
      "epoch": 0.9635811836115327,
      "grad_norm": 0.6245455741882324,
      "learning_rate": 3.7993171471927163e-06,
      "loss": 1.2592,
      "step": 635
    },
    {
      "epoch": 0.9650986342943855,
      "grad_norm": 0.7321311831474304,
      "learning_rate": 3.7974203338391507e-06,
      "loss": 1.3332,
      "step": 636
    },
    {
      "epoch": 0.9666160849772383,
      "grad_norm": 0.6734349131584167,
      "learning_rate": 3.7955235204855846e-06,
      "loss": 1.271,
      "step": 637
    },
    {
      "epoch": 0.9681335356600911,
      "grad_norm": 0.6730086207389832,
      "learning_rate": 3.7936267071320185e-06,
      "loss": 1.3342,
      "step": 638
    },
    {
      "epoch": 0.9696509863429439,
      "grad_norm": 0.5832639336585999,
      "learning_rate": 3.7917298937784525e-06,
      "loss": 1.2066,
      "step": 639
    },
    {
      "epoch": 0.9711684370257967,
      "grad_norm": 0.5471406579017639,
      "learning_rate": 3.789833080424887e-06,
      "loss": 1.1556,
      "step": 640
    },
    {
      "epoch": 0.9726858877086495,
      "grad_norm": 0.6284934282302856,
      "learning_rate": 3.7879362670713203e-06,
      "loss": 1.3044,
      "step": 641
    },
    {
      "epoch": 0.9742033383915023,
      "grad_norm": 0.7025759220123291,
      "learning_rate": 3.7860394537177547e-06,
      "loss": 1.2534,
      "step": 642
    },
    {
      "epoch": 0.9757207890743551,
      "grad_norm": 0.6747688055038452,
      "learning_rate": 3.7841426403641886e-06,
      "loss": 1.2686,
      "step": 643
    },
    {
      "epoch": 0.9772382397572079,
      "grad_norm": 0.8312848210334778,
      "learning_rate": 3.7822458270106225e-06,
      "loss": 1.3648,
      "step": 644
    },
    {
      "epoch": 0.9787556904400607,
      "grad_norm": 0.6724290251731873,
      "learning_rate": 3.7803490136570564e-06,
      "loss": 1.2822,
      "step": 645
    },
    {
      "epoch": 0.9802731411229135,
      "grad_norm": 0.7112178206443787,
      "learning_rate": 3.778452200303491e-06,
      "loss": 1.3888,
      "step": 646
    },
    {
      "epoch": 0.9817905918057663,
      "grad_norm": 0.6535385251045227,
      "learning_rate": 3.7765553869499243e-06,
      "loss": 1.1907,
      "step": 647
    },
    {
      "epoch": 0.9833080424886191,
      "grad_norm": 0.834165096282959,
      "learning_rate": 3.7746585735963586e-06,
      "loss": 1.3691,
      "step": 648
    },
    {
      "epoch": 0.9848254931714719,
      "grad_norm": 0.6047212481498718,
      "learning_rate": 3.7727617602427926e-06,
      "loss": 1.1995,
      "step": 649
    },
    {
      "epoch": 0.9863429438543247,
      "grad_norm": 0.7449448108673096,
      "learning_rate": 3.7708649468892265e-06,
      "loss": 1.3074,
      "step": 650
    },
    {
      "epoch": 0.9878603945371776,
      "grad_norm": 0.6472371220588684,
      "learning_rate": 3.7689681335356604e-06,
      "loss": 1.2421,
      "step": 651
    },
    {
      "epoch": 0.9893778452200304,
      "grad_norm": 0.6278042197227478,
      "learning_rate": 3.7670713201820948e-06,
      "loss": 1.3045,
      "step": 652
    },
    {
      "epoch": 0.9908952959028832,
      "grad_norm": 0.6448495984077454,
      "learning_rate": 3.7651745068285283e-06,
      "loss": 1.1973,
      "step": 653
    },
    {
      "epoch": 0.992412746585736,
      "grad_norm": 0.5527070760726929,
      "learning_rate": 3.7632776934749626e-06,
      "loss": 1.1143,
      "step": 654
    },
    {
      "epoch": 0.9939301972685888,
      "grad_norm": 0.6497949361801147,
      "learning_rate": 3.7613808801213965e-06,
      "loss": 1.2753,
      "step": 655
    },
    {
      "epoch": 0.9954476479514416,
      "grad_norm": 0.6626527905464172,
      "learning_rate": 3.7594840667678305e-06,
      "loss": 1.2502,
      "step": 656
    },
    {
      "epoch": 0.9969650986342944,
      "grad_norm": 0.6436811685562134,
      "learning_rate": 3.7575872534142644e-06,
      "loss": 1.2765,
      "step": 657
    },
    {
      "epoch": 0.9984825493171472,
      "grad_norm": 0.7477490901947021,
      "learning_rate": 3.7556904400606987e-06,
      "loss": 1.3165,
      "step": 658
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7066752314567566,
      "learning_rate": 3.7537936267071322e-06,
      "loss": 1.3125,
      "step": 659
    },
    {
      "epoch": 1.0015174506828528,
      "grad_norm": 0.6396254897117615,
      "learning_rate": 3.7518968133535666e-06,
      "loss": 1.2093,
      "step": 660
    },
    {
      "epoch": 1.0030349013657056,
      "grad_norm": 0.669262707233429,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 1.245,
      "step": 661
    },
    {
      "epoch": 1.0045523520485584,
      "grad_norm": 0.6743946671485901,
      "learning_rate": 3.748103186646434e-06,
      "loss": 1.2202,
      "step": 662
    },
    {
      "epoch": 1.0060698027314112,
      "grad_norm": 0.5987383723258972,
      "learning_rate": 3.7462063732928684e-06,
      "loss": 1.2145,
      "step": 663
    },
    {
      "epoch": 1.007587253414264,
      "grad_norm": 0.6607837080955505,
      "learning_rate": 3.744309559939302e-06,
      "loss": 1.2238,
      "step": 664
    },
    {
      "epoch": 1.0091047040971168,
      "grad_norm": 0.673328697681427,
      "learning_rate": 3.7424127465857362e-06,
      "loss": 1.2875,
      "step": 665
    },
    {
      "epoch": 1.0106221547799696,
      "grad_norm": 0.7234460711479187,
      "learning_rate": 3.74051593323217e-06,
      "loss": 1.183,
      "step": 666
    },
    {
      "epoch": 1.0121396054628224,
      "grad_norm": 0.6414504647254944,
      "learning_rate": 3.738619119878604e-06,
      "loss": 1.2627,
      "step": 667
    },
    {
      "epoch": 1.0136570561456753,
      "grad_norm": 0.6796361804008484,
      "learning_rate": 3.736722306525038e-06,
      "loss": 1.1968,
      "step": 668
    },
    {
      "epoch": 1.015174506828528,
      "grad_norm": 0.6571200489997864,
      "learning_rate": 3.7348254931714723e-06,
      "loss": 1.2411,
      "step": 669
    },
    {
      "epoch": 1.0166919575113809,
      "grad_norm": 0.6774140000343323,
      "learning_rate": 3.732928679817906e-06,
      "loss": 1.2399,
      "step": 670
    },
    {
      "epoch": 1.0182094081942337,
      "grad_norm": 0.7348694205284119,
      "learning_rate": 3.73103186646434e-06,
      "loss": 1.3225,
      "step": 671
    },
    {
      "epoch": 1.0197268588770865,
      "grad_norm": 0.7680826783180237,
      "learning_rate": 3.729135053110774e-06,
      "loss": 1.2883,
      "step": 672
    },
    {
      "epoch": 1.0212443095599393,
      "grad_norm": 0.835873544216156,
      "learning_rate": 3.727238239757208e-06,
      "loss": 1.3185,
      "step": 673
    },
    {
      "epoch": 1.022761760242792,
      "grad_norm": 0.7670581340789795,
      "learning_rate": 3.725341426403642e-06,
      "loss": 1.2354,
      "step": 674
    },
    {
      "epoch": 1.024279210925645,
      "grad_norm": 0.7249706387519836,
      "learning_rate": 3.7234446130500763e-06,
      "loss": 1.2846,
      "step": 675
    },
    {
      "epoch": 1.0257966616084977,
      "grad_norm": 0.6828282475471497,
      "learning_rate": 3.72154779969651e-06,
      "loss": 1.256,
      "step": 676
    },
    {
      "epoch": 1.0273141122913505,
      "grad_norm": 0.7150053381919861,
      "learning_rate": 3.719650986342944e-06,
      "loss": 1.3285,
      "step": 677
    },
    {
      "epoch": 1.0288315629742033,
      "grad_norm": 0.7554098963737488,
      "learning_rate": 3.717754172989378e-06,
      "loss": 1.1993,
      "step": 678
    },
    {
      "epoch": 1.0303490136570561,
      "grad_norm": 0.6152445077896118,
      "learning_rate": 3.715857359635812e-06,
      "loss": 1.1932,
      "step": 679
    },
    {
      "epoch": 1.031866464339909,
      "grad_norm": 0.7092688083648682,
      "learning_rate": 3.713960546282246e-06,
      "loss": 1.2865,
      "step": 680
    },
    {
      "epoch": 1.0333839150227617,
      "grad_norm": 0.7229138016700745,
      "learning_rate": 3.7120637329286803e-06,
      "loss": 1.2923,
      "step": 681
    },
    {
      "epoch": 1.0349013657056145,
      "grad_norm": 0.7221326231956482,
      "learning_rate": 3.710166919575114e-06,
      "loss": 1.1996,
      "step": 682
    },
    {
      "epoch": 1.0364188163884673,
      "grad_norm": 0.6572045683860779,
      "learning_rate": 3.708270106221548e-06,
      "loss": 1.1745,
      "step": 683
    },
    {
      "epoch": 1.0379362670713201,
      "grad_norm": 0.6624330878257751,
      "learning_rate": 3.706373292867982e-06,
      "loss": 1.1575,
      "step": 684
    },
    {
      "epoch": 1.039453717754173,
      "grad_norm": 0.613351583480835,
      "learning_rate": 3.704476479514416e-06,
      "loss": 1.1015,
      "step": 685
    },
    {
      "epoch": 1.0409711684370258,
      "grad_norm": 0.6735888719558716,
      "learning_rate": 3.70257966616085e-06,
      "loss": 1.2374,
      "step": 686
    },
    {
      "epoch": 1.0424886191198786,
      "grad_norm": 0.7145553231239319,
      "learning_rate": 3.7006828528072843e-06,
      "loss": 1.2031,
      "step": 687
    },
    {
      "epoch": 1.0440060698027314,
      "grad_norm": 0.8156076073646545,
      "learning_rate": 3.6987860394537178e-06,
      "loss": 1.1847,
      "step": 688
    },
    {
      "epoch": 1.0455235204855842,
      "grad_norm": 0.7471124529838562,
      "learning_rate": 3.696889226100152e-06,
      "loss": 1.3137,
      "step": 689
    },
    {
      "epoch": 1.047040971168437,
      "grad_norm": 0.6907208561897278,
      "learning_rate": 3.6949924127465856e-06,
      "loss": 1.2335,
      "step": 690
    },
    {
      "epoch": 1.0485584218512898,
      "grad_norm": 0.6278654336929321,
      "learning_rate": 3.69309559939302e-06,
      "loss": 1.1302,
      "step": 691
    },
    {
      "epoch": 1.0500758725341426,
      "grad_norm": 0.6987496018409729,
      "learning_rate": 3.691198786039454e-06,
      "loss": 1.17,
      "step": 692
    },
    {
      "epoch": 1.0515933232169954,
      "grad_norm": 0.7316738367080688,
      "learning_rate": 3.6893019726858883e-06,
      "loss": 1.2958,
      "step": 693
    },
    {
      "epoch": 1.0531107738998482,
      "grad_norm": 0.7620208263397217,
      "learning_rate": 3.6874051593323218e-06,
      "loss": 1.2656,
      "step": 694
    },
    {
      "epoch": 1.054628224582701,
      "grad_norm": 0.8677324652671814,
      "learning_rate": 3.685508345978756e-06,
      "loss": 1.2126,
      "step": 695
    },
    {
      "epoch": 1.0561456752655538,
      "grad_norm": 0.7534931302070618,
      "learning_rate": 3.6836115326251896e-06,
      "loss": 1.2644,
      "step": 696
    },
    {
      "epoch": 1.0576631259484066,
      "grad_norm": 0.7561370134353638,
      "learning_rate": 3.681714719271624e-06,
      "loss": 1.2518,
      "step": 697
    },
    {
      "epoch": 1.0591805766312594,
      "grad_norm": 0.7709604501724243,
      "learning_rate": 3.679817905918058e-06,
      "loss": 1.2809,
      "step": 698
    },
    {
      "epoch": 1.0606980273141122,
      "grad_norm": 0.6936511397361755,
      "learning_rate": 3.6779210925644922e-06,
      "loss": 1.1677,
      "step": 699
    },
    {
      "epoch": 1.062215477996965,
      "grad_norm": 0.7240753173828125,
      "learning_rate": 3.6760242792109257e-06,
      "loss": 1.2071,
      "step": 700
    },
    {
      "epoch": 1.0637329286798178,
      "grad_norm": 0.6825639009475708,
      "learning_rate": 3.67412746585736e-06,
      "loss": 1.194,
      "step": 701
    },
    {
      "epoch": 1.0652503793626706,
      "grad_norm": 0.758895754814148,
      "learning_rate": 3.6722306525037936e-06,
      "loss": 1.3073,
      "step": 702
    },
    {
      "epoch": 1.0667678300455234,
      "grad_norm": 0.7834292650222778,
      "learning_rate": 3.670333839150228e-06,
      "loss": 1.264,
      "step": 703
    },
    {
      "epoch": 1.0682852807283763,
      "grad_norm": 0.6831796169281006,
      "learning_rate": 3.668437025796662e-06,
      "loss": 1.1866,
      "step": 704
    },
    {
      "epoch": 1.069802731411229,
      "grad_norm": 0.6716588735580444,
      "learning_rate": 3.6665402124430958e-06,
      "loss": 1.1728,
      "step": 705
    },
    {
      "epoch": 1.0713201820940819,
      "grad_norm": 0.7964251637458801,
      "learning_rate": 3.6646433990895297e-06,
      "loss": 1.2411,
      "step": 706
    },
    {
      "epoch": 1.0728376327769347,
      "grad_norm": 0.7369524836540222,
      "learning_rate": 3.662746585735964e-06,
      "loss": 1.1385,
      "step": 707
    },
    {
      "epoch": 1.0743550834597875,
      "grad_norm": 0.7070616483688354,
      "learning_rate": 3.6608497723823976e-06,
      "loss": 1.2178,
      "step": 708
    },
    {
      "epoch": 1.0758725341426403,
      "grad_norm": 0.7373292446136475,
      "learning_rate": 3.658952959028832e-06,
      "loss": 1.2526,
      "step": 709
    },
    {
      "epoch": 1.077389984825493,
      "grad_norm": 0.7363407611846924,
      "learning_rate": 3.657056145675266e-06,
      "loss": 1.186,
      "step": 710
    },
    {
      "epoch": 1.078907435508346,
      "grad_norm": 0.6464115977287292,
      "learning_rate": 3.6551593323216998e-06,
      "loss": 1.0455,
      "step": 711
    },
    {
      "epoch": 1.0804248861911987,
      "grad_norm": 0.7508118748664856,
      "learning_rate": 3.6532625189681337e-06,
      "loss": 1.233,
      "step": 712
    },
    {
      "epoch": 1.0819423368740515,
      "grad_norm": 0.6381394267082214,
      "learning_rate": 3.651365705614568e-06,
      "loss": 1.1686,
      "step": 713
    },
    {
      "epoch": 1.0834597875569043,
      "grad_norm": 0.6651923656463623,
      "learning_rate": 3.6494688922610015e-06,
      "loss": 1.2476,
      "step": 714
    },
    {
      "epoch": 1.0849772382397571,
      "grad_norm": 0.7212094068527222,
      "learning_rate": 3.647572078907436e-06,
      "loss": 1.0863,
      "step": 715
    },
    {
      "epoch": 1.08649468892261,
      "grad_norm": 0.7574571371078491,
      "learning_rate": 3.64567526555387e-06,
      "loss": 1.247,
      "step": 716
    },
    {
      "epoch": 1.0880121396054627,
      "grad_norm": 0.7114570140838623,
      "learning_rate": 3.6437784522003037e-06,
      "loss": 1.0744,
      "step": 717
    },
    {
      "epoch": 1.0895295902883155,
      "grad_norm": 0.7623891830444336,
      "learning_rate": 3.6418816388467377e-06,
      "loss": 1.1744,
      "step": 718
    },
    {
      "epoch": 1.0910470409711683,
      "grad_norm": 0.6707708239555359,
      "learning_rate": 3.639984825493172e-06,
      "loss": 1.1341,
      "step": 719
    },
    {
      "epoch": 1.0925644916540211,
      "grad_norm": 0.739460289478302,
      "learning_rate": 3.6380880121396055e-06,
      "loss": 1.2601,
      "step": 720
    },
    {
      "epoch": 1.094081942336874,
      "grad_norm": 0.805168628692627,
      "learning_rate": 3.63619119878604e-06,
      "loss": 1.2289,
      "step": 721
    },
    {
      "epoch": 1.095599393019727,
      "grad_norm": 0.7507071495056152,
      "learning_rate": 3.6342943854324738e-06,
      "loss": 1.2245,
      "step": 722
    },
    {
      "epoch": 1.0971168437025796,
      "grad_norm": 0.6246512532234192,
      "learning_rate": 3.6323975720789077e-06,
      "loss": 1.0414,
      "step": 723
    },
    {
      "epoch": 1.0986342943854326,
      "grad_norm": 0.6786038875579834,
      "learning_rate": 3.6305007587253416e-06,
      "loss": 1.1665,
      "step": 724
    },
    {
      "epoch": 1.1001517450682852,
      "grad_norm": 0.7736911773681641,
      "learning_rate": 3.628603945371776e-06,
      "loss": 1.2045,
      "step": 725
    },
    {
      "epoch": 1.1016691957511382,
      "grad_norm": 0.6744875311851501,
      "learning_rate": 3.6267071320182095e-06,
      "loss": 1.0793,
      "step": 726
    },
    {
      "epoch": 1.1031866464339908,
      "grad_norm": 0.722253143787384,
      "learning_rate": 3.624810318664644e-06,
      "loss": 1.1281,
      "step": 727
    },
    {
      "epoch": 1.1047040971168438,
      "grad_norm": 0.6972172260284424,
      "learning_rate": 3.6229135053110773e-06,
      "loss": 1.1632,
      "step": 728
    },
    {
      "epoch": 1.1062215477996964,
      "grad_norm": 0.8170947432518005,
      "learning_rate": 3.6210166919575117e-06,
      "loss": 1.2453,
      "step": 729
    },
    {
      "epoch": 1.1077389984825494,
      "grad_norm": 0.822429358959198,
      "learning_rate": 3.6191198786039456e-06,
      "loss": 1.1592,
      "step": 730
    },
    {
      "epoch": 1.1092564491654022,
      "grad_norm": 0.7033846974372864,
      "learning_rate": 3.61722306525038e-06,
      "loss": 1.1718,
      "step": 731
    },
    {
      "epoch": 1.110773899848255,
      "grad_norm": 0.7265536189079285,
      "learning_rate": 3.6153262518968135e-06,
      "loss": 1.1292,
      "step": 732
    },
    {
      "epoch": 1.1122913505311078,
      "grad_norm": 0.7423152327537537,
      "learning_rate": 3.613429438543248e-06,
      "loss": 1.0908,
      "step": 733
    },
    {
      "epoch": 1.1138088012139606,
      "grad_norm": 0.7630518674850464,
      "learning_rate": 3.6115326251896813e-06,
      "loss": 1.2459,
      "step": 734
    },
    {
      "epoch": 1.1153262518968134,
      "grad_norm": 0.7399415373802185,
      "learning_rate": 3.6096358118361157e-06,
      "loss": 1.0566,
      "step": 735
    },
    {
      "epoch": 1.1168437025796663,
      "grad_norm": 0.7523176670074463,
      "learning_rate": 3.6077389984825496e-06,
      "loss": 1.1951,
      "step": 736
    },
    {
      "epoch": 1.118361153262519,
      "grad_norm": 0.6576088070869446,
      "learning_rate": 3.605842185128984e-06,
      "loss": 1.09,
      "step": 737
    },
    {
      "epoch": 1.1198786039453719,
      "grad_norm": 0.6988275647163391,
      "learning_rate": 3.6039453717754174e-06,
      "loss": 1.038,
      "step": 738
    },
    {
      "epoch": 1.1213960546282247,
      "grad_norm": 0.6655839085578918,
      "learning_rate": 3.602048558421852e-06,
      "loss": 1.0824,
      "step": 739
    },
    {
      "epoch": 1.1229135053110775,
      "grad_norm": 0.8261904716491699,
      "learning_rate": 3.6001517450682853e-06,
      "loss": 1.1485,
      "step": 740
    },
    {
      "epoch": 1.1244309559939303,
      "grad_norm": 0.7597054243087769,
      "learning_rate": 3.5982549317147196e-06,
      "loss": 1.2082,
      "step": 741
    },
    {
      "epoch": 1.125948406676783,
      "grad_norm": 0.8715972900390625,
      "learning_rate": 3.5963581183611536e-06,
      "loss": 1.1825,
      "step": 742
    },
    {
      "epoch": 1.127465857359636,
      "grad_norm": 0.8626406192779541,
      "learning_rate": 3.594461305007588e-06,
      "loss": 1.2112,
      "step": 743
    },
    {
      "epoch": 1.1289833080424887,
      "grad_norm": 0.742590606212616,
      "learning_rate": 3.5925644916540214e-06,
      "loss": 1.2011,
      "step": 744
    },
    {
      "epoch": 1.1305007587253415,
      "grad_norm": 0.8507884740829468,
      "learning_rate": 3.5906676783004558e-06,
      "loss": 1.2195,
      "step": 745
    },
    {
      "epoch": 1.1320182094081943,
      "grad_norm": 0.8451756834983826,
      "learning_rate": 3.5887708649468893e-06,
      "loss": 1.1602,
      "step": 746
    },
    {
      "epoch": 1.1335356600910471,
      "grad_norm": 0.6866504549980164,
      "learning_rate": 3.5868740515933236e-06,
      "loss": 1.0792,
      "step": 747
    },
    {
      "epoch": 1.1350531107739,
      "grad_norm": 0.7378876209259033,
      "learning_rate": 3.5849772382397575e-06,
      "loss": 1.1748,
      "step": 748
    },
    {
      "epoch": 1.1365705614567527,
      "grad_norm": 0.9086833596229553,
      "learning_rate": 3.5830804248861915e-06,
      "loss": 1.2417,
      "step": 749
    },
    {
      "epoch": 1.1380880121396055,
      "grad_norm": 0.8088547587394714,
      "learning_rate": 3.5811836115326254e-06,
      "loss": 1.1908,
      "step": 750
    },
    {
      "epoch": 1.1396054628224583,
      "grad_norm": 0.8445717096328735,
      "learning_rate": 3.5792867981790597e-06,
      "loss": 1.2174,
      "step": 751
    },
    {
      "epoch": 1.1411229135053111,
      "grad_norm": 0.7253536581993103,
      "learning_rate": 3.5773899848254932e-06,
      "loss": 0.9836,
      "step": 752
    },
    {
      "epoch": 1.142640364188164,
      "grad_norm": 0.7928915023803711,
      "learning_rate": 3.5754931714719276e-06,
      "loss": 1.0525,
      "step": 753
    },
    {
      "epoch": 1.1441578148710168,
      "grad_norm": 0.7930762767791748,
      "learning_rate": 3.5735963581183615e-06,
      "loss": 1.1582,
      "step": 754
    },
    {
      "epoch": 1.1456752655538696,
      "grad_norm": 1.010955810546875,
      "learning_rate": 3.5716995447647954e-06,
      "loss": 1.1517,
      "step": 755
    },
    {
      "epoch": 1.1471927162367224,
      "grad_norm": 0.7201443910598755,
      "learning_rate": 3.5698027314112294e-06,
      "loss": 1.0376,
      "step": 756
    },
    {
      "epoch": 1.1487101669195752,
      "grad_norm": 0.7524106502532959,
      "learning_rate": 3.5679059180576637e-06,
      "loss": 1.0881,
      "step": 757
    },
    {
      "epoch": 1.150227617602428,
      "grad_norm": 0.788011908531189,
      "learning_rate": 3.5660091047040972e-06,
      "loss": 1.0635,
      "step": 758
    },
    {
      "epoch": 1.1517450682852808,
      "grad_norm": 0.7461714148521423,
      "learning_rate": 3.5641122913505316e-06,
      "loss": 1.0934,
      "step": 759
    },
    {
      "epoch": 1.1532625189681336,
      "grad_norm": 0.8380299806594849,
      "learning_rate": 3.5622154779969655e-06,
      "loss": 1.1771,
      "step": 760
    },
    {
      "epoch": 1.1547799696509864,
      "grad_norm": 0.7555849552154541,
      "learning_rate": 3.5603186646433994e-06,
      "loss": 1.0715,
      "step": 761
    },
    {
      "epoch": 1.1562974203338392,
      "grad_norm": 0.8299313187599182,
      "learning_rate": 3.5584218512898333e-06,
      "loss": 1.2196,
      "step": 762
    },
    {
      "epoch": 1.157814871016692,
      "grad_norm": 0.7834205031394958,
      "learning_rate": 3.5565250379362677e-06,
      "loss": 1.0746,
      "step": 763
    },
    {
      "epoch": 1.1593323216995448,
      "grad_norm": 0.7515361309051514,
      "learning_rate": 3.554628224582701e-06,
      "loss": 1.1792,
      "step": 764
    },
    {
      "epoch": 1.1608497723823976,
      "grad_norm": 0.7813181281089783,
      "learning_rate": 3.5527314112291355e-06,
      "loss": 1.1262,
      "step": 765
    },
    {
      "epoch": 1.1623672230652504,
      "grad_norm": 0.7648390531539917,
      "learning_rate": 3.5508345978755695e-06,
      "loss": 1.1024,
      "step": 766
    },
    {
      "epoch": 1.1638846737481032,
      "grad_norm": 0.8767331838607788,
      "learning_rate": 3.5489377845220034e-06,
      "loss": 1.1019,
      "step": 767
    },
    {
      "epoch": 1.165402124430956,
      "grad_norm": 0.8119664788246155,
      "learning_rate": 3.5470409711684373e-06,
      "loss": 1.1475,
      "step": 768
    },
    {
      "epoch": 1.1669195751138088,
      "grad_norm": 0.8458890318870544,
      "learning_rate": 3.5451441578148717e-06,
      "loss": 1.17,
      "step": 769
    },
    {
      "epoch": 1.1684370257966616,
      "grad_norm": 0.6492888331413269,
      "learning_rate": 3.543247344461305e-06,
      "loss": 1.0251,
      "step": 770
    },
    {
      "epoch": 1.1699544764795144,
      "grad_norm": 0.7806167006492615,
      "learning_rate": 3.5413505311077395e-06,
      "loss": 1.046,
      "step": 771
    },
    {
      "epoch": 1.1714719271623673,
      "grad_norm": 0.8073126077651978,
      "learning_rate": 3.539453717754173e-06,
      "loss": 1.1593,
      "step": 772
    },
    {
      "epoch": 1.17298937784522,
      "grad_norm": 0.7585123777389526,
      "learning_rate": 3.5375569044006074e-06,
      "loss": 1.0811,
      "step": 773
    },
    {
      "epoch": 1.1745068285280729,
      "grad_norm": 0.7722355723381042,
      "learning_rate": 3.5356600910470413e-06,
      "loss": 1.113,
      "step": 774
    },
    {
      "epoch": 1.1760242792109257,
      "grad_norm": 1.1604024171829224,
      "learning_rate": 3.5337632776934756e-06,
      "loss": 1.131,
      "step": 775
    },
    {
      "epoch": 1.1775417298937785,
      "grad_norm": 1.2828015089035034,
      "learning_rate": 3.531866464339909e-06,
      "loss": 1.0844,
      "step": 776
    },
    {
      "epoch": 1.1790591805766313,
      "grad_norm": 0.6145550012588501,
      "learning_rate": 3.5299696509863435e-06,
      "loss": 0.9512,
      "step": 777
    },
    {
      "epoch": 1.180576631259484,
      "grad_norm": 0.7324347496032715,
      "learning_rate": 3.528072837632777e-06,
      "loss": 1.0487,
      "step": 778
    },
    {
      "epoch": 1.182094081942337,
      "grad_norm": 0.8651108741760254,
      "learning_rate": 3.5261760242792114e-06,
      "loss": 1.1189,
      "step": 779
    },
    {
      "epoch": 1.1836115326251897,
      "grad_norm": 1.0281636714935303,
      "learning_rate": 3.5242792109256453e-06,
      "loss": 1.0722,
      "step": 780
    },
    {
      "epoch": 1.1851289833080425,
      "grad_norm": 0.9196638464927673,
      "learning_rate": 3.5223823975720796e-06,
      "loss": 1.1728,
      "step": 781
    },
    {
      "epoch": 1.1866464339908953,
      "grad_norm": 0.8157622218132019,
      "learning_rate": 3.520485584218513e-06,
      "loss": 1.1435,
      "step": 782
    },
    {
      "epoch": 1.1881638846737481,
      "grad_norm": 0.8722344040870667,
      "learning_rate": 3.5185887708649475e-06,
      "loss": 1.1543,
      "step": 783
    },
    {
      "epoch": 1.189681335356601,
      "grad_norm": 0.7301714420318604,
      "learning_rate": 3.516691957511381e-06,
      "loss": 1.067,
      "step": 784
    },
    {
      "epoch": 1.1911987860394537,
      "grad_norm": 0.7382665276527405,
      "learning_rate": 3.5147951441578153e-06,
      "loss": 1.0379,
      "step": 785
    },
    {
      "epoch": 1.1927162367223065,
      "grad_norm": 0.8201307058334351,
      "learning_rate": 3.5128983308042493e-06,
      "loss": 1.0877,
      "step": 786
    },
    {
      "epoch": 1.1942336874051593,
      "grad_norm": 0.8057897090911865,
      "learning_rate": 3.5110015174506836e-06,
      "loss": 1.0912,
      "step": 787
    },
    {
      "epoch": 1.1957511380880121,
      "grad_norm": 0.9591964483261108,
      "learning_rate": 3.509104704097117e-06,
      "loss": 1.0843,
      "step": 788
    },
    {
      "epoch": 1.197268588770865,
      "grad_norm": 0.8185682892799377,
      "learning_rate": 3.5072078907435515e-06,
      "loss": 1.0664,
      "step": 789
    },
    {
      "epoch": 1.1987860394537178,
      "grad_norm": 0.779423177242279,
      "learning_rate": 3.505311077389985e-06,
      "loss": 1.1275,
      "step": 790
    },
    {
      "epoch": 1.2003034901365706,
      "grad_norm": 1.5716511011123657,
      "learning_rate": 3.5034142640364193e-06,
      "loss": 1.1361,
      "step": 791
    },
    {
      "epoch": 1.2018209408194234,
      "grad_norm": 0.6487793922424316,
      "learning_rate": 3.5015174506828532e-06,
      "loss": 0.9867,
      "step": 792
    },
    {
      "epoch": 1.2033383915022762,
      "grad_norm": 0.8055029511451721,
      "learning_rate": 3.4996206373292867e-06,
      "loss": 1.1374,
      "step": 793
    },
    {
      "epoch": 1.204855842185129,
      "grad_norm": 0.8605948090553284,
      "learning_rate": 3.497723823975721e-06,
      "loss": 1.1394,
      "step": 794
    },
    {
      "epoch": 1.2063732928679818,
      "grad_norm": 0.728450357913971,
      "learning_rate": 3.4958270106221546e-06,
      "loss": 1.0642,
      "step": 795
    },
    {
      "epoch": 1.2078907435508346,
      "grad_norm": 0.7542862296104431,
      "learning_rate": 3.493930197268589e-06,
      "loss": 1.0502,
      "step": 796
    },
    {
      "epoch": 1.2094081942336874,
      "grad_norm": 0.8250539898872375,
      "learning_rate": 3.492033383915023e-06,
      "loss": 1.1295,
      "step": 797
    },
    {
      "epoch": 1.2109256449165402,
      "grad_norm": 0.8312739133834839,
      "learning_rate": 3.490136570561457e-06,
      "loss": 1.0431,
      "step": 798
    },
    {
      "epoch": 1.212443095599393,
      "grad_norm": 1.03303861618042,
      "learning_rate": 3.4882397572078907e-06,
      "loss": 0.9893,
      "step": 799
    },
    {
      "epoch": 1.2139605462822458,
      "grad_norm": 0.8144717216491699,
      "learning_rate": 3.486342943854325e-06,
      "loss": 1.141,
      "step": 800
    },
    {
      "epoch": 1.2154779969650986,
      "grad_norm": 0.7653478980064392,
      "learning_rate": 3.4844461305007586e-06,
      "loss": 1.1104,
      "step": 801
    },
    {
      "epoch": 1.2169954476479514,
      "grad_norm": 0.6667231917381287,
      "learning_rate": 3.482549317147193e-06,
      "loss": 1.0212,
      "step": 802
    },
    {
      "epoch": 1.2185128983308042,
      "grad_norm": 0.7534983158111572,
      "learning_rate": 3.480652503793627e-06,
      "loss": 1.0023,
      "step": 803
    },
    {
      "epoch": 1.220030349013657,
      "grad_norm": 0.9331824779510498,
      "learning_rate": 3.478755690440061e-06,
      "loss": 1.1385,
      "step": 804
    },
    {
      "epoch": 1.2215477996965098,
      "grad_norm": 0.914644181728363,
      "learning_rate": 3.4768588770864947e-06,
      "loss": 1.0467,
      "step": 805
    },
    {
      "epoch": 1.2230652503793626,
      "grad_norm": 0.7564423084259033,
      "learning_rate": 3.474962063732929e-06,
      "loss": 1.0561,
      "step": 806
    },
    {
      "epoch": 1.2245827010622155,
      "grad_norm": 0.8360483646392822,
      "learning_rate": 3.4730652503793625e-06,
      "loss": 1.0848,
      "step": 807
    },
    {
      "epoch": 1.2261001517450683,
      "grad_norm": 0.8882290720939636,
      "learning_rate": 3.471168437025797e-06,
      "loss": 1.0245,
      "step": 808
    },
    {
      "epoch": 1.227617602427921,
      "grad_norm": 0.729907214641571,
      "learning_rate": 3.469271623672231e-06,
      "loss": 1.0406,
      "step": 809
    },
    {
      "epoch": 1.2291350531107739,
      "grad_norm": 0.7776599526405334,
      "learning_rate": 3.467374810318665e-06,
      "loss": 1.022,
      "step": 810
    },
    {
      "epoch": 1.2306525037936267,
      "grad_norm": 0.887126624584198,
      "learning_rate": 3.4654779969650987e-06,
      "loss": 1.0963,
      "step": 811
    },
    {
      "epoch": 1.2321699544764795,
      "grad_norm": 0.9343617558479309,
      "learning_rate": 3.463581183611533e-06,
      "loss": 1.1147,
      "step": 812
    },
    {
      "epoch": 1.2336874051593323,
      "grad_norm": 1.0189794301986694,
      "learning_rate": 3.4616843702579665e-06,
      "loss": 1.0827,
      "step": 813
    },
    {
      "epoch": 1.235204855842185,
      "grad_norm": 0.8416253924369812,
      "learning_rate": 3.459787556904401e-06,
      "loss": 1.0129,
      "step": 814
    },
    {
      "epoch": 1.236722306525038,
      "grad_norm": 0.7650587558746338,
      "learning_rate": 3.4578907435508348e-06,
      "loss": 1.0249,
      "step": 815
    },
    {
      "epoch": 1.2382397572078907,
      "grad_norm": 0.8261044025421143,
      "learning_rate": 3.4559939301972687e-06,
      "loss": 0.9998,
      "step": 816
    },
    {
      "epoch": 1.2397572078907435,
      "grad_norm": 0.9865848422050476,
      "learning_rate": 3.4540971168437026e-06,
      "loss": 1.077,
      "step": 817
    },
    {
      "epoch": 1.2412746585735963,
      "grad_norm": 0.8085364103317261,
      "learning_rate": 3.452200303490137e-06,
      "loss": 1.0408,
      "step": 818
    },
    {
      "epoch": 1.2427921092564491,
      "grad_norm": 0.8519001603126526,
      "learning_rate": 3.4503034901365705e-06,
      "loss": 1.1063,
      "step": 819
    },
    {
      "epoch": 1.244309559939302,
      "grad_norm": 1.0162992477416992,
      "learning_rate": 3.448406676783005e-06,
      "loss": 1.0846,
      "step": 820
    },
    {
      "epoch": 1.2458270106221547,
      "grad_norm": 0.9338759183883667,
      "learning_rate": 3.4465098634294388e-06,
      "loss": 1.0662,
      "step": 821
    },
    {
      "epoch": 1.2473444613050075,
      "grad_norm": 1.0082311630249023,
      "learning_rate": 3.4446130500758727e-06,
      "loss": 1.0871,
      "step": 822
    },
    {
      "epoch": 1.2488619119878603,
      "grad_norm": 1.5989502668380737,
      "learning_rate": 3.4427162367223066e-06,
      "loss": 0.9485,
      "step": 823
    },
    {
      "epoch": 1.2503793626707131,
      "grad_norm": 0.8059790730476379,
      "learning_rate": 3.440819423368741e-06,
      "loss": 0.9809,
      "step": 824
    },
    {
      "epoch": 1.251896813353566,
      "grad_norm": 1.0484849214553833,
      "learning_rate": 3.4389226100151745e-06,
      "loss": 1.0588,
      "step": 825
    },
    {
      "epoch": 1.2534142640364188,
      "grad_norm": 0.7508343458175659,
      "learning_rate": 3.437025796661609e-06,
      "loss": 0.9699,
      "step": 826
    },
    {
      "epoch": 1.2549317147192716,
      "grad_norm": 0.8714025020599365,
      "learning_rate": 3.4351289833080427e-06,
      "loss": 1.0431,
      "step": 827
    },
    {
      "epoch": 1.2564491654021244,
      "grad_norm": 0.9358300566673279,
      "learning_rate": 3.4332321699544767e-06,
      "loss": 1.0021,
      "step": 828
    },
    {
      "epoch": 1.2579666160849772,
      "grad_norm": 0.9322062134742737,
      "learning_rate": 3.4313353566009106e-06,
      "loss": 1.0444,
      "step": 829
    },
    {
      "epoch": 1.25948406676783,
      "grad_norm": 0.9379375576972961,
      "learning_rate": 3.429438543247345e-06,
      "loss": 1.0583,
      "step": 830
    },
    {
      "epoch": 1.2610015174506828,
      "grad_norm": 0.8438712358474731,
      "learning_rate": 3.4275417298937784e-06,
      "loss": 0.985,
      "step": 831
    },
    {
      "epoch": 1.2625189681335356,
      "grad_norm": 1.004164695739746,
      "learning_rate": 3.425644916540213e-06,
      "loss": 0.9512,
      "step": 832
    },
    {
      "epoch": 1.2640364188163884,
      "grad_norm": 0.9001045227050781,
      "learning_rate": 3.4237481031866467e-06,
      "loss": 0.9246,
      "step": 833
    },
    {
      "epoch": 1.2655538694992412,
      "grad_norm": 0.8749445676803589,
      "learning_rate": 3.4218512898330806e-06,
      "loss": 1.0556,
      "step": 834
    },
    {
      "epoch": 1.267071320182094,
      "grad_norm": 0.8480680584907532,
      "learning_rate": 3.4199544764795146e-06,
      "loss": 1.0188,
      "step": 835
    },
    {
      "epoch": 1.2685887708649468,
      "grad_norm": 0.9074400663375854,
      "learning_rate": 3.418057663125949e-06,
      "loss": 1.0192,
      "step": 836
    },
    {
      "epoch": 1.2701062215477996,
      "grad_norm": 0.8459684252738953,
      "learning_rate": 3.4161608497723824e-06,
      "loss": 1.0174,
      "step": 837
    },
    {
      "epoch": 1.2716236722306524,
      "grad_norm": 0.847891092300415,
      "learning_rate": 3.4142640364188168e-06,
      "loss": 0.9477,
      "step": 838
    },
    {
      "epoch": 1.2731411229135052,
      "grad_norm": 1.2299724817276,
      "learning_rate": 3.4123672230652503e-06,
      "loss": 1.1092,
      "step": 839
    },
    {
      "epoch": 1.274658573596358,
      "grad_norm": 0.904341995716095,
      "learning_rate": 3.4104704097116846e-06,
      "loss": 1.0293,
      "step": 840
    },
    {
      "epoch": 1.276176024279211,
      "grad_norm": 1.2884615659713745,
      "learning_rate": 3.4085735963581185e-06,
      "loss": 1.0641,
      "step": 841
    },
    {
      "epoch": 1.2776934749620636,
      "grad_norm": 0.823958694934845,
      "learning_rate": 3.406676783004553e-06,
      "loss": 0.9884,
      "step": 842
    },
    {
      "epoch": 1.2792109256449167,
      "grad_norm": 0.9140548706054688,
      "learning_rate": 3.4047799696509864e-06,
      "loss": 0.9819,
      "step": 843
    },
    {
      "epoch": 1.2807283763277693,
      "grad_norm": 0.8119893074035645,
      "learning_rate": 3.4028831562974207e-06,
      "loss": 1.0036,
      "step": 844
    },
    {
      "epoch": 1.2822458270106223,
      "grad_norm": 0.8125877976417542,
      "learning_rate": 3.4009863429438542e-06,
      "loss": 1.0106,
      "step": 845
    },
    {
      "epoch": 1.2837632776934749,
      "grad_norm": 0.8454040884971619,
      "learning_rate": 3.3990895295902886e-06,
      "loss": 1.0345,
      "step": 846
    },
    {
      "epoch": 1.285280728376328,
      "grad_norm": 0.9130933284759521,
      "learning_rate": 3.3971927162367225e-06,
      "loss": 1.0971,
      "step": 847
    },
    {
      "epoch": 1.2867981790591805,
      "grad_norm": 0.8906899094581604,
      "learning_rate": 3.395295902883157e-06,
      "loss": 0.9743,
      "step": 848
    },
    {
      "epoch": 1.2883156297420335,
      "grad_norm": 0.8265310525894165,
      "learning_rate": 3.3933990895295904e-06,
      "loss": 1.0058,
      "step": 849
    },
    {
      "epoch": 1.289833080424886,
      "grad_norm": 0.8817155361175537,
      "learning_rate": 3.3915022761760247e-06,
      "loss": 0.9752,
      "step": 850
    },
    {
      "epoch": 1.2913505311077391,
      "grad_norm": 0.9402693510055542,
      "learning_rate": 3.3896054628224582e-06,
      "loss": 1.0247,
      "step": 851
    },
    {
      "epoch": 1.2928679817905917,
      "grad_norm": 1.1260693073272705,
      "learning_rate": 3.3877086494688926e-06,
      "loss": 1.0285,
      "step": 852
    },
    {
      "epoch": 1.2943854324734447,
      "grad_norm": 0.8887811303138733,
      "learning_rate": 3.3858118361153265e-06,
      "loss": 0.9953,
      "step": 853
    },
    {
      "epoch": 1.2959028831562973,
      "grad_norm": 0.9329166412353516,
      "learning_rate": 3.383915022761761e-06,
      "loss": 1.0983,
      "step": 854
    },
    {
      "epoch": 1.2974203338391503,
      "grad_norm": 0.9314559102058411,
      "learning_rate": 3.3820182094081943e-06,
      "loss": 0.9131,
      "step": 855
    },
    {
      "epoch": 1.298937784522003,
      "grad_norm": 0.8477385640144348,
      "learning_rate": 3.3801213960546287e-06,
      "loss": 0.9922,
      "step": 856
    },
    {
      "epoch": 1.300455235204856,
      "grad_norm": 0.8864225149154663,
      "learning_rate": 3.378224582701062e-06,
      "loss": 1.0157,
      "step": 857
    },
    {
      "epoch": 1.3019726858877085,
      "grad_norm": 1.0086473226547241,
      "learning_rate": 3.3763277693474965e-06,
      "loss": 1.0227,
      "step": 858
    },
    {
      "epoch": 1.3034901365705616,
      "grad_norm": 0.8889028429985046,
      "learning_rate": 3.3744309559939305e-06,
      "loss": 0.9974,
      "step": 859
    },
    {
      "epoch": 1.3050075872534141,
      "grad_norm": 1.1031018495559692,
      "learning_rate": 3.3725341426403644e-06,
      "loss": 1.0278,
      "step": 860
    },
    {
      "epoch": 1.3065250379362672,
      "grad_norm": 0.8944712281227112,
      "learning_rate": 3.3706373292867983e-06,
      "loss": 1.0204,
      "step": 861
    },
    {
      "epoch": 1.3080424886191198,
      "grad_norm": 0.8984291553497314,
      "learning_rate": 3.3687405159332327e-06,
      "loss": 0.8913,
      "step": 862
    },
    {
      "epoch": 1.3095599393019728,
      "grad_norm": 1.064517855644226,
      "learning_rate": 3.366843702579666e-06,
      "loss": 1.0462,
      "step": 863
    },
    {
      "epoch": 1.3110773899848254,
      "grad_norm": 0.9340331554412842,
      "learning_rate": 3.3649468892261005e-06,
      "loss": 0.9574,
      "step": 864
    },
    {
      "epoch": 1.3125948406676784,
      "grad_norm": 0.9261365532875061,
      "learning_rate": 3.3630500758725345e-06,
      "loss": 0.9741,
      "step": 865
    },
    {
      "epoch": 1.314112291350531,
      "grad_norm": 1.2355780601501465,
      "learning_rate": 3.3611532625189684e-06,
      "loss": 0.8764,
      "step": 866
    },
    {
      "epoch": 1.315629742033384,
      "grad_norm": 0.8701640963554382,
      "learning_rate": 3.3592564491654023e-06,
      "loss": 1.0326,
      "step": 867
    },
    {
      "epoch": 1.3171471927162366,
      "grad_norm": 0.7440146207809448,
      "learning_rate": 3.3573596358118367e-06,
      "loss": 0.9261,
      "step": 868
    },
    {
      "epoch": 1.3186646433990896,
      "grad_norm": 0.9456953406333923,
      "learning_rate": 3.35546282245827e-06,
      "loss": 0.992,
      "step": 869
    },
    {
      "epoch": 1.3201820940819422,
      "grad_norm": NaN,
      "learning_rate": 3.35546282245827e-06,
      "loss": 1.4106,
      "step": 870
    },
    {
      "epoch": 1.3216995447647952,
      "grad_norm": 1.0273628234863281,
      "learning_rate": 3.3535660091047045e-06,
      "loss": 0.9724,
      "step": 871
    },
    {
      "epoch": 1.3232169954476478,
      "grad_norm": 0.9152666330337524,
      "learning_rate": 3.3516691957511384e-06,
      "loss": 0.9863,
      "step": 872
    },
    {
      "epoch": 1.3247344461305008,
      "grad_norm": 1.1976330280303955,
      "learning_rate": 3.3497723823975724e-06,
      "loss": 1.0143,
      "step": 873
    },
    {
      "epoch": 1.3262518968133536,
      "grad_norm": 1.355870008468628,
      "learning_rate": 3.3478755690440063e-06,
      "loss": 0.8997,
      "step": 874
    },
    {
      "epoch": 1.3277693474962065,
      "grad_norm": 1.0354704856872559,
      "learning_rate": 3.3459787556904406e-06,
      "loss": 0.9857,
      "step": 875
    },
    {
      "epoch": 1.3292867981790593,
      "grad_norm": 0.8697232007980347,
      "learning_rate": 3.344081942336874e-06,
      "loss": 0.9054,
      "step": 876
    },
    {
      "epoch": 1.330804248861912,
      "grad_norm": 0.9053621292114258,
      "learning_rate": 3.3421851289833085e-06,
      "loss": 1.008,
      "step": 877
    },
    {
      "epoch": 1.3323216995447649,
      "grad_norm": 0.8528845310211182,
      "learning_rate": 3.3402883156297424e-06,
      "loss": 0.9699,
      "step": 878
    },
    {
      "epoch": 1.3338391502276177,
      "grad_norm": 0.9287765026092529,
      "learning_rate": 3.3383915022761763e-06,
      "loss": 0.9702,
      "step": 879
    },
    {
      "epoch": 1.3353566009104705,
      "grad_norm": 0.9555798768997192,
      "learning_rate": 3.3364946889226103e-06,
      "loss": 0.9574,
      "step": 880
    },
    {
      "epoch": 1.3368740515933233,
      "grad_norm": 0.8935519456863403,
      "learning_rate": 3.3345978755690446e-06,
      "loss": 0.9798,
      "step": 881
    },
    {
      "epoch": 1.338391502276176,
      "grad_norm": 0.9774206876754761,
      "learning_rate": 3.332701062215478e-06,
      "loss": 0.9532,
      "step": 882
    },
    {
      "epoch": 1.339908952959029,
      "grad_norm": 0.9491626620292664,
      "learning_rate": 3.3308042488619125e-06,
      "loss": 0.8764,
      "step": 883
    },
    {
      "epoch": 1.3414264036418817,
      "grad_norm": 0.9031091928482056,
      "learning_rate": 3.328907435508346e-06,
      "loss": 0.9885,
      "step": 884
    },
    {
      "epoch": 1.3429438543247345,
      "grad_norm": 1.082543134689331,
      "learning_rate": 3.3270106221547803e-06,
      "loss": 0.9639,
      "step": 885
    },
    {
      "epoch": 1.3444613050075873,
      "grad_norm": 1.0141615867614746,
      "learning_rate": 3.3251138088012142e-06,
      "loss": 0.9724,
      "step": 886
    },
    {
      "epoch": 1.3459787556904401,
      "grad_norm": 0.9018044471740723,
      "learning_rate": 3.3232169954476486e-06,
      "loss": 0.9903,
      "step": 887
    },
    {
      "epoch": 1.347496206373293,
      "grad_norm": 0.8190234899520874,
      "learning_rate": 3.321320182094082e-06,
      "loss": 0.8942,
      "step": 888
    },
    {
      "epoch": 1.3490136570561457,
      "grad_norm": 1.1848474740982056,
      "learning_rate": 3.3194233687405164e-06,
      "loss": 0.9633,
      "step": 889
    },
    {
      "epoch": 1.3505311077389985,
      "grad_norm": 0.9124844074249268,
      "learning_rate": 3.31752655538695e-06,
      "loss": 0.8932,
      "step": 890
    },
    {
      "epoch": 1.3520485584218513,
      "grad_norm": 0.839783787727356,
      "learning_rate": 3.3156297420333843e-06,
      "loss": 0.9628,
      "step": 891
    },
    {
      "epoch": 1.3535660091047041,
      "grad_norm": 0.9875271320343018,
      "learning_rate": 3.313732928679818e-06,
      "loss": 0.8906,
      "step": 892
    },
    {
      "epoch": 1.355083459787557,
      "grad_norm": 1.272769570350647,
      "learning_rate": 3.3118361153262526e-06,
      "loss": 1.0282,
      "step": 893
    },
    {
      "epoch": 1.3566009104704098,
      "grad_norm": 0.8835585117340088,
      "learning_rate": 3.309939301972686e-06,
      "loss": 0.9484,
      "step": 894
    },
    {
      "epoch": 1.3581183611532626,
      "grad_norm": 0.8784465193748474,
      "learning_rate": 3.3080424886191204e-06,
      "loss": 0.9153,
      "step": 895
    },
    {
      "epoch": 1.3596358118361154,
      "grad_norm": 0.9341387152671814,
      "learning_rate": 3.306145675265554e-06,
      "loss": 0.9072,
      "step": 896
    },
    {
      "epoch": 1.3611532625189682,
      "grad_norm": 0.8338627219200134,
      "learning_rate": 3.3042488619119883e-06,
      "loss": 0.9187,
      "step": 897
    },
    {
      "epoch": 1.362670713201821,
      "grad_norm": 0.8358674645423889,
      "learning_rate": 3.302352048558422e-06,
      "loss": 0.8779,
      "step": 898
    },
    {
      "epoch": 1.3641881638846738,
      "grad_norm": 1.134578824043274,
      "learning_rate": 3.3004552352048565e-06,
      "loss": 0.9305,
      "step": 899
    },
    {
      "epoch": 1.3657056145675266,
      "grad_norm": 1.106629729270935,
      "learning_rate": 3.29855842185129e-06,
      "loss": 0.9077,
      "step": 900
    },
    {
      "epoch": 1.3672230652503794,
      "grad_norm": 0.9289503693580627,
      "learning_rate": 3.2966616084977244e-06,
      "loss": 0.8503,
      "step": 901
    },
    {
      "epoch": 1.3687405159332322,
      "grad_norm": 0.8763986825942993,
      "learning_rate": 3.294764795144158e-06,
      "loss": 0.8684,
      "step": 902
    },
    {
      "epoch": 1.370257966616085,
      "grad_norm": 1.1314632892608643,
      "learning_rate": 3.2928679817905922e-06,
      "loss": 0.9239,
      "step": 903
    },
    {
      "epoch": 1.3717754172989378,
      "grad_norm": 0.9684114456176758,
      "learning_rate": 3.290971168437026e-06,
      "loss": 0.9394,
      "step": 904
    },
    {
      "epoch": 1.3732928679817906,
      "grad_norm": 0.9983351230621338,
      "learning_rate": 3.28907435508346e-06,
      "loss": 0.9708,
      "step": 905
    },
    {
      "epoch": 1.3748103186646434,
      "grad_norm": 0.9794581532478333,
      "learning_rate": 3.287177541729894e-06,
      "loss": 0.9539,
      "step": 906
    },
    {
      "epoch": 1.3763277693474962,
      "grad_norm": 1.4699108600616455,
      "learning_rate": 3.2852807283763284e-06,
      "loss": 0.9451,
      "step": 907
    },
    {
      "epoch": 1.377845220030349,
      "grad_norm": 0.8858221173286438,
      "learning_rate": 3.283383915022762e-06,
      "loss": 0.8828,
      "step": 908
    },
    {
      "epoch": 1.3793626707132018,
      "grad_norm": 0.937201201915741,
      "learning_rate": 3.2814871016691962e-06,
      "loss": 0.8717,
      "step": 909
    },
    {
      "epoch": 1.3808801213960546,
      "grad_norm": 0.7658079266548157,
      "learning_rate": 3.27959028831563e-06,
      "loss": 0.863,
      "step": 910
    },
    {
      "epoch": 1.3823975720789075,
      "grad_norm": 0.9641990661621094,
      "learning_rate": 3.277693474962064e-06,
      "loss": 0.9742,
      "step": 911
    },
    {
      "epoch": 1.3839150227617603,
      "grad_norm": 0.9518501162528992,
      "learning_rate": 3.275796661608498e-06,
      "loss": 0.948,
      "step": 912
    },
    {
      "epoch": 1.385432473444613,
      "grad_norm": 0.9875105023384094,
      "learning_rate": 3.2738998482549323e-06,
      "loss": 0.9663,
      "step": 913
    },
    {
      "epoch": 1.3869499241274659,
      "grad_norm": 1.2022558450698853,
      "learning_rate": 3.272003034901366e-06,
      "loss": 0.8968,
      "step": 914
    },
    {
      "epoch": 1.3884673748103187,
      "grad_norm": 1.1994062662124634,
      "learning_rate": 3.2701062215478e-06,
      "loss": 0.9755,
      "step": 915
    },
    {
      "epoch": 1.3899848254931715,
      "grad_norm": 1.134764313697815,
      "learning_rate": 3.268209408194234e-06,
      "loss": 0.9414,
      "step": 916
    },
    {
      "epoch": 1.3915022761760243,
      "grad_norm": 0.9590386748313904,
      "learning_rate": 3.266312594840668e-06,
      "loss": 0.8571,
      "step": 917
    },
    {
      "epoch": 1.393019726858877,
      "grad_norm": 0.86861652135849,
      "learning_rate": 3.264415781487102e-06,
      "loss": 0.9194,
      "step": 918
    },
    {
      "epoch": 1.39453717754173,
      "grad_norm": 1.0890203714370728,
      "learning_rate": 3.2625189681335363e-06,
      "loss": 0.9195,
      "step": 919
    },
    {
      "epoch": 1.3960546282245827,
      "grad_norm": 0.8663861155509949,
      "learning_rate": 3.26062215477997e-06,
      "loss": 0.8491,
      "step": 920
    },
    {
      "epoch": 1.3975720789074355,
      "grad_norm": 1.0003081560134888,
      "learning_rate": 3.258725341426404e-06,
      "loss": 0.9263,
      "step": 921
    },
    {
      "epoch": 1.3990895295902883,
      "grad_norm": 0.715452253818512,
      "learning_rate": 3.256828528072838e-06,
      "loss": 0.8455,
      "step": 922
    },
    {
      "epoch": 1.4006069802731411,
      "grad_norm": 0.9461583495140076,
      "learning_rate": 3.254931714719272e-06,
      "loss": 0.9585,
      "step": 923
    },
    {
      "epoch": 1.402124430955994,
      "grad_norm": 1.183956503868103,
      "learning_rate": 3.253034901365706e-06,
      "loss": 0.9127,
      "step": 924
    },
    {
      "epoch": 1.4036418816388467,
      "grad_norm": 0.9083898067474365,
      "learning_rate": 3.2511380880121403e-06,
      "loss": 0.8463,
      "step": 925
    },
    {
      "epoch": 1.4051593323216995,
      "grad_norm": 0.9594562649726868,
      "learning_rate": 3.249241274658574e-06,
      "loss": 0.964,
      "step": 926
    },
    {
      "epoch": 1.4066767830045523,
      "grad_norm": 0.9068331122398376,
      "learning_rate": 3.2473444613050077e-06,
      "loss": 0.882,
      "step": 927
    },
    {
      "epoch": 1.4081942336874052,
      "grad_norm": 0.9005661606788635,
      "learning_rate": 3.2454476479514416e-06,
      "loss": 0.9573,
      "step": 928
    },
    {
      "epoch": 1.409711684370258,
      "grad_norm": 0.9614682197570801,
      "learning_rate": 3.2435508345978756e-06,
      "loss": 0.9016,
      "step": 929
    },
    {
      "epoch": 1.4112291350531108,
      "grad_norm": 1.2124158143997192,
      "learning_rate": 3.24165402124431e-06,
      "loss": 0.8304,
      "step": 930
    },
    {
      "epoch": 1.4127465857359636,
      "grad_norm": 0.8696630597114563,
      "learning_rate": 3.2397572078907434e-06,
      "loss": 0.8439,
      "step": 931
    },
    {
      "epoch": 1.4142640364188164,
      "grad_norm": 0.9042608737945557,
      "learning_rate": 3.2378603945371778e-06,
      "loss": 0.8845,
      "step": 932
    },
    {
      "epoch": 1.4157814871016692,
      "grad_norm": 0.9745182991027832,
      "learning_rate": 3.2359635811836117e-06,
      "loss": 0.9,
      "step": 933
    },
    {
      "epoch": 1.417298937784522,
      "grad_norm": 0.9267016053199768,
      "learning_rate": 3.2340667678300456e-06,
      "loss": 0.9313,
      "step": 934
    },
    {
      "epoch": 1.4188163884673748,
      "grad_norm": 1.0432069301605225,
      "learning_rate": 3.2321699544764795e-06,
      "loss": 0.9097,
      "step": 935
    },
    {
      "epoch": 1.4203338391502276,
      "grad_norm": 0.9933257699012756,
      "learning_rate": 3.230273141122914e-06,
      "loss": 0.777,
      "step": 936
    },
    {
      "epoch": 1.4218512898330804,
      "grad_norm": 1.0633795261383057,
      "learning_rate": 3.2283763277693474e-06,
      "loss": 0.8845,
      "step": 937
    },
    {
      "epoch": 1.4233687405159332,
      "grad_norm": 1.0623611211776733,
      "learning_rate": 3.2264795144157817e-06,
      "loss": 0.9363,
      "step": 938
    },
    {
      "epoch": 1.424886191198786,
      "grad_norm": 1.0027660131454468,
      "learning_rate": 3.2245827010622157e-06,
      "loss": 0.9353,
      "step": 939
    },
    {
      "epoch": 1.4264036418816388,
      "grad_norm": 1.023142695426941,
      "learning_rate": 3.2226858877086496e-06,
      "loss": 0.8634,
      "step": 940
    },
    {
      "epoch": 1.4279210925644916,
      "grad_norm": 0.9696964621543884,
      "learning_rate": 3.2207890743550835e-06,
      "loss": 0.92,
      "step": 941
    },
    {
      "epoch": 1.4294385432473444,
      "grad_norm": 1.0345882177352905,
      "learning_rate": 3.218892261001518e-06,
      "loss": 0.8412,
      "step": 942
    },
    {
      "epoch": 1.4309559939301972,
      "grad_norm": 1.6695038080215454,
      "learning_rate": 3.2169954476479514e-06,
      "loss": 0.8737,
      "step": 943
    },
    {
      "epoch": 1.43247344461305,
      "grad_norm": 0.9964457750320435,
      "learning_rate": 3.2150986342943857e-06,
      "loss": 0.7928,
      "step": 944
    },
    {
      "epoch": 1.4339908952959028,
      "grad_norm": 0.8616949915885925,
      "learning_rate": 3.2132018209408196e-06,
      "loss": 0.778,
      "step": 945
    },
    {
      "epoch": 1.4355083459787557,
      "grad_norm": 0.983545184135437,
      "learning_rate": 3.2113050075872536e-06,
      "loss": 0.8912,
      "step": 946
    },
    {
      "epoch": 1.4370257966616085,
      "grad_norm": 1.0133165121078491,
      "learning_rate": 3.2094081942336875e-06,
      "loss": 0.8559,
      "step": 947
    },
    {
      "epoch": 1.4385432473444613,
      "grad_norm": 0.9491091966629028,
      "learning_rate": 3.207511380880122e-06,
      "loss": 0.8902,
      "step": 948
    },
    {
      "epoch": 1.440060698027314,
      "grad_norm": 1.3940199613571167,
      "learning_rate": 3.2056145675265554e-06,
      "loss": 0.8108,
      "step": 949
    },
    {
      "epoch": 1.4415781487101669,
      "grad_norm": 1.4017459154129028,
      "learning_rate": 3.2037177541729897e-06,
      "loss": 0.8847,
      "step": 950
    },
    {
      "epoch": 1.4430955993930197,
      "grad_norm": 0.9677340984344482,
      "learning_rate": 3.201820940819423e-06,
      "loss": 0.7955,
      "step": 951
    },
    {
      "epoch": 1.4446130500758725,
      "grad_norm": 1.0415700674057007,
      "learning_rate": 3.1999241274658576e-06,
      "loss": 0.8054,
      "step": 952
    },
    {
      "epoch": 1.4461305007587253,
      "grad_norm": 1.3195655345916748,
      "learning_rate": 3.1980273141122915e-06,
      "loss": 0.8737,
      "step": 953
    },
    {
      "epoch": 1.447647951441578,
      "grad_norm": 1.067475438117981,
      "learning_rate": 3.196130500758726e-06,
      "loss": 0.8554,
      "step": 954
    },
    {
      "epoch": 1.449165402124431,
      "grad_norm": 1.026983380317688,
      "learning_rate": 3.1942336874051593e-06,
      "loss": 0.8725,
      "step": 955
    },
    {
      "epoch": 1.4506828528072837,
      "grad_norm": 1.1062065362930298,
      "learning_rate": 3.1923368740515937e-06,
      "loss": 0.9254,
      "step": 956
    },
    {
      "epoch": 1.4522003034901365,
      "grad_norm": 0.9603431224822998,
      "learning_rate": 3.190440060698027e-06,
      "loss": 0.969,
      "step": 957
    },
    {
      "epoch": 1.4537177541729893,
      "grad_norm": 0.9893978834152222,
      "learning_rate": 3.1885432473444615e-06,
      "loss": 0.9036,
      "step": 958
    },
    {
      "epoch": 1.4552352048558421,
      "grad_norm": 1.0200785398483276,
      "learning_rate": 3.1866464339908955e-06,
      "loss": 0.8858,
      "step": 959
    },
    {
      "epoch": 1.456752655538695,
      "grad_norm": 1.0387217998504639,
      "learning_rate": 3.18474962063733e-06,
      "loss": 0.884,
      "step": 960
    },
    {
      "epoch": 1.4582701062215477,
      "grad_norm": 1.3557586669921875,
      "learning_rate": 3.1828528072837633e-06,
      "loss": 0.8956,
      "step": 961
    },
    {
      "epoch": 1.4597875569044005,
      "grad_norm": 0.846711277961731,
      "learning_rate": 3.1809559939301977e-06,
      "loss": 0.8381,
      "step": 962
    },
    {
      "epoch": 1.4613050075872533,
      "grad_norm": 1.0195708274841309,
      "learning_rate": 3.179059180576631e-06,
      "loss": 0.8384,
      "step": 963
    },
    {
      "epoch": 1.4628224582701062,
      "grad_norm": 0.9697287678718567,
      "learning_rate": 3.1771623672230655e-06,
      "loss": 0.7144,
      "step": 964
    },
    {
      "epoch": 1.464339908952959,
      "grad_norm": 0.9039379358291626,
      "learning_rate": 3.1752655538694994e-06,
      "loss": 0.7624,
      "step": 965
    },
    {
      "epoch": 1.4658573596358118,
      "grad_norm": 0.889756441116333,
      "learning_rate": 3.1733687405159338e-06,
      "loss": 0.901,
      "step": 966
    },
    {
      "epoch": 1.4673748103186646,
      "grad_norm": 1.539158582687378,
      "learning_rate": 3.1714719271623673e-06,
      "loss": 0.8515,
      "step": 967
    },
    {
      "epoch": 1.4688922610015174,
      "grad_norm": 0.8367847204208374,
      "learning_rate": 3.1695751138088016e-06,
      "loss": 0.7625,
      "step": 968
    },
    {
      "epoch": 1.4704097116843702,
      "grad_norm": 0.9010112285614014,
      "learning_rate": 3.167678300455235e-06,
      "loss": 0.8543,
      "step": 969
    },
    {
      "epoch": 1.471927162367223,
      "grad_norm": 0.8496766090393066,
      "learning_rate": 3.1657814871016695e-06,
      "loss": 0.8881,
      "step": 970
    },
    {
      "epoch": 1.4734446130500758,
      "grad_norm": 1.019339919090271,
      "learning_rate": 3.1638846737481034e-06,
      "loss": 0.8718,
      "step": 971
    },
    {
      "epoch": 1.4749620637329286,
      "grad_norm": 0.998023271560669,
      "learning_rate": 3.1619878603945373e-06,
      "loss": 0.8081,
      "step": 972
    },
    {
      "epoch": 1.4764795144157814,
      "grad_norm": 1.2063703536987305,
      "learning_rate": 3.1600910470409713e-06,
      "loss": 0.8954,
      "step": 973
    },
    {
      "epoch": 1.4779969650986344,
      "grad_norm": 1.1184223890304565,
      "learning_rate": 3.1581942336874056e-06,
      "loss": 0.8457,
      "step": 974
    },
    {
      "epoch": 1.479514415781487,
      "grad_norm": 1.0713169574737549,
      "learning_rate": 3.156297420333839e-06,
      "loss": 0.7907,
      "step": 975
    },
    {
      "epoch": 1.48103186646434,
      "grad_norm": 1.0359320640563965,
      "learning_rate": 3.1544006069802735e-06,
      "loss": 0.7613,
      "step": 976
    },
    {
      "epoch": 1.4825493171471926,
      "grad_norm": 0.9677212834358215,
      "learning_rate": 3.1525037936267074e-06,
      "loss": 0.8078,
      "step": 977
    },
    {
      "epoch": 1.4840667678300457,
      "grad_norm": 1.0129510164260864,
      "learning_rate": 3.1506069802731413e-06,
      "loss": 0.8614,
      "step": 978
    },
    {
      "epoch": 1.4855842185128982,
      "grad_norm": 0.970474898815155,
      "learning_rate": 3.1487101669195752e-06,
      "loss": 0.9166,
      "step": 979
    },
    {
      "epoch": 1.4871016691957513,
      "grad_norm": 0.8813372254371643,
      "learning_rate": 3.1468133535660096e-06,
      "loss": 0.8197,
      "step": 980
    },
    {
      "epoch": 1.4886191198786038,
      "grad_norm": 0.950182318687439,
      "learning_rate": 3.144916540212443e-06,
      "loss": 0.8343,
      "step": 981
    },
    {
      "epoch": 1.4901365705614569,
      "grad_norm": 0.8041414618492126,
      "learning_rate": 3.1430197268588774e-06,
      "loss": 0.887,
      "step": 982
    },
    {
      "epoch": 1.4916540212443095,
      "grad_norm": 0.9991877675056458,
      "learning_rate": 3.1411229135053114e-06,
      "loss": 0.8183,
      "step": 983
    },
    {
      "epoch": 1.4931714719271625,
      "grad_norm": 0.8312957882881165,
      "learning_rate": 3.1392261001517453e-06,
      "loss": 0.8473,
      "step": 984
    },
    {
      "epoch": 1.494688922610015,
      "grad_norm": 1.0253421068191528,
      "learning_rate": 3.137329286798179e-06,
      "loss": 0.8363,
      "step": 985
    },
    {
      "epoch": 1.496206373292868,
      "grad_norm": 0.8239429593086243,
      "learning_rate": 3.1354324734446136e-06,
      "loss": 0.8083,
      "step": 986
    },
    {
      "epoch": 1.4977238239757207,
      "grad_norm": 1.2663823366165161,
      "learning_rate": 3.133535660091047e-06,
      "loss": 0.8762,
      "step": 987
    },
    {
      "epoch": 1.4992412746585737,
      "grad_norm": 1.0666145086288452,
      "learning_rate": 3.1316388467374814e-06,
      "loss": 0.8006,
      "step": 988
    },
    {
      "epoch": 1.5007587253414263,
      "grad_norm": 0.939082145690918,
      "learning_rate": 3.1297420333839153e-06,
      "loss": 0.8162,
      "step": 989
    },
    {
      "epoch": 1.5022761760242793,
      "grad_norm": 0.9082555174827576,
      "learning_rate": 3.1278452200303493e-06,
      "loss": 0.7764,
      "step": 990
    },
    {
      "epoch": 1.503793626707132,
      "grad_norm": 0.7648362517356873,
      "learning_rate": 3.125948406676783e-06,
      "loss": 0.7931,
      "step": 991
    },
    {
      "epoch": 1.505311077389985,
      "grad_norm": 1.1924090385437012,
      "learning_rate": 3.1240515933232175e-06,
      "loss": 0.7869,
      "step": 992
    },
    {
      "epoch": 1.5068285280728375,
      "grad_norm": 0.8979934453964233,
      "learning_rate": 3.122154779969651e-06,
      "loss": 0.8262,
      "step": 993
    },
    {
      "epoch": 1.5083459787556905,
      "grad_norm": 0.957849383354187,
      "learning_rate": 3.1202579666160854e-06,
      "loss": 0.7456,
      "step": 994
    },
    {
      "epoch": 1.5098634294385431,
      "grad_norm": 0.9636439681053162,
      "learning_rate": 3.118361153262519e-06,
      "loss": 0.6255,
      "step": 995
    },
    {
      "epoch": 1.5113808801213962,
      "grad_norm": 1.5271068811416626,
      "learning_rate": 3.1164643399089532e-06,
      "loss": 0.7693,
      "step": 996
    },
    {
      "epoch": 1.5128983308042487,
      "grad_norm": 0.8899961709976196,
      "learning_rate": 3.114567526555387e-06,
      "loss": 0.7995,
      "step": 997
    },
    {
      "epoch": 1.5144157814871018,
      "grad_norm": 1.0061731338500977,
      "learning_rate": 3.1126707132018215e-06,
      "loss": 0.8475,
      "step": 998
    },
    {
      "epoch": 1.5159332321699543,
      "grad_norm": 0.9812837839126587,
      "learning_rate": 3.110773899848255e-06,
      "loss": 0.764,
      "step": 999
    },
    {
      "epoch": 1.5174506828528074,
      "grad_norm": 1.357759714126587,
      "learning_rate": 3.1088770864946894e-06,
      "loss": 0.8008,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2636,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.67257704955904e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
