{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.7936267071320184,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015174506828528073,
      "grad_norm": 1.2296100854873657,
      "learning_rate": 4.998103186646434e-06,
      "loss": 2.543,
      "step": 1
    },
    {
      "epoch": 0.0030349013657056147,
      "grad_norm": 1.145377516746521,
      "learning_rate": 4.996206373292868e-06,
      "loss": 2.5,
      "step": 2
    },
    {
      "epoch": 0.004552352048558422,
      "grad_norm": 1.2370086908340454,
      "learning_rate": 4.994309559939302e-06,
      "loss": 2.7604,
      "step": 3
    },
    {
      "epoch": 0.006069802731411229,
      "grad_norm": 1.521728515625,
      "learning_rate": 4.992412746585736e-06,
      "loss": 3.0212,
      "step": 4
    },
    {
      "epoch": 0.007587253414264037,
      "grad_norm": 1.2180225849151611,
      "learning_rate": 4.99051593323217e-06,
      "loss": 2.7215,
      "step": 5
    },
    {
      "epoch": 0.009104704097116844,
      "grad_norm": 1.269407868385315,
      "learning_rate": 4.988619119878604e-06,
      "loss": 2.8052,
      "step": 6
    },
    {
      "epoch": 0.010622154779969651,
      "grad_norm": 1.2483986616134644,
      "learning_rate": 4.986722306525038e-06,
      "loss": 2.6528,
      "step": 7
    },
    {
      "epoch": 0.012139605462822459,
      "grad_norm": 1.3500843048095703,
      "learning_rate": 4.984825493171473e-06,
      "loss": 2.9147,
      "step": 8
    },
    {
      "epoch": 0.013657056145675266,
      "grad_norm": 1.2383064031600952,
      "learning_rate": 4.982928679817906e-06,
      "loss": 2.7425,
      "step": 9
    },
    {
      "epoch": 0.015174506828528073,
      "grad_norm": 1.4451353549957275,
      "learning_rate": 4.9810318664643405e-06,
      "loss": 2.7475,
      "step": 10
    },
    {
      "epoch": 0.01669195751138088,
      "grad_norm": 1.1594111919403076,
      "learning_rate": 4.9791350531107744e-06,
      "loss": 2.5578,
      "step": 11
    },
    {
      "epoch": 0.018209408194233688,
      "grad_norm": 1.3190877437591553,
      "learning_rate": 4.977238239757208e-06,
      "loss": 2.8807,
      "step": 12
    },
    {
      "epoch": 0.019726858877086494,
      "grad_norm": 1.3993353843688965,
      "learning_rate": 4.975341426403642e-06,
      "loss": 2.9224,
      "step": 13
    },
    {
      "epoch": 0.021244309559939303,
      "grad_norm": 1.2011858224868774,
      "learning_rate": 4.973444613050076e-06,
      "loss": 2.6071,
      "step": 14
    },
    {
      "epoch": 0.02276176024279211,
      "grad_norm": 1.2973114252090454,
      "learning_rate": 4.97154779969651e-06,
      "loss": 2.6974,
      "step": 15
    },
    {
      "epoch": 0.024279210925644917,
      "grad_norm": 1.3878428936004639,
      "learning_rate": 4.969650986342944e-06,
      "loss": 2.8255,
      "step": 16
    },
    {
      "epoch": 0.025796661608497723,
      "grad_norm": 1.5073424577713013,
      "learning_rate": 4.967754172989378e-06,
      "loss": 2.9621,
      "step": 17
    },
    {
      "epoch": 0.027314112291350532,
      "grad_norm": 1.3009450435638428,
      "learning_rate": 4.965857359635812e-06,
      "loss": 2.8005,
      "step": 18
    },
    {
      "epoch": 0.028831562974203338,
      "grad_norm": 1.2101572751998901,
      "learning_rate": 4.963960546282246e-06,
      "loss": 2.5443,
      "step": 19
    },
    {
      "epoch": 0.030349013657056147,
      "grad_norm": 1.4805757999420166,
      "learning_rate": 4.962063732928681e-06,
      "loss": 3.0018,
      "step": 20
    },
    {
      "epoch": 0.03186646433990895,
      "grad_norm": 1.3378322124481201,
      "learning_rate": 4.960166919575114e-06,
      "loss": 2.7067,
      "step": 21
    },
    {
      "epoch": 0.03338391502276176,
      "grad_norm": 1.7017117738723755,
      "learning_rate": 4.9582701062215485e-06,
      "loss": 2.9837,
      "step": 22
    },
    {
      "epoch": 0.03490136570561457,
      "grad_norm": 1.3948395252227783,
      "learning_rate": 4.956373292867982e-06,
      "loss": 2.7968,
      "step": 23
    },
    {
      "epoch": 0.036418816388467376,
      "grad_norm": 1.3768593072891235,
      "learning_rate": 4.954476479514416e-06,
      "loss": 2.5307,
      "step": 24
    },
    {
      "epoch": 0.03793626707132018,
      "grad_norm": 1.565778136253357,
      "learning_rate": 4.95257966616085e-06,
      "loss": 3.0025,
      "step": 25
    },
    {
      "epoch": 0.03945371775417299,
      "grad_norm": 1.4931375980377197,
      "learning_rate": 4.950682852807284e-06,
      "loss": 2.899,
      "step": 26
    },
    {
      "epoch": 0.0409711684370258,
      "grad_norm": 1.382623314857483,
      "learning_rate": 4.948786039453718e-06,
      "loss": 2.6828,
      "step": 27
    },
    {
      "epoch": 0.042488619119878605,
      "grad_norm": 1.616318941116333,
      "learning_rate": 4.946889226100152e-06,
      "loss": 2.7557,
      "step": 28
    },
    {
      "epoch": 0.04400606980273141,
      "grad_norm": 1.7636715173721313,
      "learning_rate": 4.944992412746586e-06,
      "loss": 2.8293,
      "step": 29
    },
    {
      "epoch": 0.04552352048558422,
      "grad_norm": 1.5515614748001099,
      "learning_rate": 4.94309559939302e-06,
      "loss": 2.625,
      "step": 30
    },
    {
      "epoch": 0.04704097116843703,
      "grad_norm": 1.5387170314788818,
      "learning_rate": 4.941198786039454e-06,
      "loss": 2.675,
      "step": 31
    },
    {
      "epoch": 0.048558421851289835,
      "grad_norm": 1.5197559595108032,
      "learning_rate": 4.9393019726858886e-06,
      "loss": 2.8956,
      "step": 32
    },
    {
      "epoch": 0.05007587253414264,
      "grad_norm": 1.2622113227844238,
      "learning_rate": 4.937405159332322e-06,
      "loss": 2.462,
      "step": 33
    },
    {
      "epoch": 0.051593323216995446,
      "grad_norm": 1.5991828441619873,
      "learning_rate": 4.935508345978756e-06,
      "loss": 2.9048,
      "step": 34
    },
    {
      "epoch": 0.05311077389984825,
      "grad_norm": 1.496391773223877,
      "learning_rate": 4.9336115326251895e-06,
      "loss": 2.8618,
      "step": 35
    },
    {
      "epoch": 0.054628224582701064,
      "grad_norm": 1.6357301473617554,
      "learning_rate": 4.931714719271624e-06,
      "loss": 2.7141,
      "step": 36
    },
    {
      "epoch": 0.05614567526555387,
      "grad_norm": 1.3496685028076172,
      "learning_rate": 4.929817905918058e-06,
      "loss": 2.508,
      "step": 37
    },
    {
      "epoch": 0.057663125948406675,
      "grad_norm": 1.3136640787124634,
      "learning_rate": 4.927921092564492e-06,
      "loss": 2.4844,
      "step": 38
    },
    {
      "epoch": 0.05918057663125948,
      "grad_norm": 1.5625697374343872,
      "learning_rate": 4.926024279210926e-06,
      "loss": 2.843,
      "step": 39
    },
    {
      "epoch": 0.06069802731411229,
      "grad_norm": 1.3695666790008545,
      "learning_rate": 4.92412746585736e-06,
      "loss": 2.4112,
      "step": 40
    },
    {
      "epoch": 0.0622154779969651,
      "grad_norm": 1.4811420440673828,
      "learning_rate": 4.922230652503794e-06,
      "loss": 2.6638,
      "step": 41
    },
    {
      "epoch": 0.0637329286798179,
      "grad_norm": 1.4535053968429565,
      "learning_rate": 4.920333839150228e-06,
      "loss": 2.6469,
      "step": 42
    },
    {
      "epoch": 0.06525037936267071,
      "grad_norm": 1.545841932296753,
      "learning_rate": 4.918437025796662e-06,
      "loss": 2.7204,
      "step": 43
    },
    {
      "epoch": 0.06676783004552352,
      "grad_norm": 1.5925236940383911,
      "learning_rate": 4.916540212443096e-06,
      "loss": 2.7436,
      "step": 44
    },
    {
      "epoch": 0.06828528072837632,
      "grad_norm": 1.7303520441055298,
      "learning_rate": 4.91464339908953e-06,
      "loss": 2.5276,
      "step": 45
    },
    {
      "epoch": 0.06980273141122914,
      "grad_norm": 1.549386978149414,
      "learning_rate": 4.912746585735964e-06,
      "loss": 2.6785,
      "step": 46
    },
    {
      "epoch": 0.07132018209408195,
      "grad_norm": 1.5825550556182861,
      "learning_rate": 4.9108497723823974e-06,
      "loss": 2.5925,
      "step": 47
    },
    {
      "epoch": 0.07283763277693475,
      "grad_norm": 2.209886312484741,
      "learning_rate": 4.908952959028832e-06,
      "loss": 2.7396,
      "step": 48
    },
    {
      "epoch": 0.07435508345978756,
      "grad_norm": 1.6495764255523682,
      "learning_rate": 4.907056145675266e-06,
      "loss": 2.7408,
      "step": 49
    },
    {
      "epoch": 0.07587253414264036,
      "grad_norm": 1.5667551755905151,
      "learning_rate": 4.9051593323217e-06,
      "loss": 2.6933,
      "step": 50
    },
    {
      "epoch": 0.07738998482549317,
      "grad_norm": 1.6338387727737427,
      "learning_rate": 4.903262518968134e-06,
      "loss": 2.8305,
      "step": 51
    },
    {
      "epoch": 0.07890743550834597,
      "grad_norm": 1.3421189785003662,
      "learning_rate": 4.901365705614568e-06,
      "loss": 2.3223,
      "step": 52
    },
    {
      "epoch": 0.08042488619119878,
      "grad_norm": 1.5097326040267944,
      "learning_rate": 4.899468892261002e-06,
      "loss": 2.5304,
      "step": 53
    },
    {
      "epoch": 0.0819423368740516,
      "grad_norm": 1.6330476999282837,
      "learning_rate": 4.897572078907436e-06,
      "loss": 2.6407,
      "step": 54
    },
    {
      "epoch": 0.0834597875569044,
      "grad_norm": 1.5086435079574585,
      "learning_rate": 4.89567526555387e-06,
      "loss": 2.4356,
      "step": 55
    },
    {
      "epoch": 0.08497723823975721,
      "grad_norm": 1.583431601524353,
      "learning_rate": 4.893778452200304e-06,
      "loss": 2.6286,
      "step": 56
    },
    {
      "epoch": 0.08649468892261002,
      "grad_norm": 1.9110954999923706,
      "learning_rate": 4.8918816388467376e-06,
      "loss": 2.7878,
      "step": 57
    },
    {
      "epoch": 0.08801213960546282,
      "grad_norm": 1.8597239255905151,
      "learning_rate": 4.889984825493172e-06,
      "loss": 2.7653,
      "step": 58
    },
    {
      "epoch": 0.08952959028831563,
      "grad_norm": 1.7935107946395874,
      "learning_rate": 4.888088012139605e-06,
      "loss": 2.8694,
      "step": 59
    },
    {
      "epoch": 0.09104704097116843,
      "grad_norm": 1.7650842666625977,
      "learning_rate": 4.88619119878604e-06,
      "loss": 2.6,
      "step": 60
    },
    {
      "epoch": 0.09256449165402124,
      "grad_norm": 1.5878636837005615,
      "learning_rate": 4.884294385432474e-06,
      "loss": 2.5853,
      "step": 61
    },
    {
      "epoch": 0.09408194233687406,
      "grad_norm": 1.4179983139038086,
      "learning_rate": 4.882397572078908e-06,
      "loss": 2.3803,
      "step": 62
    },
    {
      "epoch": 0.09559939301972686,
      "grad_norm": 1.757156252861023,
      "learning_rate": 4.880500758725342e-06,
      "loss": 2.7451,
      "step": 63
    },
    {
      "epoch": 0.09711684370257967,
      "grad_norm": 1.6446622610092163,
      "learning_rate": 4.878603945371776e-06,
      "loss": 2.5447,
      "step": 64
    },
    {
      "epoch": 0.09863429438543247,
      "grad_norm": 1.7752546072006226,
      "learning_rate": 4.87670713201821e-06,
      "loss": 2.7742,
      "step": 65
    },
    {
      "epoch": 0.10015174506828528,
      "grad_norm": 1.7238361835479736,
      "learning_rate": 4.874810318664644e-06,
      "loss": 2.6561,
      "step": 66
    },
    {
      "epoch": 0.10166919575113809,
      "grad_norm": 1.8177186250686646,
      "learning_rate": 4.872913505311078e-06,
      "loss": 2.7642,
      "step": 67
    },
    {
      "epoch": 0.10318664643399089,
      "grad_norm": 1.7101749181747437,
      "learning_rate": 4.871016691957512e-06,
      "loss": 2.572,
      "step": 68
    },
    {
      "epoch": 0.1047040971168437,
      "grad_norm": 1.8339701890945435,
      "learning_rate": 4.8691198786039455e-06,
      "loss": 2.5973,
      "step": 69
    },
    {
      "epoch": 0.1062215477996965,
      "grad_norm": 1.67083740234375,
      "learning_rate": 4.86722306525038e-06,
      "loss": 2.4302,
      "step": 70
    },
    {
      "epoch": 0.10773899848254932,
      "grad_norm": 1.6370307207107544,
      "learning_rate": 4.865326251896813e-06,
      "loss": 2.4781,
      "step": 71
    },
    {
      "epoch": 0.10925644916540213,
      "grad_norm": 1.8731797933578491,
      "learning_rate": 4.863429438543248e-06,
      "loss": 2.7356,
      "step": 72
    },
    {
      "epoch": 0.11077389984825493,
      "grad_norm": 1.8222007751464844,
      "learning_rate": 4.861532625189681e-06,
      "loss": 2.6422,
      "step": 73
    },
    {
      "epoch": 0.11229135053110774,
      "grad_norm": 2.1893908977508545,
      "learning_rate": 4.859635811836116e-06,
      "loss": 2.7446,
      "step": 74
    },
    {
      "epoch": 0.11380880121396054,
      "grad_norm": 2.137599468231201,
      "learning_rate": 4.85773899848255e-06,
      "loss": 2.5622,
      "step": 75
    },
    {
      "epoch": 0.11532625189681335,
      "grad_norm": 1.6613123416900635,
      "learning_rate": 4.855842185128984e-06,
      "loss": 2.557,
      "step": 76
    },
    {
      "epoch": 0.11684370257966616,
      "grad_norm": 1.6124509572982788,
      "learning_rate": 4.853945371775418e-06,
      "loss": 2.4292,
      "step": 77
    },
    {
      "epoch": 0.11836115326251896,
      "grad_norm": 1.6773816347122192,
      "learning_rate": 4.852048558421852e-06,
      "loss": 2.5504,
      "step": 78
    },
    {
      "epoch": 0.11987860394537178,
      "grad_norm": 1.6578483581542969,
      "learning_rate": 4.850151745068286e-06,
      "loss": 2.5181,
      "step": 79
    },
    {
      "epoch": 0.12139605462822459,
      "grad_norm": 1.7653710842132568,
      "learning_rate": 4.8482549317147195e-06,
      "loss": 2.6582,
      "step": 80
    },
    {
      "epoch": 0.12291350531107739,
      "grad_norm": 1.8063796758651733,
      "learning_rate": 4.8463581183611535e-06,
      "loss": 2.5284,
      "step": 81
    },
    {
      "epoch": 0.1244309559939302,
      "grad_norm": 1.7706851959228516,
      "learning_rate": 4.844461305007588e-06,
      "loss": 2.6159,
      "step": 82
    },
    {
      "epoch": 0.125948406676783,
      "grad_norm": 1.6004217863082886,
      "learning_rate": 4.842564491654021e-06,
      "loss": 2.4233,
      "step": 83
    },
    {
      "epoch": 0.1274658573596358,
      "grad_norm": 1.7620840072631836,
      "learning_rate": 4.840667678300456e-06,
      "loss": 2.5243,
      "step": 84
    },
    {
      "epoch": 0.12898330804248861,
      "grad_norm": 1.6738554239273071,
      "learning_rate": 4.838770864946889e-06,
      "loss": 2.4312,
      "step": 85
    },
    {
      "epoch": 0.13050075872534142,
      "grad_norm": 1.9580169916152954,
      "learning_rate": 4.836874051593324e-06,
      "loss": 2.7399,
      "step": 86
    },
    {
      "epoch": 0.13201820940819423,
      "grad_norm": 1.8906543254852295,
      "learning_rate": 4.834977238239758e-06,
      "loss": 2.6992,
      "step": 87
    },
    {
      "epoch": 0.13353566009104703,
      "grad_norm": 2.0459909439086914,
      "learning_rate": 4.833080424886192e-06,
      "loss": 2.4593,
      "step": 88
    },
    {
      "epoch": 0.13505311077389984,
      "grad_norm": 1.7769277095794678,
      "learning_rate": 4.831183611532626e-06,
      "loss": 2.5526,
      "step": 89
    },
    {
      "epoch": 0.13657056145675264,
      "grad_norm": 1.8310178518295288,
      "learning_rate": 4.82928679817906e-06,
      "loss": 2.5688,
      "step": 90
    },
    {
      "epoch": 0.13808801213960548,
      "grad_norm": 1.4965262413024902,
      "learning_rate": 4.8273899848254936e-06,
      "loss": 2.154,
      "step": 91
    },
    {
      "epoch": 0.13960546282245828,
      "grad_norm": 1.5629067420959473,
      "learning_rate": 4.8254931714719275e-06,
      "loss": 2.2992,
      "step": 92
    },
    {
      "epoch": 0.1411229135053111,
      "grad_norm": 1.671036958694458,
      "learning_rate": 4.823596358118361e-06,
      "loss": 2.3575,
      "step": 93
    },
    {
      "epoch": 0.1426403641881639,
      "grad_norm": 1.560393214225769,
      "learning_rate": 4.821699544764795e-06,
      "loss": 2.1257,
      "step": 94
    },
    {
      "epoch": 0.1441578148710167,
      "grad_norm": 1.8713446855545044,
      "learning_rate": 4.819802731411229e-06,
      "loss": 2.5003,
      "step": 95
    },
    {
      "epoch": 0.1456752655538695,
      "grad_norm": 1.6972792148590088,
      "learning_rate": 4.817905918057664e-06,
      "loss": 2.3571,
      "step": 96
    },
    {
      "epoch": 0.1471927162367223,
      "grad_norm": 1.9290107488632202,
      "learning_rate": 4.816009104704097e-06,
      "loss": 2.5497,
      "step": 97
    },
    {
      "epoch": 0.14871016691957512,
      "grad_norm": 1.8083423376083374,
      "learning_rate": 4.814112291350532e-06,
      "loss": 2.4797,
      "step": 98
    },
    {
      "epoch": 0.15022761760242792,
      "grad_norm": 2.070162296295166,
      "learning_rate": 4.812215477996966e-06,
      "loss": 2.5869,
      "step": 99
    },
    {
      "epoch": 0.15174506828528073,
      "grad_norm": 1.7607768774032593,
      "learning_rate": 4.8103186646434e-06,
      "loss": 2.3919,
      "step": 100
    },
    {
      "epoch": 0.15326251896813353,
      "grad_norm": 1.9955432415008545,
      "learning_rate": 4.808421851289834e-06,
      "loss": 2.6112,
      "step": 101
    },
    {
      "epoch": 0.15477996965098634,
      "grad_norm": 1.6870702505111694,
      "learning_rate": 4.806525037936268e-06,
      "loss": 2.3246,
      "step": 102
    },
    {
      "epoch": 0.15629742033383914,
      "grad_norm": 1.7994812726974487,
      "learning_rate": 4.8046282245827015e-06,
      "loss": 2.4466,
      "step": 103
    },
    {
      "epoch": 0.15781487101669195,
      "grad_norm": 1.6542423963546753,
      "learning_rate": 4.8027314112291354e-06,
      "loss": 2.3006,
      "step": 104
    },
    {
      "epoch": 0.15933232169954475,
      "grad_norm": 1.8085426092147827,
      "learning_rate": 4.800834597875569e-06,
      "loss": 2.402,
      "step": 105
    },
    {
      "epoch": 0.16084977238239756,
      "grad_norm": 1.8828628063201904,
      "learning_rate": 4.798937784522003e-06,
      "loss": 2.472,
      "step": 106
    },
    {
      "epoch": 0.16236722306525037,
      "grad_norm": 1.7661021947860718,
      "learning_rate": 4.797040971168437e-06,
      "loss": 2.3524,
      "step": 107
    },
    {
      "epoch": 0.1638846737481032,
      "grad_norm": 1.6450202465057373,
      "learning_rate": 4.795144157814872e-06,
      "loss": 2.3313,
      "step": 108
    },
    {
      "epoch": 0.165402124430956,
      "grad_norm": 1.7230068445205688,
      "learning_rate": 4.793247344461305e-06,
      "loss": 2.286,
      "step": 109
    },
    {
      "epoch": 0.1669195751138088,
      "grad_norm": 1.5638422966003418,
      "learning_rate": 4.79135053110774e-06,
      "loss": 2.1131,
      "step": 110
    },
    {
      "epoch": 0.16843702579666162,
      "grad_norm": 1.9233044385910034,
      "learning_rate": 4.789453717754173e-06,
      "loss": 2.5089,
      "step": 111
    },
    {
      "epoch": 0.16995447647951442,
      "grad_norm": 1.2560926675796509,
      "learning_rate": 4.787556904400608e-06,
      "loss": 1.8277,
      "step": 112
    },
    {
      "epoch": 0.17147192716236723,
      "grad_norm": 1.8069604635238647,
      "learning_rate": 4.785660091047042e-06,
      "loss": 2.3653,
      "step": 113
    },
    {
      "epoch": 0.17298937784522003,
      "grad_norm": 1.6074119806289673,
      "learning_rate": 4.7837632776934755e-06,
      "loss": 2.2936,
      "step": 114
    },
    {
      "epoch": 0.17450682852807284,
      "grad_norm": 1.6495989561080933,
      "learning_rate": 4.7818664643399095e-06,
      "loss": 2.2697,
      "step": 115
    },
    {
      "epoch": 0.17602427921092564,
      "grad_norm": 1.6007108688354492,
      "learning_rate": 4.779969650986343e-06,
      "loss": 2.1329,
      "step": 116
    },
    {
      "epoch": 0.17754172989377845,
      "grad_norm": 1.5364031791687012,
      "learning_rate": 4.778072837632777e-06,
      "loss": 2.1355,
      "step": 117
    },
    {
      "epoch": 0.17905918057663125,
      "grad_norm": 1.5489548444747925,
      "learning_rate": 4.776176024279211e-06,
      "loss": 2.0527,
      "step": 118
    },
    {
      "epoch": 0.18057663125948406,
      "grad_norm": 1.6051255464553833,
      "learning_rate": 4.774279210925645e-06,
      "loss": 2.1953,
      "step": 119
    },
    {
      "epoch": 0.18209408194233687,
      "grad_norm": 1.6677589416503906,
      "learning_rate": 4.77238239757208e-06,
      "loss": 2.219,
      "step": 120
    },
    {
      "epoch": 0.18361153262518967,
      "grad_norm": 1.539976954460144,
      "learning_rate": 4.770485584218513e-06,
      "loss": 2.148,
      "step": 121
    },
    {
      "epoch": 0.18512898330804248,
      "grad_norm": 1.8917796611785889,
      "learning_rate": 4.768588770864948e-06,
      "loss": 2.4748,
      "step": 122
    },
    {
      "epoch": 0.18664643399089528,
      "grad_norm": 1.7332128286361694,
      "learning_rate": 4.766691957511381e-06,
      "loss": 2.3227,
      "step": 123
    },
    {
      "epoch": 0.18816388467374812,
      "grad_norm": 1.4659174680709839,
      "learning_rate": 4.764795144157816e-06,
      "loss": 1.9598,
      "step": 124
    },
    {
      "epoch": 0.18968133535660092,
      "grad_norm": 1.8245052099227905,
      "learning_rate": 4.7628983308042496e-06,
      "loss": 2.3087,
      "step": 125
    },
    {
      "epoch": 0.19119878603945373,
      "grad_norm": 1.5405339002609253,
      "learning_rate": 4.7610015174506835e-06,
      "loss": 2.1431,
      "step": 126
    },
    {
      "epoch": 0.19271623672230653,
      "grad_norm": 1.6003291606903076,
      "learning_rate": 4.759104704097117e-06,
      "loss": 2.0882,
      "step": 127
    },
    {
      "epoch": 0.19423368740515934,
      "grad_norm": 1.7538659572601318,
      "learning_rate": 4.757207890743551e-06,
      "loss": 2.3274,
      "step": 128
    },
    {
      "epoch": 0.19575113808801214,
      "grad_norm": 1.7648649215698242,
      "learning_rate": 4.755311077389985e-06,
      "loss": 2.3067,
      "step": 129
    },
    {
      "epoch": 0.19726858877086495,
      "grad_norm": 1.6103942394256592,
      "learning_rate": 4.753414264036419e-06,
      "loss": 2.1792,
      "step": 130
    },
    {
      "epoch": 0.19878603945371776,
      "grad_norm": 1.264354944229126,
      "learning_rate": 4.751517450682853e-06,
      "loss": 1.7866,
      "step": 131
    },
    {
      "epoch": 0.20030349013657056,
      "grad_norm": 1.670746088027954,
      "learning_rate": 4.749620637329287e-06,
      "loss": 2.2239,
      "step": 132
    },
    {
      "epoch": 0.20182094081942337,
      "grad_norm": 1.7092370986938477,
      "learning_rate": 4.747723823975721e-06,
      "loss": 2.2586,
      "step": 133
    },
    {
      "epoch": 0.20333839150227617,
      "grad_norm": 1.509729266166687,
      "learning_rate": 4.745827010622155e-06,
      "loss": 2.172,
      "step": 134
    },
    {
      "epoch": 0.20485584218512898,
      "grad_norm": 1.1190706491470337,
      "learning_rate": 4.743930197268589e-06,
      "loss": 1.6223,
      "step": 135
    },
    {
      "epoch": 0.20637329286798178,
      "grad_norm": 1.6933025121688843,
      "learning_rate": 4.742033383915023e-06,
      "loss": 2.2748,
      "step": 136
    },
    {
      "epoch": 0.2078907435508346,
      "grad_norm": 1.743527889251709,
      "learning_rate": 4.7401365705614575e-06,
      "loss": 2.2566,
      "step": 137
    },
    {
      "epoch": 0.2094081942336874,
      "grad_norm": 1.4296270608901978,
      "learning_rate": 4.738239757207891e-06,
      "loss": 1.9629,
      "step": 138
    },
    {
      "epoch": 0.2109256449165402,
      "grad_norm": 1.556756615638733,
      "learning_rate": 4.736342943854325e-06,
      "loss": 2.1553,
      "step": 139
    },
    {
      "epoch": 0.212443095599393,
      "grad_norm": 1.7096643447875977,
      "learning_rate": 4.7344461305007585e-06,
      "loss": 2.2276,
      "step": 140
    },
    {
      "epoch": 0.21396054628224584,
      "grad_norm": 1.2482569217681885,
      "learning_rate": 4.732549317147193e-06,
      "loss": 1.7357,
      "step": 141
    },
    {
      "epoch": 0.21547799696509864,
      "grad_norm": 1.624021291732788,
      "learning_rate": 4.730652503793627e-06,
      "loss": 2.2533,
      "step": 142
    },
    {
      "epoch": 0.21699544764795145,
      "grad_norm": 1.467234492301941,
      "learning_rate": 4.728755690440061e-06,
      "loss": 1.9622,
      "step": 143
    },
    {
      "epoch": 0.21851289833080426,
      "grad_norm": 1.5414761304855347,
      "learning_rate": 4.726858877086495e-06,
      "loss": 2.1367,
      "step": 144
    },
    {
      "epoch": 0.22003034901365706,
      "grad_norm": 1.5240756273269653,
      "learning_rate": 4.724962063732929e-06,
      "loss": 2.0415,
      "step": 145
    },
    {
      "epoch": 0.22154779969650987,
      "grad_norm": 1.5205750465393066,
      "learning_rate": 4.723065250379363e-06,
      "loss": 2.05,
      "step": 146
    },
    {
      "epoch": 0.22306525037936267,
      "grad_norm": 1.7188549041748047,
      "learning_rate": 4.721168437025797e-06,
      "loss": 2.2876,
      "step": 147
    },
    {
      "epoch": 0.22458270106221548,
      "grad_norm": 1.4150031805038452,
      "learning_rate": 4.719271623672231e-06,
      "loss": 1.938,
      "step": 148
    },
    {
      "epoch": 0.22610015174506828,
      "grad_norm": 1.5926215648651123,
      "learning_rate": 4.7173748103186655e-06,
      "loss": 2.1085,
      "step": 149
    },
    {
      "epoch": 0.2276176024279211,
      "grad_norm": 1.6348233222961426,
      "learning_rate": 4.7154779969650986e-06,
      "loss": 2.1934,
      "step": 150
    },
    {
      "epoch": 0.2291350531107739,
      "grad_norm": 1.5909978151321411,
      "learning_rate": 4.713581183611533e-06,
      "loss": 2.1288,
      "step": 151
    },
    {
      "epoch": 0.2306525037936267,
      "grad_norm": 1.4529144763946533,
      "learning_rate": 4.711684370257966e-06,
      "loss": 2.0636,
      "step": 152
    },
    {
      "epoch": 0.2321699544764795,
      "grad_norm": 1.3962119817733765,
      "learning_rate": 4.709787556904401e-06,
      "loss": 1.9554,
      "step": 153
    },
    {
      "epoch": 0.2336874051593323,
      "grad_norm": 1.4730618000030518,
      "learning_rate": 4.707890743550835e-06,
      "loss": 2.0616,
      "step": 154
    },
    {
      "epoch": 0.23520485584218512,
      "grad_norm": 1.1904081106185913,
      "learning_rate": 4.705993930197269e-06,
      "loss": 1.6726,
      "step": 155
    },
    {
      "epoch": 0.23672230652503792,
      "grad_norm": 1.429807424545288,
      "learning_rate": 4.704097116843703e-06,
      "loss": 1.936,
      "step": 156
    },
    {
      "epoch": 0.23823975720789076,
      "grad_norm": 1.6244840621948242,
      "learning_rate": 4.702200303490137e-06,
      "loss": 2.1656,
      "step": 157
    },
    {
      "epoch": 0.23975720789074356,
      "grad_norm": 1.4424347877502441,
      "learning_rate": 4.700303490136571e-06,
      "loss": 2.057,
      "step": 158
    },
    {
      "epoch": 0.24127465857359637,
      "grad_norm": 1.3128242492675781,
      "learning_rate": 4.698406676783005e-06,
      "loss": 1.8506,
      "step": 159
    },
    {
      "epoch": 0.24279210925644917,
      "grad_norm": 1.4419329166412354,
      "learning_rate": 4.696509863429439e-06,
      "loss": 2.0287,
      "step": 160
    },
    {
      "epoch": 0.24430955993930198,
      "grad_norm": 1.2898619174957275,
      "learning_rate": 4.694613050075873e-06,
      "loss": 1.8001,
      "step": 161
    },
    {
      "epoch": 0.24582701062215478,
      "grad_norm": 1.3028984069824219,
      "learning_rate": 4.6927162367223065e-06,
      "loss": 1.8427,
      "step": 162
    },
    {
      "epoch": 0.2473444613050076,
      "grad_norm": 1.3546850681304932,
      "learning_rate": 4.690819423368741e-06,
      "loss": 1.9491,
      "step": 163
    },
    {
      "epoch": 0.2488619119878604,
      "grad_norm": 1.8225573301315308,
      "learning_rate": 4.688922610015174e-06,
      "loss": 1.9993,
      "step": 164
    },
    {
      "epoch": 0.2503793626707132,
      "grad_norm": 1.2807976007461548,
      "learning_rate": 4.687025796661609e-06,
      "loss": 1.8407,
      "step": 165
    },
    {
      "epoch": 0.251896813353566,
      "grad_norm": 1.6566849946975708,
      "learning_rate": 4.685128983308043e-06,
      "loss": 1.9509,
      "step": 166
    },
    {
      "epoch": 0.2534142640364188,
      "grad_norm": 1.2932612895965576,
      "learning_rate": 4.683232169954477e-06,
      "loss": 1.9019,
      "step": 167
    },
    {
      "epoch": 0.2549317147192716,
      "grad_norm": 1.3724206686019897,
      "learning_rate": 4.681335356600911e-06,
      "loss": 2.015,
      "step": 168
    },
    {
      "epoch": 0.2564491654021244,
      "grad_norm": 1.2060550451278687,
      "learning_rate": 4.679438543247345e-06,
      "loss": 1.7887,
      "step": 169
    },
    {
      "epoch": 0.25796661608497723,
      "grad_norm": 1.2706100940704346,
      "learning_rate": 4.677541729893779e-06,
      "loss": 1.9232,
      "step": 170
    },
    {
      "epoch": 0.25948406676783003,
      "grad_norm": 1.351408839225769,
      "learning_rate": 4.675644916540213e-06,
      "loss": 1.9743,
      "step": 171
    },
    {
      "epoch": 0.26100151745068284,
      "grad_norm": 1.2413102388381958,
      "learning_rate": 4.673748103186647e-06,
      "loss": 1.8977,
      "step": 172
    },
    {
      "epoch": 0.26251896813353565,
      "grad_norm": 1.3321024179458618,
      "learning_rate": 4.6718512898330805e-06,
      "loss": 1.8982,
      "step": 173
    },
    {
      "epoch": 0.26403641881638845,
      "grad_norm": 1.7211295366287231,
      "learning_rate": 4.6699544764795145e-06,
      "loss": 1.7033,
      "step": 174
    },
    {
      "epoch": 0.26555386949924126,
      "grad_norm": 1.3274794816970825,
      "learning_rate": 4.668057663125949e-06,
      "loss": 1.9187,
      "step": 175
    },
    {
      "epoch": 0.26707132018209406,
      "grad_norm": 1.1590737104415894,
      "learning_rate": 4.666160849772382e-06,
      "loss": 1.8387,
      "step": 176
    },
    {
      "epoch": 0.26858877086494687,
      "grad_norm": 1.1518858671188354,
      "learning_rate": 4.664264036418817e-06,
      "loss": 1.7285,
      "step": 177
    },
    {
      "epoch": 0.2701062215477997,
      "grad_norm": 1.3349288702011108,
      "learning_rate": 4.66236722306525e-06,
      "loss": 1.9766,
      "step": 178
    },
    {
      "epoch": 0.2716236722306525,
      "grad_norm": 1.2497448921203613,
      "learning_rate": 4.660470409711685e-06,
      "loss": 1.8839,
      "step": 179
    },
    {
      "epoch": 0.2731411229135053,
      "grad_norm": 1.278490662574768,
      "learning_rate": 4.658573596358119e-06,
      "loss": 1.9232,
      "step": 180
    },
    {
      "epoch": 0.2746585735963581,
      "grad_norm": 1.4222418069839478,
      "learning_rate": 4.656676783004553e-06,
      "loss": 1.9597,
      "step": 181
    },
    {
      "epoch": 0.27617602427921095,
      "grad_norm": 0.9622682332992554,
      "learning_rate": 4.654779969650987e-06,
      "loss": 1.5176,
      "step": 182
    },
    {
      "epoch": 0.27769347496206376,
      "grad_norm": 1.2329496145248413,
      "learning_rate": 4.652883156297421e-06,
      "loss": 1.9833,
      "step": 183
    },
    {
      "epoch": 0.27921092564491656,
      "grad_norm": 1.3727004528045654,
      "learning_rate": 4.6509863429438546e-06,
      "loss": 2.0225,
      "step": 184
    },
    {
      "epoch": 0.28072837632776937,
      "grad_norm": 1.0548685789108276,
      "learning_rate": 4.6490895295902885e-06,
      "loss": 1.6745,
      "step": 185
    },
    {
      "epoch": 0.2822458270106222,
      "grad_norm": 1.19306218624115,
      "learning_rate": 4.647192716236722e-06,
      "loss": 1.8693,
      "step": 186
    },
    {
      "epoch": 0.283763277693475,
      "grad_norm": 1.0536706447601318,
      "learning_rate": 4.645295902883157e-06,
      "loss": 1.6836,
      "step": 187
    },
    {
      "epoch": 0.2852807283763278,
      "grad_norm": 1.2326853275299072,
      "learning_rate": 4.64339908952959e-06,
      "loss": 1.9389,
      "step": 188
    },
    {
      "epoch": 0.2867981790591806,
      "grad_norm": 1.2013968229293823,
      "learning_rate": 4.641502276176025e-06,
      "loss": 1.8592,
      "step": 189
    },
    {
      "epoch": 0.2883156297420334,
      "grad_norm": 1.1070467233657837,
      "learning_rate": 4.639605462822458e-06,
      "loss": 1.8925,
      "step": 190
    },
    {
      "epoch": 0.2898330804248862,
      "grad_norm": 1.0844887495040894,
      "learning_rate": 4.637708649468893e-06,
      "loss": 1.778,
      "step": 191
    },
    {
      "epoch": 0.291350531107739,
      "grad_norm": 1.0748296976089478,
      "learning_rate": 4.635811836115327e-06,
      "loss": 1.8245,
      "step": 192
    },
    {
      "epoch": 0.2928679817905918,
      "grad_norm": 1.1487575769424438,
      "learning_rate": 4.633915022761761e-06,
      "loss": 1.8182,
      "step": 193
    },
    {
      "epoch": 0.2943854324734446,
      "grad_norm": 1.0821006298065186,
      "learning_rate": 4.632018209408195e-06,
      "loss": 1.8209,
      "step": 194
    },
    {
      "epoch": 0.2959028831562974,
      "grad_norm": 1.0914114713668823,
      "learning_rate": 4.630121396054629e-06,
      "loss": 1.8477,
      "step": 195
    },
    {
      "epoch": 0.29742033383915023,
      "grad_norm": 1.160266399383545,
      "learning_rate": 4.6282245827010625e-06,
      "loss": 1.9103,
      "step": 196
    },
    {
      "epoch": 0.29893778452200304,
      "grad_norm": 1.0351605415344238,
      "learning_rate": 4.6263277693474964e-06,
      "loss": 1.89,
      "step": 197
    },
    {
      "epoch": 0.30045523520485584,
      "grad_norm": 1.0213708877563477,
      "learning_rate": 4.62443095599393e-06,
      "loss": 1.8812,
      "step": 198
    },
    {
      "epoch": 0.30197268588770865,
      "grad_norm": 1.0142707824707031,
      "learning_rate": 4.622534142640364e-06,
      "loss": 1.8085,
      "step": 199
    },
    {
      "epoch": 0.30349013657056145,
      "grad_norm": 1.1340233087539673,
      "learning_rate": 4.620637329286798e-06,
      "loss": 1.7665,
      "step": 200
    },
    {
      "epoch": 0.30500758725341426,
      "grad_norm": 0.9440214037895203,
      "learning_rate": 4.618740515933233e-06,
      "loss": 1.7901,
      "step": 201
    },
    {
      "epoch": 0.30652503793626706,
      "grad_norm": 0.9075629115104675,
      "learning_rate": 4.616843702579666e-06,
      "loss": 1.6295,
      "step": 202
    },
    {
      "epoch": 0.30804248861911987,
      "grad_norm": 0.9166465401649475,
      "learning_rate": 4.614946889226101e-06,
      "loss": 1.6512,
      "step": 203
    },
    {
      "epoch": 0.3095599393019727,
      "grad_norm": 0.8504083752632141,
      "learning_rate": 4.613050075872535e-06,
      "loss": 1.6313,
      "step": 204
    },
    {
      "epoch": 0.3110773899848255,
      "grad_norm": 0.9522998929023743,
      "learning_rate": 4.611153262518969e-06,
      "loss": 1.8097,
      "step": 205
    },
    {
      "epoch": 0.3125948406676783,
      "grad_norm": 0.9600613117218018,
      "learning_rate": 4.609256449165403e-06,
      "loss": 1.7349,
      "step": 206
    },
    {
      "epoch": 0.3141122913505311,
      "grad_norm": 0.9477100968360901,
      "learning_rate": 4.6073596358118365e-06,
      "loss": 1.8517,
      "step": 207
    },
    {
      "epoch": 0.3156297420333839,
      "grad_norm": 1.028922200202942,
      "learning_rate": 4.6054628224582705e-06,
      "loss": 1.8493,
      "step": 208
    },
    {
      "epoch": 0.3171471927162367,
      "grad_norm": 0.9665132164955139,
      "learning_rate": 4.603566009104704e-06,
      "loss": 1.7947,
      "step": 209
    },
    {
      "epoch": 0.3186646433990895,
      "grad_norm": 0.9483089447021484,
      "learning_rate": 4.601669195751138e-06,
      "loss": 1.8028,
      "step": 210
    },
    {
      "epoch": 0.3201820940819423,
      "grad_norm": 0.8424877524375916,
      "learning_rate": 4.599772382397572e-06,
      "loss": 1.7032,
      "step": 211
    },
    {
      "epoch": 0.3216995447647951,
      "grad_norm": 0.7557017803192139,
      "learning_rate": 4.597875569044006e-06,
      "loss": 1.4693,
      "step": 212
    },
    {
      "epoch": 0.3232169954476479,
      "grad_norm": 0.9225509762763977,
      "learning_rate": 4.595978755690441e-06,
      "loss": 1.7779,
      "step": 213
    },
    {
      "epoch": 0.32473444613050073,
      "grad_norm": 0.8795595169067383,
      "learning_rate": 4.594081942336874e-06,
      "loss": 1.7734,
      "step": 214
    },
    {
      "epoch": 0.3262518968133536,
      "grad_norm": 0.9041030406951904,
      "learning_rate": 4.592185128983309e-06,
      "loss": 1.8097,
      "step": 215
    },
    {
      "epoch": 0.3277693474962064,
      "grad_norm": 0.8540870547294617,
      "learning_rate": 4.590288315629743e-06,
      "loss": 1.7429,
      "step": 216
    },
    {
      "epoch": 0.3292867981790592,
      "grad_norm": 0.8875912427902222,
      "learning_rate": 4.588391502276177e-06,
      "loss": 1.8364,
      "step": 217
    },
    {
      "epoch": 0.330804248861912,
      "grad_norm": 0.9395864009857178,
      "learning_rate": 4.5864946889226106e-06,
      "loss": 1.8707,
      "step": 218
    },
    {
      "epoch": 0.3323216995447648,
      "grad_norm": 0.8605385422706604,
      "learning_rate": 4.5845978755690445e-06,
      "loss": 1.728,
      "step": 219
    },
    {
      "epoch": 0.3338391502276176,
      "grad_norm": 1.0024092197418213,
      "learning_rate": 4.582701062215478e-06,
      "loss": 1.8567,
      "step": 220
    },
    {
      "epoch": 0.3353566009104704,
      "grad_norm": 0.9037143588066101,
      "learning_rate": 4.580804248861912e-06,
      "loss": 1.7992,
      "step": 221
    },
    {
      "epoch": 0.33687405159332323,
      "grad_norm": 0.7638265490531921,
      "learning_rate": 4.578907435508346e-06,
      "loss": 1.6962,
      "step": 222
    },
    {
      "epoch": 0.33839150227617604,
      "grad_norm": 0.759544849395752,
      "learning_rate": 4.57701062215478e-06,
      "loss": 1.6146,
      "step": 223
    },
    {
      "epoch": 0.33990895295902884,
      "grad_norm": 0.6839285492897034,
      "learning_rate": 4.575113808801214e-06,
      "loss": 1.4171,
      "step": 224
    },
    {
      "epoch": 0.34142640364188165,
      "grad_norm": 0.7583320736885071,
      "learning_rate": 4.573216995447649e-06,
      "loss": 1.6634,
      "step": 225
    },
    {
      "epoch": 0.34294385432473445,
      "grad_norm": 0.7534310221672058,
      "learning_rate": 4.571320182094082e-06,
      "loss": 1.6962,
      "step": 226
    },
    {
      "epoch": 0.34446130500758726,
      "grad_norm": 0.8407227993011475,
      "learning_rate": 4.569423368740517e-06,
      "loss": 1.696,
      "step": 227
    },
    {
      "epoch": 0.34597875569044007,
      "grad_norm": 0.7586120367050171,
      "learning_rate": 4.56752655538695e-06,
      "loss": 1.6649,
      "step": 228
    },
    {
      "epoch": 0.34749620637329287,
      "grad_norm": 0.7961316704750061,
      "learning_rate": 4.565629742033385e-06,
      "loss": 1.6997,
      "step": 229
    },
    {
      "epoch": 0.3490136570561457,
      "grad_norm": 0.7867332100868225,
      "learning_rate": 4.5637329286798185e-06,
      "loss": 1.7057,
      "step": 230
    },
    {
      "epoch": 0.3505311077389985,
      "grad_norm": 0.7489578723907471,
      "learning_rate": 4.5618361153262524e-06,
      "loss": 1.6946,
      "step": 231
    },
    {
      "epoch": 0.3520485584218513,
      "grad_norm": 0.7921286225318909,
      "learning_rate": 4.559939301972686e-06,
      "loss": 1.7626,
      "step": 232
    },
    {
      "epoch": 0.3535660091047041,
      "grad_norm": 0.7463523149490356,
      "learning_rate": 4.55804248861912e-06,
      "loss": 1.6869,
      "step": 233
    },
    {
      "epoch": 0.3550834597875569,
      "grad_norm": 0.7653331756591797,
      "learning_rate": 4.556145675265554e-06,
      "loss": 1.7146,
      "step": 234
    },
    {
      "epoch": 0.3566009104704097,
      "grad_norm": 0.811520516872406,
      "learning_rate": 4.554248861911988e-06,
      "loss": 1.7669,
      "step": 235
    },
    {
      "epoch": 0.3581183611532625,
      "grad_norm": 0.6037833094596863,
      "learning_rate": 4.552352048558422e-06,
      "loss": 1.4121,
      "step": 236
    },
    {
      "epoch": 0.3596358118361153,
      "grad_norm": 0.7463830709457397,
      "learning_rate": 4.550455235204857e-06,
      "loss": 1.4742,
      "step": 237
    },
    {
      "epoch": 0.3611532625189681,
      "grad_norm": 0.676983654499054,
      "learning_rate": 4.54855842185129e-06,
      "loss": 1.6082,
      "step": 238
    },
    {
      "epoch": 0.3626707132018209,
      "grad_norm": 0.6406673789024353,
      "learning_rate": 4.546661608497725e-06,
      "loss": 1.3501,
      "step": 239
    },
    {
      "epoch": 0.36418816388467373,
      "grad_norm": 0.7165535092353821,
      "learning_rate": 4.544764795144158e-06,
      "loss": 1.664,
      "step": 240
    },
    {
      "epoch": 0.36570561456752654,
      "grad_norm": 0.6555314064025879,
      "learning_rate": 4.5428679817905926e-06,
      "loss": 1.5698,
      "step": 241
    },
    {
      "epoch": 0.36722306525037934,
      "grad_norm": 0.7273505330085754,
      "learning_rate": 4.5409711684370265e-06,
      "loss": 1.7161,
      "step": 242
    },
    {
      "epoch": 0.36874051593323215,
      "grad_norm": 0.778892993927002,
      "learning_rate": 4.53907435508346e-06,
      "loss": 1.8381,
      "step": 243
    },
    {
      "epoch": 0.37025796661608495,
      "grad_norm": 0.8184201717376709,
      "learning_rate": 4.537177541729894e-06,
      "loss": 1.7959,
      "step": 244
    },
    {
      "epoch": 0.37177541729893776,
      "grad_norm": 0.6690226197242737,
      "learning_rate": 4.535280728376328e-06,
      "loss": 1.6266,
      "step": 245
    },
    {
      "epoch": 0.37329286798179057,
      "grad_norm": 0.727482795715332,
      "learning_rate": 4.533383915022762e-06,
      "loss": 1.718,
      "step": 246
    },
    {
      "epoch": 0.37481031866464337,
      "grad_norm": 0.7129247188568115,
      "learning_rate": 4.531487101669196e-06,
      "loss": 1.7276,
      "step": 247
    },
    {
      "epoch": 0.37632776934749623,
      "grad_norm": 0.6813459396362305,
      "learning_rate": 4.52959028831563e-06,
      "loss": 1.6575,
      "step": 248
    },
    {
      "epoch": 0.37784522003034904,
      "grad_norm": 0.6502346992492676,
      "learning_rate": 4.527693474962064e-06,
      "loss": 1.6113,
      "step": 249
    },
    {
      "epoch": 0.37936267071320184,
      "grad_norm": 0.5612689852714539,
      "learning_rate": 4.525796661608498e-06,
      "loss": 1.3899,
      "step": 250
    },
    {
      "epoch": 0.38088012139605465,
      "grad_norm": 0.7948272824287415,
      "learning_rate": 4.523899848254933e-06,
      "loss": 1.7862,
      "step": 251
    },
    {
      "epoch": 0.38239757207890746,
      "grad_norm": 0.7175472974777222,
      "learning_rate": 4.522003034901366e-06,
      "loss": 1.7384,
      "step": 252
    },
    {
      "epoch": 0.38391502276176026,
      "grad_norm": 0.8448014855384827,
      "learning_rate": 4.5201062215478005e-06,
      "loss": 1.6782,
      "step": 253
    },
    {
      "epoch": 0.38543247344461307,
      "grad_norm": 0.6463996767997742,
      "learning_rate": 4.5182094081942344e-06,
      "loss": 1.5445,
      "step": 254
    },
    {
      "epoch": 0.38694992412746587,
      "grad_norm": 0.6482776403427124,
      "learning_rate": 4.516312594840668e-06,
      "loss": 1.5233,
      "step": 255
    },
    {
      "epoch": 0.3884673748103187,
      "grad_norm": 0.7124293446540833,
      "learning_rate": 4.514415781487102e-06,
      "loss": 1.6339,
      "step": 256
    },
    {
      "epoch": 0.3899848254931715,
      "grad_norm": 0.6895076632499695,
      "learning_rate": 4.512518968133536e-06,
      "loss": 1.6522,
      "step": 257
    },
    {
      "epoch": 0.3915022761760243,
      "grad_norm": 0.6682535409927368,
      "learning_rate": 4.51062215477997e-06,
      "loss": 1.69,
      "step": 258
    },
    {
      "epoch": 0.3930197268588771,
      "grad_norm": 0.7031059861183167,
      "learning_rate": 4.508725341426404e-06,
      "loss": 1.7842,
      "step": 259
    },
    {
      "epoch": 0.3945371775417299,
      "grad_norm": 0.7328448295593262,
      "learning_rate": 4.506828528072838e-06,
      "loss": 1.5789,
      "step": 260
    },
    {
      "epoch": 0.3960546282245827,
      "grad_norm": 0.6091205477714539,
      "learning_rate": 4.504931714719272e-06,
      "loss": 1.5694,
      "step": 261
    },
    {
      "epoch": 0.3975720789074355,
      "grad_norm": 0.6856762766838074,
      "learning_rate": 4.503034901365706e-06,
      "loss": 1.6089,
      "step": 262
    },
    {
      "epoch": 0.3990895295902883,
      "grad_norm": 0.6023194193840027,
      "learning_rate": 4.501138088012141e-06,
      "loss": 1.5262,
      "step": 263
    },
    {
      "epoch": 0.4006069802731411,
      "grad_norm": 0.6661161780357361,
      "learning_rate": 4.499241274658574e-06,
      "loss": 1.6776,
      "step": 264
    },
    {
      "epoch": 0.40212443095599393,
      "grad_norm": 0.6994673013687134,
      "learning_rate": 4.497344461305008e-06,
      "loss": 1.7318,
      "step": 265
    },
    {
      "epoch": 0.40364188163884673,
      "grad_norm": 0.733079731464386,
      "learning_rate": 4.4954476479514415e-06,
      "loss": 1.762,
      "step": 266
    },
    {
      "epoch": 0.40515933232169954,
      "grad_norm": 0.7290741801261902,
      "learning_rate": 4.4935508345978755e-06,
      "loss": 1.5935,
      "step": 267
    },
    {
      "epoch": 0.40667678300455234,
      "grad_norm": 0.6454800367355347,
      "learning_rate": 4.49165402124431e-06,
      "loss": 1.7276,
      "step": 268
    },
    {
      "epoch": 0.40819423368740515,
      "grad_norm": 0.6923502683639526,
      "learning_rate": 4.489757207890743e-06,
      "loss": 1.6077,
      "step": 269
    },
    {
      "epoch": 0.40971168437025796,
      "grad_norm": 0.6698258519172668,
      "learning_rate": 4.487860394537178e-06,
      "loss": 1.6654,
      "step": 270
    },
    {
      "epoch": 0.41122913505311076,
      "grad_norm": 0.6251970529556274,
      "learning_rate": 4.485963581183612e-06,
      "loss": 1.6774,
      "step": 271
    },
    {
      "epoch": 0.41274658573596357,
      "grad_norm": 0.7134875655174255,
      "learning_rate": 4.484066767830046e-06,
      "loss": 1.7781,
      "step": 272
    },
    {
      "epoch": 0.4142640364188164,
      "grad_norm": 0.6270570158958435,
      "learning_rate": 4.48216995447648e-06,
      "loss": 1.6279,
      "step": 273
    },
    {
      "epoch": 0.4157814871016692,
      "grad_norm": 0.7383782267570496,
      "learning_rate": 4.480273141122914e-06,
      "loss": 1.7337,
      "step": 274
    },
    {
      "epoch": 0.417298937784522,
      "grad_norm": 0.682374119758606,
      "learning_rate": 4.478376327769348e-06,
      "loss": 1.7108,
      "step": 275
    },
    {
      "epoch": 0.4188163884673748,
      "grad_norm": 0.6371250748634338,
      "learning_rate": 4.476479514415782e-06,
      "loss": 1.6171,
      "step": 276
    },
    {
      "epoch": 0.4203338391502276,
      "grad_norm": 0.5635488629341125,
      "learning_rate": 4.4745827010622156e-06,
      "loss": 1.4765,
      "step": 277
    },
    {
      "epoch": 0.4218512898330804,
      "grad_norm": 0.7518844604492188,
      "learning_rate": 4.4726858877086495e-06,
      "loss": 1.7189,
      "step": 278
    },
    {
      "epoch": 0.4233687405159332,
      "grad_norm": 0.6932361721992493,
      "learning_rate": 4.470789074355083e-06,
      "loss": 1.7133,
      "step": 279
    },
    {
      "epoch": 0.424886191198786,
      "grad_norm": 0.6024936437606812,
      "learning_rate": 4.468892261001518e-06,
      "loss": 1.5319,
      "step": 280
    },
    {
      "epoch": 0.4264036418816389,
      "grad_norm": 0.6698088049888611,
      "learning_rate": 4.466995447647951e-06,
      "loss": 1.6117,
      "step": 281
    },
    {
      "epoch": 0.4279210925644917,
      "grad_norm": 0.6805720329284668,
      "learning_rate": 4.465098634294386e-06,
      "loss": 1.756,
      "step": 282
    },
    {
      "epoch": 0.4294385432473445,
      "grad_norm": 0.6549474596977234,
      "learning_rate": 4.46320182094082e-06,
      "loss": 1.597,
      "step": 283
    },
    {
      "epoch": 0.4309559939301973,
      "grad_norm": 0.6891216039657593,
      "learning_rate": 4.461305007587254e-06,
      "loss": 1.7095,
      "step": 284
    },
    {
      "epoch": 0.4324734446130501,
      "grad_norm": 0.6396589279174805,
      "learning_rate": 4.459408194233688e-06,
      "loss": 1.5392,
      "step": 285
    },
    {
      "epoch": 0.4339908952959029,
      "grad_norm": 0.6449142098426819,
      "learning_rate": 4.457511380880122e-06,
      "loss": 1.6843,
      "step": 286
    },
    {
      "epoch": 0.4355083459787557,
      "grad_norm": 0.7474581003189087,
      "learning_rate": 4.455614567526556e-06,
      "loss": 1.5867,
      "step": 287
    },
    {
      "epoch": 0.4370257966616085,
      "grad_norm": 0.6382227540016174,
      "learning_rate": 4.45371775417299e-06,
      "loss": 1.6125,
      "step": 288
    },
    {
      "epoch": 0.4385432473444613,
      "grad_norm": 0.5860352516174316,
      "learning_rate": 4.4518209408194235e-06,
      "loss": 1.5229,
      "step": 289
    },
    {
      "epoch": 0.4400606980273141,
      "grad_norm": 0.6173549294471741,
      "learning_rate": 4.4499241274658574e-06,
      "loss": 1.6002,
      "step": 290
    },
    {
      "epoch": 0.44157814871016693,
      "grad_norm": 0.7263356447219849,
      "learning_rate": 4.448027314112291e-06,
      "loss": 1.6308,
      "step": 291
    },
    {
      "epoch": 0.44309559939301973,
      "grad_norm": 0.7004325985908508,
      "learning_rate": 4.446130500758726e-06,
      "loss": 1.6365,
      "step": 292
    },
    {
      "epoch": 0.44461305007587254,
      "grad_norm": 0.5995455384254456,
      "learning_rate": 4.444233687405159e-06,
      "loss": 1.5612,
      "step": 293
    },
    {
      "epoch": 0.44613050075872535,
      "grad_norm": 0.5806179642677307,
      "learning_rate": 4.442336874051594e-06,
      "loss": 1.5718,
      "step": 294
    },
    {
      "epoch": 0.44764795144157815,
      "grad_norm": 0.5942299962043762,
      "learning_rate": 4.440440060698027e-06,
      "loss": 1.6212,
      "step": 295
    },
    {
      "epoch": 0.44916540212443096,
      "grad_norm": 0.6263571977615356,
      "learning_rate": 4.438543247344462e-06,
      "loss": 1.5959,
      "step": 296
    },
    {
      "epoch": 0.45068285280728376,
      "grad_norm": 0.6709257960319519,
      "learning_rate": 4.436646433990896e-06,
      "loss": 1.7135,
      "step": 297
    },
    {
      "epoch": 0.45220030349013657,
      "grad_norm": 0.6300745606422424,
      "learning_rate": 4.43474962063733e-06,
      "loss": 1.5444,
      "step": 298
    },
    {
      "epoch": 0.4537177541729894,
      "grad_norm": 0.6544375419616699,
      "learning_rate": 4.432852807283764e-06,
      "loss": 1.6546,
      "step": 299
    },
    {
      "epoch": 0.4552352048558422,
      "grad_norm": 0.6862886548042297,
      "learning_rate": 4.4309559939301975e-06,
      "loss": 1.4746,
      "step": 300
    },
    {
      "epoch": 0.456752655538695,
      "grad_norm": 0.6507607698440552,
      "learning_rate": 4.4290591805766315e-06,
      "loss": 1.6124,
      "step": 301
    },
    {
      "epoch": 0.4582701062215478,
      "grad_norm": 0.6257960200309753,
      "learning_rate": 4.427162367223065e-06,
      "loss": 1.6371,
      "step": 302
    },
    {
      "epoch": 0.4597875569044006,
      "grad_norm": 0.6678638458251953,
      "learning_rate": 4.425265553869499e-06,
      "loss": 1.6746,
      "step": 303
    },
    {
      "epoch": 0.4613050075872534,
      "grad_norm": 0.6769396662712097,
      "learning_rate": 4.423368740515934e-06,
      "loss": 1.6711,
      "step": 304
    },
    {
      "epoch": 0.4628224582701062,
      "grad_norm": 0.6222069263458252,
      "learning_rate": 4.421471927162367e-06,
      "loss": 1.5486,
      "step": 305
    },
    {
      "epoch": 0.464339908952959,
      "grad_norm": 0.5828348398208618,
      "learning_rate": 4.419575113808802e-06,
      "loss": 1.5311,
      "step": 306
    },
    {
      "epoch": 0.4658573596358118,
      "grad_norm": 0.5915746688842773,
      "learning_rate": 4.417678300455235e-06,
      "loss": 1.393,
      "step": 307
    },
    {
      "epoch": 0.4673748103186646,
      "grad_norm": 0.6565119624137878,
      "learning_rate": 4.41578148710167e-06,
      "loss": 1.429,
      "step": 308
    },
    {
      "epoch": 0.46889226100151743,
      "grad_norm": 0.6431642770767212,
      "learning_rate": 4.413884673748104e-06,
      "loss": 1.6678,
      "step": 309
    },
    {
      "epoch": 0.47040971168437024,
      "grad_norm": 0.5368162393569946,
      "learning_rate": 4.411987860394538e-06,
      "loss": 1.4738,
      "step": 310
    },
    {
      "epoch": 0.47192716236722304,
      "grad_norm": 0.607896625995636,
      "learning_rate": 4.4100910470409716e-06,
      "loss": 1.5851,
      "step": 311
    },
    {
      "epoch": 0.47344461305007585,
      "grad_norm": 0.656377375125885,
      "learning_rate": 4.4081942336874055e-06,
      "loss": 1.6345,
      "step": 312
    },
    {
      "epoch": 0.47496206373292865,
      "grad_norm": 0.6604123115539551,
      "learning_rate": 4.406297420333839e-06,
      "loss": 1.6715,
      "step": 313
    },
    {
      "epoch": 0.4764795144157815,
      "grad_norm": 0.5649489164352417,
      "learning_rate": 4.404400606980273e-06,
      "loss": 1.4805,
      "step": 314
    },
    {
      "epoch": 0.4779969650986343,
      "grad_norm": 0.5791256427764893,
      "learning_rate": 4.402503793626707e-06,
      "loss": 1.4711,
      "step": 315
    },
    {
      "epoch": 0.4795144157814871,
      "grad_norm": 0.6056892275810242,
      "learning_rate": 4.400606980273141e-06,
      "loss": 1.5976,
      "step": 316
    },
    {
      "epoch": 0.48103186646433993,
      "grad_norm": 0.5920920372009277,
      "learning_rate": 4.398710166919575e-06,
      "loss": 1.5325,
      "step": 317
    },
    {
      "epoch": 0.48254931714719274,
      "grad_norm": 0.5900841951370239,
      "learning_rate": 4.39681335356601e-06,
      "loss": 1.4368,
      "step": 318
    },
    {
      "epoch": 0.48406676783004554,
      "grad_norm": 0.5513509511947632,
      "learning_rate": 4.394916540212443e-06,
      "loss": 1.5463,
      "step": 319
    },
    {
      "epoch": 0.48558421851289835,
      "grad_norm": 0.5559934377670288,
      "learning_rate": 4.393019726858878e-06,
      "loss": 1.497,
      "step": 320
    },
    {
      "epoch": 0.48710166919575115,
      "grad_norm": 0.5739576816558838,
      "learning_rate": 4.391122913505312e-06,
      "loss": 1.5317,
      "step": 321
    },
    {
      "epoch": 0.48861911987860396,
      "grad_norm": 0.621339738368988,
      "learning_rate": 4.389226100151746e-06,
      "loss": 1.5647,
      "step": 322
    },
    {
      "epoch": 0.49013657056145676,
      "grad_norm": 0.5411561131477356,
      "learning_rate": 4.3873292867981795e-06,
      "loss": 1.4771,
      "step": 323
    },
    {
      "epoch": 0.49165402124430957,
      "grad_norm": 0.539316713809967,
      "learning_rate": 4.3854324734446134e-06,
      "loss": 1.2411,
      "step": 324
    },
    {
      "epoch": 0.4931714719271624,
      "grad_norm": 0.6225400567054749,
      "learning_rate": 4.383535660091047e-06,
      "loss": 1.606,
      "step": 325
    },
    {
      "epoch": 0.4946889226100152,
      "grad_norm": 0.5934780240058899,
      "learning_rate": 4.381638846737481e-06,
      "loss": 1.4464,
      "step": 326
    },
    {
      "epoch": 0.496206373292868,
      "grad_norm": 0.6879992485046387,
      "learning_rate": 4.379742033383915e-06,
      "loss": 1.5816,
      "step": 327
    },
    {
      "epoch": 0.4977238239757208,
      "grad_norm": 0.640099287033081,
      "learning_rate": 4.377845220030349e-06,
      "loss": 1.6544,
      "step": 328
    },
    {
      "epoch": 0.4992412746585736,
      "grad_norm": 0.5341420769691467,
      "learning_rate": 4.375948406676783e-06,
      "loss": 1.4902,
      "step": 329
    },
    {
      "epoch": 0.5007587253414264,
      "grad_norm": 0.6039739847183228,
      "learning_rate": 4.374051593323218e-06,
      "loss": 1.6415,
      "step": 330
    },
    {
      "epoch": 0.5022761760242792,
      "grad_norm": 0.47283196449279785,
      "learning_rate": 4.372154779969651e-06,
      "loss": 1.2187,
      "step": 331
    },
    {
      "epoch": 0.503793626707132,
      "grad_norm": 0.5269982218742371,
      "learning_rate": 4.370257966616086e-06,
      "loss": 1.4469,
      "step": 332
    },
    {
      "epoch": 0.5053110773899848,
      "grad_norm": 0.6039966940879822,
      "learning_rate": 4.368361153262519e-06,
      "loss": 1.5749,
      "step": 333
    },
    {
      "epoch": 0.5068285280728376,
      "grad_norm": 0.5686982274055481,
      "learning_rate": 4.3664643399089536e-06,
      "loss": 1.5411,
      "step": 334
    },
    {
      "epoch": 0.5083459787556904,
      "grad_norm": 0.6026314496994019,
      "learning_rate": 4.3645675265553875e-06,
      "loss": 1.476,
      "step": 335
    },
    {
      "epoch": 0.5098634294385432,
      "grad_norm": 0.6377545595169067,
      "learning_rate": 4.362670713201821e-06,
      "loss": 1.6189,
      "step": 336
    },
    {
      "epoch": 0.511380880121396,
      "grad_norm": 0.5221042633056641,
      "learning_rate": 4.360773899848255e-06,
      "loss": 1.4637,
      "step": 337
    },
    {
      "epoch": 0.5128983308042488,
      "grad_norm": 0.6348833441734314,
      "learning_rate": 4.358877086494689e-06,
      "loss": 1.6695,
      "step": 338
    },
    {
      "epoch": 0.5144157814871017,
      "grad_norm": 0.6248000860214233,
      "learning_rate": 4.356980273141123e-06,
      "loss": 1.6875,
      "step": 339
    },
    {
      "epoch": 0.5159332321699545,
      "grad_norm": 0.6159707903862,
      "learning_rate": 4.355083459787557e-06,
      "loss": 1.6149,
      "step": 340
    },
    {
      "epoch": 0.5174506828528073,
      "grad_norm": 0.629008948802948,
      "learning_rate": 4.353186646433991e-06,
      "loss": 1.5596,
      "step": 341
    },
    {
      "epoch": 0.5189681335356601,
      "grad_norm": 0.5674412846565247,
      "learning_rate": 4.351289833080426e-06,
      "loss": 1.556,
      "step": 342
    },
    {
      "epoch": 0.5204855842185129,
      "grad_norm": 0.6316165924072266,
      "learning_rate": 4.349393019726859e-06,
      "loss": 1.588,
      "step": 343
    },
    {
      "epoch": 0.5220030349013657,
      "grad_norm": 0.6546992659568787,
      "learning_rate": 4.347496206373294e-06,
      "loss": 1.5751,
      "step": 344
    },
    {
      "epoch": 0.5235204855842185,
      "grad_norm": 0.5448107123374939,
      "learning_rate": 4.345599393019727e-06,
      "loss": 1.5136,
      "step": 345
    },
    {
      "epoch": 0.5250379362670713,
      "grad_norm": 0.6148433089256287,
      "learning_rate": 4.3437025796661615e-06,
      "loss": 1.6177,
      "step": 346
    },
    {
      "epoch": 0.5265553869499241,
      "grad_norm": 0.5003474950790405,
      "learning_rate": 4.3418057663125954e-06,
      "loss": 1.4505,
      "step": 347
    },
    {
      "epoch": 0.5280728376327769,
      "grad_norm": 0.6357369422912598,
      "learning_rate": 4.339908952959029e-06,
      "loss": 1.6333,
      "step": 348
    },
    {
      "epoch": 0.5295902883156297,
      "grad_norm": 0.5947277545928955,
      "learning_rate": 4.338012139605463e-06,
      "loss": 1.5235,
      "step": 349
    },
    {
      "epoch": 0.5311077389984825,
      "grad_norm": 0.5818940997123718,
      "learning_rate": 4.336115326251897e-06,
      "loss": 1.5431,
      "step": 350
    },
    {
      "epoch": 0.5326251896813353,
      "grad_norm": 0.6041730046272278,
      "learning_rate": 4.334218512898331e-06,
      "loss": 1.5737,
      "step": 351
    },
    {
      "epoch": 0.5341426403641881,
      "grad_norm": 0.6039009690284729,
      "learning_rate": 4.332321699544765e-06,
      "loss": 1.6319,
      "step": 352
    },
    {
      "epoch": 0.5356600910470409,
      "grad_norm": 0.5340460538864136,
      "learning_rate": 4.330424886191199e-06,
      "loss": 1.4312,
      "step": 353
    },
    {
      "epoch": 0.5371775417298937,
      "grad_norm": 0.5981981754302979,
      "learning_rate": 4.328528072837633e-06,
      "loss": 1.5653,
      "step": 354
    },
    {
      "epoch": 0.5386949924127465,
      "grad_norm": 0.6118205189704895,
      "learning_rate": 4.326631259484067e-06,
      "loss": 1.586,
      "step": 355
    },
    {
      "epoch": 0.5402124430955993,
      "grad_norm": 0.5704741477966309,
      "learning_rate": 4.324734446130502e-06,
      "loss": 1.5239,
      "step": 356
    },
    {
      "epoch": 0.5417298937784522,
      "grad_norm": 0.5773799419403076,
      "learning_rate": 4.322837632776935e-06,
      "loss": 1.5813,
      "step": 357
    },
    {
      "epoch": 0.543247344461305,
      "grad_norm": 0.5947864055633545,
      "learning_rate": 4.3209408194233695e-06,
      "loss": 1.5401,
      "step": 358
    },
    {
      "epoch": 0.5447647951441578,
      "grad_norm": 0.526710033416748,
      "learning_rate": 4.319044006069803e-06,
      "loss": 1.4376,
      "step": 359
    },
    {
      "epoch": 0.5462822458270106,
      "grad_norm": 0.5983788967132568,
      "learning_rate": 4.317147192716237e-06,
      "loss": 1.6355,
      "step": 360
    },
    {
      "epoch": 0.5477996965098634,
      "grad_norm": 0.5097243785858154,
      "learning_rate": 4.315250379362671e-06,
      "loss": 1.3633,
      "step": 361
    },
    {
      "epoch": 0.5493171471927162,
      "grad_norm": 0.5372440814971924,
      "learning_rate": 4.313353566009105e-06,
      "loss": 1.4779,
      "step": 362
    },
    {
      "epoch": 0.5508345978755691,
      "grad_norm": 0.5431463122367859,
      "learning_rate": 4.311456752655539e-06,
      "loss": 1.4059,
      "step": 363
    },
    {
      "epoch": 0.5523520485584219,
      "grad_norm": 0.5251713395118713,
      "learning_rate": 4.309559939301973e-06,
      "loss": 1.3281,
      "step": 364
    },
    {
      "epoch": 0.5538694992412747,
      "grad_norm": 0.5596709847450256,
      "learning_rate": 4.307663125948407e-06,
      "loss": 1.3896,
      "step": 365
    },
    {
      "epoch": 0.5553869499241275,
      "grad_norm": 0.5759207010269165,
      "learning_rate": 4.305766312594841e-06,
      "loss": 1.5542,
      "step": 366
    },
    {
      "epoch": 0.5569044006069803,
      "grad_norm": 0.6063926815986633,
      "learning_rate": 4.303869499241275e-06,
      "loss": 1.5598,
      "step": 367
    },
    {
      "epoch": 0.5584218512898331,
      "grad_norm": 0.5655776858329773,
      "learning_rate": 4.3019726858877096e-06,
      "loss": 1.5393,
      "step": 368
    },
    {
      "epoch": 0.5599393019726859,
      "grad_norm": 0.5337826013565063,
      "learning_rate": 4.300075872534143e-06,
      "loss": 1.4272,
      "step": 369
    },
    {
      "epoch": 0.5614567526555387,
      "grad_norm": 0.5345029830932617,
      "learning_rate": 4.298179059180577e-06,
      "loss": 1.4592,
      "step": 370
    },
    {
      "epoch": 0.5629742033383915,
      "grad_norm": 0.5669544339179993,
      "learning_rate": 4.296282245827011e-06,
      "loss": 1.4995,
      "step": 371
    },
    {
      "epoch": 0.5644916540212443,
      "grad_norm": 0.5549417734146118,
      "learning_rate": 4.294385432473445e-06,
      "loss": 1.4412,
      "step": 372
    },
    {
      "epoch": 0.5660091047040972,
      "grad_norm": 0.5613023042678833,
      "learning_rate": 4.292488619119879e-06,
      "loss": 1.5664,
      "step": 373
    },
    {
      "epoch": 0.56752655538695,
      "grad_norm": 0.5973164439201355,
      "learning_rate": 4.290591805766313e-06,
      "loss": 1.6008,
      "step": 374
    },
    {
      "epoch": 0.5690440060698028,
      "grad_norm": 0.6398727297782898,
      "learning_rate": 4.288694992412747e-06,
      "loss": 1.5964,
      "step": 375
    },
    {
      "epoch": 0.5705614567526556,
      "grad_norm": 0.6034932732582092,
      "learning_rate": 4.286798179059181e-06,
      "loss": 1.574,
      "step": 376
    },
    {
      "epoch": 0.5720789074355084,
      "grad_norm": 0.5680099129676819,
      "learning_rate": 4.284901365705615e-06,
      "loss": 1.4148,
      "step": 377
    },
    {
      "epoch": 0.5735963581183612,
      "grad_norm": 0.5267028212547302,
      "learning_rate": 4.283004552352049e-06,
      "loss": 1.4375,
      "step": 378
    },
    {
      "epoch": 0.575113808801214,
      "grad_norm": 0.5374861359596252,
      "learning_rate": 4.281107738998483e-06,
      "loss": 1.4015,
      "step": 379
    },
    {
      "epoch": 0.5766312594840668,
      "grad_norm": 0.6280297636985779,
      "learning_rate": 4.2792109256449175e-06,
      "loss": 1.6314,
      "step": 380
    },
    {
      "epoch": 0.5781487101669196,
      "grad_norm": 0.5112261176109314,
      "learning_rate": 4.277314112291351e-06,
      "loss": 1.2869,
      "step": 381
    },
    {
      "epoch": 0.5796661608497724,
      "grad_norm": 0.543584942817688,
      "learning_rate": 4.275417298937785e-06,
      "loss": 1.4995,
      "step": 382
    },
    {
      "epoch": 0.5811836115326252,
      "grad_norm": 0.5986610651016235,
      "learning_rate": 4.2735204855842184e-06,
      "loss": 1.4887,
      "step": 383
    },
    {
      "epoch": 0.582701062215478,
      "grad_norm": 0.535126805305481,
      "learning_rate": 4.271623672230653e-06,
      "loss": 1.4933,
      "step": 384
    },
    {
      "epoch": 0.5842185128983308,
      "grad_norm": 0.5898280143737793,
      "learning_rate": 4.269726858877087e-06,
      "loss": 1.5241,
      "step": 385
    },
    {
      "epoch": 0.5857359635811836,
      "grad_norm": 0.5706352591514587,
      "learning_rate": 4.267830045523521e-06,
      "loss": 1.5089,
      "step": 386
    },
    {
      "epoch": 0.5872534142640364,
      "grad_norm": 0.7310394048690796,
      "learning_rate": 4.265933232169955e-06,
      "loss": 1.5774,
      "step": 387
    },
    {
      "epoch": 0.5887708649468892,
      "grad_norm": 0.611628532409668,
      "learning_rate": 4.264036418816389e-06,
      "loss": 1.5579,
      "step": 388
    },
    {
      "epoch": 0.590288315629742,
      "grad_norm": 0.6000181436538696,
      "learning_rate": 4.262139605462823e-06,
      "loss": 1.4628,
      "step": 389
    },
    {
      "epoch": 0.5918057663125948,
      "grad_norm": 0.551677942276001,
      "learning_rate": 4.260242792109257e-06,
      "loss": 1.358,
      "step": 390
    },
    {
      "epoch": 0.5933232169954477,
      "grad_norm": 0.5693620443344116,
      "learning_rate": 4.258345978755691e-06,
      "loss": 1.4223,
      "step": 391
    },
    {
      "epoch": 0.5948406676783005,
      "grad_norm": 0.5624873638153076,
      "learning_rate": 4.2564491654021255e-06,
      "loss": 1.4506,
      "step": 392
    },
    {
      "epoch": 0.5963581183611533,
      "grad_norm": 0.5493528842926025,
      "learning_rate": 4.2545523520485585e-06,
      "loss": 1.1984,
      "step": 393
    },
    {
      "epoch": 0.5978755690440061,
      "grad_norm": 0.557110071182251,
      "learning_rate": 4.252655538694993e-06,
      "loss": 1.5242,
      "step": 394
    },
    {
      "epoch": 0.5993930197268589,
      "grad_norm": 0.5433549880981445,
      "learning_rate": 4.250758725341426e-06,
      "loss": 1.4271,
      "step": 395
    },
    {
      "epoch": 0.6009104704097117,
      "grad_norm": 0.5263792872428894,
      "learning_rate": 4.24886191198786e-06,
      "loss": 1.3816,
      "step": 396
    },
    {
      "epoch": 0.6024279210925645,
      "grad_norm": 0.5964055061340332,
      "learning_rate": 4.246965098634295e-06,
      "loss": 1.5163,
      "step": 397
    },
    {
      "epoch": 0.6039453717754173,
      "grad_norm": 0.4954032599925995,
      "learning_rate": 4.245068285280728e-06,
      "loss": 1.3144,
      "step": 398
    },
    {
      "epoch": 0.6054628224582701,
      "grad_norm": 0.604634165763855,
      "learning_rate": 4.243171471927163e-06,
      "loss": 1.5565,
      "step": 399
    },
    {
      "epoch": 0.6069802731411229,
      "grad_norm": 0.49928390979766846,
      "learning_rate": 4.241274658573596e-06,
      "loss": 1.3523,
      "step": 400
    },
    {
      "epoch": 0.6084977238239757,
      "grad_norm": 0.5421729683876038,
      "learning_rate": 4.239377845220031e-06,
      "loss": 1.4907,
      "step": 401
    },
    {
      "epoch": 0.6100151745068285,
      "grad_norm": 0.5622231960296631,
      "learning_rate": 4.237481031866465e-06,
      "loss": 1.4901,
      "step": 402
    },
    {
      "epoch": 0.6115326251896813,
      "grad_norm": 0.455873042345047,
      "learning_rate": 4.235584218512899e-06,
      "loss": 1.1994,
      "step": 403
    },
    {
      "epoch": 0.6130500758725341,
      "grad_norm": 0.6127259731292725,
      "learning_rate": 4.2336874051593326e-06,
      "loss": 1.5477,
      "step": 404
    },
    {
      "epoch": 0.6145675265553869,
      "grad_norm": 0.566137969493866,
      "learning_rate": 4.2317905918057665e-06,
      "loss": 1.478,
      "step": 405
    },
    {
      "epoch": 0.6160849772382397,
      "grad_norm": 0.5281153321266174,
      "learning_rate": 4.2298937784522004e-06,
      "loss": 1.3986,
      "step": 406
    },
    {
      "epoch": 0.6176024279210925,
      "grad_norm": 0.4907263219356537,
      "learning_rate": 4.227996965098634e-06,
      "loss": 1.322,
      "step": 407
    },
    {
      "epoch": 0.6191198786039454,
      "grad_norm": 0.5937967300415039,
      "learning_rate": 4.226100151745068e-06,
      "loss": 1.4733,
      "step": 408
    },
    {
      "epoch": 0.6206373292867982,
      "grad_norm": 0.5622734427452087,
      "learning_rate": 4.224203338391503e-06,
      "loss": 1.5215,
      "step": 409
    },
    {
      "epoch": 0.622154779969651,
      "grad_norm": 0.5772116184234619,
      "learning_rate": 4.222306525037936e-06,
      "loss": 1.4661,
      "step": 410
    },
    {
      "epoch": 0.6236722306525038,
      "grad_norm": 0.5607485771179199,
      "learning_rate": 4.220409711684371e-06,
      "loss": 1.4161,
      "step": 411
    },
    {
      "epoch": 0.6251896813353566,
      "grad_norm": 0.5763262510299683,
      "learning_rate": 4.218512898330804e-06,
      "loss": 1.5283,
      "step": 412
    },
    {
      "epoch": 0.6267071320182094,
      "grad_norm": 0.534227192401886,
      "learning_rate": 4.216616084977239e-06,
      "loss": 1.3359,
      "step": 413
    },
    {
      "epoch": 0.6282245827010622,
      "grad_norm": 0.49940866231918335,
      "learning_rate": 4.214719271623673e-06,
      "loss": 1.3678,
      "step": 414
    },
    {
      "epoch": 0.629742033383915,
      "grad_norm": 0.5653464794158936,
      "learning_rate": 4.212822458270107e-06,
      "loss": 1.4693,
      "step": 415
    },
    {
      "epoch": 0.6312594840667678,
      "grad_norm": 0.5381935238838196,
      "learning_rate": 4.2109256449165405e-06,
      "loss": 1.4128,
      "step": 416
    },
    {
      "epoch": 0.6327769347496206,
      "grad_norm": 0.7321341633796692,
      "learning_rate": 4.2090288315629745e-06,
      "loss": 1.5258,
      "step": 417
    },
    {
      "epoch": 0.6342943854324734,
      "grad_norm": 0.49959489703178406,
      "learning_rate": 4.207132018209408e-06,
      "loss": 1.3451,
      "step": 418
    },
    {
      "epoch": 0.6358118361153262,
      "grad_norm": 0.5754705667495728,
      "learning_rate": 4.205235204855842e-06,
      "loss": 1.4525,
      "step": 419
    },
    {
      "epoch": 0.637329286798179,
      "grad_norm": 0.5571062564849854,
      "learning_rate": 4.203338391502276e-06,
      "loss": 1.4471,
      "step": 420
    },
    {
      "epoch": 0.6388467374810318,
      "grad_norm": 0.4880090057849884,
      "learning_rate": 4.20144157814871e-06,
      "loss": 1.2586,
      "step": 421
    },
    {
      "epoch": 0.6403641881638846,
      "grad_norm": 0.5454453825950623,
      "learning_rate": 4.199544764795144e-06,
      "loss": 1.4197,
      "step": 422
    },
    {
      "epoch": 0.6418816388467374,
      "grad_norm": 0.5665869116783142,
      "learning_rate": 4.197647951441579e-06,
      "loss": 1.3844,
      "step": 423
    },
    {
      "epoch": 0.6433990895295902,
      "grad_norm": 0.5569076538085938,
      "learning_rate": 4.195751138088012e-06,
      "loss": 1.4706,
      "step": 424
    },
    {
      "epoch": 0.644916540212443,
      "grad_norm": 0.5874656438827515,
      "learning_rate": 4.193854324734447e-06,
      "loss": 1.4706,
      "step": 425
    },
    {
      "epoch": 0.6464339908952959,
      "grad_norm": 0.5947632789611816,
      "learning_rate": 4.191957511380881e-06,
      "loss": 1.5445,
      "step": 426
    },
    {
      "epoch": 0.6479514415781487,
      "grad_norm": 0.5843673944473267,
      "learning_rate": 4.1900606980273146e-06,
      "loss": 1.4651,
      "step": 427
    },
    {
      "epoch": 0.6494688922610015,
      "grad_norm": 0.6201871633529663,
      "learning_rate": 4.1881638846737485e-06,
      "loss": 1.4466,
      "step": 428
    },
    {
      "epoch": 0.6509863429438544,
      "grad_norm": 0.5590775012969971,
      "learning_rate": 4.186267071320182e-06,
      "loss": 1.4154,
      "step": 429
    },
    {
      "epoch": 0.6525037936267072,
      "grad_norm": 0.6429518461227417,
      "learning_rate": 4.184370257966616e-06,
      "loss": 1.3915,
      "step": 430
    },
    {
      "epoch": 0.65402124430956,
      "grad_norm": 0.6168590188026428,
      "learning_rate": 4.18247344461305e-06,
      "loss": 1.524,
      "step": 431
    },
    {
      "epoch": 0.6555386949924128,
      "grad_norm": 0.5458989143371582,
      "learning_rate": 4.180576631259484e-06,
      "loss": 1.3496,
      "step": 432
    },
    {
      "epoch": 0.6570561456752656,
      "grad_norm": 0.5254637598991394,
      "learning_rate": 4.178679817905918e-06,
      "loss": 1.4041,
      "step": 433
    },
    {
      "epoch": 0.6585735963581184,
      "grad_norm": 0.6290822625160217,
      "learning_rate": 4.176783004552352e-06,
      "loss": 1.525,
      "step": 434
    },
    {
      "epoch": 0.6600910470409712,
      "grad_norm": 0.51552414894104,
      "learning_rate": 4.174886191198787e-06,
      "loss": 1.2929,
      "step": 435
    },
    {
      "epoch": 0.661608497723824,
      "grad_norm": 0.5317790508270264,
      "learning_rate": 4.17298937784522e-06,
      "loss": 1.4006,
      "step": 436
    },
    {
      "epoch": 0.6631259484066768,
      "grad_norm": 0.5863569378852844,
      "learning_rate": 4.171092564491655e-06,
      "loss": 1.514,
      "step": 437
    },
    {
      "epoch": 0.6646433990895296,
      "grad_norm": 0.5978503823280334,
      "learning_rate": 4.169195751138089e-06,
      "loss": 1.5157,
      "step": 438
    },
    {
      "epoch": 0.6661608497723824,
      "grad_norm": 0.49966782331466675,
      "learning_rate": 4.1672989377845225e-06,
      "loss": 1.2509,
      "step": 439
    },
    {
      "epoch": 0.6676783004552352,
      "grad_norm": 0.6166907548904419,
      "learning_rate": 4.1654021244309564e-06,
      "loss": 1.4841,
      "step": 440
    },
    {
      "epoch": 0.669195751138088,
      "grad_norm": 0.5208405256271362,
      "learning_rate": 4.16350531107739e-06,
      "loss": 1.3073,
      "step": 441
    },
    {
      "epoch": 0.6707132018209409,
      "grad_norm": 0.563643217086792,
      "learning_rate": 4.161608497723824e-06,
      "loss": 1.346,
      "step": 442
    },
    {
      "epoch": 0.6722306525037937,
      "grad_norm": 0.5620986223220825,
      "learning_rate": 4.159711684370258e-06,
      "loss": 1.4223,
      "step": 443
    },
    {
      "epoch": 0.6737481031866465,
      "grad_norm": 0.6089151501655579,
      "learning_rate": 4.157814871016692e-06,
      "loss": 1.5104,
      "step": 444
    },
    {
      "epoch": 0.6752655538694993,
      "grad_norm": 0.5502729415893555,
      "learning_rate": 4.155918057663126e-06,
      "loss": 1.3744,
      "step": 445
    },
    {
      "epoch": 0.6767830045523521,
      "grad_norm": 0.6057721376419067,
      "learning_rate": 4.15402124430956e-06,
      "loss": 1.4936,
      "step": 446
    },
    {
      "epoch": 0.6783004552352049,
      "grad_norm": 0.5744244456291199,
      "learning_rate": 4.152124430955995e-06,
      "loss": 1.4308,
      "step": 447
    },
    {
      "epoch": 0.6798179059180577,
      "grad_norm": 0.5685344338417053,
      "learning_rate": 4.150227617602428e-06,
      "loss": 1.4295,
      "step": 448
    },
    {
      "epoch": 0.6813353566009105,
      "grad_norm": 0.5779551267623901,
      "learning_rate": 4.148330804248863e-06,
      "loss": 1.3701,
      "step": 449
    },
    {
      "epoch": 0.6828528072837633,
      "grad_norm": 0.5613082051277161,
      "learning_rate": 4.146433990895296e-06,
      "loss": 1.3036,
      "step": 450
    },
    {
      "epoch": 0.6843702579666161,
      "grad_norm": 0.668230414390564,
      "learning_rate": 4.1445371775417305e-06,
      "loss": 1.4198,
      "step": 451
    },
    {
      "epoch": 0.6858877086494689,
      "grad_norm": 0.5357564687728882,
      "learning_rate": 4.142640364188164e-06,
      "loss": 1.3605,
      "step": 452
    },
    {
      "epoch": 0.6874051593323217,
      "grad_norm": 0.5597037672996521,
      "learning_rate": 4.140743550834598e-06,
      "loss": 1.3554,
      "step": 453
    },
    {
      "epoch": 0.6889226100151745,
      "grad_norm": 0.5845338106155396,
      "learning_rate": 4.138846737481032e-06,
      "loss": 1.2758,
      "step": 454
    },
    {
      "epoch": 0.6904400606980273,
      "grad_norm": 0.5538659691810608,
      "learning_rate": 4.136949924127466e-06,
      "loss": 1.3762,
      "step": 455
    },
    {
      "epoch": 0.6919575113808801,
      "grad_norm": 0.6102843880653381,
      "learning_rate": 4.1350531107739e-06,
      "loss": 1.3954,
      "step": 456
    },
    {
      "epoch": 0.6934749620637329,
      "grad_norm": 0.5929787755012512,
      "learning_rate": 4.133156297420334e-06,
      "loss": 1.444,
      "step": 457
    },
    {
      "epoch": 0.6949924127465857,
      "grad_norm": 0.4879250228404999,
      "learning_rate": 4.131259484066768e-06,
      "loss": 1.1886,
      "step": 458
    },
    {
      "epoch": 0.6965098634294385,
      "grad_norm": 0.720162034034729,
      "learning_rate": 4.129362670713203e-06,
      "loss": 1.3789,
      "step": 459
    },
    {
      "epoch": 0.6980273141122914,
      "grad_norm": 0.5712143778800964,
      "learning_rate": 4.127465857359636e-06,
      "loss": 1.3938,
      "step": 460
    },
    {
      "epoch": 0.6995447647951442,
      "grad_norm": 0.6945801973342896,
      "learning_rate": 4.1255690440060706e-06,
      "loss": 1.3043,
      "step": 461
    },
    {
      "epoch": 0.701062215477997,
      "grad_norm": 0.5791314244270325,
      "learning_rate": 4.123672230652504e-06,
      "loss": 1.3537,
      "step": 462
    },
    {
      "epoch": 0.7025796661608498,
      "grad_norm": 0.514592170715332,
      "learning_rate": 4.121775417298938e-06,
      "loss": 1.3275,
      "step": 463
    },
    {
      "epoch": 0.7040971168437026,
      "grad_norm": 0.6030213832855225,
      "learning_rate": 4.119878603945372e-06,
      "loss": 1.3267,
      "step": 464
    },
    {
      "epoch": 0.7056145675265554,
      "grad_norm": 0.5351337790489197,
      "learning_rate": 4.117981790591806e-06,
      "loss": 1.3214,
      "step": 465
    },
    {
      "epoch": 0.7071320182094082,
      "grad_norm": 0.6597126722335815,
      "learning_rate": 4.11608497723824e-06,
      "loss": 1.4733,
      "step": 466
    },
    {
      "epoch": 0.708649468892261,
      "grad_norm": 0.5972892642021179,
      "learning_rate": 4.114188163884674e-06,
      "loss": 1.4286,
      "step": 467
    },
    {
      "epoch": 0.7101669195751138,
      "grad_norm": 0.5242651104927063,
      "learning_rate": 4.112291350531108e-06,
      "loss": 1.3223,
      "step": 468
    },
    {
      "epoch": 0.7116843702579666,
      "grad_norm": 0.5747799277305603,
      "learning_rate": 4.110394537177542e-06,
      "loss": 1.4173,
      "step": 469
    },
    {
      "epoch": 0.7132018209408194,
      "grad_norm": 0.6069474220275879,
      "learning_rate": 4.108497723823976e-06,
      "loss": 1.3409,
      "step": 470
    },
    {
      "epoch": 0.7147192716236722,
      "grad_norm": 0.6043670177459717,
      "learning_rate": 4.10660091047041e-06,
      "loss": 1.3155,
      "step": 471
    },
    {
      "epoch": 0.716236722306525,
      "grad_norm": 0.5714666247367859,
      "learning_rate": 4.104704097116844e-06,
      "loss": 1.3863,
      "step": 472
    },
    {
      "epoch": 0.7177541729893778,
      "grad_norm": 0.5791792869567871,
      "learning_rate": 4.1028072837632785e-06,
      "loss": 1.3398,
      "step": 473
    },
    {
      "epoch": 0.7192716236722306,
      "grad_norm": 0.656827449798584,
      "learning_rate": 4.100910470409712e-06,
      "loss": 1.3589,
      "step": 474
    },
    {
      "epoch": 0.7207890743550834,
      "grad_norm": 0.5827882885932922,
      "learning_rate": 4.099013657056146e-06,
      "loss": 1.3461,
      "step": 475
    },
    {
      "epoch": 0.7223065250379362,
      "grad_norm": 0.5464465618133545,
      "learning_rate": 4.09711684370258e-06,
      "loss": 1.2828,
      "step": 476
    },
    {
      "epoch": 0.723823975720789,
      "grad_norm": 0.5064337849617004,
      "learning_rate": 4.095220030349014e-06,
      "loss": 1.123,
      "step": 477
    },
    {
      "epoch": 0.7253414264036419,
      "grad_norm": 0.5905984044075012,
      "learning_rate": 4.093323216995448e-06,
      "loss": 1.3723,
      "step": 478
    },
    {
      "epoch": 0.7268588770864947,
      "grad_norm": 0.605913519859314,
      "learning_rate": 4.091426403641882e-06,
      "loss": 1.4082,
      "step": 479
    },
    {
      "epoch": 0.7283763277693475,
      "grad_norm": 0.5312521457672119,
      "learning_rate": 4.089529590288316e-06,
      "loss": 1.3023,
      "step": 480
    },
    {
      "epoch": 0.7298937784522003,
      "grad_norm": 0.5919853448867798,
      "learning_rate": 4.08763277693475e-06,
      "loss": 1.4491,
      "step": 481
    },
    {
      "epoch": 0.7314112291350531,
      "grad_norm": 0.8036760687828064,
      "learning_rate": 4.085735963581184e-06,
      "loss": 1.4703,
      "step": 482
    },
    {
      "epoch": 0.7329286798179059,
      "grad_norm": 0.5746173858642578,
      "learning_rate": 4.083839150227618e-06,
      "loss": 1.3924,
      "step": 483
    },
    {
      "epoch": 0.7344461305007587,
      "grad_norm": 0.6384495496749878,
      "learning_rate": 4.081942336874052e-06,
      "loss": 1.4382,
      "step": 484
    },
    {
      "epoch": 0.7359635811836115,
      "grad_norm": 0.6528049111366272,
      "learning_rate": 4.0800455235204865e-06,
      "loss": 1.3985,
      "step": 485
    },
    {
      "epoch": 0.7374810318664643,
      "grad_norm": 0.5686211585998535,
      "learning_rate": 4.0781487101669195e-06,
      "loss": 1.3371,
      "step": 486
    },
    {
      "epoch": 0.7389984825493171,
      "grad_norm": 0.5533775091171265,
      "learning_rate": 4.076251896813354e-06,
      "loss": 1.2848,
      "step": 487
    },
    {
      "epoch": 0.7405159332321699,
      "grad_norm": 0.5539667010307312,
      "learning_rate": 4.074355083459787e-06,
      "loss": 1.3247,
      "step": 488
    },
    {
      "epoch": 0.7420333839150227,
      "grad_norm": 0.5494842529296875,
      "learning_rate": 4.072458270106222e-06,
      "loss": 1.2866,
      "step": 489
    },
    {
      "epoch": 0.7435508345978755,
      "grad_norm": 0.5410556197166443,
      "learning_rate": 4.070561456752656e-06,
      "loss": 1.1601,
      "step": 490
    },
    {
      "epoch": 0.7450682852807283,
      "grad_norm": 0.5904690027236938,
      "learning_rate": 4.06866464339909e-06,
      "loss": 1.3999,
      "step": 491
    },
    {
      "epoch": 0.7465857359635811,
      "grad_norm": 0.5558508038520813,
      "learning_rate": 4.066767830045524e-06,
      "loss": 1.292,
      "step": 492
    },
    {
      "epoch": 0.7481031866464339,
      "grad_norm": 0.6429516673088074,
      "learning_rate": 4.064871016691958e-06,
      "loss": 1.3913,
      "step": 493
    },
    {
      "epoch": 0.7496206373292867,
      "grad_norm": 0.5553253889083862,
      "learning_rate": 4.062974203338392e-06,
      "loss": 1.327,
      "step": 494
    },
    {
      "epoch": 0.7511380880121397,
      "grad_norm": 0.5266602039337158,
      "learning_rate": 4.061077389984826e-06,
      "loss": 1.226,
      "step": 495
    },
    {
      "epoch": 0.7526555386949925,
      "grad_norm": 0.5489621758460999,
      "learning_rate": 4.05918057663126e-06,
      "loss": 1.1249,
      "step": 496
    },
    {
      "epoch": 0.7541729893778453,
      "grad_norm": 0.5787691473960876,
      "learning_rate": 4.057283763277694e-06,
      "loss": 1.3184,
      "step": 497
    },
    {
      "epoch": 0.7556904400606981,
      "grad_norm": 0.6036232709884644,
      "learning_rate": 4.0553869499241275e-06,
      "loss": 1.3527,
      "step": 498
    },
    {
      "epoch": 0.7572078907435509,
      "grad_norm": 0.5443717241287231,
      "learning_rate": 4.053490136570562e-06,
      "loss": 1.2655,
      "step": 499
    },
    {
      "epoch": 0.7587253414264037,
      "grad_norm": 0.5390751361846924,
      "learning_rate": 4.051593323216995e-06,
      "loss": 1.1065,
      "step": 500
    },
    {
      "epoch": 0.7602427921092565,
      "grad_norm": 0.5936874747276306,
      "learning_rate": 4.04969650986343e-06,
      "loss": 1.3637,
      "step": 501
    },
    {
      "epoch": 0.7617602427921093,
      "grad_norm": 0.6232985258102417,
      "learning_rate": 4.047799696509864e-06,
      "loss": 1.3332,
      "step": 502
    },
    {
      "epoch": 0.7632776934749621,
      "grad_norm": 0.5746336579322815,
      "learning_rate": 4.045902883156298e-06,
      "loss": 1.3142,
      "step": 503
    },
    {
      "epoch": 0.7647951441578149,
      "grad_norm": 0.6048758029937744,
      "learning_rate": 4.044006069802732e-06,
      "loss": 1.3021,
      "step": 504
    },
    {
      "epoch": 0.7663125948406677,
      "grad_norm": 0.5871798396110535,
      "learning_rate": 4.042109256449166e-06,
      "loss": 1.2891,
      "step": 505
    },
    {
      "epoch": 0.7678300455235205,
      "grad_norm": 0.7579604983329773,
      "learning_rate": 4.0402124430956e-06,
      "loss": 1.3731,
      "step": 506
    },
    {
      "epoch": 0.7693474962063733,
      "grad_norm": 0.6365904808044434,
      "learning_rate": 4.038315629742034e-06,
      "loss": 1.3714,
      "step": 507
    },
    {
      "epoch": 0.7708649468892261,
      "grad_norm": 0.5318761467933655,
      "learning_rate": 4.036418816388468e-06,
      "loss": 1.1206,
      "step": 508
    },
    {
      "epoch": 0.7723823975720789,
      "grad_norm": 0.6556257009506226,
      "learning_rate": 4.0345220030349015e-06,
      "loss": 1.4171,
      "step": 509
    },
    {
      "epoch": 0.7738998482549317,
      "grad_norm": 0.5792012810707092,
      "learning_rate": 4.0326251896813355e-06,
      "loss": 1.3461,
      "step": 510
    },
    {
      "epoch": 0.7754172989377845,
      "grad_norm": 0.5866755247116089,
      "learning_rate": 4.03072837632777e-06,
      "loss": 1.2107,
      "step": 511
    },
    {
      "epoch": 0.7769347496206374,
      "grad_norm": 0.6344605088233948,
      "learning_rate": 4.028831562974203e-06,
      "loss": 1.2891,
      "step": 512
    },
    {
      "epoch": 0.7784522003034902,
      "grad_norm": 0.6137851476669312,
      "learning_rate": 4.026934749620638e-06,
      "loss": 1.3848,
      "step": 513
    },
    {
      "epoch": 0.779969650986343,
      "grad_norm": 0.5667396783828735,
      "learning_rate": 4.025037936267072e-06,
      "loss": 1.304,
      "step": 514
    },
    {
      "epoch": 0.7814871016691958,
      "grad_norm": 0.6420179605484009,
      "learning_rate": 4.023141122913506e-06,
      "loss": 1.3565,
      "step": 515
    },
    {
      "epoch": 0.7830045523520486,
      "grad_norm": 0.6154142618179321,
      "learning_rate": 4.02124430955994e-06,
      "loss": 1.3131,
      "step": 516
    },
    {
      "epoch": 0.7845220030349014,
      "grad_norm": 0.6223462224006653,
      "learning_rate": 4.019347496206374e-06,
      "loss": 1.3492,
      "step": 517
    },
    {
      "epoch": 0.7860394537177542,
      "grad_norm": 0.5994542241096497,
      "learning_rate": 4.017450682852808e-06,
      "loss": 1.3487,
      "step": 518
    },
    {
      "epoch": 0.787556904400607,
      "grad_norm": 0.7692631483078003,
      "learning_rate": 4.015553869499242e-06,
      "loss": 1.2037,
      "step": 519
    },
    {
      "epoch": 0.7890743550834598,
      "grad_norm": 0.5956696271896362,
      "learning_rate": 4.0136570561456756e-06,
      "loss": 1.2654,
      "step": 520
    },
    {
      "epoch": 0.7905918057663126,
      "grad_norm": 0.6287387609481812,
      "learning_rate": 4.0117602427921095e-06,
      "loss": 1.3377,
      "step": 521
    },
    {
      "epoch": 0.7921092564491654,
      "grad_norm": 0.6997042894363403,
      "learning_rate": 4.009863429438543e-06,
      "loss": 1.3448,
      "step": 522
    },
    {
      "epoch": 0.7936267071320182,
      "grad_norm": 0.676548421382904,
      "learning_rate": 4.007966616084978e-06,
      "loss": 1.3991,
      "step": 523
    },
    {
      "epoch": 0.795144157814871,
      "grad_norm": 0.6693485379219055,
      "learning_rate": 4.006069802731411e-06,
      "loss": 1.3636,
      "step": 524
    },
    {
      "epoch": 0.7966616084977238,
      "grad_norm": 0.60329270362854,
      "learning_rate": 4.004172989377846e-06,
      "loss": 1.3421,
      "step": 525
    },
    {
      "epoch": 0.7981790591805766,
      "grad_norm": 0.6439431309700012,
      "learning_rate": 4.00227617602428e-06,
      "loss": 1.3668,
      "step": 526
    },
    {
      "epoch": 0.7996965098634294,
      "grad_norm": 0.6625509858131409,
      "learning_rate": 4.000379362670714e-06,
      "loss": 1.3977,
      "step": 527
    },
    {
      "epoch": 0.8012139605462822,
      "grad_norm": 0.5714672803878784,
      "learning_rate": 3.998482549317148e-06,
      "loss": 1.1971,
      "step": 528
    },
    {
      "epoch": 0.802731411229135,
      "grad_norm": 0.48881494998931885,
      "learning_rate": 3.996585735963581e-06,
      "loss": 1.0821,
      "step": 529
    },
    {
      "epoch": 0.8042488619119879,
      "grad_norm": 0.6553323864936829,
      "learning_rate": 3.994688922610016e-06,
      "loss": 1.3872,
      "step": 530
    },
    {
      "epoch": 0.8057663125948407,
      "grad_norm": 0.6148123741149902,
      "learning_rate": 3.99279210925645e-06,
      "loss": 1.283,
      "step": 531
    },
    {
      "epoch": 0.8072837632776935,
      "grad_norm": 0.6315016746520996,
      "learning_rate": 3.9908952959028835e-06,
      "loss": 1.3377,
      "step": 532
    },
    {
      "epoch": 0.8088012139605463,
      "grad_norm": 0.6089475750923157,
      "learning_rate": 3.9889984825493174e-06,
      "loss": 1.3413,
      "step": 533
    },
    {
      "epoch": 0.8103186646433991,
      "grad_norm": 0.6164594292640686,
      "learning_rate": 3.987101669195751e-06,
      "loss": 1.2519,
      "step": 534
    },
    {
      "epoch": 0.8118361153262519,
      "grad_norm": 0.6173480749130249,
      "learning_rate": 3.985204855842185e-06,
      "loss": 1.2907,
      "step": 535
    },
    {
      "epoch": 0.8133535660091047,
      "grad_norm": 0.6444141864776611,
      "learning_rate": 3.983308042488619e-06,
      "loss": 1.2565,
      "step": 536
    },
    {
      "epoch": 0.8148710166919575,
      "grad_norm": 0.6523893475532532,
      "learning_rate": 3.981411229135053e-06,
      "loss": 1.2185,
      "step": 537
    },
    {
      "epoch": 0.8163884673748103,
      "grad_norm": 0.7135000228881836,
      "learning_rate": 3.979514415781487e-06,
      "loss": 1.3711,
      "step": 538
    },
    {
      "epoch": 0.8179059180576631,
      "grad_norm": 0.6239281296730042,
      "learning_rate": 3.977617602427921e-06,
      "loss": 1.3593,
      "step": 539
    },
    {
      "epoch": 0.8194233687405159,
      "grad_norm": 0.6924601197242737,
      "learning_rate": 3.975720789074356e-06,
      "loss": 1.3718,
      "step": 540
    },
    {
      "epoch": 0.8209408194233687,
      "grad_norm": 0.6840732097625732,
      "learning_rate": 3.973823975720789e-06,
      "loss": 1.274,
      "step": 541
    },
    {
      "epoch": 0.8224582701062215,
      "grad_norm": 0.6234981417655945,
      "learning_rate": 3.971927162367224e-06,
      "loss": 1.3084,
      "step": 542
    },
    {
      "epoch": 0.8239757207890743,
      "grad_norm": 0.6240392923355103,
      "learning_rate": 3.9700303490136575e-06,
      "loss": 1.368,
      "step": 543
    },
    {
      "epoch": 0.8254931714719271,
      "grad_norm": 0.7347214221954346,
      "learning_rate": 3.9681335356600915e-06,
      "loss": 1.3174,
      "step": 544
    },
    {
      "epoch": 0.8270106221547799,
      "grad_norm": 0.586729884147644,
      "learning_rate": 3.966236722306525e-06,
      "loss": 1.2496,
      "step": 545
    },
    {
      "epoch": 0.8285280728376327,
      "grad_norm": 0.6665019989013672,
      "learning_rate": 3.964339908952959e-06,
      "loss": 1.3545,
      "step": 546
    },
    {
      "epoch": 0.8300455235204856,
      "grad_norm": 0.6150568127632141,
      "learning_rate": 3.962443095599393e-06,
      "loss": 1.2275,
      "step": 547
    },
    {
      "epoch": 0.8315629742033384,
      "grad_norm": 0.6060002446174622,
      "learning_rate": 3.960546282245827e-06,
      "loss": 1.1595,
      "step": 548
    },
    {
      "epoch": 0.8330804248861912,
      "grad_norm": 0.642095148563385,
      "learning_rate": 3.958649468892261e-06,
      "loss": 1.2511,
      "step": 549
    },
    {
      "epoch": 0.834597875569044,
      "grad_norm": 0.6025371551513672,
      "learning_rate": 3.956752655538695e-06,
      "loss": 1.2213,
      "step": 550
    },
    {
      "epoch": 0.8361153262518968,
      "grad_norm": 0.6165017485618591,
      "learning_rate": 3.954855842185129e-06,
      "loss": 1.192,
      "step": 551
    },
    {
      "epoch": 0.8376327769347496,
      "grad_norm": 0.6700395941734314,
      "learning_rate": 3.952959028831564e-06,
      "loss": 1.3489,
      "step": 552
    },
    {
      "epoch": 0.8391502276176024,
      "grad_norm": 0.6863000392913818,
      "learning_rate": 3.951062215477997e-06,
      "loss": 1.2902,
      "step": 553
    },
    {
      "epoch": 0.8406676783004552,
      "grad_norm": 0.6347008347511292,
      "learning_rate": 3.9491654021244316e-06,
      "loss": 1.2424,
      "step": 554
    },
    {
      "epoch": 0.842185128983308,
      "grad_norm": 0.7164322733879089,
      "learning_rate": 3.947268588770865e-06,
      "loss": 1.368,
      "step": 555
    },
    {
      "epoch": 0.8437025796661608,
      "grad_norm": 0.618610680103302,
      "learning_rate": 3.945371775417299e-06,
      "loss": 1.2511,
      "step": 556
    },
    {
      "epoch": 0.8452200303490136,
      "grad_norm": 0.6236634850502014,
      "learning_rate": 3.943474962063733e-06,
      "loss": 1.2773,
      "step": 557
    },
    {
      "epoch": 0.8467374810318664,
      "grad_norm": 0.5780504941940308,
      "learning_rate": 3.941578148710167e-06,
      "loss": 1.1959,
      "step": 558
    },
    {
      "epoch": 0.8482549317147192,
      "grad_norm": 0.5395984649658203,
      "learning_rate": 3.939681335356601e-06,
      "loss": 1.2074,
      "step": 559
    },
    {
      "epoch": 0.849772382397572,
      "grad_norm": 0.684339165687561,
      "learning_rate": 3.937784522003035e-06,
      "loss": 1.2994,
      "step": 560
    },
    {
      "epoch": 0.8512898330804249,
      "grad_norm": 0.6802208423614502,
      "learning_rate": 3.935887708649469e-06,
      "loss": 1.3226,
      "step": 561
    },
    {
      "epoch": 0.8528072837632777,
      "grad_norm": 0.6320262551307678,
      "learning_rate": 3.933990895295903e-06,
      "loss": 1.3224,
      "step": 562
    },
    {
      "epoch": 0.8543247344461306,
      "grad_norm": 0.6579378843307495,
      "learning_rate": 3.932094081942337e-06,
      "loss": 1.3222,
      "step": 563
    },
    {
      "epoch": 0.8558421851289834,
      "grad_norm": 0.6394983530044556,
      "learning_rate": 3.930197268588772e-06,
      "loss": 1.2538,
      "step": 564
    },
    {
      "epoch": 0.8573596358118362,
      "grad_norm": 0.5847620964050293,
      "learning_rate": 3.928300455235205e-06,
      "loss": 1.2461,
      "step": 565
    },
    {
      "epoch": 0.858877086494689,
      "grad_norm": 0.6988146305084229,
      "learning_rate": 3.9264036418816395e-06,
      "loss": 1.2873,
      "step": 566
    },
    {
      "epoch": 0.8603945371775418,
      "grad_norm": 0.6621590852737427,
      "learning_rate": 3.924506828528073e-06,
      "loss": 1.2746,
      "step": 567
    },
    {
      "epoch": 0.8619119878603946,
      "grad_norm": 0.6258567571640015,
      "learning_rate": 3.922610015174507e-06,
      "loss": 1.188,
      "step": 568
    },
    {
      "epoch": 0.8634294385432474,
      "grad_norm": 0.6787793040275574,
      "learning_rate": 3.920713201820941e-06,
      "loss": 1.2807,
      "step": 569
    },
    {
      "epoch": 0.8649468892261002,
      "grad_norm": 0.5721497535705566,
      "learning_rate": 3.918816388467375e-06,
      "loss": 1.1837,
      "step": 570
    },
    {
      "epoch": 0.866464339908953,
      "grad_norm": 1.0213912725448608,
      "learning_rate": 3.916919575113809e-06,
      "loss": 1.1029,
      "step": 571
    },
    {
      "epoch": 0.8679817905918058,
      "grad_norm": 0.6511566042900085,
      "learning_rate": 3.915022761760243e-06,
      "loss": 1.2004,
      "step": 572
    },
    {
      "epoch": 0.8694992412746586,
      "grad_norm": 0.7098206281661987,
      "learning_rate": 3.913125948406677e-06,
      "loss": 1.2645,
      "step": 573
    },
    {
      "epoch": 0.8710166919575114,
      "grad_norm": 0.6265614032745361,
      "learning_rate": 3.911229135053111e-06,
      "loss": 1.2078,
      "step": 574
    },
    {
      "epoch": 0.8725341426403642,
      "grad_norm": 0.7319982051849365,
      "learning_rate": 3.909332321699545e-06,
      "loss": 1.2876,
      "step": 575
    },
    {
      "epoch": 0.874051593323217,
      "grad_norm": 0.6547860503196716,
      "learning_rate": 3.907435508345979e-06,
      "loss": 1.1775,
      "step": 576
    },
    {
      "epoch": 0.8755690440060698,
      "grad_norm": 0.6680166721343994,
      "learning_rate": 3.905538694992413e-06,
      "loss": 1.2421,
      "step": 577
    },
    {
      "epoch": 0.8770864946889226,
      "grad_norm": 0.6396723985671997,
      "learning_rate": 3.9036418816388475e-06,
      "loss": 1.1978,
      "step": 578
    },
    {
      "epoch": 0.8786039453717754,
      "grad_norm": 0.7300524115562439,
      "learning_rate": 3.9017450682852805e-06,
      "loss": 1.3011,
      "step": 579
    },
    {
      "epoch": 0.8801213960546282,
      "grad_norm": 0.5258088707923889,
      "learning_rate": 3.899848254931715e-06,
      "loss": 1.0386,
      "step": 580
    },
    {
      "epoch": 0.881638846737481,
      "grad_norm": 0.7085638046264648,
      "learning_rate": 3.897951441578149e-06,
      "loss": 1.2711,
      "step": 581
    },
    {
      "epoch": 0.8831562974203339,
      "grad_norm": 0.5980825424194336,
      "learning_rate": 3.896054628224583e-06,
      "loss": 1.0996,
      "step": 582
    },
    {
      "epoch": 0.8846737481031867,
      "grad_norm": 0.6004820466041565,
      "learning_rate": 3.894157814871017e-06,
      "loss": 1.0882,
      "step": 583
    },
    {
      "epoch": 0.8861911987860395,
      "grad_norm": 0.6392678618431091,
      "learning_rate": 3.892261001517451e-06,
      "loss": 1.2252,
      "step": 584
    },
    {
      "epoch": 0.8877086494688923,
      "grad_norm": 0.5787937045097351,
      "learning_rate": 3.890364188163885e-06,
      "loss": 1.0591,
      "step": 585
    },
    {
      "epoch": 0.8892261001517451,
      "grad_norm": 0.6121287941932678,
      "learning_rate": 3.888467374810319e-06,
      "loss": 1.1832,
      "step": 586
    },
    {
      "epoch": 0.8907435508345979,
      "grad_norm": 0.6463755965232849,
      "learning_rate": 3.886570561456753e-06,
      "loss": 1.1851,
      "step": 587
    },
    {
      "epoch": 0.8922610015174507,
      "grad_norm": 0.6421238780021667,
      "learning_rate": 3.884673748103187e-06,
      "loss": 1.1997,
      "step": 588
    },
    {
      "epoch": 0.8937784522003035,
      "grad_norm": 0.6779330372810364,
      "learning_rate": 3.882776934749621e-06,
      "loss": 1.1678,
      "step": 589
    },
    {
      "epoch": 0.8952959028831563,
      "grad_norm": 0.7773991227149963,
      "learning_rate": 3.880880121396055e-06,
      "loss": 1.307,
      "step": 590
    },
    {
      "epoch": 0.8968133535660091,
      "grad_norm": 1.0197347402572632,
      "learning_rate": 3.8789833080424885e-06,
      "loss": 1.2993,
      "step": 591
    },
    {
      "epoch": 0.8983308042488619,
      "grad_norm": 0.6807075142860413,
      "learning_rate": 3.877086494688923e-06,
      "loss": 1.2123,
      "step": 592
    },
    {
      "epoch": 0.8998482549317147,
      "grad_norm": 0.7916385531425476,
      "learning_rate": 3.875189681335357e-06,
      "loss": 1.2551,
      "step": 593
    },
    {
      "epoch": 0.9013657056145675,
      "grad_norm": 0.6429006457328796,
      "learning_rate": 3.873292867981791e-06,
      "loss": 1.2656,
      "step": 594
    },
    {
      "epoch": 0.9028831562974203,
      "grad_norm": 0.6683565378189087,
      "learning_rate": 3.871396054628225e-06,
      "loss": 1.2436,
      "step": 595
    },
    {
      "epoch": 0.9044006069802731,
      "grad_norm": 0.7129449844360352,
      "learning_rate": 3.869499241274659e-06,
      "loss": 1.2503,
      "step": 596
    },
    {
      "epoch": 0.9059180576631259,
      "grad_norm": 0.7577820420265198,
      "learning_rate": 3.867602427921093e-06,
      "loss": 1.1825,
      "step": 597
    },
    {
      "epoch": 0.9074355083459787,
      "grad_norm": 0.6665180325508118,
      "learning_rate": 3.865705614567527e-06,
      "loss": 1.1657,
      "step": 598
    },
    {
      "epoch": 0.9089529590288316,
      "grad_norm": 0.7560456395149231,
      "learning_rate": 3.863808801213961e-06,
      "loss": 1.2675,
      "step": 599
    },
    {
      "epoch": 0.9104704097116844,
      "grad_norm": 0.7175353765487671,
      "learning_rate": 3.861911987860395e-06,
      "loss": 1.2358,
      "step": 600
    },
    {
      "epoch": 0.9119878603945372,
      "grad_norm": 0.6829220056533813,
      "learning_rate": 3.860015174506829e-06,
      "loss": 1.1194,
      "step": 601
    },
    {
      "epoch": 0.91350531107739,
      "grad_norm": 0.6745370030403137,
      "learning_rate": 3.858118361153263e-06,
      "loss": 1.1417,
      "step": 602
    },
    {
      "epoch": 0.9150227617602428,
      "grad_norm": 0.6791129112243652,
      "learning_rate": 3.8562215477996965e-06,
      "loss": 1.1768,
      "step": 603
    },
    {
      "epoch": 0.9165402124430956,
      "grad_norm": 0.57466059923172,
      "learning_rate": 3.854324734446131e-06,
      "loss": 0.9936,
      "step": 604
    },
    {
      "epoch": 0.9180576631259484,
      "grad_norm": 0.7228608727455139,
      "learning_rate": 3.852427921092564e-06,
      "loss": 1.2562,
      "step": 605
    },
    {
      "epoch": 0.9195751138088012,
      "grad_norm": 0.7318534851074219,
      "learning_rate": 3.850531107738999e-06,
      "loss": 1.2056,
      "step": 606
    },
    {
      "epoch": 0.921092564491654,
      "grad_norm": 0.5700934529304504,
      "learning_rate": 3.848634294385433e-06,
      "loss": 0.9731,
      "step": 607
    },
    {
      "epoch": 0.9226100151745068,
      "grad_norm": 0.6936195492744446,
      "learning_rate": 3.846737481031867e-06,
      "loss": 1.1928,
      "step": 608
    },
    {
      "epoch": 0.9241274658573596,
      "grad_norm": 0.7053388953208923,
      "learning_rate": 3.844840667678301e-06,
      "loss": 1.2003,
      "step": 609
    },
    {
      "epoch": 0.9256449165402124,
      "grad_norm": 0.751331627368927,
      "learning_rate": 3.842943854324735e-06,
      "loss": 1.2133,
      "step": 610
    },
    {
      "epoch": 0.9271623672230652,
      "grad_norm": 0.6334368586540222,
      "learning_rate": 3.841047040971169e-06,
      "loss": 1.1608,
      "step": 611
    },
    {
      "epoch": 0.928679817905918,
      "grad_norm": 0.6568412184715271,
      "learning_rate": 3.839150227617603e-06,
      "loss": 1.086,
      "step": 612
    },
    {
      "epoch": 0.9301972685887708,
      "grad_norm": 0.7076822519302368,
      "learning_rate": 3.8372534142640366e-06,
      "loss": 1.1327,
      "step": 613
    },
    {
      "epoch": 0.9317147192716236,
      "grad_norm": 0.7151392102241516,
      "learning_rate": 3.835356600910471e-06,
      "loss": 1.1717,
      "step": 614
    },
    {
      "epoch": 0.9332321699544764,
      "grad_norm": 0.7994325160980225,
      "learning_rate": 3.833459787556904e-06,
      "loss": 1.2384,
      "step": 615
    },
    {
      "epoch": 0.9347496206373292,
      "grad_norm": 0.7164725661277771,
      "learning_rate": 3.831562974203339e-06,
      "loss": 1.2042,
      "step": 616
    },
    {
      "epoch": 0.936267071320182,
      "grad_norm": 0.7342996001243591,
      "learning_rate": 3.829666160849772e-06,
      "loss": 1.2031,
      "step": 617
    },
    {
      "epoch": 0.9377845220030349,
      "grad_norm": 0.6222565174102783,
      "learning_rate": 3.827769347496207e-06,
      "loss": 1.1363,
      "step": 618
    },
    {
      "epoch": 0.9393019726858877,
      "grad_norm": 0.7366554737091064,
      "learning_rate": 3.825872534142641e-06,
      "loss": 1.1863,
      "step": 619
    },
    {
      "epoch": 0.9408194233687405,
      "grad_norm": 0.7937423586845398,
      "learning_rate": 3.823975720789075e-06,
      "loss": 1.2077,
      "step": 620
    },
    {
      "epoch": 0.9423368740515933,
      "grad_norm": 0.7241978645324707,
      "learning_rate": 3.822078907435509e-06,
      "loss": 1.1541,
      "step": 621
    },
    {
      "epoch": 0.9438543247344461,
      "grad_norm": 0.7280402183532715,
      "learning_rate": 3.820182094081943e-06,
      "loss": 1.1299,
      "step": 622
    },
    {
      "epoch": 0.9453717754172989,
      "grad_norm": 0.6718699336051941,
      "learning_rate": 3.818285280728377e-06,
      "loss": 1.1583,
      "step": 623
    },
    {
      "epoch": 0.9468892261001517,
      "grad_norm": 0.7083509564399719,
      "learning_rate": 3.816388467374811e-06,
      "loss": 1.1646,
      "step": 624
    },
    {
      "epoch": 0.9484066767830045,
      "grad_norm": 0.7444880604743958,
      "learning_rate": 3.8144916540212445e-06,
      "loss": 1.2374,
      "step": 625
    },
    {
      "epoch": 0.9499241274658573,
      "grad_norm": 0.6976372003555298,
      "learning_rate": 3.812594840667679e-06,
      "loss": 1.1022,
      "step": 626
    },
    {
      "epoch": 0.9514415781487102,
      "grad_norm": 0.7776029706001282,
      "learning_rate": 3.8106980273141124e-06,
      "loss": 1.2199,
      "step": 627
    },
    {
      "epoch": 0.952959028831563,
      "grad_norm": 0.6921577453613281,
      "learning_rate": 3.8088012139605467e-06,
      "loss": 1.102,
      "step": 628
    },
    {
      "epoch": 0.9544764795144158,
      "grad_norm": 0.7744995951652527,
      "learning_rate": 3.8069044006069806e-06,
      "loss": 1.1814,
      "step": 629
    },
    {
      "epoch": 0.9559939301972686,
      "grad_norm": 0.7066841125488281,
      "learning_rate": 3.805007587253415e-06,
      "loss": 1.1839,
      "step": 630
    },
    {
      "epoch": 0.9575113808801214,
      "grad_norm": 0.7560507655143738,
      "learning_rate": 3.8031107738998485e-06,
      "loss": 1.1629,
      "step": 631
    },
    {
      "epoch": 0.9590288315629742,
      "grad_norm": 0.6510953903198242,
      "learning_rate": 3.801213960546283e-06,
      "loss": 1.0758,
      "step": 632
    },
    {
      "epoch": 0.960546282245827,
      "grad_norm": 0.7333802580833435,
      "learning_rate": 3.7993171471927163e-06,
      "loss": 1.226,
      "step": 633
    },
    {
      "epoch": 0.9620637329286799,
      "grad_norm": 0.6596143245697021,
      "learning_rate": 3.7974203338391507e-06,
      "loss": 1.0839,
      "step": 634
    },
    {
      "epoch": 0.9635811836115327,
      "grad_norm": 0.7016305327415466,
      "learning_rate": 3.7955235204855846e-06,
      "loss": 1.1317,
      "step": 635
    },
    {
      "epoch": 0.9650986342943855,
      "grad_norm": 0.774504542350769,
      "learning_rate": 3.7936267071320185e-06,
      "loss": 1.1657,
      "step": 636
    },
    {
      "epoch": 0.9666160849772383,
      "grad_norm": 0.7246546745300293,
      "learning_rate": 3.7917298937784525e-06,
      "loss": 1.1261,
      "step": 637
    },
    {
      "epoch": 0.9681335356600911,
      "grad_norm": 0.7328459024429321,
      "learning_rate": 3.789833080424887e-06,
      "loss": 1.1655,
      "step": 638
    },
    {
      "epoch": 0.9696509863429439,
      "grad_norm": 0.6376906633377075,
      "learning_rate": 3.7879362670713203e-06,
      "loss": 1.0833,
      "step": 639
    },
    {
      "epoch": 0.9711684370257967,
      "grad_norm": 0.6539663076400757,
      "learning_rate": 3.7860394537177547e-06,
      "loss": 1.0287,
      "step": 640
    },
    {
      "epoch": 0.9726858877086495,
      "grad_norm": 0.7560637593269348,
      "learning_rate": 3.7841426403641886e-06,
      "loss": 1.1567,
      "step": 641
    },
    {
      "epoch": 0.9742033383915023,
      "grad_norm": 0.7377043962478638,
      "learning_rate": 3.7822458270106225e-06,
      "loss": 1.0916,
      "step": 642
    },
    {
      "epoch": 0.9757207890743551,
      "grad_norm": 0.798874020576477,
      "learning_rate": 3.7803490136570564e-06,
      "loss": 1.1115,
      "step": 643
    },
    {
      "epoch": 0.9772382397572079,
      "grad_norm": 0.7908024787902832,
      "learning_rate": 3.778452200303491e-06,
      "loss": 1.1696,
      "step": 644
    },
    {
      "epoch": 0.9787556904400607,
      "grad_norm": 0.7623255848884583,
      "learning_rate": 3.7765553869499243e-06,
      "loss": 1.1287,
      "step": 645
    },
    {
      "epoch": 0.9802731411229135,
      "grad_norm": 0.7493880987167358,
      "learning_rate": 3.7746585735963586e-06,
      "loss": 1.2109,
      "step": 646
    },
    {
      "epoch": 0.9817905918057663,
      "grad_norm": 0.7325438261032104,
      "learning_rate": 3.7727617602427926e-06,
      "loss": 1.0409,
      "step": 647
    },
    {
      "epoch": 0.9833080424886191,
      "grad_norm": 0.7877883911132812,
      "learning_rate": 3.7708649468892265e-06,
      "loss": 1.196,
      "step": 648
    },
    {
      "epoch": 0.9848254931714719,
      "grad_norm": 0.6598672866821289,
      "learning_rate": 3.7689681335356604e-06,
      "loss": 1.0673,
      "step": 649
    },
    {
      "epoch": 0.9863429438543247,
      "grad_norm": 1.0724830627441406,
      "learning_rate": 3.7670713201820948e-06,
      "loss": 1.1299,
      "step": 650
    },
    {
      "epoch": 0.9878603945371776,
      "grad_norm": 0.7076230049133301,
      "learning_rate": 3.7651745068285283e-06,
      "loss": 1.0843,
      "step": 651
    },
    {
      "epoch": 0.9893778452200304,
      "grad_norm": 0.7395166158676147,
      "learning_rate": 3.7632776934749626e-06,
      "loss": 1.1451,
      "step": 652
    },
    {
      "epoch": 0.9908952959028832,
      "grad_norm": 0.7459083199501038,
      "learning_rate": 3.7613808801213965e-06,
      "loss": 1.0464,
      "step": 653
    },
    {
      "epoch": 0.992412746585736,
      "grad_norm": 0.6613230109214783,
      "learning_rate": 3.7594840667678305e-06,
      "loss": 0.9887,
      "step": 654
    },
    {
      "epoch": 0.9939301972685888,
      "grad_norm": 0.7344444990158081,
      "learning_rate": 3.7575872534142644e-06,
      "loss": 1.1264,
      "step": 655
    },
    {
      "epoch": 0.9954476479514416,
      "grad_norm": 0.7575323581695557,
      "learning_rate": 3.7556904400606987e-06,
      "loss": 1.0888,
      "step": 656
    },
    {
      "epoch": 0.9969650986342944,
      "grad_norm": 0.6891176104545593,
      "learning_rate": 3.7537936267071322e-06,
      "loss": 1.1168,
      "step": 657
    },
    {
      "epoch": 0.9984825493171472,
      "grad_norm": 0.9107339978218079,
      "learning_rate": 3.7518968133535666e-06,
      "loss": 1.1433,
      "step": 658
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8037164211273193,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 1.1529,
      "step": 659
    },
    {
      "epoch": 1.0015174506828528,
      "grad_norm": 0.7204551100730896,
      "learning_rate": 3.748103186646434e-06,
      "loss": 1.0383,
      "step": 660
    },
    {
      "epoch": 1.0030349013657056,
      "grad_norm": 0.7586669921875,
      "learning_rate": 3.7462063732928684e-06,
      "loss": 1.0726,
      "step": 661
    },
    {
      "epoch": 1.0045523520485584,
      "grad_norm": 0.7692720890045166,
      "learning_rate": 3.744309559939302e-06,
      "loss": 1.0534,
      "step": 662
    },
    {
      "epoch": 1.0060698027314112,
      "grad_norm": 0.7287057042121887,
      "learning_rate": 3.7424127465857362e-06,
      "loss": 1.0778,
      "step": 663
    },
    {
      "epoch": 1.007587253414264,
      "grad_norm": 0.7262711524963379,
      "learning_rate": 3.74051593323217e-06,
      "loss": 1.0724,
      "step": 664
    },
    {
      "epoch": 1.0091047040971168,
      "grad_norm": 0.8808585405349731,
      "learning_rate": 3.738619119878604e-06,
      "loss": 1.1022,
      "step": 665
    },
    {
      "epoch": 1.0106221547799696,
      "grad_norm": 0.7418603897094727,
      "learning_rate": 3.736722306525038e-06,
      "loss": 0.997,
      "step": 666
    },
    {
      "epoch": 1.0121396054628224,
      "grad_norm": 0.7548208832740784,
      "learning_rate": 3.7348254931714723e-06,
      "loss": 1.0938,
      "step": 667
    },
    {
      "epoch": 1.0136570561456753,
      "grad_norm": 0.7534995079040527,
      "learning_rate": 3.732928679817906e-06,
      "loss": 1.0405,
      "step": 668
    },
    {
      "epoch": 1.015174506828528,
      "grad_norm": 0.6921838521957397,
      "learning_rate": 3.73103186646434e-06,
      "loss": 1.0625,
      "step": 669
    },
    {
      "epoch": 1.0166919575113809,
      "grad_norm": 0.7528606653213501,
      "learning_rate": 3.729135053110774e-06,
      "loss": 1.0848,
      "step": 670
    },
    {
      "epoch": 1.0182094081942337,
      "grad_norm": 0.8860468864440918,
      "learning_rate": 3.727238239757208e-06,
      "loss": 1.1591,
      "step": 671
    },
    {
      "epoch": 1.0197268588770865,
      "grad_norm": 0.7821629047393799,
      "learning_rate": 3.725341426403642e-06,
      "loss": 1.0945,
      "step": 672
    },
    {
      "epoch": 1.0212443095599393,
      "grad_norm": 0.7699531316757202,
      "learning_rate": 3.7234446130500763e-06,
      "loss": 1.1151,
      "step": 673
    },
    {
      "epoch": 1.022761760242792,
      "grad_norm": 0.8526583313941956,
      "learning_rate": 3.72154779969651e-06,
      "loss": 1.0469,
      "step": 674
    },
    {
      "epoch": 1.024279210925645,
      "grad_norm": 0.7682433724403381,
      "learning_rate": 3.719650986342944e-06,
      "loss": 1.1032,
      "step": 675
    },
    {
      "epoch": 1.0257966616084977,
      "grad_norm": 0.7897148728370667,
      "learning_rate": 3.717754172989378e-06,
      "loss": 1.069,
      "step": 676
    },
    {
      "epoch": 1.0273141122913505,
      "grad_norm": 0.808472216129303,
      "learning_rate": 3.715857359635812e-06,
      "loss": 1.114,
      "step": 677
    },
    {
      "epoch": 1.0288315629742033,
      "grad_norm": 0.8366085290908813,
      "learning_rate": 3.713960546282246e-06,
      "loss": 1.0016,
      "step": 678
    },
    {
      "epoch": 1.0303490136570561,
      "grad_norm": 0.7146348357200623,
      "learning_rate": 3.7120637329286803e-06,
      "loss": 1.0339,
      "step": 679
    },
    {
      "epoch": 1.031866464339909,
      "grad_norm": 0.7936529517173767,
      "learning_rate": 3.710166919575114e-06,
      "loss": 1.0971,
      "step": 680
    },
    {
      "epoch": 1.0333839150227617,
      "grad_norm": 0.7844485640525818,
      "learning_rate": 3.708270106221548e-06,
      "loss": 1.075,
      "step": 681
    },
    {
      "epoch": 1.0349013657056145,
      "grad_norm": 0.8085949420928955,
      "learning_rate": 3.706373292867982e-06,
      "loss": 1.0115,
      "step": 682
    },
    {
      "epoch": 1.0364188163884673,
      "grad_norm": 0.7284925580024719,
      "learning_rate": 3.704476479514416e-06,
      "loss": 0.9853,
      "step": 683
    },
    {
      "epoch": 1.0379362670713201,
      "grad_norm": 0.7037591934204102,
      "learning_rate": 3.70257966616085e-06,
      "loss": 0.9893,
      "step": 684
    },
    {
      "epoch": 1.039453717754173,
      "grad_norm": 0.6778113842010498,
      "learning_rate": 3.7006828528072843e-06,
      "loss": 0.894,
      "step": 685
    },
    {
      "epoch": 1.0409711684370258,
      "grad_norm": 0.7623596787452698,
      "learning_rate": 3.6987860394537178e-06,
      "loss": 1.059,
      "step": 686
    },
    {
      "epoch": 1.0424886191198786,
      "grad_norm": 0.8411406874656677,
      "learning_rate": 3.696889226100152e-06,
      "loss": 0.9923,
      "step": 687
    },
    {
      "epoch": 1.0440060698027314,
      "grad_norm": 0.7257373332977295,
      "learning_rate": 3.6949924127465856e-06,
      "loss": 1.0259,
      "step": 688
    },
    {
      "epoch": 1.0455235204855842,
      "grad_norm": 0.8259684443473816,
      "learning_rate": 3.69309559939302e-06,
      "loss": 1.1014,
      "step": 689
    },
    {
      "epoch": 1.047040971168437,
      "grad_norm": 0.7834537029266357,
      "learning_rate": 3.691198786039454e-06,
      "loss": 1.0446,
      "step": 690
    },
    {
      "epoch": 1.0485584218512898,
      "grad_norm": 0.7754220366477966,
      "learning_rate": 3.6893019726858883e-06,
      "loss": 0.9689,
      "step": 691
    },
    {
      "epoch": 1.0500758725341426,
      "grad_norm": 0.7399963140487671,
      "learning_rate": 3.6874051593323218e-06,
      "loss": 0.9852,
      "step": 692
    },
    {
      "epoch": 1.0515933232169954,
      "grad_norm": 0.8115289211273193,
      "learning_rate": 3.685508345978756e-06,
      "loss": 1.1009,
      "step": 693
    },
    {
      "epoch": 1.0531107738998482,
      "grad_norm": 0.8503049612045288,
      "learning_rate": 3.6836115326251896e-06,
      "loss": 1.0601,
      "step": 694
    },
    {
      "epoch": 1.054628224582701,
      "grad_norm": 0.7317591905593872,
      "learning_rate": 3.681714719271624e-06,
      "loss": 1.0136,
      "step": 695
    },
    {
      "epoch": 1.0561456752655538,
      "grad_norm": 0.8762601017951965,
      "learning_rate": 3.679817905918058e-06,
      "loss": 1.0467,
      "step": 696
    },
    {
      "epoch": 1.0576631259484066,
      "grad_norm": 0.9027112722396851,
      "learning_rate": 3.6779210925644922e-06,
      "loss": 1.0392,
      "step": 697
    },
    {
      "epoch": 1.0591805766312594,
      "grad_norm": 0.9217508435249329,
      "learning_rate": 3.6760242792109257e-06,
      "loss": 1.0661,
      "step": 698
    },
    {
      "epoch": 1.0606980273141122,
      "grad_norm": 0.7270858883857727,
      "learning_rate": 3.67412746585736e-06,
      "loss": 0.9786,
      "step": 699
    },
    {
      "epoch": 1.062215477996965,
      "grad_norm": 0.7824820280075073,
      "learning_rate": 3.6722306525037936e-06,
      "loss": 1.0027,
      "step": 700
    },
    {
      "epoch": 1.0637329286798178,
      "grad_norm": 0.7568990588188171,
      "learning_rate": 3.670333839150228e-06,
      "loss": 0.9788,
      "step": 701
    },
    {
      "epoch": 1.0652503793626706,
      "grad_norm": 0.8278170824050903,
      "learning_rate": 3.668437025796662e-06,
      "loss": 1.0998,
      "step": 702
    },
    {
      "epoch": 1.0667678300455234,
      "grad_norm": 0.9132586717605591,
      "learning_rate": 3.6665402124430958e-06,
      "loss": 1.0487,
      "step": 703
    },
    {
      "epoch": 1.0682852807283763,
      "grad_norm": 1.1458830833435059,
      "learning_rate": 3.6646433990895297e-06,
      "loss": 0.9931,
      "step": 704
    },
    {
      "epoch": 1.069802731411229,
      "grad_norm": 0.8139266967773438,
      "learning_rate": 3.662746585735964e-06,
      "loss": 0.9834,
      "step": 705
    },
    {
      "epoch": 1.0713201820940819,
      "grad_norm": 0.9916530251502991,
      "learning_rate": 3.6608497723823976e-06,
      "loss": 1.0158,
      "step": 706
    },
    {
      "epoch": 1.0728376327769347,
      "grad_norm": 0.7564611434936523,
      "learning_rate": 3.658952959028832e-06,
      "loss": 0.944,
      "step": 707
    },
    {
      "epoch": 1.0743550834597875,
      "grad_norm": 0.8698639273643494,
      "learning_rate": 3.657056145675266e-06,
      "loss": 1.0284,
      "step": 708
    },
    {
      "epoch": 1.0758725341426403,
      "grad_norm": 0.912381112575531,
      "learning_rate": 3.6551593323216998e-06,
      "loss": 1.0367,
      "step": 709
    },
    {
      "epoch": 1.077389984825493,
      "grad_norm": 0.808841347694397,
      "learning_rate": 3.6532625189681337e-06,
      "loss": 0.9765,
      "step": 710
    },
    {
      "epoch": 1.078907435508346,
      "grad_norm": 0.7296853065490723,
      "learning_rate": 3.651365705614568e-06,
      "loss": 0.8472,
      "step": 711
    },
    {
      "epoch": 1.0804248861911987,
      "grad_norm": 0.8214021325111389,
      "learning_rate": 3.6494688922610015e-06,
      "loss": 1.0126,
      "step": 712
    },
    {
      "epoch": 1.0819423368740515,
      "grad_norm": 0.7338147759437561,
      "learning_rate": 3.647572078907436e-06,
      "loss": 0.9684,
      "step": 713
    },
    {
      "epoch": 1.0834597875569043,
      "grad_norm": 0.8313868641853333,
      "learning_rate": 3.64567526555387e-06,
      "loss": 1.0459,
      "step": 714
    },
    {
      "epoch": 1.0849772382397571,
      "grad_norm": 0.7836718559265137,
      "learning_rate": 3.6437784522003037e-06,
      "loss": 0.8715,
      "step": 715
    },
    {
      "epoch": 1.08649468892261,
      "grad_norm": 0.8701501488685608,
      "learning_rate": 3.6418816388467377e-06,
      "loss": 1.0133,
      "step": 716
    },
    {
      "epoch": 1.0880121396054627,
      "grad_norm": 0.8458018898963928,
      "learning_rate": 3.639984825493172e-06,
      "loss": 0.8699,
      "step": 717
    },
    {
      "epoch": 1.0895295902883155,
      "grad_norm": 0.8846885561943054,
      "learning_rate": 3.6380880121396055e-06,
      "loss": 0.9535,
      "step": 718
    },
    {
      "epoch": 1.0910470409711683,
      "grad_norm": 0.7917983531951904,
      "learning_rate": 3.63619119878604e-06,
      "loss": 0.9391,
      "step": 719
    },
    {
      "epoch": 1.0925644916540211,
      "grad_norm": 0.9286705851554871,
      "learning_rate": 3.6342943854324738e-06,
      "loss": 1.0306,
      "step": 720
    },
    {
      "epoch": 1.094081942336874,
      "grad_norm": 0.8905529379844666,
      "learning_rate": 3.6323975720789077e-06,
      "loss": 1.0044,
      "step": 721
    },
    {
      "epoch": 1.095599393019727,
      "grad_norm": 0.8306246995925903,
      "learning_rate": 3.6305007587253416e-06,
      "loss": 0.9914,
      "step": 722
    },
    {
      "epoch": 1.0971168437025796,
      "grad_norm": 0.781680703163147,
      "learning_rate": 3.628603945371776e-06,
      "loss": 0.8425,
      "step": 723
    },
    {
      "epoch": 1.0986342943854326,
      "grad_norm": 0.8504987359046936,
      "learning_rate": 3.6267071320182095e-06,
      "loss": 0.933,
      "step": 724
    },
    {
      "epoch": 1.1001517450682852,
      "grad_norm": 0.8606308102607727,
      "learning_rate": 3.624810318664644e-06,
      "loss": 0.9672,
      "step": 725
    },
    {
      "epoch": 1.1016691957511382,
      "grad_norm": 0.7370347380638123,
      "learning_rate": 3.6229135053110773e-06,
      "loss": 0.8917,
      "step": 726
    },
    {
      "epoch": 1.1031866464339908,
      "grad_norm": 0.7889446020126343,
      "learning_rate": 3.6210166919575117e-06,
      "loss": 0.9143,
      "step": 727
    },
    {
      "epoch": 1.1047040971168438,
      "grad_norm": 0.8202736377716064,
      "learning_rate": 3.6191198786039456e-06,
      "loss": 0.928,
      "step": 728
    },
    {
      "epoch": 1.1062215477996964,
      "grad_norm": 0.8588501214981079,
      "learning_rate": 3.61722306525038e-06,
      "loss": 1.0159,
      "step": 729
    },
    {
      "epoch": 1.1077389984825494,
      "grad_norm": 0.8991169929504395,
      "learning_rate": 3.6153262518968135e-06,
      "loss": 0.9166,
      "step": 730
    },
    {
      "epoch": 1.1092564491654022,
      "grad_norm": 0.7844178080558777,
      "learning_rate": 3.613429438543248e-06,
      "loss": 0.9163,
      "step": 731
    },
    {
      "epoch": 1.110773899848255,
      "grad_norm": 1.0611954927444458,
      "learning_rate": 3.6115326251896813e-06,
      "loss": 0.9195,
      "step": 732
    },
    {
      "epoch": 1.1122913505311078,
      "grad_norm": 0.8818607926368713,
      "learning_rate": 3.6096358118361157e-06,
      "loss": 0.8639,
      "step": 733
    },
    {
      "epoch": 1.1138088012139606,
      "grad_norm": 0.8941583037376404,
      "learning_rate": 3.6077389984825496e-06,
      "loss": 0.9946,
      "step": 734
    },
    {
      "epoch": 1.1153262518968134,
      "grad_norm": 0.9234739542007446,
      "learning_rate": 3.605842185128984e-06,
      "loss": 0.8455,
      "step": 735
    },
    {
      "epoch": 1.1168437025796663,
      "grad_norm": 0.8314835429191589,
      "learning_rate": 3.6039453717754174e-06,
      "loss": 0.9794,
      "step": 736
    },
    {
      "epoch": 1.118361153262519,
      "grad_norm": 0.7929196953773499,
      "learning_rate": 3.602048558421852e-06,
      "loss": 0.8812,
      "step": 737
    },
    {
      "epoch": 1.1198786039453719,
      "grad_norm": 0.7820455431938171,
      "learning_rate": 3.6001517450682853e-06,
      "loss": 0.8325,
      "step": 738
    },
    {
      "epoch": 1.1213960546282247,
      "grad_norm": 0.7500519156455994,
      "learning_rate": 3.5982549317147196e-06,
      "loss": 0.8831,
      "step": 739
    },
    {
      "epoch": 1.1229135053110775,
      "grad_norm": 0.8219907283782959,
      "learning_rate": 3.5963581183611536e-06,
      "loss": 0.9331,
      "step": 740
    },
    {
      "epoch": 1.1244309559939303,
      "grad_norm": 0.9043349623680115,
      "learning_rate": 3.594461305007588e-06,
      "loss": 0.95,
      "step": 741
    },
    {
      "epoch": 1.125948406676783,
      "grad_norm": 0.8849051594734192,
      "learning_rate": 3.5925644916540214e-06,
      "loss": 0.934,
      "step": 742
    },
    {
      "epoch": 1.127465857359636,
      "grad_norm": 0.9561841487884521,
      "learning_rate": 3.5906676783004558e-06,
      "loss": 0.9553,
      "step": 743
    },
    {
      "epoch": 1.1289833080424887,
      "grad_norm": 0.8328596949577332,
      "learning_rate": 3.5887708649468893e-06,
      "loss": 0.9604,
      "step": 744
    },
    {
      "epoch": 1.1305007587253415,
      "grad_norm": 0.9572125673294067,
      "learning_rate": 3.5868740515933236e-06,
      "loss": 0.9382,
      "step": 745
    },
    {
      "epoch": 1.1320182094081943,
      "grad_norm": 0.903351902961731,
      "learning_rate": 3.5849772382397575e-06,
      "loss": 0.8958,
      "step": 746
    },
    {
      "epoch": 1.1335356600910471,
      "grad_norm": 0.7612574100494385,
      "learning_rate": 3.5830804248861915e-06,
      "loss": 0.8631,
      "step": 747
    },
    {
      "epoch": 1.1350531107739,
      "grad_norm": 0.8477659225463867,
      "learning_rate": 3.5811836115326254e-06,
      "loss": 0.9275,
      "step": 748
    },
    {
      "epoch": 1.1365705614567527,
      "grad_norm": 0.9165145754814148,
      "learning_rate": 3.5792867981790597e-06,
      "loss": 0.9871,
      "step": 749
    },
    {
      "epoch": 1.1380880121396055,
      "grad_norm": 1.063443660736084,
      "learning_rate": 3.5773899848254932e-06,
      "loss": 0.9268,
      "step": 750
    },
    {
      "epoch": 1.1396054628224583,
      "grad_norm": 0.9806431531906128,
      "learning_rate": 3.5754931714719276e-06,
      "loss": 0.9459,
      "step": 751
    },
    {
      "epoch": 1.1411229135053111,
      "grad_norm": 0.7876414060592651,
      "learning_rate": 3.5735963581183615e-06,
      "loss": 0.7399,
      "step": 752
    },
    {
      "epoch": 1.142640364188164,
      "grad_norm": 0.8632305264472961,
      "learning_rate": 3.5716995447647954e-06,
      "loss": 0.8029,
      "step": 753
    },
    {
      "epoch": 1.1441578148710168,
      "grad_norm": 1.2995400428771973,
      "learning_rate": 3.5698027314112294e-06,
      "loss": 0.9053,
      "step": 754
    },
    {
      "epoch": 1.1456752655538696,
      "grad_norm": 1.0133686065673828,
      "learning_rate": 3.5679059180576637e-06,
      "loss": 0.8854,
      "step": 755
    },
    {
      "epoch": 1.1471927162367224,
      "grad_norm": 0.7251290678977966,
      "learning_rate": 3.5660091047040972e-06,
      "loss": 0.8196,
      "step": 756
    },
    {
      "epoch": 1.1487101669195752,
      "grad_norm": 0.8500257134437561,
      "learning_rate": 3.5641122913505316e-06,
      "loss": 0.8429,
      "step": 757
    },
    {
      "epoch": 1.150227617602428,
      "grad_norm": 0.8225701451301575,
      "learning_rate": 3.5622154779969655e-06,
      "loss": 0.8013,
      "step": 758
    },
    {
      "epoch": 1.1517450682852808,
      "grad_norm": 0.9426960349082947,
      "learning_rate": 3.5603186646433994e-06,
      "loss": 0.8319,
      "step": 759
    },
    {
      "epoch": 1.1532625189681336,
      "grad_norm": 0.9014292359352112,
      "learning_rate": 3.5584218512898333e-06,
      "loss": 0.9278,
      "step": 760
    },
    {
      "epoch": 1.1547799696509864,
      "grad_norm": 0.8785642385482788,
      "learning_rate": 3.5565250379362677e-06,
      "loss": 0.8485,
      "step": 761
    },
    {
      "epoch": 1.1562974203338392,
      "grad_norm": 0.9171194434165955,
      "learning_rate": 3.554628224582701e-06,
      "loss": 0.9298,
      "step": 762
    },
    {
      "epoch": 1.157814871016692,
      "grad_norm": 0.873210608959198,
      "learning_rate": 3.5527314112291355e-06,
      "loss": 0.8179,
      "step": 763
    },
    {
      "epoch": 1.1593323216995448,
      "grad_norm": 0.9531263113021851,
      "learning_rate": 3.5508345978755695e-06,
      "loss": 0.9071,
      "step": 764
    },
    {
      "epoch": 1.1608497723823976,
      "grad_norm": 0.8633113503456116,
      "learning_rate": 3.5489377845220034e-06,
      "loss": 0.8757,
      "step": 765
    },
    {
      "epoch": 1.1623672230652504,
      "grad_norm": 0.8305951952934265,
      "learning_rate": 3.5470409711684373e-06,
      "loss": 0.875,
      "step": 766
    },
    {
      "epoch": 1.1638846737481032,
      "grad_norm": 1.4308189153671265,
      "learning_rate": 3.5451441578148717e-06,
      "loss": 0.8431,
      "step": 767
    },
    {
      "epoch": 1.165402124430956,
      "grad_norm": 0.8671628832817078,
      "learning_rate": 3.543247344461305e-06,
      "loss": 0.9082,
      "step": 768
    },
    {
      "epoch": 1.1669195751138088,
      "grad_norm": 0.8962685465812683,
      "learning_rate": 3.5413505311077395e-06,
      "loss": 0.8924,
      "step": 769
    },
    {
      "epoch": 1.1684370257966616,
      "grad_norm": 0.792935311794281,
      "learning_rate": 3.539453717754173e-06,
      "loss": 0.7704,
      "step": 770
    },
    {
      "epoch": 1.1699544764795144,
      "grad_norm": 0.8903762102127075,
      "learning_rate": 3.5375569044006074e-06,
      "loss": 0.7932,
      "step": 771
    },
    {
      "epoch": 1.1714719271623673,
      "grad_norm": 0.8513233661651611,
      "learning_rate": 3.5356600910470413e-06,
      "loss": 0.9185,
      "step": 772
    },
    {
      "epoch": 1.17298937784522,
      "grad_norm": 0.8638866543769836,
      "learning_rate": 3.5337632776934756e-06,
      "loss": 0.83,
      "step": 773
    },
    {
      "epoch": 1.1745068285280729,
      "grad_norm": 0.8612803220748901,
      "learning_rate": 3.531866464339909e-06,
      "loss": 0.8855,
      "step": 774
    },
    {
      "epoch": 1.1760242792109257,
      "grad_norm": 0.9043928384780884,
      "learning_rate": 3.5299696509863435e-06,
      "loss": 0.8418,
      "step": 775
    },
    {
      "epoch": 1.1775417298937785,
      "grad_norm": 0.8649753928184509,
      "learning_rate": 3.528072837632777e-06,
      "loss": 0.8247,
      "step": 776
    },
    {
      "epoch": 1.1790591805766313,
      "grad_norm": 0.7467991709709167,
      "learning_rate": 3.5261760242792114e-06,
      "loss": 0.7276,
      "step": 777
    },
    {
      "epoch": 1.180576631259484,
      "grad_norm": 1.8949743509292603,
      "learning_rate": 3.5242792109256453e-06,
      "loss": 0.8264,
      "step": 778
    },
    {
      "epoch": 1.182094081942337,
      "grad_norm": 0.8981977701187134,
      "learning_rate": 3.5223823975720796e-06,
      "loss": 0.8719,
      "step": 779
    },
    {
      "epoch": 1.1836115326251897,
      "grad_norm": 0.9274281859397888,
      "learning_rate": 3.520485584218513e-06,
      "loss": 0.8198,
      "step": 780
    },
    {
      "epoch": 1.1851289833080425,
      "grad_norm": 0.9007733464241028,
      "learning_rate": 3.5185887708649475e-06,
      "loss": 0.8936,
      "step": 781
    },
    {
      "epoch": 1.1866464339908953,
      "grad_norm": 0.9588742256164551,
      "learning_rate": 3.516691957511381e-06,
      "loss": 0.8961,
      "step": 782
    },
    {
      "epoch": 1.1881638846737481,
      "grad_norm": 0.95916348695755,
      "learning_rate": 3.5147951441578153e-06,
      "loss": 0.8626,
      "step": 783
    },
    {
      "epoch": 1.189681335356601,
      "grad_norm": 0.7935614585876465,
      "learning_rate": 3.5128983308042493e-06,
      "loss": 0.8198,
      "step": 784
    },
    {
      "epoch": 1.1911987860394537,
      "grad_norm": 0.9131838083267212,
      "learning_rate": 3.5110015174506836e-06,
      "loss": 0.7912,
      "step": 785
    },
    {
      "epoch": 1.1927162367223065,
      "grad_norm": 0.8453381657600403,
      "learning_rate": 3.509104704097117e-06,
      "loss": 0.8266,
      "step": 786
    },
    {
      "epoch": 1.1942336874051593,
      "grad_norm": 0.9421883821487427,
      "learning_rate": 3.5072078907435515e-06,
      "loss": 0.8129,
      "step": 787
    },
    {
      "epoch": 1.1957511380880121,
      "grad_norm": 0.8241940140724182,
      "learning_rate": 3.505311077389985e-06,
      "loss": 0.8362,
      "step": 788
    },
    {
      "epoch": 1.197268588770865,
      "grad_norm": 0.8369482159614563,
      "learning_rate": 3.5034142640364193e-06,
      "loss": 0.8253,
      "step": 789
    },
    {
      "epoch": 1.1987860394537178,
      "grad_norm": 0.8514634370803833,
      "learning_rate": 3.5015174506828532e-06,
      "loss": 0.8716,
      "step": 790
    },
    {
      "epoch": 1.2003034901365706,
      "grad_norm": 0.9026839137077332,
      "learning_rate": 3.4996206373292867e-06,
      "loss": 0.8477,
      "step": 791
    },
    {
      "epoch": 1.2018209408194234,
      "grad_norm": 0.7174123525619507,
      "learning_rate": 3.497723823975721e-06,
      "loss": 0.7678,
      "step": 792
    },
    {
      "epoch": 1.2033383915022762,
      "grad_norm": 0.869878888130188,
      "learning_rate": 3.4958270106221546e-06,
      "loss": 0.8618,
      "step": 793
    },
    {
      "epoch": 1.204855842185129,
      "grad_norm": 0.967435896396637,
      "learning_rate": 3.493930197268589e-06,
      "loss": 0.8374,
      "step": 794
    },
    {
      "epoch": 1.2063732928679818,
      "grad_norm": 0.8405280113220215,
      "learning_rate": 3.492033383915023e-06,
      "loss": 0.8245,
      "step": 795
    },
    {
      "epoch": 1.2078907435508346,
      "grad_norm": 0.8377997279167175,
      "learning_rate": 3.490136570561457e-06,
      "loss": 0.7877,
      "step": 796
    },
    {
      "epoch": 1.2094081942336874,
      "grad_norm": 0.9020735025405884,
      "learning_rate": 3.4882397572078907e-06,
      "loss": 0.8463,
      "step": 797
    },
    {
      "epoch": 1.2109256449165402,
      "grad_norm": 0.8685819506645203,
      "learning_rate": 3.486342943854325e-06,
      "loss": 0.7558,
      "step": 798
    },
    {
      "epoch": 1.212443095599393,
      "grad_norm": 0.8740023374557495,
      "learning_rate": 3.4844461305007586e-06,
      "loss": 0.712,
      "step": 799
    },
    {
      "epoch": 1.2139605462822458,
      "grad_norm": 0.9512165784835815,
      "learning_rate": 3.482549317147193e-06,
      "loss": 0.8548,
      "step": 800
    },
    {
      "epoch": 1.2154779969650986,
      "grad_norm": 0.8413642644882202,
      "learning_rate": 3.480652503793627e-06,
      "loss": 0.8798,
      "step": 801
    },
    {
      "epoch": 1.2169954476479514,
      "grad_norm": 0.7056971788406372,
      "learning_rate": 3.478755690440061e-06,
      "loss": 0.8069,
      "step": 802
    },
    {
      "epoch": 1.2185128983308042,
      "grad_norm": 0.8253024816513062,
      "learning_rate": 3.4768588770864947e-06,
      "loss": 0.7295,
      "step": 803
    },
    {
      "epoch": 1.220030349013657,
      "grad_norm": 0.96793133020401,
      "learning_rate": 3.474962063732929e-06,
      "loss": 0.8492,
      "step": 804
    },
    {
      "epoch": 1.2215477996965098,
      "grad_norm": 0.9356053471565247,
      "learning_rate": 3.4730652503793625e-06,
      "loss": 0.792,
      "step": 805
    },
    {
      "epoch": 1.2230652503793626,
      "grad_norm": 0.8436589241027832,
      "learning_rate": 3.471168437025797e-06,
      "loss": 0.8083,
      "step": 806
    },
    {
      "epoch": 1.2245827010622155,
      "grad_norm": 0.9279831051826477,
      "learning_rate": 3.469271623672231e-06,
      "loss": 0.8016,
      "step": 807
    },
    {
      "epoch": 1.2261001517450683,
      "grad_norm": 0.8621704578399658,
      "learning_rate": 3.467374810318665e-06,
      "loss": 0.7643,
      "step": 808
    },
    {
      "epoch": 1.227617602427921,
      "grad_norm": 0.8044596910476685,
      "learning_rate": 3.4654779969650987e-06,
      "loss": 0.7841,
      "step": 809
    },
    {
      "epoch": 1.2291350531107739,
      "grad_norm": 0.8671068549156189,
      "learning_rate": 3.463581183611533e-06,
      "loss": 0.7601,
      "step": 810
    },
    {
      "epoch": 1.2306525037936267,
      "grad_norm": 0.90746009349823,
      "learning_rate": 3.4616843702579665e-06,
      "loss": 0.8049,
      "step": 811
    },
    {
      "epoch": 1.2321699544764795,
      "grad_norm": 1.0012075901031494,
      "learning_rate": 3.459787556904401e-06,
      "loss": 0.8088,
      "step": 812
    },
    {
      "epoch": 1.2336874051593323,
      "grad_norm": 0.9815060496330261,
      "learning_rate": 3.4578907435508348e-06,
      "loss": 0.7772,
      "step": 813
    },
    {
      "epoch": 1.235204855842185,
      "grad_norm": 0.9048492908477783,
      "learning_rate": 3.4559939301972687e-06,
      "loss": 0.7492,
      "step": 814
    },
    {
      "epoch": 1.236722306525038,
      "grad_norm": 0.8303017616271973,
      "learning_rate": 3.4540971168437026e-06,
      "loss": 0.7639,
      "step": 815
    },
    {
      "epoch": 1.2382397572078907,
      "grad_norm": 1.2029954195022583,
      "learning_rate": 3.452200303490137e-06,
      "loss": 0.7446,
      "step": 816
    },
    {
      "epoch": 1.2397572078907435,
      "grad_norm": 0.944161057472229,
      "learning_rate": 3.4503034901365705e-06,
      "loss": 0.788,
      "step": 817
    },
    {
      "epoch": 1.2412746585735963,
      "grad_norm": 0.8233780860900879,
      "learning_rate": 3.448406676783005e-06,
      "loss": 0.7751,
      "step": 818
    },
    {
      "epoch": 1.2427921092564491,
      "grad_norm": 0.8295745253562927,
      "learning_rate": 3.4465098634294388e-06,
      "loss": 0.8464,
      "step": 819
    },
    {
      "epoch": 1.244309559939302,
      "grad_norm": 0.9425657987594604,
      "learning_rate": 3.4446130500758727e-06,
      "loss": 0.8116,
      "step": 820
    },
    {
      "epoch": 1.2458270106221547,
      "grad_norm": 1.0104968547821045,
      "learning_rate": 3.4427162367223066e-06,
      "loss": 0.7707,
      "step": 821
    },
    {
      "epoch": 1.2473444613050075,
      "grad_norm": 0.870525062084198,
      "learning_rate": 3.440819423368741e-06,
      "loss": 0.7716,
      "step": 822
    },
    {
      "epoch": 1.2488619119878603,
      "grad_norm": 0.8690431118011475,
      "learning_rate": 3.4389226100151745e-06,
      "loss": 0.6686,
      "step": 823
    },
    {
      "epoch": 1.2503793626707131,
      "grad_norm": 0.8193434476852417,
      "learning_rate": 3.437025796661609e-06,
      "loss": 0.7301,
      "step": 824
    },
    {
      "epoch": 1.251896813353566,
      "grad_norm": 0.9702070355415344,
      "learning_rate": 3.4351289833080427e-06,
      "loss": 0.7875,
      "step": 825
    },
    {
      "epoch": 1.2534142640364188,
      "grad_norm": 0.8037180304527283,
      "learning_rate": 3.4332321699544767e-06,
      "loss": 0.7148,
      "step": 826
    },
    {
      "epoch": 1.2549317147192716,
      "grad_norm": 0.9701418280601501,
      "learning_rate": 3.4313353566009106e-06,
      "loss": 0.7491,
      "step": 827
    },
    {
      "epoch": 1.2564491654021244,
      "grad_norm": 0.9124203324317932,
      "learning_rate": 3.429438543247345e-06,
      "loss": 0.7081,
      "step": 828
    },
    {
      "epoch": 1.2579666160849772,
      "grad_norm": 0.9974834322929382,
      "learning_rate": 3.4275417298937784e-06,
      "loss": 0.756,
      "step": 829
    },
    {
      "epoch": 1.25948406676783,
      "grad_norm": 0.9322922825813293,
      "learning_rate": 3.425644916540213e-06,
      "loss": 0.7733,
      "step": 830
    },
    {
      "epoch": 1.2610015174506828,
      "grad_norm": 0.8947255611419678,
      "learning_rate": 3.4237481031866467e-06,
      "loss": 0.6677,
      "step": 831
    },
    {
      "epoch": 1.2625189681335356,
      "grad_norm": 0.969436764717102,
      "learning_rate": 3.4218512898330806e-06,
      "loss": 0.66,
      "step": 832
    },
    {
      "epoch": 1.2640364188163884,
      "grad_norm": 0.7329368591308594,
      "learning_rate": 3.4199544764795146e-06,
      "loss": 0.6768,
      "step": 833
    },
    {
      "epoch": 1.2655538694992412,
      "grad_norm": 0.9579295516014099,
      "learning_rate": 3.418057663125949e-06,
      "loss": 0.7797,
      "step": 834
    },
    {
      "epoch": 1.267071320182094,
      "grad_norm": 0.8941435813903809,
      "learning_rate": 3.4161608497723824e-06,
      "loss": 0.7306,
      "step": 835
    },
    {
      "epoch": 1.2685887708649468,
      "grad_norm": 0.9485209584236145,
      "learning_rate": 3.4142640364188168e-06,
      "loss": 0.723,
      "step": 836
    },
    {
      "epoch": 1.2701062215477996,
      "grad_norm": 0.7877125144004822,
      "learning_rate": 3.4123672230652503e-06,
      "loss": 0.7475,
      "step": 837
    },
    {
      "epoch": 1.2716236722306524,
      "grad_norm": 2.2392053604125977,
      "learning_rate": 3.4104704097116846e-06,
      "loss": 0.6719,
      "step": 838
    },
    {
      "epoch": 1.2731411229135052,
      "grad_norm": 0.9681890606880188,
      "learning_rate": 3.4085735963581185e-06,
      "loss": 0.8103,
      "step": 839
    },
    {
      "epoch": 1.274658573596358,
      "grad_norm": 0.9420495629310608,
      "learning_rate": 3.406676783004553e-06,
      "loss": 0.7166,
      "step": 840
    },
    {
      "epoch": 1.276176024279211,
      "grad_norm": 0.9697649478912354,
      "learning_rate": 3.4047799696509864e-06,
      "loss": 0.7696,
      "step": 841
    },
    {
      "epoch": 1.2776934749620636,
      "grad_norm": 0.8299065828323364,
      "learning_rate": 3.4028831562974207e-06,
      "loss": 0.6899,
      "step": 842
    },
    {
      "epoch": 1.2792109256449167,
      "grad_norm": 0.8322023153305054,
      "learning_rate": 3.4009863429438542e-06,
      "loss": 0.6931,
      "step": 843
    },
    {
      "epoch": 1.2807283763277693,
      "grad_norm": 0.8462271094322205,
      "learning_rate": 3.3990895295902886e-06,
      "loss": 0.7281,
      "step": 844
    },
    {
      "epoch": 1.2822458270106223,
      "grad_norm": 0.8508765697479248,
      "learning_rate": 3.3971927162367225e-06,
      "loss": 0.7639,
      "step": 845
    },
    {
      "epoch": 1.2837632776934749,
      "grad_norm": 1.0239074230194092,
      "learning_rate": 3.395295902883157e-06,
      "loss": 0.7633,
      "step": 846
    },
    {
      "epoch": 1.285280728376328,
      "grad_norm": 0.9603191614151001,
      "learning_rate": 3.3933990895295904e-06,
      "loss": 0.7886,
      "step": 847
    },
    {
      "epoch": 1.2867981790591805,
      "grad_norm": 0.9261913299560547,
      "learning_rate": 3.3915022761760247e-06,
      "loss": 0.7027,
      "step": 848
    },
    {
      "epoch": 1.2883156297420335,
      "grad_norm": 0.8447955846786499,
      "learning_rate": 3.3896054628224582e-06,
      "loss": 0.7224,
      "step": 849
    },
    {
      "epoch": 1.289833080424886,
      "grad_norm": 0.8561683893203735,
      "learning_rate": 3.3877086494688926e-06,
      "loss": 0.7189,
      "step": 850
    },
    {
      "epoch": 1.2913505311077391,
      "grad_norm": 0.9868314862251282,
      "learning_rate": 3.3858118361153265e-06,
      "loss": 0.7313,
      "step": 851
    },
    {
      "epoch": 1.2928679817905917,
      "grad_norm": 0.959457278251648,
      "learning_rate": 3.383915022761761e-06,
      "loss": 0.7654,
      "step": 852
    },
    {
      "epoch": 1.2943854324734447,
      "grad_norm": 0.984119176864624,
      "learning_rate": 3.3820182094081943e-06,
      "loss": 0.7152,
      "step": 853
    },
    {
      "epoch": 1.2959028831562973,
      "grad_norm": 1.0127884149551392,
      "learning_rate": 3.3801213960546287e-06,
      "loss": 0.8,
      "step": 854
    },
    {
      "epoch": 1.2974203338391503,
      "grad_norm": 0.835495114326477,
      "learning_rate": 3.378224582701062e-06,
      "loss": 0.6469,
      "step": 855
    },
    {
      "epoch": 1.298937784522003,
      "grad_norm": 0.962474524974823,
      "learning_rate": 3.3763277693474965e-06,
      "loss": 0.7059,
      "step": 856
    },
    {
      "epoch": 1.300455235204856,
      "grad_norm": 0.946350634098053,
      "learning_rate": 3.3744309559939305e-06,
      "loss": 0.7346,
      "step": 857
    },
    {
      "epoch": 1.3019726858877085,
      "grad_norm": 0.9343305826187134,
      "learning_rate": 3.3725341426403644e-06,
      "loss": 0.7192,
      "step": 858
    },
    {
      "epoch": 1.3034901365705616,
      "grad_norm": 0.8674973845481873,
      "learning_rate": 3.3706373292867983e-06,
      "loss": 0.6979,
      "step": 859
    },
    {
      "epoch": 1.3050075872534141,
      "grad_norm": 0.8927565813064575,
      "learning_rate": 3.3687405159332327e-06,
      "loss": 0.7316,
      "step": 860
    },
    {
      "epoch": 1.3065250379362672,
      "grad_norm": 0.9598598480224609,
      "learning_rate": 3.366843702579666e-06,
      "loss": 0.7469,
      "step": 861
    },
    {
      "epoch": 1.3080424886191198,
      "grad_norm": 0.8926438093185425,
      "learning_rate": 3.3649468892261005e-06,
      "loss": 0.6232,
      "step": 862
    },
    {
      "epoch": 1.3095599393019728,
      "grad_norm": 0.9558469653129578,
      "learning_rate": 3.3630500758725345e-06,
      "loss": 0.7256,
      "step": 863
    },
    {
      "epoch": 1.3110773899848254,
      "grad_norm": 0.8982704281806946,
      "learning_rate": 3.3611532625189684e-06,
      "loss": 0.6787,
      "step": 864
    },
    {
      "epoch": 1.3125948406676784,
      "grad_norm": 0.9646546244621277,
      "learning_rate": 3.3592564491654023e-06,
      "loss": 0.6886,
      "step": 865
    },
    {
      "epoch": 1.314112291350531,
      "grad_norm": 0.8506848216056824,
      "learning_rate": 3.3573596358118367e-06,
      "loss": 0.6214,
      "step": 866
    },
    {
      "epoch": 1.315629742033384,
      "grad_norm": 0.9470043182373047,
      "learning_rate": 3.35546282245827e-06,
      "loss": 0.7624,
      "step": 867
    },
    {
      "epoch": 1.3171471927162366,
      "grad_norm": 0.7901824712753296,
      "learning_rate": 3.3535660091047045e-06,
      "loss": 0.6937,
      "step": 868
    },
    {
      "epoch": 1.3186646433990896,
      "grad_norm": 0.9177944660186768,
      "learning_rate": 3.3516691957511384e-06,
      "loss": 0.7061,
      "step": 869
    },
    {
      "epoch": 1.3201820940819422,
      "grad_norm": 0.929663360118866,
      "learning_rate": 3.3497723823975724e-06,
      "loss": 0.7251,
      "step": 870
    },
    {
      "epoch": 1.3216995447647952,
      "grad_norm": 0.8863811492919922,
      "learning_rate": 3.3478755690440063e-06,
      "loss": 0.6799,
      "step": 871
    },
    {
      "epoch": 1.3232169954476478,
      "grad_norm": 0.8859876394271851,
      "learning_rate": 3.3459787556904406e-06,
      "loss": 0.6948,
      "step": 872
    },
    {
      "epoch": 1.3247344461305008,
      "grad_norm": 0.9627453684806824,
      "learning_rate": 3.344081942336874e-06,
      "loss": 0.7105,
      "step": 873
    },
    {
      "epoch": 1.3262518968133536,
      "grad_norm": 0.8309656977653503,
      "learning_rate": 3.3421851289833085e-06,
      "loss": 0.6266,
      "step": 874
    },
    {
      "epoch": 1.3277693474962065,
      "grad_norm": 1.0156337022781372,
      "learning_rate": 3.3402883156297424e-06,
      "loss": 0.6668,
      "step": 875
    },
    {
      "epoch": 1.3292867981790593,
      "grad_norm": 0.8506255745887756,
      "learning_rate": 3.3383915022761763e-06,
      "loss": 0.6268,
      "step": 876
    },
    {
      "epoch": 1.330804248861912,
      "grad_norm": 0.932571291923523,
      "learning_rate": 3.3364946889226103e-06,
      "loss": 0.7014,
      "step": 877
    },
    {
      "epoch": 1.3323216995447649,
      "grad_norm": 0.8097352385520935,
      "learning_rate": 3.3345978755690446e-06,
      "loss": 0.7115,
      "step": 878
    },
    {
      "epoch": 1.3338391502276177,
      "grad_norm": 0.9394879937171936,
      "learning_rate": 3.332701062215478e-06,
      "loss": 0.6648,
      "step": 879
    },
    {
      "epoch": 1.3353566009104705,
      "grad_norm": 1.023410439491272,
      "learning_rate": 3.3308042488619125e-06,
      "loss": 0.6578,
      "step": 880
    },
    {
      "epoch": 1.3368740515933233,
      "grad_norm": 0.888619065284729,
      "learning_rate": 3.328907435508346e-06,
      "loss": 0.7082,
      "step": 881
    },
    {
      "epoch": 1.338391502276176,
      "grad_norm": 0.9105077981948853,
      "learning_rate": 3.3270106221547803e-06,
      "loss": 0.6456,
      "step": 882
    },
    {
      "epoch": 1.339908952959029,
      "grad_norm": 0.9075903296470642,
      "learning_rate": 3.3251138088012142e-06,
      "loss": 0.5873,
      "step": 883
    },
    {
      "epoch": 1.3414264036418817,
      "grad_norm": 0.9294325113296509,
      "learning_rate": 3.3232169954476486e-06,
      "loss": 0.7127,
      "step": 884
    },
    {
      "epoch": 1.3429438543247345,
      "grad_norm": 0.8971956372261047,
      "learning_rate": 3.321320182094082e-06,
      "loss": 0.6657,
      "step": 885
    },
    {
      "epoch": 1.3444613050075873,
      "grad_norm": 0.9002825617790222,
      "learning_rate": 3.3194233687405164e-06,
      "loss": 0.6866,
      "step": 886
    },
    {
      "epoch": 1.3459787556904401,
      "grad_norm": 0.9324438571929932,
      "learning_rate": 3.31752655538695e-06,
      "loss": 0.7064,
      "step": 887
    },
    {
      "epoch": 1.347496206373293,
      "grad_norm": 0.8676284551620483,
      "learning_rate": 3.3156297420333843e-06,
      "loss": 0.6146,
      "step": 888
    },
    {
      "epoch": 1.3490136570561457,
      "grad_norm": 0.9482198357582092,
      "learning_rate": 3.313732928679818e-06,
      "loss": 0.6631,
      "step": 889
    },
    {
      "epoch": 1.3505311077389985,
      "grad_norm": 0.8713312745094299,
      "learning_rate": 3.3118361153262526e-06,
      "loss": 0.6056,
      "step": 890
    },
    {
      "epoch": 1.3520485584218513,
      "grad_norm": 0.8658629655838013,
      "learning_rate": 3.309939301972686e-06,
      "loss": 0.703,
      "step": 891
    },
    {
      "epoch": 1.3535660091047041,
      "grad_norm": 0.9013953804969788,
      "learning_rate": 3.3080424886191204e-06,
      "loss": 0.6055,
      "step": 892
    },
    {
      "epoch": 1.355083459787557,
      "grad_norm": 0.8898760080337524,
      "learning_rate": 3.306145675265554e-06,
      "loss": 0.7424,
      "step": 893
    },
    {
      "epoch": 1.3566009104704098,
      "grad_norm": 0.8383095264434814,
      "learning_rate": 3.3042488619119883e-06,
      "loss": 0.6792,
      "step": 894
    },
    {
      "epoch": 1.3581183611532626,
      "grad_norm": 0.841491162776947,
      "learning_rate": 3.302352048558422e-06,
      "loss": 0.6484,
      "step": 895
    },
    {
      "epoch": 1.3596358118361154,
      "grad_norm": 0.9516908526420593,
      "learning_rate": 3.3004552352048565e-06,
      "loss": 0.6305,
      "step": 896
    },
    {
      "epoch": 1.3611532625189682,
      "grad_norm": 0.8657616376876831,
      "learning_rate": 3.29855842185129e-06,
      "loss": 0.6797,
      "step": 897
    },
    {
      "epoch": 1.362670713201821,
      "grad_norm": 0.7937811017036438,
      "learning_rate": 3.2966616084977244e-06,
      "loss": 0.6003,
      "step": 898
    },
    {
      "epoch": 1.3641881638846738,
      "grad_norm": 0.8031069040298462,
      "learning_rate": 3.294764795144158e-06,
      "loss": 0.6605,
      "step": 899
    },
    {
      "epoch": 1.3657056145675266,
      "grad_norm": 0.9242746233940125,
      "learning_rate": 3.2928679817905922e-06,
      "loss": 0.6327,
      "step": 900
    },
    {
      "epoch": 1.3672230652503794,
      "grad_norm": 0.8258346915245056,
      "learning_rate": 3.290971168437026e-06,
      "loss": 0.5864,
      "step": 901
    },
    {
      "epoch": 1.3687405159332322,
      "grad_norm": 0.86337810754776,
      "learning_rate": 3.28907435508346e-06,
      "loss": 0.5888,
      "step": 902
    },
    {
      "epoch": 1.370257966616085,
      "grad_norm": 0.8730962872505188,
      "learning_rate": 3.287177541729894e-06,
      "loss": 0.6291,
      "step": 903
    },
    {
      "epoch": 1.3717754172989378,
      "grad_norm": 0.9498186111450195,
      "learning_rate": 3.2852807283763284e-06,
      "loss": 0.6392,
      "step": 904
    },
    {
      "epoch": 1.3732928679817906,
      "grad_norm": 0.9330300092697144,
      "learning_rate": 3.283383915022762e-06,
      "loss": 0.651,
      "step": 905
    },
    {
      "epoch": 1.3748103186646434,
      "grad_norm": 0.928959846496582,
      "learning_rate": 3.2814871016691962e-06,
      "loss": 0.6469,
      "step": 906
    },
    {
      "epoch": 1.3763277693474962,
      "grad_norm": 0.8889382481575012,
      "learning_rate": 3.27959028831563e-06,
      "loss": 0.6428,
      "step": 907
    },
    {
      "epoch": 1.377845220030349,
      "grad_norm": 0.8757660388946533,
      "learning_rate": 3.277693474962064e-06,
      "loss": 0.5994,
      "step": 908
    },
    {
      "epoch": 1.3793626707132018,
      "grad_norm": 0.8018273115158081,
      "learning_rate": 3.275796661608498e-06,
      "loss": 0.6131,
      "step": 909
    },
    {
      "epoch": 1.3808801213960546,
      "grad_norm": 0.8682140111923218,
      "learning_rate": 3.2738998482549323e-06,
      "loss": 0.6131,
      "step": 910
    },
    {
      "epoch": 1.3823975720789075,
      "grad_norm": 0.9281781315803528,
      "learning_rate": 3.272003034901366e-06,
      "loss": 0.6955,
      "step": 911
    },
    {
      "epoch": 1.3839150227617603,
      "grad_norm": 0.8263450264930725,
      "learning_rate": 3.2701062215478e-06,
      "loss": 0.6829,
      "step": 912
    },
    {
      "epoch": 1.385432473444613,
      "grad_norm": 0.8712936043739319,
      "learning_rate": 3.268209408194234e-06,
      "loss": 0.682,
      "step": 913
    },
    {
      "epoch": 1.3869499241274659,
      "grad_norm": 0.8060970902442932,
      "learning_rate": 3.266312594840668e-06,
      "loss": 0.6297,
      "step": 914
    },
    {
      "epoch": 1.3884673748103187,
      "grad_norm": 0.8618897795677185,
      "learning_rate": 3.264415781487102e-06,
      "loss": 0.6708,
      "step": 915
    },
    {
      "epoch": 1.3899848254931715,
      "grad_norm": 0.973439633846283,
      "learning_rate": 3.2625189681335363e-06,
      "loss": 0.6534,
      "step": 916
    },
    {
      "epoch": 1.3915022761760243,
      "grad_norm": 0.9058233499526978,
      "learning_rate": 3.26062215477997e-06,
      "loss": 0.5778,
      "step": 917
    },
    {
      "epoch": 1.393019726858877,
      "grad_norm": 1.0713438987731934,
      "learning_rate": 3.258725341426404e-06,
      "loss": 0.6641,
      "step": 918
    },
    {
      "epoch": 1.39453717754173,
      "grad_norm": 0.8987117409706116,
      "learning_rate": 3.256828528072838e-06,
      "loss": 0.6174,
      "step": 919
    },
    {
      "epoch": 1.3960546282245827,
      "grad_norm": 0.9065254926681519,
      "learning_rate": 3.254931714719272e-06,
      "loss": 0.5605,
      "step": 920
    },
    {
      "epoch": 1.3975720789074355,
      "grad_norm": 0.870563268661499,
      "learning_rate": 3.253034901365706e-06,
      "loss": 0.6694,
      "step": 921
    },
    {
      "epoch": 1.3990895295902883,
      "grad_norm": 0.6909657716751099,
      "learning_rate": 3.2511380880121403e-06,
      "loss": 0.6156,
      "step": 922
    },
    {
      "epoch": 1.4006069802731411,
      "grad_norm": 0.8358914256095886,
      "learning_rate": 3.249241274658574e-06,
      "loss": 0.706,
      "step": 923
    },
    {
      "epoch": 1.402124430955994,
      "grad_norm": 0.8372401595115662,
      "learning_rate": 3.2473444613050077e-06,
      "loss": 0.6308,
      "step": 924
    },
    {
      "epoch": 1.4036418816388467,
      "grad_norm": 0.8220028281211853,
      "learning_rate": 3.2454476479514416e-06,
      "loss": 0.5547,
      "step": 925
    },
    {
      "epoch": 1.4051593323216995,
      "grad_norm": 0.9070355296134949,
      "learning_rate": 3.2435508345978756e-06,
      "loss": 0.667,
      "step": 926
    },
    {
      "epoch": 1.4066767830045523,
      "grad_norm": 0.8094897866249084,
      "learning_rate": 3.24165402124431e-06,
      "loss": 0.6056,
      "step": 927
    },
    {
      "epoch": 1.4081942336874052,
      "grad_norm": 0.8393376469612122,
      "learning_rate": 3.2397572078907434e-06,
      "loss": 0.674,
      "step": 928
    },
    {
      "epoch": 1.409711684370258,
      "grad_norm": 0.9030462503433228,
      "learning_rate": 3.2378603945371778e-06,
      "loss": 0.6578,
      "step": 929
    },
    {
      "epoch": 1.4112291350531108,
      "grad_norm": 0.7783934473991394,
      "learning_rate": 3.2359635811836117e-06,
      "loss": 0.6061,
      "step": 930
    },
    {
      "epoch": 1.4127465857359636,
      "grad_norm": 0.835570216178894,
      "learning_rate": 3.2340667678300456e-06,
      "loss": 0.5508,
      "step": 931
    },
    {
      "epoch": 1.4142640364188164,
      "grad_norm": 0.8608207106590271,
      "learning_rate": 3.2321699544764795e-06,
      "loss": 0.6094,
      "step": 932
    },
    {
      "epoch": 1.4157814871016692,
      "grad_norm": 0.8180117607116699,
      "learning_rate": 3.230273141122914e-06,
      "loss": 0.6639,
      "step": 933
    },
    {
      "epoch": 1.417298937784522,
      "grad_norm": 0.9810336232185364,
      "learning_rate": 3.2283763277693474e-06,
      "loss": 0.6641,
      "step": 934
    },
    {
      "epoch": 1.4188163884673748,
      "grad_norm": 0.8523558378219604,
      "learning_rate": 3.2264795144157817e-06,
      "loss": 0.6087,
      "step": 935
    },
    {
      "epoch": 1.4203338391502276,
      "grad_norm": 0.7662287950515747,
      "learning_rate": 3.2245827010622157e-06,
      "loss": 0.5046,
      "step": 936
    },
    {
      "epoch": 1.4218512898330804,
      "grad_norm": 1.8126938343048096,
      "learning_rate": 3.2226858877086496e-06,
      "loss": 0.6275,
      "step": 937
    },
    {
      "epoch": 1.4233687405159332,
      "grad_norm": 0.8907243013381958,
      "learning_rate": 3.2207890743550835e-06,
      "loss": 0.641,
      "step": 938
    },
    {
      "epoch": 1.424886191198786,
      "grad_norm": 1.0125117301940918,
      "learning_rate": 3.218892261001518e-06,
      "loss": 0.6272,
      "step": 939
    },
    {
      "epoch": 1.4264036418816388,
      "grad_norm": 1.007325291633606,
      "learning_rate": 3.2169954476479514e-06,
      "loss": 0.5889,
      "step": 940
    },
    {
      "epoch": 1.4279210925644916,
      "grad_norm": 1.0651479959487915,
      "learning_rate": 3.2150986342943857e-06,
      "loss": 0.6405,
      "step": 941
    },
    {
      "epoch": 1.4294385432473444,
      "grad_norm": 0.7472161650657654,
      "learning_rate": 3.2132018209408196e-06,
      "loss": 0.6002,
      "step": 942
    },
    {
      "epoch": 1.4309559939301972,
      "grad_norm": 0.8282257318496704,
      "learning_rate": 3.2113050075872536e-06,
      "loss": 0.5904,
      "step": 943
    },
    {
      "epoch": 1.43247344461305,
      "grad_norm": 1.0276248455047607,
      "learning_rate": 3.2094081942336875e-06,
      "loss": 0.533,
      "step": 944
    },
    {
      "epoch": 1.4339908952959028,
      "grad_norm": 0.7988645434379578,
      "learning_rate": 3.207511380880122e-06,
      "loss": 0.5214,
      "step": 945
    },
    {
      "epoch": 1.4355083459787557,
      "grad_norm": 1.0929768085479736,
      "learning_rate": 3.2056145675265554e-06,
      "loss": 0.6262,
      "step": 946
    },
    {
      "epoch": 1.4370257966616085,
      "grad_norm": 1.3922351598739624,
      "learning_rate": 3.2037177541729897e-06,
      "loss": 0.5706,
      "step": 947
    },
    {
      "epoch": 1.4385432473444613,
      "grad_norm": 0.9457386136054993,
      "learning_rate": 3.201820940819423e-06,
      "loss": 0.6236,
      "step": 948
    },
    {
      "epoch": 1.440060698027314,
      "grad_norm": 0.8224130272865295,
      "learning_rate": 3.1999241274658576e-06,
      "loss": 0.5402,
      "step": 949
    },
    {
      "epoch": 1.4415781487101669,
      "grad_norm": 0.9256688356399536,
      "learning_rate": 3.1980273141122915e-06,
      "loss": 0.5914,
      "step": 950
    },
    {
      "epoch": 1.4430955993930197,
      "grad_norm": 0.7671840786933899,
      "learning_rate": 3.196130500758726e-06,
      "loss": 0.5215,
      "step": 951
    },
    {
      "epoch": 1.4446130500758725,
      "grad_norm": 0.868060827255249,
      "learning_rate": 3.1942336874051593e-06,
      "loss": 0.5193,
      "step": 952
    },
    {
      "epoch": 1.4461305007587253,
      "grad_norm": 0.9000276923179626,
      "learning_rate": 3.1923368740515937e-06,
      "loss": 0.609,
      "step": 953
    },
    {
      "epoch": 1.447647951441578,
      "grad_norm": 0.8513280749320984,
      "learning_rate": 3.190440060698027e-06,
      "loss": 0.5758,
      "step": 954
    },
    {
      "epoch": 1.449165402124431,
      "grad_norm": 0.9384821057319641,
      "learning_rate": 3.1885432473444615e-06,
      "loss": 0.5887,
      "step": 955
    },
    {
      "epoch": 1.4506828528072837,
      "grad_norm": 0.8367178440093994,
      "learning_rate": 3.1866464339908955e-06,
      "loss": 0.6466,
      "step": 956
    },
    {
      "epoch": 1.4522003034901365,
      "grad_norm": 0.8944584131240845,
      "learning_rate": 3.18474962063733e-06,
      "loss": 0.6522,
      "step": 957
    },
    {
      "epoch": 1.4537177541729893,
      "grad_norm": 0.9053120017051697,
      "learning_rate": 3.1828528072837633e-06,
      "loss": 0.5892,
      "step": 958
    },
    {
      "epoch": 1.4552352048558421,
      "grad_norm": 0.8654170036315918,
      "learning_rate": 3.1809559939301977e-06,
      "loss": 0.6273,
      "step": 959
    },
    {
      "epoch": 1.456752655538695,
      "grad_norm": 0.7813900709152222,
      "learning_rate": 3.179059180576631e-06,
      "loss": 0.6295,
      "step": 960
    },
    {
      "epoch": 1.4582701062215477,
      "grad_norm": 0.9418593645095825,
      "learning_rate": 3.1771623672230655e-06,
      "loss": 0.5945,
      "step": 961
    },
    {
      "epoch": 1.4597875569044005,
      "grad_norm": 0.9540605545043945,
      "learning_rate": 3.1752655538694994e-06,
      "loss": 0.5938,
      "step": 962
    },
    {
      "epoch": 1.4613050075872533,
      "grad_norm": 0.9568831324577332,
      "learning_rate": 3.1733687405159338e-06,
      "loss": 0.5588,
      "step": 963
    },
    {
      "epoch": 1.4628224582701062,
      "grad_norm": 0.8497644662857056,
      "learning_rate": 3.1714719271623673e-06,
      "loss": 0.4649,
      "step": 964
    },
    {
      "epoch": 1.464339908952959,
      "grad_norm": 0.8769261240959167,
      "learning_rate": 3.1695751138088016e-06,
      "loss": 0.5185,
      "step": 965
    },
    {
      "epoch": 1.4658573596358118,
      "grad_norm": 0.8513078093528748,
      "learning_rate": 3.167678300455235e-06,
      "loss": 0.6472,
      "step": 966
    },
    {
      "epoch": 1.4673748103186646,
      "grad_norm": 0.8383026719093323,
      "learning_rate": 3.1657814871016695e-06,
      "loss": 0.5896,
      "step": 967
    },
    {
      "epoch": 1.4688922610015174,
      "grad_norm": 0.8582555651664734,
      "learning_rate": 3.1638846737481034e-06,
      "loss": 0.4956,
      "step": 968
    },
    {
      "epoch": 1.4704097116843702,
      "grad_norm": 0.8513321280479431,
      "learning_rate": 3.1619878603945373e-06,
      "loss": 0.5772,
      "step": 969
    },
    {
      "epoch": 1.471927162367223,
      "grad_norm": 0.8101665377616882,
      "learning_rate": 3.1600910470409713e-06,
      "loss": 0.6226,
      "step": 970
    },
    {
      "epoch": 1.4734446130500758,
      "grad_norm": 0.860826849937439,
      "learning_rate": 3.1581942336874056e-06,
      "loss": 0.6051,
      "step": 971
    },
    {
      "epoch": 1.4749620637329286,
      "grad_norm": 1.0713157653808594,
      "learning_rate": 3.156297420333839e-06,
      "loss": 0.5467,
      "step": 972
    },
    {
      "epoch": 1.4764795144157814,
      "grad_norm": 0.8317447304725647,
      "learning_rate": 3.1544006069802735e-06,
      "loss": 0.6448,
      "step": 973
    },
    {
      "epoch": 1.4779969650986344,
      "grad_norm": 2.0341761112213135,
      "learning_rate": 3.1525037936267074e-06,
      "loss": 0.57,
      "step": 974
    },
    {
      "epoch": 1.479514415781487,
      "grad_norm": 0.9277472496032715,
      "learning_rate": 3.1506069802731413e-06,
      "loss": 0.5394,
      "step": 975
    },
    {
      "epoch": 1.48103186646434,
      "grad_norm": 0.7957863807678223,
      "learning_rate": 3.1487101669195752e-06,
      "loss": 0.5187,
      "step": 976
    },
    {
      "epoch": 1.4825493171471926,
      "grad_norm": 0.9492657780647278,
      "learning_rate": 3.1468133535660096e-06,
      "loss": 0.5345,
      "step": 977
    },
    {
      "epoch": 1.4840667678300457,
      "grad_norm": 0.9059072732925415,
      "learning_rate": 3.144916540212443e-06,
      "loss": 0.608,
      "step": 978
    },
    {
      "epoch": 1.4855842185128982,
      "grad_norm": 1.1321309804916382,
      "learning_rate": 3.1430197268588774e-06,
      "loss": 0.648,
      "step": 979
    },
    {
      "epoch": 1.4871016691957513,
      "grad_norm": 0.8518458604812622,
      "learning_rate": 3.1411229135053114e-06,
      "loss": 0.5727,
      "step": 980
    },
    {
      "epoch": 1.4886191198786038,
      "grad_norm": 0.9628214836120605,
      "learning_rate": 3.1392261001517453e-06,
      "loss": 0.5881,
      "step": 981
    },
    {
      "epoch": 1.4901365705614569,
      "grad_norm": 0.8033422231674194,
      "learning_rate": 3.137329286798179e-06,
      "loss": 0.6817,
      "step": 982
    },
    {
      "epoch": 1.4916540212443095,
      "grad_norm": 0.8863669633865356,
      "learning_rate": 3.1354324734446136e-06,
      "loss": 0.5419,
      "step": 983
    },
    {
      "epoch": 1.4931714719271625,
      "grad_norm": 0.8062348365783691,
      "learning_rate": 3.133535660091047e-06,
      "loss": 0.6017,
      "step": 984
    },
    {
      "epoch": 1.494688922610015,
      "grad_norm": 0.9734047055244446,
      "learning_rate": 3.1316388467374814e-06,
      "loss": 0.5921,
      "step": 985
    },
    {
      "epoch": 1.496206373292868,
      "grad_norm": 0.9415066242218018,
      "learning_rate": 3.1297420333839153e-06,
      "loss": 0.5833,
      "step": 986
    },
    {
      "epoch": 1.4977238239757207,
      "grad_norm": 0.9106006622314453,
      "learning_rate": 3.1278452200303493e-06,
      "loss": 0.6229,
      "step": 987
    },
    {
      "epoch": 1.4992412746585737,
      "grad_norm": 0.8795501589775085,
      "learning_rate": 3.125948406676783e-06,
      "loss": 0.534,
      "step": 988
    },
    {
      "epoch": 1.5007587253414263,
      "grad_norm": 0.8950515389442444,
      "learning_rate": 3.1240515933232175e-06,
      "loss": 0.5485,
      "step": 989
    },
    {
      "epoch": 1.5022761760242793,
      "grad_norm": 0.7698360681533813,
      "learning_rate": 3.122154779969651e-06,
      "loss": 0.5509,
      "step": 990
    },
    {
      "epoch": 1.503793626707132,
      "grad_norm": 0.9636415243148804,
      "learning_rate": 3.1202579666160854e-06,
      "loss": 0.528,
      "step": 991
    },
    {
      "epoch": 1.505311077389985,
      "grad_norm": 0.9208599925041199,
      "learning_rate": 3.118361153262519e-06,
      "loss": 0.5014,
      "step": 992
    },
    {
      "epoch": 1.5068285280728375,
      "grad_norm": 0.858135998249054,
      "learning_rate": 3.1164643399089532e-06,
      "loss": 0.5848,
      "step": 993
    },
    {
      "epoch": 1.5083459787556905,
      "grad_norm": 0.8136850595474243,
      "learning_rate": 3.114567526555387e-06,
      "loss": 0.492,
      "step": 994
    },
    {
      "epoch": 1.5098634294385431,
      "grad_norm": 0.8705328106880188,
      "learning_rate": 3.1126707132018215e-06,
      "loss": 0.4031,
      "step": 995
    },
    {
      "epoch": 1.5113808801213962,
      "grad_norm": 0.9318100810050964,
      "learning_rate": 3.110773899848255e-06,
      "loss": 0.5227,
      "step": 996
    },
    {
      "epoch": 1.5128983308042487,
      "grad_norm": 0.8172624707221985,
      "learning_rate": 3.1088770864946894e-06,
      "loss": 0.567,
      "step": 997
    },
    {
      "epoch": 1.5144157814871018,
      "grad_norm": 0.8022767901420593,
      "learning_rate": 3.106980273141123e-06,
      "loss": 0.5682,
      "step": 998
    },
    {
      "epoch": 1.5159332321699543,
      "grad_norm": 0.8361572027206421,
      "learning_rate": 3.1050834597875572e-06,
      "loss": 0.5065,
      "step": 999
    },
    {
      "epoch": 1.5174506828528074,
      "grad_norm": 0.8725852370262146,
      "learning_rate": 3.103186646433991e-06,
      "loss": 0.5438,
      "step": 1000
    },
    {
      "epoch": 1.51896813353566,
      "grad_norm": 0.886681318283081,
      "learning_rate": 3.1012898330804255e-06,
      "loss": 0.5287,
      "step": 1001
    },
    {
      "epoch": 1.520485584218513,
      "grad_norm": 2.808351516723633,
      "learning_rate": 3.099393019726859e-06,
      "loss": 0.5643,
      "step": 1002
    },
    {
      "epoch": 1.5220030349013656,
      "grad_norm": 0.8927615880966187,
      "learning_rate": 3.0974962063732933e-06,
      "loss": 0.5647,
      "step": 1003
    },
    {
      "epoch": 1.5235204855842186,
      "grad_norm": 0.8199148178100586,
      "learning_rate": 3.095599393019727e-06,
      "loss": 0.621,
      "step": 1004
    },
    {
      "epoch": 1.5250379362670712,
      "grad_norm": 0.8336735963821411,
      "learning_rate": 3.093702579666161e-06,
      "loss": 0.5016,
      "step": 1005
    },
    {
      "epoch": 1.5265553869499242,
      "grad_norm": 0.7894304394721985,
      "learning_rate": 3.091805766312595e-06,
      "loss": 0.5551,
      "step": 1006
    },
    {
      "epoch": 1.5280728376327768,
      "grad_norm": 0.8263694643974304,
      "learning_rate": 3.0899089529590295e-06,
      "loss": 0.5675,
      "step": 1007
    },
    {
      "epoch": 1.5295902883156298,
      "grad_norm": 1.0307930707931519,
      "learning_rate": 3.088012139605463e-06,
      "loss": 0.6118,
      "step": 1008
    },
    {
      "epoch": 1.5311077389984824,
      "grad_norm": 0.8347984552383423,
      "learning_rate": 3.0861153262518973e-06,
      "loss": 0.5735,
      "step": 1009
    },
    {
      "epoch": 1.5326251896813354,
      "grad_norm": 0.827668309211731,
      "learning_rate": 3.084218512898331e-06,
      "loss": 0.4442,
      "step": 1010
    },
    {
      "epoch": 1.534142640364188,
      "grad_norm": 0.8633823990821838,
      "learning_rate": 3.082321699544765e-06,
      "loss": 0.4825,
      "step": 1011
    },
    {
      "epoch": 1.535660091047041,
      "grad_norm": 1.2769697904586792,
      "learning_rate": 3.080424886191199e-06,
      "loss": 0.5722,
      "step": 1012
    },
    {
      "epoch": 1.5371775417298936,
      "grad_norm": 0.8457627296447754,
      "learning_rate": 3.078528072837633e-06,
      "loss": 0.5835,
      "step": 1013
    },
    {
      "epoch": 1.5386949924127467,
      "grad_norm": 0.7600122094154358,
      "learning_rate": 3.076631259484067e-06,
      "loss": 0.6555,
      "step": 1014
    },
    {
      "epoch": 1.5402124430955992,
      "grad_norm": 0.8575071096420288,
      "learning_rate": 3.0747344461305013e-06,
      "loss": 0.5352,
      "step": 1015
    },
    {
      "epoch": 1.5417298937784523,
      "grad_norm": 0.827841579914093,
      "learning_rate": 3.072837632776935e-06,
      "loss": 0.5726,
      "step": 1016
    },
    {
      "epoch": 1.5432473444613048,
      "grad_norm": 0.8846891522407532,
      "learning_rate": 3.070940819423369e-06,
      "loss": 0.5262,
      "step": 1017
    },
    {
      "epoch": 1.5447647951441579,
      "grad_norm": 0.9075151085853577,
      "learning_rate": 3.069044006069803e-06,
      "loss": 0.5958,
      "step": 1018
    },
    {
      "epoch": 1.5462822458270105,
      "grad_norm": 0.8575437664985657,
      "learning_rate": 3.067147192716237e-06,
      "loss": 0.5247,
      "step": 1019
    },
    {
      "epoch": 1.5477996965098635,
      "grad_norm": 0.8287276029586792,
      "learning_rate": 3.065250379362671e-06,
      "loss": 0.523,
      "step": 1020
    },
    {
      "epoch": 1.549317147192716,
      "grad_norm": 0.8702021241188049,
      "learning_rate": 3.0633535660091053e-06,
      "loss": 0.6188,
      "step": 1021
    },
    {
      "epoch": 1.550834597875569,
      "grad_norm": 0.8432260155677795,
      "learning_rate": 3.0614567526555388e-06,
      "loss": 0.5683,
      "step": 1022
    },
    {
      "epoch": 1.552352048558422,
      "grad_norm": 0.7402470111846924,
      "learning_rate": 3.059559939301973e-06,
      "loss": 0.4847,
      "step": 1023
    },
    {
      "epoch": 1.5538694992412747,
      "grad_norm": 0.9040923714637756,
      "learning_rate": 3.057663125948407e-06,
      "loss": 0.5665,
      "step": 1024
    },
    {
      "epoch": 1.5553869499241275,
      "grad_norm": 0.9040977358818054,
      "learning_rate": 3.055766312594841e-06,
      "loss": 0.538,
      "step": 1025
    },
    {
      "epoch": 1.5569044006069803,
      "grad_norm": 0.8151480555534363,
      "learning_rate": 3.053869499241275e-06,
      "loss": 0.551,
      "step": 1026
    },
    {
      "epoch": 1.5584218512898331,
      "grad_norm": 0.7539666891098022,
      "learning_rate": 3.0519726858877092e-06,
      "loss": 0.5501,
      "step": 1027
    },
    {
      "epoch": 1.559939301972686,
      "grad_norm": 0.7452706098556519,
      "learning_rate": 3.0500758725341427e-06,
      "loss": 0.5799,
      "step": 1028
    },
    {
      "epoch": 1.5614567526555387,
      "grad_norm": 0.8184561729431152,
      "learning_rate": 3.048179059180577e-06,
      "loss": 0.4687,
      "step": 1029
    },
    {
      "epoch": 1.5629742033383915,
      "grad_norm": 0.8531137704849243,
      "learning_rate": 3.046282245827011e-06,
      "loss": 0.5414,
      "step": 1030
    },
    {
      "epoch": 1.5644916540212443,
      "grad_norm": 0.8311110138893127,
      "learning_rate": 3.044385432473445e-06,
      "loss": 0.5009,
      "step": 1031
    },
    {
      "epoch": 1.5660091047040972,
      "grad_norm": 0.7800692915916443,
      "learning_rate": 3.042488619119879e-06,
      "loss": 0.5302,
      "step": 1032
    },
    {
      "epoch": 1.56752655538695,
      "grad_norm": 0.7878679633140564,
      "learning_rate": 3.0405918057663132e-06,
      "loss": 0.4093,
      "step": 1033
    },
    {
      "epoch": 1.5690440060698028,
      "grad_norm": 0.754623293876648,
      "learning_rate": 3.0386949924127467e-06,
      "loss": 0.5425,
      "step": 1034
    },
    {
      "epoch": 1.5705614567526556,
      "grad_norm": 0.7931473851203918,
      "learning_rate": 3.036798179059181e-06,
      "loss": 0.4886,
      "step": 1035
    },
    {
      "epoch": 1.5720789074355084,
      "grad_norm": 0.7151297926902771,
      "learning_rate": 3.0349013657056146e-06,
      "loss": 0.4467,
      "step": 1036
    },
    {
      "epoch": 1.5735963581183612,
      "grad_norm": 0.8170958757400513,
      "learning_rate": 3.033004552352049e-06,
      "loss": 0.4751,
      "step": 1037
    },
    {
      "epoch": 1.575113808801214,
      "grad_norm": 0.7487092018127441,
      "learning_rate": 3.031107738998483e-06,
      "loss": 0.5081,
      "step": 1038
    },
    {
      "epoch": 1.5766312594840668,
      "grad_norm": 0.712166965007782,
      "learning_rate": 3.029210925644917e-06,
      "loss": 0.5383,
      "step": 1039
    },
    {
      "epoch": 1.5781487101669196,
      "grad_norm": 0.7151088714599609,
      "learning_rate": 3.0273141122913507e-06,
      "loss": 0.5685,
      "step": 1040
    },
    {
      "epoch": 1.5796661608497724,
      "grad_norm": 0.8010864853858948,
      "learning_rate": 3.025417298937785e-06,
      "loss": 0.4538,
      "step": 1041
    },
    {
      "epoch": 1.5811836115326252,
      "grad_norm": 0.8142756223678589,
      "learning_rate": 3.0235204855842186e-06,
      "loss": 0.4202,
      "step": 1042
    },
    {
      "epoch": 1.582701062215478,
      "grad_norm": 0.8050506114959717,
      "learning_rate": 3.021623672230653e-06,
      "loss": 0.5277,
      "step": 1043
    },
    {
      "epoch": 1.5842185128983308,
      "grad_norm": 0.8635820746421814,
      "learning_rate": 3.019726858877087e-06,
      "loss": 0.5604,
      "step": 1044
    },
    {
      "epoch": 1.5857359635811836,
      "grad_norm": 0.9416060447692871,
      "learning_rate": 3.017830045523521e-06,
      "loss": 0.4924,
      "step": 1045
    },
    {
      "epoch": 1.5872534142640364,
      "grad_norm": 0.7861348986625671,
      "learning_rate": 3.0159332321699547e-06,
      "loss": 0.4914,
      "step": 1046
    },
    {
      "epoch": 1.5887708649468892,
      "grad_norm": 0.8847095370292664,
      "learning_rate": 3.014036418816389e-06,
      "loss": 0.5563,
      "step": 1047
    },
    {
      "epoch": 1.590288315629742,
      "grad_norm": 0.7602783441543579,
      "learning_rate": 3.0121396054628225e-06,
      "loss": 0.506,
      "step": 1048
    },
    {
      "epoch": 1.5918057663125948,
      "grad_norm": 0.714981198310852,
      "learning_rate": 3.010242792109257e-06,
      "loss": 0.4955,
      "step": 1049
    },
    {
      "epoch": 1.5933232169954477,
      "grad_norm": 0.8954324126243591,
      "learning_rate": 3.008345978755691e-06,
      "loss": 0.4987,
      "step": 1050
    },
    {
      "epoch": 1.5948406676783005,
      "grad_norm": 0.7828902006149292,
      "learning_rate": 3.006449165402125e-06,
      "loss": 0.4678,
      "step": 1051
    },
    {
      "epoch": 1.5963581183611533,
      "grad_norm": 0.7251822352409363,
      "learning_rate": 3.0045523520485587e-06,
      "loss": 0.4785,
      "step": 1052
    },
    {
      "epoch": 1.597875569044006,
      "grad_norm": 0.953097403049469,
      "learning_rate": 3.002655538694993e-06,
      "loss": 0.6011,
      "step": 1053
    },
    {
      "epoch": 1.5993930197268589,
      "grad_norm": 0.7500038743019104,
      "learning_rate": 3.0007587253414265e-06,
      "loss": 0.49,
      "step": 1054
    },
    {
      "epoch": 1.6009104704097117,
      "grad_norm": 0.8721365928649902,
      "learning_rate": 2.9988619119878604e-06,
      "loss": 0.4983,
      "step": 1055
    },
    {
      "epoch": 1.6024279210925645,
      "grad_norm": 0.7843972444534302,
      "learning_rate": 2.9969650986342948e-06,
      "loss": 0.5268,
      "step": 1056
    },
    {
      "epoch": 1.6039453717754173,
      "grad_norm": 0.7329304814338684,
      "learning_rate": 2.9950682852807283e-06,
      "loss": 0.5568,
      "step": 1057
    },
    {
      "epoch": 1.60546282245827,
      "grad_norm": 0.8249422907829285,
      "learning_rate": 2.9931714719271626e-06,
      "loss": 0.4371,
      "step": 1058
    },
    {
      "epoch": 1.606980273141123,
      "grad_norm": 0.8548746109008789,
      "learning_rate": 2.991274658573596e-06,
      "loss": 0.4888,
      "step": 1059
    },
    {
      "epoch": 1.6084977238239757,
      "grad_norm": 0.919590175151825,
      "learning_rate": 2.9893778452200305e-06,
      "loss": 0.468,
      "step": 1060
    },
    {
      "epoch": 1.6100151745068285,
      "grad_norm": 0.7875679731369019,
      "learning_rate": 2.9874810318664644e-06,
      "loss": 0.4922,
      "step": 1061
    },
    {
      "epoch": 1.6115326251896813,
      "grad_norm": 0.7887643575668335,
      "learning_rate": 2.9855842185128988e-06,
      "loss": 0.5298,
      "step": 1062
    },
    {
      "epoch": 1.6130500758725341,
      "grad_norm": 0.8229043483734131,
      "learning_rate": 2.9836874051593323e-06,
      "loss": 0.5109,
      "step": 1063
    },
    {
      "epoch": 1.614567526555387,
      "grad_norm": 0.7610735893249512,
      "learning_rate": 2.9817905918057666e-06,
      "loss": 0.5162,
      "step": 1064
    },
    {
      "epoch": 1.6160849772382397,
      "grad_norm": 0.7907794117927551,
      "learning_rate": 2.9798937784522e-06,
      "loss": 0.597,
      "step": 1065
    },
    {
      "epoch": 1.6176024279210925,
      "grad_norm": 0.7840317487716675,
      "learning_rate": 2.9779969650986345e-06,
      "loss": 0.4686,
      "step": 1066
    },
    {
      "epoch": 1.6191198786039454,
      "grad_norm": 0.837986171245575,
      "learning_rate": 2.9761001517450684e-06,
      "loss": 0.5316,
      "step": 1067
    },
    {
      "epoch": 1.6206373292867982,
      "grad_norm": 0.8557603359222412,
      "learning_rate": 2.9742033383915027e-06,
      "loss": 0.5626,
      "step": 1068
    },
    {
      "epoch": 1.622154779969651,
      "grad_norm": 1.0622498989105225,
      "learning_rate": 2.9723065250379362e-06,
      "loss": 0.5155,
      "step": 1069
    },
    {
      "epoch": 1.6236722306525038,
      "grad_norm": 0.7927647233009338,
      "learning_rate": 2.9704097116843706e-06,
      "loss": 0.5337,
      "step": 1070
    },
    {
      "epoch": 1.6251896813353566,
      "grad_norm": 0.8295977711677551,
      "learning_rate": 2.968512898330804e-06,
      "loss": 0.5264,
      "step": 1071
    },
    {
      "epoch": 1.6267071320182094,
      "grad_norm": 0.8355529308319092,
      "learning_rate": 2.9666160849772384e-06,
      "loss": 0.4659,
      "step": 1072
    },
    {
      "epoch": 1.6282245827010622,
      "grad_norm": 0.8137402534484863,
      "learning_rate": 2.9647192716236724e-06,
      "loss": 0.49,
      "step": 1073
    },
    {
      "epoch": 1.629742033383915,
      "grad_norm": 0.8184390664100647,
      "learning_rate": 2.9628224582701067e-06,
      "loss": 0.5217,
      "step": 1074
    },
    {
      "epoch": 1.6312594840667678,
      "grad_norm": 0.7192495465278625,
      "learning_rate": 2.9609256449165402e-06,
      "loss": 0.5002,
      "step": 1075
    },
    {
      "epoch": 1.6327769347496206,
      "grad_norm": 0.7448078989982605,
      "learning_rate": 2.9590288315629746e-06,
      "loss": 0.4377,
      "step": 1076
    },
    {
      "epoch": 1.6342943854324734,
      "grad_norm": 0.7785364389419556,
      "learning_rate": 2.957132018209408e-06,
      "loss": 0.5369,
      "step": 1077
    },
    {
      "epoch": 1.6358118361153262,
      "grad_norm": 0.7646759748458862,
      "learning_rate": 2.9552352048558424e-06,
      "loss": 0.5271,
      "step": 1078
    },
    {
      "epoch": 1.637329286798179,
      "grad_norm": 0.7773253321647644,
      "learning_rate": 2.9533383915022763e-06,
      "loss": 0.4772,
      "step": 1079
    },
    {
      "epoch": 1.6388467374810318,
      "grad_norm": 0.8224021196365356,
      "learning_rate": 2.9514415781487103e-06,
      "loss": 0.463,
      "step": 1080
    },
    {
      "epoch": 1.6403641881638846,
      "grad_norm": 0.8192982077598572,
      "learning_rate": 2.949544764795144e-06,
      "loss": 0.5538,
      "step": 1081
    },
    {
      "epoch": 1.6418816388467374,
      "grad_norm": 0.793633759021759,
      "learning_rate": 2.9476479514415785e-06,
      "loss": 0.5421,
      "step": 1082
    },
    {
      "epoch": 1.6433990895295902,
      "grad_norm": 0.8590916395187378,
      "learning_rate": 2.945751138088012e-06,
      "loss": 0.4663,
      "step": 1083
    },
    {
      "epoch": 1.644916540212443,
      "grad_norm": 0.8717281222343445,
      "learning_rate": 2.9438543247344464e-06,
      "loss": 0.5239,
      "step": 1084
    },
    {
      "epoch": 1.6464339908952959,
      "grad_norm": 0.7502239942550659,
      "learning_rate": 2.9419575113808803e-06,
      "loss": 0.4701,
      "step": 1085
    },
    {
      "epoch": 1.6479514415781487,
      "grad_norm": 0.8296588063240051,
      "learning_rate": 2.9400606980273142e-06,
      "loss": 0.5133,
      "step": 1086
    },
    {
      "epoch": 1.6494688922610015,
      "grad_norm": 0.7583321928977966,
      "learning_rate": 2.938163884673748e-06,
      "loss": 0.5471,
      "step": 1087
    },
    {
      "epoch": 1.6509863429438543,
      "grad_norm": 0.7970783710479736,
      "learning_rate": 2.9362670713201825e-06,
      "loss": 0.4758,
      "step": 1088
    },
    {
      "epoch": 1.6525037936267073,
      "grad_norm": 0.8703566193580627,
      "learning_rate": 2.934370257966616e-06,
      "loss": 0.5213,
      "step": 1089
    },
    {
      "epoch": 1.6540212443095599,
      "grad_norm": 0.7784585356712341,
      "learning_rate": 2.9324734446130504e-06,
      "loss": 0.478,
      "step": 1090
    },
    {
      "epoch": 1.655538694992413,
      "grad_norm": 0.7153319716453552,
      "learning_rate": 2.9305766312594843e-06,
      "loss": 0.554,
      "step": 1091
    },
    {
      "epoch": 1.6570561456752655,
      "grad_norm": 0.78010094165802,
      "learning_rate": 2.9286798179059182e-06,
      "loss": 0.4983,
      "step": 1092
    },
    {
      "epoch": 1.6585735963581185,
      "grad_norm": 0.7205672860145569,
      "learning_rate": 2.926783004552352e-06,
      "loss": 0.4613,
      "step": 1093
    },
    {
      "epoch": 1.660091047040971,
      "grad_norm": 0.7065385580062866,
      "learning_rate": 2.9248861911987865e-06,
      "loss": 0.5147,
      "step": 1094
    },
    {
      "epoch": 1.6616084977238241,
      "grad_norm": 0.8234506249427795,
      "learning_rate": 2.92298937784522e-06,
      "loss": 0.5744,
      "step": 1095
    },
    {
      "epoch": 1.6631259484066767,
      "grad_norm": 0.7810950875282288,
      "learning_rate": 2.9210925644916543e-06,
      "loss": 0.4235,
      "step": 1096
    },
    {
      "epoch": 1.6646433990895297,
      "grad_norm": 0.6804543733596802,
      "learning_rate": 2.9191957511380883e-06,
      "loss": 0.4734,
      "step": 1097
    },
    {
      "epoch": 1.6661608497723823,
      "grad_norm": 0.7811937928199768,
      "learning_rate": 2.917298937784522e-06,
      "loss": 0.4715,
      "step": 1098
    },
    {
      "epoch": 1.6676783004552354,
      "grad_norm": 0.8141264319419861,
      "learning_rate": 2.915402124430956e-06,
      "loss": 0.5145,
      "step": 1099
    },
    {
      "epoch": 1.669195751138088,
      "grad_norm": 0.7463986873626709,
      "learning_rate": 2.9135053110773905e-06,
      "loss": 0.5335,
      "step": 1100
    },
    {
      "epoch": 1.670713201820941,
      "grad_norm": 0.7541671395301819,
      "learning_rate": 2.911608497723824e-06,
      "loss": 0.486,
      "step": 1101
    },
    {
      "epoch": 1.6722306525037935,
      "grad_norm": 0.7670480012893677,
      "learning_rate": 2.9097116843702583e-06,
      "loss": 0.4611,
      "step": 1102
    },
    {
      "epoch": 1.6737481031866466,
      "grad_norm": 0.8342136144638062,
      "learning_rate": 2.907814871016692e-06,
      "loss": 0.5016,
      "step": 1103
    },
    {
      "epoch": 1.6752655538694992,
      "grad_norm": 0.7873482704162598,
      "learning_rate": 2.905918057663126e-06,
      "loss": 0.5087,
      "step": 1104
    },
    {
      "epoch": 1.6767830045523522,
      "grad_norm": 0.8104230761528015,
      "learning_rate": 2.90402124430956e-06,
      "loss": 0.5574,
      "step": 1105
    },
    {
      "epoch": 1.6783004552352048,
      "grad_norm": 0.7508617043495178,
      "learning_rate": 2.9021244309559944e-06,
      "loss": 0.497,
      "step": 1106
    },
    {
      "epoch": 1.6798179059180578,
      "grad_norm": 0.8419861197471619,
      "learning_rate": 2.900227617602428e-06,
      "loss": 0.5275,
      "step": 1107
    },
    {
      "epoch": 1.6813353566009104,
      "grad_norm": 0.8369075655937195,
      "learning_rate": 2.8983308042488623e-06,
      "loss": 0.5131,
      "step": 1108
    },
    {
      "epoch": 1.6828528072837634,
      "grad_norm": 0.7458667755126953,
      "learning_rate": 2.896433990895296e-06,
      "loss": 0.485,
      "step": 1109
    },
    {
      "epoch": 1.684370257966616,
      "grad_norm": 0.7576282620429993,
      "learning_rate": 2.89453717754173e-06,
      "loss": 0.5015,
      "step": 1110
    },
    {
      "epoch": 1.685887708649469,
      "grad_norm": 0.9149157404899597,
      "learning_rate": 2.892640364188164e-06,
      "loss": 0.4516,
      "step": 1111
    },
    {
      "epoch": 1.6874051593323216,
      "grad_norm": 0.7604202628135681,
      "learning_rate": 2.8907435508345984e-06,
      "loss": 0.4308,
      "step": 1112
    },
    {
      "epoch": 1.6889226100151746,
      "grad_norm": 0.783974826335907,
      "learning_rate": 2.888846737481032e-06,
      "loss": 0.5363,
      "step": 1113
    },
    {
      "epoch": 1.6904400606980272,
      "grad_norm": 0.7234585285186768,
      "learning_rate": 2.8869499241274663e-06,
      "loss": 0.4533,
      "step": 1114
    },
    {
      "epoch": 1.6919575113808802,
      "grad_norm": 0.7958245873451233,
      "learning_rate": 2.8850531107738998e-06,
      "loss": 0.4476,
      "step": 1115
    },
    {
      "epoch": 1.6934749620637328,
      "grad_norm": 0.7682774662971497,
      "learning_rate": 2.883156297420334e-06,
      "loss": 0.4893,
      "step": 1116
    },
    {
      "epoch": 1.6949924127465859,
      "grad_norm": 0.9309331774711609,
      "learning_rate": 2.881259484066768e-06,
      "loss": 0.4562,
      "step": 1117
    },
    {
      "epoch": 1.6965098634294384,
      "grad_norm": 0.7655642032623291,
      "learning_rate": 2.8793626707132024e-06,
      "loss": 0.5435,
      "step": 1118
    },
    {
      "epoch": 1.6980273141122915,
      "grad_norm": 0.7752047181129456,
      "learning_rate": 2.877465857359636e-06,
      "loss": 0.4729,
      "step": 1119
    },
    {
      "epoch": 1.699544764795144,
      "grad_norm": 0.8520053625106812,
      "learning_rate": 2.8755690440060702e-06,
      "loss": 0.4819,
      "step": 1120
    },
    {
      "epoch": 1.701062215477997,
      "grad_norm": 0.7350866198539734,
      "learning_rate": 2.8736722306525038e-06,
      "loss": 0.4201,
      "step": 1121
    },
    {
      "epoch": 1.7025796661608497,
      "grad_norm": 0.7612974047660828,
      "learning_rate": 2.871775417298938e-06,
      "loss": 0.3981,
      "step": 1122
    },
    {
      "epoch": 1.7040971168437027,
      "grad_norm": 0.7504494190216064,
      "learning_rate": 2.869878603945372e-06,
      "loss": 0.4409,
      "step": 1123
    },
    {
      "epoch": 1.7056145675265553,
      "grad_norm": 0.8565976619720459,
      "learning_rate": 2.867981790591806e-06,
      "loss": 0.5613,
      "step": 1124
    },
    {
      "epoch": 1.7071320182094083,
      "grad_norm": 0.9035297632217407,
      "learning_rate": 2.86608497723824e-06,
      "loss": 0.46,
      "step": 1125
    },
    {
      "epoch": 1.7086494688922609,
      "grad_norm": 0.7188514471054077,
      "learning_rate": 2.8641881638846742e-06,
      "loss": 0.427,
      "step": 1126
    },
    {
      "epoch": 1.710166919575114,
      "grad_norm": 0.7493747472763062,
      "learning_rate": 2.8622913505311077e-06,
      "loss": 0.4145,
      "step": 1127
    },
    {
      "epoch": 1.7116843702579665,
      "grad_norm": 0.8304914236068726,
      "learning_rate": 2.860394537177542e-06,
      "loss": 0.4686,
      "step": 1128
    },
    {
      "epoch": 1.7132018209408195,
      "grad_norm": 0.8214012980461121,
      "learning_rate": 2.858497723823976e-06,
      "loss": 0.4534,
      "step": 1129
    },
    {
      "epoch": 1.714719271623672,
      "grad_norm": 0.7487799525260925,
      "learning_rate": 2.85660091047041e-06,
      "loss": 0.4664,
      "step": 1130
    },
    {
      "epoch": 1.7162367223065251,
      "grad_norm": 0.8238638639450073,
      "learning_rate": 2.854704097116844e-06,
      "loss": 0.4854,
      "step": 1131
    },
    {
      "epoch": 1.7177541729893777,
      "grad_norm": 0.7973813414573669,
      "learning_rate": 2.852807283763278e-06,
      "loss": 0.4051,
      "step": 1132
    },
    {
      "epoch": 1.7192716236722307,
      "grad_norm": 0.7769657373428345,
      "learning_rate": 2.8509104704097117e-06,
      "loss": 0.4485,
      "step": 1133
    },
    {
      "epoch": 1.7207890743550833,
      "grad_norm": 0.8271666765213013,
      "learning_rate": 2.849013657056146e-06,
      "loss": 0.5047,
      "step": 1134
    },
    {
      "epoch": 1.7223065250379364,
      "grad_norm": 0.763364315032959,
      "learning_rate": 2.84711684370258e-06,
      "loss": 0.4835,
      "step": 1135
    },
    {
      "epoch": 1.723823975720789,
      "grad_norm": 0.6795865297317505,
      "learning_rate": 2.845220030349014e-06,
      "loss": 0.4766,
      "step": 1136
    },
    {
      "epoch": 1.725341426403642,
      "grad_norm": 0.7652872204780579,
      "learning_rate": 2.843323216995448e-06,
      "loss": 0.4658,
      "step": 1137
    },
    {
      "epoch": 1.7268588770864945,
      "grad_norm": 0.7817854285240173,
      "learning_rate": 2.841426403641882e-06,
      "loss": 0.5601,
      "step": 1138
    },
    {
      "epoch": 1.7283763277693476,
      "grad_norm": 0.9774952530860901,
      "learning_rate": 2.8395295902883157e-06,
      "loss": 0.4793,
      "step": 1139
    },
    {
      "epoch": 1.7298937784522002,
      "grad_norm": 0.8113053441047668,
      "learning_rate": 2.83763277693475e-06,
      "loss": 0.4678,
      "step": 1140
    },
    {
      "epoch": 1.7314112291350532,
      "grad_norm": 0.7143362760543823,
      "learning_rate": 2.835735963581184e-06,
      "loss": 0.4744,
      "step": 1141
    },
    {
      "epoch": 1.7329286798179058,
      "grad_norm": 1.0483052730560303,
      "learning_rate": 2.833839150227618e-06,
      "loss": 0.449,
      "step": 1142
    },
    {
      "epoch": 1.7344461305007588,
      "grad_norm": 0.7322930097579956,
      "learning_rate": 2.831942336874052e-06,
      "loss": 0.5038,
      "step": 1143
    },
    {
      "epoch": 1.7359635811836114,
      "grad_norm": 0.8502449989318848,
      "learning_rate": 2.830045523520486e-06,
      "loss": 0.4635,
      "step": 1144
    },
    {
      "epoch": 1.7374810318664644,
      "grad_norm": 0.7772228717803955,
      "learning_rate": 2.8281487101669197e-06,
      "loss": 0.5002,
      "step": 1145
    },
    {
      "epoch": 1.738998482549317,
      "grad_norm": 0.7970730662345886,
      "learning_rate": 2.826251896813354e-06,
      "loss": 0.4618,
      "step": 1146
    },
    {
      "epoch": 1.74051593323217,
      "grad_norm": 0.8028997182846069,
      "learning_rate": 2.8243550834597875e-06,
      "loss": 0.4059,
      "step": 1147
    },
    {
      "epoch": 1.7420333839150226,
      "grad_norm": 0.8604562878608704,
      "learning_rate": 2.822458270106222e-06,
      "loss": 0.4232,
      "step": 1148
    },
    {
      "epoch": 1.7435508345978756,
      "grad_norm": 0.7215724587440491,
      "learning_rate": 2.8205614567526558e-06,
      "loss": 0.4408,
      "step": 1149
    },
    {
      "epoch": 1.7450682852807282,
      "grad_norm": 0.8030796647071838,
      "learning_rate": 2.81866464339909e-06,
      "loss": 0.5059,
      "step": 1150
    },
    {
      "epoch": 1.7465857359635812,
      "grad_norm": 0.7283950448036194,
      "learning_rate": 2.8167678300455236e-06,
      "loss": 0.4207,
      "step": 1151
    },
    {
      "epoch": 1.7481031866464338,
      "grad_norm": 0.8560608625411987,
      "learning_rate": 2.814871016691958e-06,
      "loss": 0.5338,
      "step": 1152
    },
    {
      "epoch": 1.7496206373292869,
      "grad_norm": 0.8373098969459534,
      "learning_rate": 2.8129742033383915e-06,
      "loss": 0.4629,
      "step": 1153
    },
    {
      "epoch": 1.7511380880121397,
      "grad_norm": 0.7689189910888672,
      "learning_rate": 2.811077389984826e-06,
      "loss": 0.3903,
      "step": 1154
    },
    {
      "epoch": 1.7526555386949925,
      "grad_norm": 0.7022128105163574,
      "learning_rate": 2.8091805766312598e-06,
      "loss": 0.4794,
      "step": 1155
    },
    {
      "epoch": 1.7541729893778453,
      "grad_norm": 0.8499449491500854,
      "learning_rate": 2.807283763277694e-06,
      "loss": 0.5006,
      "step": 1156
    },
    {
      "epoch": 1.755690440060698,
      "grad_norm": 0.7649245262145996,
      "learning_rate": 2.8053869499241276e-06,
      "loss": 0.5181,
      "step": 1157
    },
    {
      "epoch": 1.7572078907435509,
      "grad_norm": 0.690108060836792,
      "learning_rate": 2.803490136570562e-06,
      "loss": 0.513,
      "step": 1158
    },
    {
      "epoch": 1.7587253414264037,
      "grad_norm": 0.8072371482849121,
      "learning_rate": 2.8015933232169955e-06,
      "loss": 0.5165,
      "step": 1159
    },
    {
      "epoch": 1.7602427921092565,
      "grad_norm": 0.73676598072052,
      "learning_rate": 2.79969650986343e-06,
      "loss": 0.4831,
      "step": 1160
    },
    {
      "epoch": 1.7617602427921093,
      "grad_norm": 0.7249675393104553,
      "learning_rate": 2.7977996965098637e-06,
      "loss": 0.5394,
      "step": 1161
    },
    {
      "epoch": 1.763277693474962,
      "grad_norm": 0.7786719799041748,
      "learning_rate": 2.795902883156298e-06,
      "loss": 0.4998,
      "step": 1162
    },
    {
      "epoch": 1.764795144157815,
      "grad_norm": 0.8132042288780212,
      "learning_rate": 2.7940060698027316e-06,
      "loss": 0.357,
      "step": 1163
    },
    {
      "epoch": 1.7663125948406677,
      "grad_norm": 0.6923694014549255,
      "learning_rate": 2.792109256449166e-06,
      "loss": 0.5096,
      "step": 1164
    },
    {
      "epoch": 1.7678300455235205,
      "grad_norm": 0.7018610835075378,
      "learning_rate": 2.7902124430955994e-06,
      "loss": 0.4078,
      "step": 1165
    },
    {
      "epoch": 1.7693474962063733,
      "grad_norm": 0.7467529773712158,
      "learning_rate": 2.7883156297420338e-06,
      "loss": 0.4246,
      "step": 1166
    },
    {
      "epoch": 1.7708649468892261,
      "grad_norm": NaN,
      "learning_rate": 2.7883156297420338e-06,
      "loss": 0.4695,
      "step": 1167
    },
    {
      "epoch": 1.772382397572079,
      "grad_norm": 0.8197860717773438,
      "learning_rate": 2.7864188163884677e-06,
      "loss": 0.455,
      "step": 1168
    },
    {
      "epoch": 1.7738998482549317,
      "grad_norm": 0.6713728308677673,
      "learning_rate": 2.7845220030349016e-06,
      "loss": 0.4245,
      "step": 1169
    },
    {
      "epoch": 1.7754172989377845,
      "grad_norm": 1.0045093297958374,
      "learning_rate": 2.7826251896813356e-06,
      "loss": 0.5143,
      "step": 1170
    },
    {
      "epoch": 1.7769347496206374,
      "grad_norm": 0.7411584258079529,
      "learning_rate": 2.78072837632777e-06,
      "loss": 0.4809,
      "step": 1171
    },
    {
      "epoch": 1.7784522003034902,
      "grad_norm": 1.0757373571395874,
      "learning_rate": 2.7788315629742034e-06,
      "loss": 0.5055,
      "step": 1172
    },
    {
      "epoch": 1.779969650986343,
      "grad_norm": 0.7348597645759583,
      "learning_rate": 2.7769347496206378e-06,
      "loss": 0.4575,
      "step": 1173
    },
    {
      "epoch": 1.7814871016691958,
      "grad_norm": 0.861450731754303,
      "learning_rate": 2.7750379362670717e-06,
      "loss": 0.4673,
      "step": 1174
    },
    {
      "epoch": 1.7830045523520486,
      "grad_norm": 0.8325413465499878,
      "learning_rate": 2.7731411229135056e-06,
      "loss": 0.4751,
      "step": 1175
    },
    {
      "epoch": 1.7845220030349014,
      "grad_norm": 0.7130002975463867,
      "learning_rate": 2.7712443095599395e-06,
      "loss": 0.422,
      "step": 1176
    },
    {
      "epoch": 1.7860394537177542,
      "grad_norm": 0.784915566444397,
      "learning_rate": 2.769347496206374e-06,
      "loss": 0.4521,
      "step": 1177
    },
    {
      "epoch": 1.787556904400607,
      "grad_norm": 0.6993827819824219,
      "learning_rate": 2.7674506828528074e-06,
      "loss": 0.4802,
      "step": 1178
    },
    {
      "epoch": 1.7890743550834598,
      "grad_norm": 0.9044429063796997,
      "learning_rate": 2.7655538694992417e-06,
      "loss": 0.4927,
      "step": 1179
    },
    {
      "epoch": 1.7905918057663126,
      "grad_norm": 0.6629013419151306,
      "learning_rate": 2.7636570561456757e-06,
      "loss": 0.4229,
      "step": 1180
    },
    {
      "epoch": 1.7921092564491654,
      "grad_norm": 0.7300974726676941,
      "learning_rate": 2.7617602427921096e-06,
      "loss": 0.497,
      "step": 1181
    },
    {
      "epoch": 1.7936267071320182,
      "grad_norm": 0.7630741000175476,
      "learning_rate": 2.7598634294385435e-06,
      "loss": 0.4548,
      "step": 1182
    },
    {
      "epoch": 1.795144157814871,
      "grad_norm": 0.8485010266304016,
      "learning_rate": 2.757966616084978e-06,
      "loss": 0.5129,
      "step": 1183
    },
    {
      "epoch": 1.7966616084977238,
      "grad_norm": 0.8764451146125793,
      "learning_rate": 2.7560698027314114e-06,
      "loss": 0.4898,
      "step": 1184
    },
    {
      "epoch": 1.7981790591805766,
      "grad_norm": 0.6982226967811584,
      "learning_rate": 2.7541729893778457e-06,
      "loss": 0.3797,
      "step": 1185
    },
    {
      "epoch": 1.7996965098634294,
      "grad_norm": 0.7284002304077148,
      "learning_rate": 2.7522761760242796e-06,
      "loss": 0.4818,
      "step": 1186
    },
    {
      "epoch": 1.8012139605462822,
      "grad_norm": 0.7189427614212036,
      "learning_rate": 2.7503793626707136e-06,
      "loss": 0.4885,
      "step": 1187
    },
    {
      "epoch": 1.802731411229135,
      "grad_norm": 0.8278812766075134,
      "learning_rate": 2.7484825493171475e-06,
      "loss": 0.4799,
      "step": 1188
    },
    {
      "epoch": 1.8042488619119879,
      "grad_norm": 0.6948935389518738,
      "learning_rate": 2.746585735963581e-06,
      "loss": 0.4335,
      "step": 1189
    },
    {
      "epoch": 1.8057663125948407,
      "grad_norm": 0.778026819229126,
      "learning_rate": 2.7446889226100153e-06,
      "loss": 0.3954,
      "step": 1190
    },
    {
      "epoch": 1.8072837632776935,
      "grad_norm": 0.8068287968635559,
      "learning_rate": 2.7427921092564493e-06,
      "loss": 0.4866,
      "step": 1191
    },
    {
      "epoch": 1.8088012139605463,
      "grad_norm": 0.9128187298774719,
      "learning_rate": 2.740895295902883e-06,
      "loss": 0.4469,
      "step": 1192
    },
    {
      "epoch": 1.810318664643399,
      "grad_norm": 0.8219955563545227,
      "learning_rate": 2.738998482549317e-06,
      "loss": 0.4028,
      "step": 1193
    },
    {
      "epoch": 1.8118361153262519,
      "grad_norm": 0.7784366011619568,
      "learning_rate": 2.7371016691957515e-06,
      "loss": 0.5191,
      "step": 1194
    },
    {
      "epoch": 1.8133535660091047,
      "grad_norm": 0.8223621249198914,
      "learning_rate": 2.735204855842185e-06,
      "loss": 0.4317,
      "step": 1195
    },
    {
      "epoch": 1.8148710166919575,
      "grad_norm": 0.8092402815818787,
      "learning_rate": 2.7333080424886193e-06,
      "loss": 0.4729,
      "step": 1196
    },
    {
      "epoch": 1.8163884673748103,
      "grad_norm": 0.7776556611061096,
      "learning_rate": 2.7314112291350532e-06,
      "loss": 0.486,
      "step": 1197
    },
    {
      "epoch": 1.817905918057663,
      "grad_norm": 0.7990955114364624,
      "learning_rate": 2.729514415781487e-06,
      "loss": 0.5264,
      "step": 1198
    },
    {
      "epoch": 1.819423368740516,
      "grad_norm": 0.7954473495483398,
      "learning_rate": 2.727617602427921e-06,
      "loss": 0.4864,
      "step": 1199
    },
    {
      "epoch": 1.8209408194233687,
      "grad_norm": 0.7431261539459229,
      "learning_rate": 2.7257207890743554e-06,
      "loss": 0.4813,
      "step": 1200
    },
    {
      "epoch": 1.8224582701062215,
      "grad_norm": 0.787602961063385,
      "learning_rate": 2.723823975720789e-06,
      "loss": 0.4433,
      "step": 1201
    },
    {
      "epoch": 1.8239757207890743,
      "grad_norm": 0.7266238927841187,
      "learning_rate": 2.7219271623672233e-06,
      "loss": 0.4001,
      "step": 1202
    },
    {
      "epoch": 1.8254931714719271,
      "grad_norm": 0.6875113844871521,
      "learning_rate": 2.7200303490136572e-06,
      "loss": 0.5007,
      "step": 1203
    },
    {
      "epoch": 1.82701062215478,
      "grad_norm": 0.7876385450363159,
      "learning_rate": 2.718133535660091e-06,
      "loss": 0.4767,
      "step": 1204
    },
    {
      "epoch": 1.8285280728376327,
      "grad_norm": 0.8228778839111328,
      "learning_rate": 2.716236722306525e-06,
      "loss": 0.5019,
      "step": 1205
    },
    {
      "epoch": 1.8300455235204856,
      "grad_norm": 0.7017737627029419,
      "learning_rate": 2.7143399089529594e-06,
      "loss": 0.5307,
      "step": 1206
    },
    {
      "epoch": 1.8315629742033384,
      "grad_norm": 0.7926883101463318,
      "learning_rate": 2.712443095599393e-06,
      "loss": 0.4586,
      "step": 1207
    },
    {
      "epoch": 1.8330804248861912,
      "grad_norm": 0.7919149398803711,
      "learning_rate": 2.7105462822458273e-06,
      "loss": 0.4597,
      "step": 1208
    },
    {
      "epoch": 1.834597875569044,
      "grad_norm": 0.7349370718002319,
      "learning_rate": 2.708649468892261e-06,
      "loss": 0.4438,
      "step": 1209
    },
    {
      "epoch": 1.8361153262518968,
      "grad_norm": 0.7187139391899109,
      "learning_rate": 2.706752655538695e-06,
      "loss": 0.4138,
      "step": 1210
    },
    {
      "epoch": 1.8376327769347496,
      "grad_norm": 0.7292289733886719,
      "learning_rate": 2.704855842185129e-06,
      "loss": 0.4459,
      "step": 1211
    },
    {
      "epoch": 1.8391502276176024,
      "grad_norm": 0.7881174683570862,
      "learning_rate": 2.7029590288315634e-06,
      "loss": 0.4635,
      "step": 1212
    },
    {
      "epoch": 1.8406676783004552,
      "grad_norm": 0.8131877183914185,
      "learning_rate": 2.701062215477997e-06,
      "loss": 0.4424,
      "step": 1213
    },
    {
      "epoch": 1.842185128983308,
      "grad_norm": 0.8002461791038513,
      "learning_rate": 2.6991654021244312e-06,
      "loss": 0.4225,
      "step": 1214
    },
    {
      "epoch": 1.8437025796661608,
      "grad_norm": 0.7235583662986755,
      "learning_rate": 2.6972685887708648e-06,
      "loss": 0.3664,
      "step": 1215
    },
    {
      "epoch": 1.8452200303490136,
      "grad_norm": 0.7126868963241577,
      "learning_rate": 2.695371775417299e-06,
      "loss": 0.442,
      "step": 1216
    },
    {
      "epoch": 1.8467374810318664,
      "grad_norm": 0.8299351334571838,
      "learning_rate": 2.693474962063733e-06,
      "loss": 0.4025,
      "step": 1217
    },
    {
      "epoch": 1.8482549317147192,
      "grad_norm": 0.7000476717948914,
      "learning_rate": 2.6915781487101674e-06,
      "loss": 0.4477,
      "step": 1218
    },
    {
      "epoch": 1.849772382397572,
      "grad_norm": 0.8251737952232361,
      "learning_rate": 2.689681335356601e-06,
      "loss": 0.4849,
      "step": 1219
    },
    {
      "epoch": 1.851289833080425,
      "grad_norm": 0.7009702324867249,
      "learning_rate": 2.6877845220030352e-06,
      "loss": 0.3717,
      "step": 1220
    },
    {
      "epoch": 1.8528072837632776,
      "grad_norm": 0.9555925130844116,
      "learning_rate": 2.6858877086494687e-06,
      "loss": 0.4452,
      "step": 1221
    },
    {
      "epoch": 1.8543247344461307,
      "grad_norm": 0.7121559977531433,
      "learning_rate": 2.683990895295903e-06,
      "loss": 0.4538,
      "step": 1222
    },
    {
      "epoch": 1.8558421851289832,
      "grad_norm": 0.6253114342689514,
      "learning_rate": 2.682094081942337e-06,
      "loss": 0.4417,
      "step": 1223
    },
    {
      "epoch": 1.8573596358118363,
      "grad_norm": 0.8100435733795166,
      "learning_rate": 2.6801972685887714e-06,
      "loss": 0.3839,
      "step": 1224
    },
    {
      "epoch": 1.8588770864946889,
      "grad_norm": 0.7771409153938293,
      "learning_rate": 2.678300455235205e-06,
      "loss": 0.4349,
      "step": 1225
    },
    {
      "epoch": 1.8603945371775419,
      "grad_norm": 0.7794637680053711,
      "learning_rate": 2.676403641881639e-06,
      "loss": 0.4294,
      "step": 1226
    },
    {
      "epoch": 1.8619119878603945,
      "grad_norm": 0.7303665280342102,
      "learning_rate": 2.6745068285280727e-06,
      "loss": 0.5124,
      "step": 1227
    },
    {
      "epoch": 1.8634294385432475,
      "grad_norm": 0.733555018901825,
      "learning_rate": 2.672610015174507e-06,
      "loss": 0.4203,
      "step": 1228
    },
    {
      "epoch": 1.8649468892261,
      "grad_norm": 0.7123723030090332,
      "learning_rate": 2.670713201820941e-06,
      "loss": 0.4898,
      "step": 1229
    },
    {
      "epoch": 1.866464339908953,
      "grad_norm": 0.7563650608062744,
      "learning_rate": 2.6688163884673753e-06,
      "loss": 0.4199,
      "step": 1230
    },
    {
      "epoch": 1.8679817905918057,
      "grad_norm": 0.7991386651992798,
      "learning_rate": 2.666919575113809e-06,
      "loss": 0.4536,
      "step": 1231
    },
    {
      "epoch": 1.8694992412746587,
      "grad_norm": 0.7601667046546936,
      "learning_rate": 2.665022761760243e-06,
      "loss": 0.4889,
      "step": 1232
    },
    {
      "epoch": 1.8710166919575113,
      "grad_norm": 0.7535145878791809,
      "learning_rate": 2.6631259484066767e-06,
      "loss": 0.5034,
      "step": 1233
    },
    {
      "epoch": 1.8725341426403643,
      "grad_norm": 0.678016185760498,
      "learning_rate": 2.661229135053111e-06,
      "loss": 0.4465,
      "step": 1234
    },
    {
      "epoch": 1.874051593323217,
      "grad_norm": 0.6946050524711609,
      "learning_rate": 2.659332321699545e-06,
      "loss": 0.4502,
      "step": 1235
    },
    {
      "epoch": 1.87556904400607,
      "grad_norm": 0.7529787421226501,
      "learning_rate": 2.657435508345979e-06,
      "loss": 0.418,
      "step": 1236
    },
    {
      "epoch": 1.8770864946889225,
      "grad_norm": 0.7594640254974365,
      "learning_rate": 2.655538694992413e-06,
      "loss": 0.4367,
      "step": 1237
    },
    {
      "epoch": 1.8786039453717756,
      "grad_norm": 0.7945356369018555,
      "learning_rate": 2.653641881638847e-06,
      "loss": 0.3967,
      "step": 1238
    },
    {
      "epoch": 1.8801213960546281,
      "grad_norm": 0.6905581951141357,
      "learning_rate": 2.6517450682852807e-06,
      "loss": 0.4619,
      "step": 1239
    },
    {
      "epoch": 1.8816388467374812,
      "grad_norm": 0.7269334197044373,
      "learning_rate": 2.649848254931715e-06,
      "loss": 0.4089,
      "step": 1240
    },
    {
      "epoch": 1.8831562974203337,
      "grad_norm": 0.7403880953788757,
      "learning_rate": 2.647951441578149e-06,
      "loss": 0.393,
      "step": 1241
    },
    {
      "epoch": 1.8846737481031868,
      "grad_norm": 0.6573333144187927,
      "learning_rate": 2.646054628224583e-06,
      "loss": 0.4321,
      "step": 1242
    },
    {
      "epoch": 1.8861911987860394,
      "grad_norm": 0.7443058490753174,
      "learning_rate": 2.6441578148710168e-06,
      "loss": 0.4483,
      "step": 1243
    },
    {
      "epoch": 1.8877086494688924,
      "grad_norm": 0.6805448532104492,
      "learning_rate": 2.642261001517451e-06,
      "loss": 0.3449,
      "step": 1244
    },
    {
      "epoch": 1.889226100151745,
      "grad_norm": 0.8844950199127197,
      "learning_rate": 2.6403641881638846e-06,
      "loss": 0.3713,
      "step": 1245
    },
    {
      "epoch": 1.890743550834598,
      "grad_norm": 0.7295020222663879,
      "learning_rate": 2.638467374810319e-06,
      "loss": 0.3728,
      "step": 1246
    },
    {
      "epoch": 1.8922610015174506,
      "grad_norm": 0.844834566116333,
      "learning_rate": 2.636570561456753e-06,
      "loss": 0.4424,
      "step": 1247
    },
    {
      "epoch": 1.8937784522003036,
      "grad_norm": 0.7239975929260254,
      "learning_rate": 2.634673748103187e-06,
      "loss": 0.4304,
      "step": 1248
    },
    {
      "epoch": 1.8952959028831562,
      "grad_norm": 0.6344717741012573,
      "learning_rate": 2.6327769347496208e-06,
      "loss": 0.4448,
      "step": 1249
    },
    {
      "epoch": 1.8968133535660092,
      "grad_norm": 0.7826501727104187,
      "learning_rate": 2.630880121396055e-06,
      "loss": 0.4137,
      "step": 1250
    },
    {
      "epoch": 1.8983308042488618,
      "grad_norm": 0.9958877563476562,
      "learning_rate": 2.6289833080424886e-06,
      "loss": 0.4419,
      "step": 1251
    },
    {
      "epoch": 1.8998482549317148,
      "grad_norm": 0.7187129855155945,
      "learning_rate": 2.627086494688923e-06,
      "loss": 0.3431,
      "step": 1252
    },
    {
      "epoch": 1.9013657056145674,
      "grad_norm": 0.7693142294883728,
      "learning_rate": 2.625189681335357e-06,
      "loss": 0.3775,
      "step": 1253
    },
    {
      "epoch": 1.9028831562974204,
      "grad_norm": 0.7232300043106079,
      "learning_rate": 2.623292867981791e-06,
      "loss": 0.4741,
      "step": 1254
    },
    {
      "epoch": 1.904400606980273,
      "grad_norm": 0.8153777122497559,
      "learning_rate": 2.6213960546282247e-06,
      "loss": 0.4504,
      "step": 1255
    },
    {
      "epoch": 1.905918057663126,
      "grad_norm": 0.7313961982727051,
      "learning_rate": 2.619499241274659e-06,
      "loss": 0.4557,
      "step": 1256
    },
    {
      "epoch": 1.9074355083459786,
      "grad_norm": 0.7854203581809998,
      "learning_rate": 2.6176024279210926e-06,
      "loss": 0.4038,
      "step": 1257
    },
    {
      "epoch": 1.9089529590288317,
      "grad_norm": 0.6291289925575256,
      "learning_rate": 2.615705614567527e-06,
      "loss": 0.4318,
      "step": 1258
    },
    {
      "epoch": 1.9104704097116842,
      "grad_norm": 0.8342536091804504,
      "learning_rate": 2.6138088012139604e-06,
      "loss": 0.3828,
      "step": 1259
    },
    {
      "epoch": 1.9119878603945373,
      "grad_norm": 0.7162157893180847,
      "learning_rate": 2.6119119878603948e-06,
      "loss": 0.4198,
      "step": 1260
    },
    {
      "epoch": 1.9135053110773899,
      "grad_norm": 0.8628060817718506,
      "learning_rate": 2.6100151745068287e-06,
      "loss": 0.4313,
      "step": 1261
    },
    {
      "epoch": 1.9150227617602429,
      "grad_norm": 0.6630631685256958,
      "learning_rate": 2.608118361153263e-06,
      "loss": 0.5134,
      "step": 1262
    },
    {
      "epoch": 1.9165402124430955,
      "grad_norm": 0.697038471698761,
      "learning_rate": 2.6062215477996966e-06,
      "loss": 0.4483,
      "step": 1263
    },
    {
      "epoch": 1.9180576631259485,
      "grad_norm": 0.7190819382667542,
      "learning_rate": 2.604324734446131e-06,
      "loss": 0.4786,
      "step": 1264
    },
    {
      "epoch": 1.919575113808801,
      "grad_norm": 0.9300246834754944,
      "learning_rate": 2.6024279210925644e-06,
      "loss": 0.3812,
      "step": 1265
    },
    {
      "epoch": 1.921092564491654,
      "grad_norm": 0.7811073660850525,
      "learning_rate": 2.6005311077389988e-06,
      "loss": 0.4449,
      "step": 1266
    },
    {
      "epoch": 1.9226100151745067,
      "grad_norm": 0.720337986946106,
      "learning_rate": 2.5986342943854327e-06,
      "loss": 0.3208,
      "step": 1267
    },
    {
      "epoch": 1.9241274658573597,
      "grad_norm": 0.9240683913230896,
      "learning_rate": 2.596737481031867e-06,
      "loss": 0.4364,
      "step": 1268
    },
    {
      "epoch": 1.9256449165402123,
      "grad_norm": 0.756955087184906,
      "learning_rate": 2.5948406676783005e-06,
      "loss": 0.3642,
      "step": 1269
    },
    {
      "epoch": 1.9271623672230653,
      "grad_norm": 0.7105963826179504,
      "learning_rate": 2.592943854324735e-06,
      "loss": 0.3866,
      "step": 1270
    },
    {
      "epoch": 1.928679817905918,
      "grad_norm": 0.7827079892158508,
      "learning_rate": 2.5910470409711684e-06,
      "loss": 0.3235,
      "step": 1271
    },
    {
      "epoch": 1.930197268588771,
      "grad_norm": 0.6633968949317932,
      "learning_rate": 2.5891502276176027e-06,
      "loss": 0.3974,
      "step": 1272
    },
    {
      "epoch": 1.9317147192716235,
      "grad_norm": 0.7732729315757751,
      "learning_rate": 2.5872534142640367e-06,
      "loss": 0.4034,
      "step": 1273
    },
    {
      "epoch": 1.9332321699544766,
      "grad_norm": 0.6753467321395874,
      "learning_rate": 2.585356600910471e-06,
      "loss": 0.41,
      "step": 1274
    },
    {
      "epoch": 1.9347496206373291,
      "grad_norm": 0.7675151228904724,
      "learning_rate": 2.5834597875569045e-06,
      "loss": 0.4407,
      "step": 1275
    },
    {
      "epoch": 1.9362670713201822,
      "grad_norm": 0.8066815137863159,
      "learning_rate": 2.581562974203339e-06,
      "loss": 0.4674,
      "step": 1276
    },
    {
      "epoch": 1.9377845220030347,
      "grad_norm": 0.7789056897163391,
      "learning_rate": 2.5796661608497724e-06,
      "loss": 0.4616,
      "step": 1277
    },
    {
      "epoch": 1.9393019726858878,
      "grad_norm": 0.7990955114364624,
      "learning_rate": 2.5777693474962067e-06,
      "loss": 0.3869,
      "step": 1278
    },
    {
      "epoch": 1.9408194233687404,
      "grad_norm": 0.7542016506195068,
      "learning_rate": 2.5758725341426406e-06,
      "loss": 0.3728,
      "step": 1279
    },
    {
      "epoch": 1.9423368740515934,
      "grad_norm": 0.7470676302909851,
      "learning_rate": 2.5739757207890746e-06,
      "loss": 0.4675,
      "step": 1280
    },
    {
      "epoch": 1.943854324734446,
      "grad_norm": 0.836834728717804,
      "learning_rate": 2.5720789074355085e-06,
      "loss": 0.4582,
      "step": 1281
    },
    {
      "epoch": 1.945371775417299,
      "grad_norm": 0.8209420442581177,
      "learning_rate": 2.570182094081943e-06,
      "loss": 0.4456,
      "step": 1282
    },
    {
      "epoch": 1.9468892261001516,
      "grad_norm": 0.795702338218689,
      "learning_rate": 2.5682852807283763e-06,
      "loss": 0.4298,
      "step": 1283
    },
    {
      "epoch": 1.9484066767830046,
      "grad_norm": 0.7565538287162781,
      "learning_rate": 2.5663884673748107e-06,
      "loss": 0.4696,
      "step": 1284
    },
    {
      "epoch": 1.9499241274658572,
      "grad_norm": 0.7103140354156494,
      "learning_rate": 2.5644916540212446e-06,
      "loss": 0.5046,
      "step": 1285
    },
    {
      "epoch": 1.9514415781487102,
      "grad_norm": 0.7898181676864624,
      "learning_rate": 2.5625948406676785e-06,
      "loss": 0.4518,
      "step": 1286
    },
    {
      "epoch": 1.952959028831563,
      "grad_norm": 0.7237702012062073,
      "learning_rate": 2.5606980273141125e-06,
      "loss": 0.5407,
      "step": 1287
    },
    {
      "epoch": 1.9544764795144158,
      "grad_norm": 0.7530133724212646,
      "learning_rate": 2.558801213960547e-06,
      "loss": 0.395,
      "step": 1288
    },
    {
      "epoch": 1.9559939301972686,
      "grad_norm": 0.6966849565505981,
      "learning_rate": 2.5569044006069803e-06,
      "loss": 0.4165,
      "step": 1289
    },
    {
      "epoch": 1.9575113808801214,
      "grad_norm": 0.8981953263282776,
      "learning_rate": 2.5550075872534147e-06,
      "loss": 0.4711,
      "step": 1290
    },
    {
      "epoch": 1.9590288315629742,
      "grad_norm": 0.7146081328392029,
      "learning_rate": 2.5531107738998486e-06,
      "loss": 0.4277,
      "step": 1291
    },
    {
      "epoch": 1.960546282245827,
      "grad_norm": 0.7302681803703308,
      "learning_rate": 2.5512139605462825e-06,
      "loss": 0.4803,
      "step": 1292
    },
    {
      "epoch": 1.9620637329286799,
      "grad_norm": 0.8188121318817139,
      "learning_rate": 2.5493171471927164e-06,
      "loss": 0.4394,
      "step": 1293
    },
    {
      "epoch": 1.9635811836115327,
      "grad_norm": 0.7257273197174072,
      "learning_rate": 2.547420333839151e-06,
      "loss": 0.3477,
      "step": 1294
    },
    {
      "epoch": 1.9650986342943855,
      "grad_norm": 0.7446451187133789,
      "learning_rate": 2.5455235204855843e-06,
      "loss": 0.4198,
      "step": 1295
    },
    {
      "epoch": 1.9666160849772383,
      "grad_norm": 0.7203855514526367,
      "learning_rate": 2.5436267071320186e-06,
      "loss": 0.3845,
      "step": 1296
    },
    {
      "epoch": 1.968133535660091,
      "grad_norm": 0.833365261554718,
      "learning_rate": 2.5417298937784526e-06,
      "loss": 0.467,
      "step": 1297
    },
    {
      "epoch": 1.9696509863429439,
      "grad_norm": 0.7118340730667114,
      "learning_rate": 2.5398330804248865e-06,
      "loss": 0.3207,
      "step": 1298
    },
    {
      "epoch": 1.9711684370257967,
      "grad_norm": 0.6493121981620789,
      "learning_rate": 2.5379362670713204e-06,
      "loss": 0.4396,
      "step": 1299
    },
    {
      "epoch": 1.9726858877086495,
      "grad_norm": 0.894439697265625,
      "learning_rate": 2.5360394537177548e-06,
      "loss": 0.4429,
      "step": 1300
    },
    {
      "epoch": 1.9742033383915023,
      "grad_norm": 0.7804149389266968,
      "learning_rate": 2.5341426403641883e-06,
      "loss": 0.3943,
      "step": 1301
    },
    {
      "epoch": 1.975720789074355,
      "grad_norm": 0.6968331336975098,
      "learning_rate": 2.5322458270106226e-06,
      "loss": 0.3967,
      "step": 1302
    },
    {
      "epoch": 1.977238239757208,
      "grad_norm": 0.654218316078186,
      "learning_rate": 2.530349013657056e-06,
      "loss": 0.4293,
      "step": 1303
    },
    {
      "epoch": 1.9787556904400607,
      "grad_norm": 0.8898016214370728,
      "learning_rate": 2.5284522003034905e-06,
      "loss": 0.4788,
      "step": 1304
    },
    {
      "epoch": 1.9802731411229135,
      "grad_norm": 0.7122480273246765,
      "learning_rate": 2.5265553869499244e-06,
      "loss": 0.4263,
      "step": 1305
    },
    {
      "epoch": 1.9817905918057663,
      "grad_norm": 0.730893611907959,
      "learning_rate": 2.5246585735963587e-06,
      "loss": 0.4671,
      "step": 1306
    },
    {
      "epoch": 1.9833080424886191,
      "grad_norm": 0.9359156489372253,
      "learning_rate": 2.5227617602427923e-06,
      "loss": 0.4223,
      "step": 1307
    },
    {
      "epoch": 1.984825493171472,
      "grad_norm": 0.6411389708518982,
      "learning_rate": 2.5208649468892266e-06,
      "loss": 0.4868,
      "step": 1308
    },
    {
      "epoch": 1.9863429438543247,
      "grad_norm": 0.6370247006416321,
      "learning_rate": 2.51896813353566e-06,
      "loss": 0.4477,
      "step": 1309
    },
    {
      "epoch": 1.9878603945371776,
      "grad_norm": 0.7732454538345337,
      "learning_rate": 2.5170713201820945e-06,
      "loss": 0.4062,
      "step": 1310
    },
    {
      "epoch": 1.9893778452200304,
      "grad_norm": 0.7351574301719666,
      "learning_rate": 2.5151745068285284e-06,
      "loss": 0.4064,
      "step": 1311
    },
    {
      "epoch": 1.9908952959028832,
      "grad_norm": 0.9328382015228271,
      "learning_rate": 2.5132776934749627e-06,
      "loss": 0.424,
      "step": 1312
    },
    {
      "epoch": 1.992412746585736,
      "grad_norm": 1.8798032999038696,
      "learning_rate": 2.5113808801213962e-06,
      "loss": 0.4662,
      "step": 1313
    },
    {
      "epoch": 1.9939301972685888,
      "grad_norm": 0.7345114350318909,
      "learning_rate": 2.5094840667678306e-06,
      "loss": 0.3869,
      "step": 1314
    },
    {
      "epoch": 1.9954476479514416,
      "grad_norm": 0.6708837151527405,
      "learning_rate": 2.507587253414264e-06,
      "loss": 0.4526,
      "step": 1315
    },
    {
      "epoch": 1.9969650986342944,
      "grad_norm": 0.8634695410728455,
      "learning_rate": 2.5056904400606984e-06,
      "loss": 0.4102,
      "step": 1316
    },
    {
      "epoch": 1.9984825493171472,
      "grad_norm": 0.6483496427536011,
      "learning_rate": 2.5037936267071324e-06,
      "loss": 0.4878,
      "step": 1317
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.7182413935661316,
      "learning_rate": 2.5018968133535663e-06,
      "loss": 0.3912,
      "step": 1318
    },
    {
      "epoch": 2.001517450682853,
      "grad_norm": 0.7738302946090698,
      "learning_rate": 2.5e-06,
      "loss": 0.3436,
      "step": 1319
    },
    {
      "epoch": 2.0030349013657056,
      "grad_norm": 0.9107718467712402,
      "learning_rate": 2.498103186646434e-06,
      "loss": 0.3834,
      "step": 1320
    },
    {
      "epoch": 2.0045523520485586,
      "grad_norm": 0.6262805461883545,
      "learning_rate": 2.496206373292868e-06,
      "loss": 0.4185,
      "step": 1321
    },
    {
      "epoch": 2.0060698027314112,
      "grad_norm": 0.7611541152000427,
      "learning_rate": 2.494309559939302e-06,
      "loss": 0.4598,
      "step": 1322
    },
    {
      "epoch": 2.0075872534142643,
      "grad_norm": 0.6597197651863098,
      "learning_rate": 2.4924127465857363e-06,
      "loss": 0.4392,
      "step": 1323
    },
    {
      "epoch": 2.009104704097117,
      "grad_norm": 0.7689185738563538,
      "learning_rate": 2.4905159332321703e-06,
      "loss": 0.4337,
      "step": 1324
    },
    {
      "epoch": 2.01062215477997,
      "grad_norm": 0.6649989485740662,
      "learning_rate": 2.488619119878604e-06,
      "loss": 0.3661,
      "step": 1325
    },
    {
      "epoch": 2.0121396054628224,
      "grad_norm": 0.7546749711036682,
      "learning_rate": 2.486722306525038e-06,
      "loss": 0.4606,
      "step": 1326
    },
    {
      "epoch": 2.0136570561456755,
      "grad_norm": 0.7398785352706909,
      "learning_rate": 2.484825493171472e-06,
      "loss": 0.3847,
      "step": 1327
    },
    {
      "epoch": 2.015174506828528,
      "grad_norm": 0.7687200307846069,
      "learning_rate": 2.482928679817906e-06,
      "loss": 0.3873,
      "step": 1328
    },
    {
      "epoch": 2.016691957511381,
      "grad_norm": 0.7715986371040344,
      "learning_rate": 2.4810318664643403e-06,
      "loss": 0.4353,
      "step": 1329
    },
    {
      "epoch": 2.0182094081942337,
      "grad_norm": 0.7450002431869507,
      "learning_rate": 2.4791350531107742e-06,
      "loss": 0.2985,
      "step": 1330
    },
    {
      "epoch": 2.0197268588770867,
      "grad_norm": 0.8309657573699951,
      "learning_rate": 2.477238239757208e-06,
      "loss": 0.4788,
      "step": 1331
    },
    {
      "epoch": 2.0212443095599393,
      "grad_norm": 0.6827001571655273,
      "learning_rate": 2.475341426403642e-06,
      "loss": 0.4024,
      "step": 1332
    },
    {
      "epoch": 2.0227617602427923,
      "grad_norm": 0.777490496635437,
      "learning_rate": 2.473444613050076e-06,
      "loss": 0.4208,
      "step": 1333
    },
    {
      "epoch": 2.024279210925645,
      "grad_norm": 0.6682451963424683,
      "learning_rate": 2.47154779969651e-06,
      "loss": 0.3921,
      "step": 1334
    },
    {
      "epoch": 2.025796661608498,
      "grad_norm": 0.6859871745109558,
      "learning_rate": 2.4696509863429443e-06,
      "loss": 0.4148,
      "step": 1335
    },
    {
      "epoch": 2.0273141122913505,
      "grad_norm": 0.7619078755378723,
      "learning_rate": 2.467754172989378e-06,
      "loss": 0.4327,
      "step": 1336
    },
    {
      "epoch": 2.0288315629742035,
      "grad_norm": 0.7534498572349548,
      "learning_rate": 2.465857359635812e-06,
      "loss": 0.3924,
      "step": 1337
    },
    {
      "epoch": 2.030349013657056,
      "grad_norm": 0.6967165470123291,
      "learning_rate": 2.463960546282246e-06,
      "loss": 0.3914,
      "step": 1338
    },
    {
      "epoch": 2.031866464339909,
      "grad_norm": 0.7031086683273315,
      "learning_rate": 2.46206373292868e-06,
      "loss": 0.452,
      "step": 1339
    },
    {
      "epoch": 2.0333839150227617,
      "grad_norm": 2.5306715965270996,
      "learning_rate": 2.460166919575114e-06,
      "loss": 0.4518,
      "step": 1340
    },
    {
      "epoch": 2.0349013657056148,
      "grad_norm": 0.7270659804344177,
      "learning_rate": 2.458270106221548e-06,
      "loss": 0.4245,
      "step": 1341
    },
    {
      "epoch": 2.0364188163884673,
      "grad_norm": 0.6706833839416504,
      "learning_rate": 2.456373292867982e-06,
      "loss": 0.5136,
      "step": 1342
    },
    {
      "epoch": 2.0379362670713204,
      "grad_norm": 0.6386483311653137,
      "learning_rate": 2.454476479514416e-06,
      "loss": 0.4417,
      "step": 1343
    },
    {
      "epoch": 2.039453717754173,
      "grad_norm": 0.7736127972602844,
      "learning_rate": 2.45257966616085e-06,
      "loss": 0.4284,
      "step": 1344
    },
    {
      "epoch": 2.040971168437026,
      "grad_norm": 0.8206384778022766,
      "learning_rate": 2.450682852807284e-06,
      "loss": 0.3959,
      "step": 1345
    },
    {
      "epoch": 2.0424886191198786,
      "grad_norm": 0.696505069732666,
      "learning_rate": 2.448786039453718e-06,
      "loss": 0.3959,
      "step": 1346
    },
    {
      "epoch": 2.0440060698027316,
      "grad_norm": 0.9228935241699219,
      "learning_rate": 2.446889226100152e-06,
      "loss": 0.4221,
      "step": 1347
    },
    {
      "epoch": 2.045523520485584,
      "grad_norm": 1.0192068815231323,
      "learning_rate": 2.444992412746586e-06,
      "loss": 0.4164,
      "step": 1348
    },
    {
      "epoch": 2.047040971168437,
      "grad_norm": 0.7536095380783081,
      "learning_rate": 2.44309559939302e-06,
      "loss": 0.4092,
      "step": 1349
    },
    {
      "epoch": 2.04855842185129,
      "grad_norm": 0.740201473236084,
      "learning_rate": 2.441198786039454e-06,
      "loss": 0.3587,
      "step": 1350
    },
    {
      "epoch": 2.050075872534143,
      "grad_norm": 0.6190667152404785,
      "learning_rate": 2.439301972685888e-06,
      "loss": 0.4297,
      "step": 1351
    },
    {
      "epoch": 2.0515933232169954,
      "grad_norm": 0.708543598651886,
      "learning_rate": 2.437405159332322e-06,
      "loss": 0.3828,
      "step": 1352
    },
    {
      "epoch": 2.0531107738998484,
      "grad_norm": 0.6568596959114075,
      "learning_rate": 2.435508345978756e-06,
      "loss": 0.3277,
      "step": 1353
    },
    {
      "epoch": 2.054628224582701,
      "grad_norm": 0.7098693251609802,
      "learning_rate": 2.43361153262519e-06,
      "loss": 0.3943,
      "step": 1354
    },
    {
      "epoch": 2.056145675265554,
      "grad_norm": 0.6657254695892334,
      "learning_rate": 2.431714719271624e-06,
      "loss": 0.4262,
      "step": 1355
    },
    {
      "epoch": 2.0576631259484066,
      "grad_norm": 0.6139259338378906,
      "learning_rate": 2.429817905918058e-06,
      "loss": 0.4464,
      "step": 1356
    },
    {
      "epoch": 2.0591805766312596,
      "grad_norm": 0.706188976764679,
      "learning_rate": 2.427921092564492e-06,
      "loss": 0.3506,
      "step": 1357
    },
    {
      "epoch": 2.0606980273141122,
      "grad_norm": 0.6815840601921082,
      "learning_rate": 2.426024279210926e-06,
      "loss": 0.3678,
      "step": 1358
    },
    {
      "epoch": 2.0622154779969653,
      "grad_norm": 0.7010625004768372,
      "learning_rate": 2.4241274658573598e-06,
      "loss": 0.3956,
      "step": 1359
    },
    {
      "epoch": 2.063732928679818,
      "grad_norm": 0.7473686337471008,
      "learning_rate": 2.422230652503794e-06,
      "loss": 0.4428,
      "step": 1360
    },
    {
      "epoch": 2.065250379362671,
      "grad_norm": 0.6525387763977051,
      "learning_rate": 2.420333839150228e-06,
      "loss": 0.3474,
      "step": 1361
    },
    {
      "epoch": 2.0667678300455234,
      "grad_norm": 0.8056809306144714,
      "learning_rate": 2.418437025796662e-06,
      "loss": 0.4433,
      "step": 1362
    },
    {
      "epoch": 2.0682852807283765,
      "grad_norm": 0.7753369212150574,
      "learning_rate": 2.416540212443096e-06,
      "loss": 0.4362,
      "step": 1363
    },
    {
      "epoch": 2.069802731411229,
      "grad_norm": 0.8102440237998962,
      "learning_rate": 2.41464339908953e-06,
      "loss": 0.4938,
      "step": 1364
    },
    {
      "epoch": 2.071320182094082,
      "grad_norm": 0.663090169429779,
      "learning_rate": 2.4127465857359637e-06,
      "loss": 0.4191,
      "step": 1365
    },
    {
      "epoch": 2.0728376327769347,
      "grad_norm": 0.7184640169143677,
      "learning_rate": 2.4108497723823977e-06,
      "loss": 0.3651,
      "step": 1366
    },
    {
      "epoch": 2.0743550834597877,
      "grad_norm": 0.7098435759544373,
      "learning_rate": 2.408952959028832e-06,
      "loss": 0.3598,
      "step": 1367
    },
    {
      "epoch": 2.0758725341426403,
      "grad_norm": 0.7758377194404602,
      "learning_rate": 2.407056145675266e-06,
      "loss": 0.4515,
      "step": 1368
    },
    {
      "epoch": 2.0773899848254933,
      "grad_norm": 0.6154193878173828,
      "learning_rate": 2.4051593323217e-06,
      "loss": 0.4428,
      "step": 1369
    },
    {
      "epoch": 2.078907435508346,
      "grad_norm": 0.8213388919830322,
      "learning_rate": 2.403262518968134e-06,
      "loss": 0.4768,
      "step": 1370
    },
    {
      "epoch": 2.080424886191199,
      "grad_norm": 0.7056933045387268,
      "learning_rate": 2.4013657056145677e-06,
      "loss": 0.4816,
      "step": 1371
    },
    {
      "epoch": 2.0819423368740515,
      "grad_norm": 0.8266693949699402,
      "learning_rate": 2.3994688922610016e-06,
      "loss": 0.4846,
      "step": 1372
    },
    {
      "epoch": 2.0834597875569045,
      "grad_norm": 0.7175130844116211,
      "learning_rate": 2.397572078907436e-06,
      "loss": 0.4414,
      "step": 1373
    },
    {
      "epoch": 2.084977238239757,
      "grad_norm": 0.6942849159240723,
      "learning_rate": 2.39567526555387e-06,
      "loss": 0.4423,
      "step": 1374
    },
    {
      "epoch": 2.08649468892261,
      "grad_norm": 0.7888792753219604,
      "learning_rate": 2.393778452200304e-06,
      "loss": 0.4824,
      "step": 1375
    },
    {
      "epoch": 2.0880121396054627,
      "grad_norm": 0.6790125370025635,
      "learning_rate": 2.3918816388467378e-06,
      "loss": 0.4361,
      "step": 1376
    },
    {
      "epoch": 2.0895295902883158,
      "grad_norm": 0.7012373805046082,
      "learning_rate": 2.3899848254931717e-06,
      "loss": 0.415,
      "step": 1377
    },
    {
      "epoch": 2.0910470409711683,
      "grad_norm": 0.8415109515190125,
      "learning_rate": 2.3880880121396056e-06,
      "loss": 0.4698,
      "step": 1378
    },
    {
      "epoch": 2.0925644916540214,
      "grad_norm": 0.793003261089325,
      "learning_rate": 2.38619119878604e-06,
      "loss": 0.4141,
      "step": 1379
    },
    {
      "epoch": 2.094081942336874,
      "grad_norm": 0.691160261631012,
      "learning_rate": 2.384294385432474e-06,
      "loss": 0.4537,
      "step": 1380
    },
    {
      "epoch": 2.095599393019727,
      "grad_norm": 0.7835822105407715,
      "learning_rate": 2.382397572078908e-06,
      "loss": 0.3451,
      "step": 1381
    },
    {
      "epoch": 2.0971168437025796,
      "grad_norm": 0.7598285675048828,
      "learning_rate": 2.3805007587253417e-06,
      "loss": 0.4713,
      "step": 1382
    },
    {
      "epoch": 2.0986342943854326,
      "grad_norm": 0.7061788439750671,
      "learning_rate": 2.3786039453717757e-06,
      "loss": 0.4184,
      "step": 1383
    },
    {
      "epoch": 2.100151745068285,
      "grad_norm": 0.6655016541481018,
      "learning_rate": 2.3767071320182096e-06,
      "loss": 0.3706,
      "step": 1384
    },
    {
      "epoch": 2.101669195751138,
      "grad_norm": 0.7309941053390503,
      "learning_rate": 2.3748103186646435e-06,
      "loss": 0.4559,
      "step": 1385
    },
    {
      "epoch": 2.103186646433991,
      "grad_norm": 0.7263609170913696,
      "learning_rate": 2.3729135053110774e-06,
      "loss": 0.3973,
      "step": 1386
    },
    {
      "epoch": 2.104704097116844,
      "grad_norm": 0.7345076203346252,
      "learning_rate": 2.3710166919575114e-06,
      "loss": 0.4226,
      "step": 1387
    },
    {
      "epoch": 2.1062215477996964,
      "grad_norm": 0.6783868670463562,
      "learning_rate": 2.3691198786039453e-06,
      "loss": 0.4341,
      "step": 1388
    },
    {
      "epoch": 2.1077389984825494,
      "grad_norm": 0.8477240204811096,
      "learning_rate": 2.3672230652503792e-06,
      "loss": 0.3917,
      "step": 1389
    },
    {
      "epoch": 2.109256449165402,
      "grad_norm": 0.7486142516136169,
      "learning_rate": 2.3653262518968136e-06,
      "loss": 0.4018,
      "step": 1390
    },
    {
      "epoch": 2.110773899848255,
      "grad_norm": 0.7030133605003357,
      "learning_rate": 2.3634294385432475e-06,
      "loss": 0.4296,
      "step": 1391
    },
    {
      "epoch": 2.1122913505311076,
      "grad_norm": 0.7158125042915344,
      "learning_rate": 2.3615326251896814e-06,
      "loss": 0.5016,
      "step": 1392
    },
    {
      "epoch": 2.1138088012139606,
      "grad_norm": 0.6827885508537292,
      "learning_rate": 2.3596358118361154e-06,
      "loss": 0.4187,
      "step": 1393
    },
    {
      "epoch": 2.1153262518968132,
      "grad_norm": 0.7484778761863708,
      "learning_rate": 2.3577389984825493e-06,
      "loss": 0.4071,
      "step": 1394
    },
    {
      "epoch": 2.1168437025796663,
      "grad_norm": 0.678970992565155,
      "learning_rate": 2.355842185128983e-06,
      "loss": 0.4488,
      "step": 1395
    },
    {
      "epoch": 2.118361153262519,
      "grad_norm": 0.8760374784469604,
      "learning_rate": 2.3539453717754176e-06,
      "loss": 0.4859,
      "step": 1396
    },
    {
      "epoch": 2.119878603945372,
      "grad_norm": 0.7137235403060913,
      "learning_rate": 2.3520485584218515e-06,
      "loss": 0.3512,
      "step": 1397
    },
    {
      "epoch": 2.1213960546282244,
      "grad_norm": 0.7088404297828674,
      "learning_rate": 2.3501517450682854e-06,
      "loss": 0.4082,
      "step": 1398
    },
    {
      "epoch": 2.1229135053110775,
      "grad_norm": 0.7204476594924927,
      "learning_rate": 2.3482549317147193e-06,
      "loss": 0.3881,
      "step": 1399
    },
    {
      "epoch": 2.12443095599393,
      "grad_norm": 0.8407230973243713,
      "learning_rate": 2.3463581183611533e-06,
      "loss": 0.3592,
      "step": 1400
    },
    {
      "epoch": 2.125948406676783,
      "grad_norm": 0.629682719707489,
      "learning_rate": 2.344461305007587e-06,
      "loss": 0.381,
      "step": 1401
    },
    {
      "epoch": 2.1274658573596357,
      "grad_norm": 0.6359724998474121,
      "learning_rate": 2.3425644916540215e-06,
      "loss": 0.3924,
      "step": 1402
    },
    {
      "epoch": 2.1289833080424887,
      "grad_norm": 0.6666708588600159,
      "learning_rate": 2.3406676783004555e-06,
      "loss": 0.4067,
      "step": 1403
    },
    {
      "epoch": 2.1305007587253413,
      "grad_norm": 0.7437430024147034,
      "learning_rate": 2.3387708649468894e-06,
      "loss": 0.3266,
      "step": 1404
    },
    {
      "epoch": 2.1320182094081943,
      "grad_norm": 0.8418055176734924,
      "learning_rate": 2.3368740515933233e-06,
      "loss": 0.34,
      "step": 1405
    },
    {
      "epoch": 2.133535660091047,
      "grad_norm": 0.6250756978988647,
      "learning_rate": 2.3349772382397572e-06,
      "loss": 0.3732,
      "step": 1406
    },
    {
      "epoch": 2.1350531107739,
      "grad_norm": 0.7844383120536804,
      "learning_rate": 2.333080424886191e-06,
      "loss": 0.4026,
      "step": 1407
    },
    {
      "epoch": 2.1365705614567525,
      "grad_norm": 0.7241693139076233,
      "learning_rate": 2.331183611532625e-06,
      "loss": 0.3831,
      "step": 1408
    },
    {
      "epoch": 2.1380880121396055,
      "grad_norm": 0.7171072959899902,
      "learning_rate": 2.3292867981790594e-06,
      "loss": 0.3563,
      "step": 1409
    },
    {
      "epoch": 2.139605462822458,
      "grad_norm": 0.6481499671936035,
      "learning_rate": 2.3273899848254934e-06,
      "loss": 0.3847,
      "step": 1410
    },
    {
      "epoch": 2.141122913505311,
      "grad_norm": 0.7682173848152161,
      "learning_rate": 2.3254931714719273e-06,
      "loss": 0.402,
      "step": 1411
    },
    {
      "epoch": 2.1426403641881637,
      "grad_norm": 1.0210436582565308,
      "learning_rate": 2.323596358118361e-06,
      "loss": 0.4333,
      "step": 1412
    },
    {
      "epoch": 2.1441578148710168,
      "grad_norm": 1.006227731704712,
      "learning_rate": 2.321699544764795e-06,
      "loss": 0.32,
      "step": 1413
    },
    {
      "epoch": 2.1456752655538693,
      "grad_norm": 0.717557430267334,
      "learning_rate": 2.319802731411229e-06,
      "loss": 0.4152,
      "step": 1414
    },
    {
      "epoch": 2.1471927162367224,
      "grad_norm": 0.6416487693786621,
      "learning_rate": 2.3179059180576634e-06,
      "loss": 0.4109,
      "step": 1415
    },
    {
      "epoch": 2.148710166919575,
      "grad_norm": 0.7001078128814697,
      "learning_rate": 2.3160091047040973e-06,
      "loss": 0.4591,
      "step": 1416
    },
    {
      "epoch": 2.150227617602428,
      "grad_norm": 0.6478511691093445,
      "learning_rate": 2.3141122913505313e-06,
      "loss": 0.3954,
      "step": 1417
    },
    {
      "epoch": 2.1517450682852806,
      "grad_norm": 0.6325688362121582,
      "learning_rate": 2.312215477996965e-06,
      "loss": 0.3995,
      "step": 1418
    },
    {
      "epoch": 2.1532625189681336,
      "grad_norm": 0.6399153470993042,
      "learning_rate": 2.310318664643399e-06,
      "loss": 0.4224,
      "step": 1419
    },
    {
      "epoch": 2.154779969650986,
      "grad_norm": 0.7463342547416687,
      "learning_rate": 2.308421851289833e-06,
      "loss": 0.3931,
      "step": 1420
    },
    {
      "epoch": 2.156297420333839,
      "grad_norm": 0.6521073579788208,
      "learning_rate": 2.3065250379362674e-06,
      "loss": 0.376,
      "step": 1421
    },
    {
      "epoch": 2.157814871016692,
      "grad_norm": 0.6852390170097351,
      "learning_rate": 2.3046282245827013e-06,
      "loss": 0.4221,
      "step": 1422
    },
    {
      "epoch": 2.159332321699545,
      "grad_norm": 0.6590631604194641,
      "learning_rate": 2.3027314112291352e-06,
      "loss": 0.3891,
      "step": 1423
    },
    {
      "epoch": 2.1608497723823974,
      "grad_norm": 0.6492985486984253,
      "learning_rate": 2.300834597875569e-06,
      "loss": 0.3752,
      "step": 1424
    },
    {
      "epoch": 2.1623672230652504,
      "grad_norm": 0.830288290977478,
      "learning_rate": 2.298937784522003e-06,
      "loss": 0.4487,
      "step": 1425
    },
    {
      "epoch": 2.163884673748103,
      "grad_norm": 0.8994640707969666,
      "learning_rate": 2.297040971168437e-06,
      "loss": 0.414,
      "step": 1426
    },
    {
      "epoch": 2.165402124430956,
      "grad_norm": 0.8591932654380798,
      "learning_rate": 2.2951441578148714e-06,
      "loss": 0.3302,
      "step": 1427
    },
    {
      "epoch": 2.1669195751138086,
      "grad_norm": 0.6953637003898621,
      "learning_rate": 2.2932473444613053e-06,
      "loss": 0.4098,
      "step": 1428
    },
    {
      "epoch": 2.1684370257966616,
      "grad_norm": 0.6786372661590576,
      "learning_rate": 2.291350531107739e-06,
      "loss": 0.3726,
      "step": 1429
    },
    {
      "epoch": 2.1699544764795142,
      "grad_norm": 0.6929110288619995,
      "learning_rate": 2.289453717754173e-06,
      "loss": 0.4115,
      "step": 1430
    },
    {
      "epoch": 2.1714719271623673,
      "grad_norm": 0.7120795249938965,
      "learning_rate": 2.287556904400607e-06,
      "loss": 0.3838,
      "step": 1431
    },
    {
      "epoch": 2.17298937784522,
      "grad_norm": 0.8368760943412781,
      "learning_rate": 2.285660091047041e-06,
      "loss": 0.4473,
      "step": 1432
    },
    {
      "epoch": 2.174506828528073,
      "grad_norm": 0.732052743434906,
      "learning_rate": 2.283763277693475e-06,
      "loss": 0.3909,
      "step": 1433
    },
    {
      "epoch": 2.1760242792109254,
      "grad_norm": 0.7150756120681763,
      "learning_rate": 2.2818664643399093e-06,
      "loss": 0.3784,
      "step": 1434
    },
    {
      "epoch": 2.1775417298937785,
      "grad_norm": 0.6019081473350525,
      "learning_rate": 2.279969650986343e-06,
      "loss": 0.4079,
      "step": 1435
    },
    {
      "epoch": 2.179059180576631,
      "grad_norm": 0.6102845072746277,
      "learning_rate": 2.278072837632777e-06,
      "loss": 0.4034,
      "step": 1436
    },
    {
      "epoch": 2.180576631259484,
      "grad_norm": 0.6363476514816284,
      "learning_rate": 2.276176024279211e-06,
      "loss": 0.3583,
      "step": 1437
    },
    {
      "epoch": 2.1820940819423367,
      "grad_norm": 0.6560015678405762,
      "learning_rate": 2.274279210925645e-06,
      "loss": 0.3647,
      "step": 1438
    },
    {
      "epoch": 2.1836115326251897,
      "grad_norm": 0.6991040706634521,
      "learning_rate": 2.272382397572079e-06,
      "loss": 0.3285,
      "step": 1439
    },
    {
      "epoch": 2.1851289833080423,
      "grad_norm": 0.7211271524429321,
      "learning_rate": 2.2704855842185132e-06,
      "loss": 0.3426,
      "step": 1440
    },
    {
      "epoch": 2.1866464339908953,
      "grad_norm": 0.6530520915985107,
      "learning_rate": 2.268588770864947e-06,
      "loss": 0.4299,
      "step": 1441
    },
    {
      "epoch": 2.188163884673748,
      "grad_norm": 0.7247563600540161,
      "learning_rate": 2.266691957511381e-06,
      "loss": 0.3951,
      "step": 1442
    },
    {
      "epoch": 2.189681335356601,
      "grad_norm": 0.6684522032737732,
      "learning_rate": 2.264795144157815e-06,
      "loss": 0.3477,
      "step": 1443
    },
    {
      "epoch": 2.191198786039454,
      "grad_norm": 0.7252140045166016,
      "learning_rate": 2.262898330804249e-06,
      "loss": 0.3809,
      "step": 1444
    },
    {
      "epoch": 2.1927162367223065,
      "grad_norm": 0.7199464440345764,
      "learning_rate": 2.261001517450683e-06,
      "loss": 0.3597,
      "step": 1445
    },
    {
      "epoch": 2.194233687405159,
      "grad_norm": 0.6861783266067505,
      "learning_rate": 2.2591047040971172e-06,
      "loss": 0.4127,
      "step": 1446
    },
    {
      "epoch": 2.195751138088012,
      "grad_norm": 0.7135829925537109,
      "learning_rate": 2.257207890743551e-06,
      "loss": 0.327,
      "step": 1447
    },
    {
      "epoch": 2.197268588770865,
      "grad_norm": 0.6549074649810791,
      "learning_rate": 2.255311077389985e-06,
      "loss": 0.4108,
      "step": 1448
    },
    {
      "epoch": 2.1987860394537178,
      "grad_norm": 0.7120893597602844,
      "learning_rate": 2.253414264036419e-06,
      "loss": 0.3564,
      "step": 1449
    },
    {
      "epoch": 2.2003034901365703,
      "grad_norm": 0.7396847605705261,
      "learning_rate": 2.251517450682853e-06,
      "loss": 0.4113,
      "step": 1450
    },
    {
      "epoch": 2.2018209408194234,
      "grad_norm": 0.7141941785812378,
      "learning_rate": 2.249620637329287e-06,
      "loss": 0.3875,
      "step": 1451
    },
    {
      "epoch": 2.2033383915022764,
      "grad_norm": 0.7339722514152527,
      "learning_rate": 2.2477238239757208e-06,
      "loss": 0.3715,
      "step": 1452
    },
    {
      "epoch": 2.204855842185129,
      "grad_norm": 0.6525372266769409,
      "learning_rate": 2.245827010622155e-06,
      "loss": 0.3721,
      "step": 1453
    },
    {
      "epoch": 2.2063732928679816,
      "grad_norm": 0.754374086856842,
      "learning_rate": 2.243930197268589e-06,
      "loss": 0.3347,
      "step": 1454
    },
    {
      "epoch": 2.2078907435508346,
      "grad_norm": 0.66275954246521,
      "learning_rate": 2.242033383915023e-06,
      "loss": 0.2943,
      "step": 1455
    },
    {
      "epoch": 2.2094081942336876,
      "grad_norm": 0.6865372657775879,
      "learning_rate": 2.240136570561457e-06,
      "loss": 0.464,
      "step": 1456
    },
    {
      "epoch": 2.21092564491654,
      "grad_norm": 0.8266329169273376,
      "learning_rate": 2.238239757207891e-06,
      "loss": 0.3486,
      "step": 1457
    },
    {
      "epoch": 2.212443095599393,
      "grad_norm": 0.7081322073936462,
      "learning_rate": 2.2363429438543247e-06,
      "loss": 0.4045,
      "step": 1458
    },
    {
      "epoch": 2.213960546282246,
      "grad_norm": 0.7839081287384033,
      "learning_rate": 2.234446130500759e-06,
      "loss": 0.4305,
      "step": 1459
    },
    {
      "epoch": 2.215477996965099,
      "grad_norm": 0.5979506373405457,
      "learning_rate": 2.232549317147193e-06,
      "loss": 0.4142,
      "step": 1460
    },
    {
      "epoch": 2.2169954476479514,
      "grad_norm": 0.6477890014648438,
      "learning_rate": 2.230652503793627e-06,
      "loss": 0.3931,
      "step": 1461
    },
    {
      "epoch": 2.2185128983308045,
      "grad_norm": 0.6616842746734619,
      "learning_rate": 2.228755690440061e-06,
      "loss": 0.3783,
      "step": 1462
    },
    {
      "epoch": 2.220030349013657,
      "grad_norm": 0.6498293280601501,
      "learning_rate": 2.226858877086495e-06,
      "loss": 0.4484,
      "step": 1463
    },
    {
      "epoch": 2.22154779969651,
      "grad_norm": 0.6380702257156372,
      "learning_rate": 2.2249620637329287e-06,
      "loss": 0.2617,
      "step": 1464
    },
    {
      "epoch": 2.2230652503793626,
      "grad_norm": 0.8045085072517395,
      "learning_rate": 2.223065250379363e-06,
      "loss": 0.4401,
      "step": 1465
    },
    {
      "epoch": 2.2245827010622157,
      "grad_norm": 0.8154208660125732,
      "learning_rate": 2.221168437025797e-06,
      "loss": 0.418,
      "step": 1466
    },
    {
      "epoch": 2.2261001517450683,
      "grad_norm": 0.7651209235191345,
      "learning_rate": 2.219271623672231e-06,
      "loss": 0.4105,
      "step": 1467
    },
    {
      "epoch": 2.2276176024279213,
      "grad_norm": 0.7541279792785645,
      "learning_rate": 2.217374810318665e-06,
      "loss": 0.393,
      "step": 1468
    },
    {
      "epoch": 2.229135053110774,
      "grad_norm": 0.7377870082855225,
      "learning_rate": 2.2154779969650988e-06,
      "loss": 0.3883,
      "step": 1469
    },
    {
      "epoch": 2.230652503793627,
      "grad_norm": 0.8093578815460205,
      "learning_rate": 2.2135811836115327e-06,
      "loss": 0.3959,
      "step": 1470
    },
    {
      "epoch": 2.2321699544764795,
      "grad_norm": 0.6909343004226685,
      "learning_rate": 2.211684370257967e-06,
      "loss": 0.3626,
      "step": 1471
    },
    {
      "epoch": 2.2336874051593325,
      "grad_norm": 0.6305730938911438,
      "learning_rate": 2.209787556904401e-06,
      "loss": 0.3866,
      "step": 1472
    },
    {
      "epoch": 2.235204855842185,
      "grad_norm": 0.7345577478408813,
      "learning_rate": 2.207890743550835e-06,
      "loss": 0.3928,
      "step": 1473
    },
    {
      "epoch": 2.236722306525038,
      "grad_norm": 0.6632226705551147,
      "learning_rate": 2.205993930197269e-06,
      "loss": 0.3704,
      "step": 1474
    },
    {
      "epoch": 2.2382397572078907,
      "grad_norm": 0.9039681553840637,
      "learning_rate": 2.2040971168437027e-06,
      "loss": 0.4105,
      "step": 1475
    },
    {
      "epoch": 2.2397572078907437,
      "grad_norm": 0.6354726552963257,
      "learning_rate": 2.2022003034901367e-06,
      "loss": 0.3339,
      "step": 1476
    },
    {
      "epoch": 2.2412746585735963,
      "grad_norm": 0.6464208960533142,
      "learning_rate": 2.2003034901365706e-06,
      "loss": 0.3269,
      "step": 1477
    },
    {
      "epoch": 2.2427921092564493,
      "grad_norm": 0.6902570724487305,
      "learning_rate": 2.198406676783005e-06,
      "loss": 0.4341,
      "step": 1478
    },
    {
      "epoch": 2.244309559939302,
      "grad_norm": 0.6273630261421204,
      "learning_rate": 2.196509863429439e-06,
      "loss": 0.3758,
      "step": 1479
    },
    {
      "epoch": 2.245827010622155,
      "grad_norm": 0.6530746221542358,
      "learning_rate": 2.194613050075873e-06,
      "loss": 0.4546,
      "step": 1480
    },
    {
      "epoch": 2.2473444613050075,
      "grad_norm": 0.9210697412490845,
      "learning_rate": 2.1927162367223067e-06,
      "loss": 0.3579,
      "step": 1481
    },
    {
      "epoch": 2.2488619119878606,
      "grad_norm": 0.644134521484375,
      "learning_rate": 2.1908194233687407e-06,
      "loss": 0.4312,
      "step": 1482
    },
    {
      "epoch": 2.250379362670713,
      "grad_norm": 1.5996127128601074,
      "learning_rate": 2.1889226100151746e-06,
      "loss": 0.3429,
      "step": 1483
    },
    {
      "epoch": 2.251896813353566,
      "grad_norm": 0.7162794470787048,
      "learning_rate": 2.187025796661609e-06,
      "loss": 0.3875,
      "step": 1484
    },
    {
      "epoch": 2.2534142640364188,
      "grad_norm": 0.6471013426780701,
      "learning_rate": 2.185128983308043e-06,
      "loss": 0.3997,
      "step": 1485
    },
    {
      "epoch": 2.254931714719272,
      "grad_norm": 0.7548422813415527,
      "learning_rate": 2.1832321699544768e-06,
      "loss": 0.3936,
      "step": 1486
    },
    {
      "epoch": 2.2564491654021244,
      "grad_norm": 0.737548291683197,
      "learning_rate": 2.1813353566009107e-06,
      "loss": 0.351,
      "step": 1487
    },
    {
      "epoch": 2.2579666160849774,
      "grad_norm": 0.6789474487304688,
      "learning_rate": 2.1794385432473446e-06,
      "loss": 0.384,
      "step": 1488
    },
    {
      "epoch": 2.25948406676783,
      "grad_norm": 0.7063108682632446,
      "learning_rate": 2.1775417298937786e-06,
      "loss": 0.4065,
      "step": 1489
    },
    {
      "epoch": 2.261001517450683,
      "grad_norm": 0.6617575883865356,
      "learning_rate": 2.175644916540213e-06,
      "loss": 0.3621,
      "step": 1490
    },
    {
      "epoch": 2.2625189681335356,
      "grad_norm": 0.7100099921226501,
      "learning_rate": 2.173748103186647e-06,
      "loss": 0.4019,
      "step": 1491
    },
    {
      "epoch": 2.2640364188163886,
      "grad_norm": 0.7851284146308899,
      "learning_rate": 2.1718512898330808e-06,
      "loss": 0.4018,
      "step": 1492
    },
    {
      "epoch": 2.265553869499241,
      "grad_norm": 0.6673569083213806,
      "learning_rate": 2.1699544764795147e-06,
      "loss": 0.4532,
      "step": 1493
    },
    {
      "epoch": 2.2670713201820942,
      "grad_norm": 0.6229077577590942,
      "learning_rate": 2.1680576631259486e-06,
      "loss": 0.3693,
      "step": 1494
    },
    {
      "epoch": 2.268588770864947,
      "grad_norm": 0.7087063789367676,
      "learning_rate": 2.1661608497723825e-06,
      "loss": 0.4583,
      "step": 1495
    },
    {
      "epoch": 2.2701062215478,
      "grad_norm": 0.6562153697013855,
      "learning_rate": 2.1642640364188165e-06,
      "loss": 0.3615,
      "step": 1496
    },
    {
      "epoch": 2.2716236722306524,
      "grad_norm": 0.707072377204895,
      "learning_rate": 2.162367223065251e-06,
      "loss": 0.4513,
      "step": 1497
    },
    {
      "epoch": 2.2731411229135055,
      "grad_norm": 0.7132171988487244,
      "learning_rate": 2.1604704097116847e-06,
      "loss": 0.3917,
      "step": 1498
    },
    {
      "epoch": 2.274658573596358,
      "grad_norm": 0.7517818212509155,
      "learning_rate": 2.1585735963581187e-06,
      "loss": 0.4725,
      "step": 1499
    },
    {
      "epoch": 2.276176024279211,
      "grad_norm": 0.6900469660758972,
      "learning_rate": 2.1566767830045526e-06,
      "loss": 0.3956,
      "step": 1500
    },
    {
      "epoch": 2.2776934749620636,
      "grad_norm": 0.7434033751487732,
      "learning_rate": 2.1547799696509865e-06,
      "loss": 0.4084,
      "step": 1501
    },
    {
      "epoch": 2.2792109256449167,
      "grad_norm": 0.7999261617660522,
      "learning_rate": 2.1528831562974204e-06,
      "loss": 0.3963,
      "step": 1502
    },
    {
      "epoch": 2.2807283763277693,
      "grad_norm": 0.6476234793663025,
      "learning_rate": 2.1509863429438548e-06,
      "loss": 0.3799,
      "step": 1503
    },
    {
      "epoch": 2.2822458270106223,
      "grad_norm": 0.6424363255500793,
      "learning_rate": 2.1490895295902887e-06,
      "loss": 0.372,
      "step": 1504
    },
    {
      "epoch": 2.283763277693475,
      "grad_norm": 0.6876583099365234,
      "learning_rate": 2.1471927162367226e-06,
      "loss": 0.4285,
      "step": 1505
    },
    {
      "epoch": 2.285280728376328,
      "grad_norm": 0.7512415647506714,
      "learning_rate": 2.1452959028831566e-06,
      "loss": 0.3775,
      "step": 1506
    },
    {
      "epoch": 2.2867981790591805,
      "grad_norm": 0.6716144680976868,
      "learning_rate": 2.1433990895295905e-06,
      "loss": 0.3281,
      "step": 1507
    },
    {
      "epoch": 2.2883156297420335,
      "grad_norm": 0.6683937907218933,
      "learning_rate": 2.1415022761760244e-06,
      "loss": 0.3677,
      "step": 1508
    },
    {
      "epoch": 2.289833080424886,
      "grad_norm": 0.69082111120224,
      "learning_rate": 2.1396054628224588e-06,
      "loss": 0.3675,
      "step": 1509
    },
    {
      "epoch": 2.291350531107739,
      "grad_norm": 0.678083062171936,
      "learning_rate": 2.1377086494688927e-06,
      "loss": 0.3388,
      "step": 1510
    },
    {
      "epoch": 2.2928679817905917,
      "grad_norm": 0.7109081745147705,
      "learning_rate": 2.1358118361153266e-06,
      "loss": 0.2973,
      "step": 1511
    },
    {
      "epoch": 2.2943854324734447,
      "grad_norm": 0.7542926669120789,
      "learning_rate": 2.1339150227617605e-06,
      "loss": 0.3733,
      "step": 1512
    },
    {
      "epoch": 2.2959028831562973,
      "grad_norm": 0.6914696097373962,
      "learning_rate": 2.1320182094081945e-06,
      "loss": 0.4443,
      "step": 1513
    },
    {
      "epoch": 2.2974203338391503,
      "grad_norm": 0.8196873664855957,
      "learning_rate": 2.1301213960546284e-06,
      "loss": 0.3376,
      "step": 1514
    },
    {
      "epoch": 2.298937784522003,
      "grad_norm": 0.8607761263847351,
      "learning_rate": 2.1282245827010627e-06,
      "loss": 0.3207,
      "step": 1515
    },
    {
      "epoch": 2.300455235204856,
      "grad_norm": 0.6824743151664734,
      "learning_rate": 2.1263277693474967e-06,
      "loss": 0.3957,
      "step": 1516
    },
    {
      "epoch": 2.3019726858877085,
      "grad_norm": 0.8046515583992004,
      "learning_rate": 2.12443095599393e-06,
      "loss": 0.4233,
      "step": 1517
    },
    {
      "epoch": 2.3034901365705616,
      "grad_norm": 0.6243926286697388,
      "learning_rate": 2.122534142640364e-06,
      "loss": 0.4045,
      "step": 1518
    },
    {
      "epoch": 2.305007587253414,
      "grad_norm": 0.6802630424499512,
      "learning_rate": 2.120637329286798e-06,
      "loss": 0.3753,
      "step": 1519
    },
    {
      "epoch": 2.306525037936267,
      "grad_norm": 0.6532068252563477,
      "learning_rate": 2.1187405159332324e-06,
      "loss": 0.4344,
      "step": 1520
    },
    {
      "epoch": 2.3080424886191198,
      "grad_norm": 0.7960065603256226,
      "learning_rate": 2.1168437025796663e-06,
      "loss": 0.4529,
      "step": 1521
    },
    {
      "epoch": 2.309559939301973,
      "grad_norm": 0.6616641283035278,
      "learning_rate": 2.1149468892261002e-06,
      "loss": 0.3493,
      "step": 1522
    },
    {
      "epoch": 2.3110773899848254,
      "grad_norm": 0.7310425639152527,
      "learning_rate": 2.113050075872534e-06,
      "loss": 0.3422,
      "step": 1523
    },
    {
      "epoch": 2.3125948406676784,
      "grad_norm": 0.7776210308074951,
      "learning_rate": 2.111153262518968e-06,
      "loss": 0.3137,
      "step": 1524
    },
    {
      "epoch": 2.314112291350531,
      "grad_norm": 0.708541214466095,
      "learning_rate": 2.109256449165402e-06,
      "loss": 0.3499,
      "step": 1525
    },
    {
      "epoch": 2.315629742033384,
      "grad_norm": 0.6559802293777466,
      "learning_rate": 2.1073596358118363e-06,
      "loss": 0.3541,
      "step": 1526
    },
    {
      "epoch": 2.3171471927162366,
      "grad_norm": 0.6503825187683105,
      "learning_rate": 2.1054628224582703e-06,
      "loss": 0.3914,
      "step": 1527
    },
    {
      "epoch": 2.3186646433990896,
      "grad_norm": 0.7042705416679382,
      "learning_rate": 2.103566009104704e-06,
      "loss": 0.437,
      "step": 1528
    },
    {
      "epoch": 2.320182094081942,
      "grad_norm": 0.6314390301704407,
      "learning_rate": 2.101669195751138e-06,
      "loss": 0.3885,
      "step": 1529
    },
    {
      "epoch": 2.3216995447647952,
      "grad_norm": 0.5830575227737427,
      "learning_rate": 2.099772382397572e-06,
      "loss": 0.4368,
      "step": 1530
    },
    {
      "epoch": 2.323216995447648,
      "grad_norm": 0.6583192348480225,
      "learning_rate": 2.097875569044006e-06,
      "loss": 0.3794,
      "step": 1531
    },
    {
      "epoch": 2.324734446130501,
      "grad_norm": 0.8404437303543091,
      "learning_rate": 2.0959787556904403e-06,
      "loss": 0.4257,
      "step": 1532
    },
    {
      "epoch": 2.3262518968133534,
      "grad_norm": 0.7736722230911255,
      "learning_rate": 2.0940819423368742e-06,
      "loss": 0.3991,
      "step": 1533
    },
    {
      "epoch": 2.3277693474962065,
      "grad_norm": 0.7747715711593628,
      "learning_rate": 2.092185128983308e-06,
      "loss": 0.4862,
      "step": 1534
    },
    {
      "epoch": 2.329286798179059,
      "grad_norm": 0.6958097815513611,
      "learning_rate": 2.090288315629742e-06,
      "loss": 0.3611,
      "step": 1535
    },
    {
      "epoch": 2.330804248861912,
      "grad_norm": 0.6955814957618713,
      "learning_rate": 2.088391502276176e-06,
      "loss": 0.3887,
      "step": 1536
    },
    {
      "epoch": 2.3323216995447646,
      "grad_norm": 0.5810850858688354,
      "learning_rate": 2.08649468892261e-06,
      "loss": 0.3708,
      "step": 1537
    },
    {
      "epoch": 2.3338391502276177,
      "grad_norm": 0.7051094174385071,
      "learning_rate": 2.0845978755690443e-06,
      "loss": 0.4036,
      "step": 1538
    },
    {
      "epoch": 2.3353566009104703,
      "grad_norm": 0.6338982582092285,
      "learning_rate": 2.0827010622154782e-06,
      "loss": 0.3947,
      "step": 1539
    },
    {
      "epoch": 2.3368740515933233,
      "grad_norm": 0.6545442342758179,
      "learning_rate": 2.080804248861912e-06,
      "loss": 0.4536,
      "step": 1540
    },
    {
      "epoch": 2.338391502276176,
      "grad_norm": 0.6579464077949524,
      "learning_rate": 2.078907435508346e-06,
      "loss": 0.368,
      "step": 1541
    },
    {
      "epoch": 2.339908952959029,
      "grad_norm": 0.6153895854949951,
      "learning_rate": 2.07701062215478e-06,
      "loss": 0.4266,
      "step": 1542
    },
    {
      "epoch": 2.3414264036418815,
      "grad_norm": 0.6058775782585144,
      "learning_rate": 2.075113808801214e-06,
      "loss": 0.4178,
      "step": 1543
    },
    {
      "epoch": 2.3429438543247345,
      "grad_norm": 0.709796130657196,
      "learning_rate": 2.073216995447648e-06,
      "loss": 0.3456,
      "step": 1544
    },
    {
      "epoch": 2.344461305007587,
      "grad_norm": 0.6294386386871338,
      "learning_rate": 2.071320182094082e-06,
      "loss": 0.2997,
      "step": 1545
    },
    {
      "epoch": 2.34597875569044,
      "grad_norm": 0.7245036959648132,
      "learning_rate": 2.069423368740516e-06,
      "loss": 0.316,
      "step": 1546
    },
    {
      "epoch": 2.3474962063732927,
      "grad_norm": 0.6629536151885986,
      "learning_rate": 2.06752655538695e-06,
      "loss": 0.4473,
      "step": 1547
    },
    {
      "epoch": 2.3490136570561457,
      "grad_norm": 0.712742030620575,
      "learning_rate": 2.065629742033384e-06,
      "loss": 0.393,
      "step": 1548
    },
    {
      "epoch": 2.3505311077389983,
      "grad_norm": 0.7057610750198364,
      "learning_rate": 2.063732928679818e-06,
      "loss": 0.3577,
      "step": 1549
    },
    {
      "epoch": 2.3520485584218513,
      "grad_norm": 0.7632089853286743,
      "learning_rate": 2.061836115326252e-06,
      "loss": 0.3489,
      "step": 1550
    },
    {
      "epoch": 2.353566009104704,
      "grad_norm": 0.6426003575325012,
      "learning_rate": 2.059939301972686e-06,
      "loss": 0.3396,
      "step": 1551
    },
    {
      "epoch": 2.355083459787557,
      "grad_norm": 0.7282283306121826,
      "learning_rate": 2.05804248861912e-06,
      "loss": 0.363,
      "step": 1552
    },
    {
      "epoch": 2.3566009104704095,
      "grad_norm": 0.6685829758644104,
      "learning_rate": 2.056145675265554e-06,
      "loss": 0.4261,
      "step": 1553
    },
    {
      "epoch": 2.3581183611532626,
      "grad_norm": 0.840048611164093,
      "learning_rate": 2.054248861911988e-06,
      "loss": 0.3901,
      "step": 1554
    },
    {
      "epoch": 2.359635811836115,
      "grad_norm": 0.8258536458015442,
      "learning_rate": 2.052352048558422e-06,
      "loss": 0.3943,
      "step": 1555
    },
    {
      "epoch": 2.361153262518968,
      "grad_norm": 0.7020835876464844,
      "learning_rate": 2.050455235204856e-06,
      "loss": 0.3713,
      "step": 1556
    },
    {
      "epoch": 2.3626707132018208,
      "grad_norm": 0.7464252710342407,
      "learning_rate": 2.04855842185129e-06,
      "loss": 0.4948,
      "step": 1557
    },
    {
      "epoch": 2.364188163884674,
      "grad_norm": 0.6462410092353821,
      "learning_rate": 2.046661608497724e-06,
      "loss": 0.3464,
      "step": 1558
    },
    {
      "epoch": 2.3657056145675264,
      "grad_norm": 0.652110755443573,
      "learning_rate": 2.044764795144158e-06,
      "loss": 0.296,
      "step": 1559
    },
    {
      "epoch": 2.3672230652503794,
      "grad_norm": 0.7185195684432983,
      "learning_rate": 2.042867981790592e-06,
      "loss": 0.2633,
      "step": 1560
    },
    {
      "epoch": 2.368740515933232,
      "grad_norm": 0.6569268703460693,
      "learning_rate": 2.040971168437026e-06,
      "loss": 0.3542,
      "step": 1561
    },
    {
      "epoch": 2.370257966616085,
      "grad_norm": 0.7644569873809814,
      "learning_rate": 2.0390743550834598e-06,
      "loss": 0.3733,
      "step": 1562
    },
    {
      "epoch": 2.3717754172989376,
      "grad_norm": 0.8580999970436096,
      "learning_rate": 2.0371775417298937e-06,
      "loss": 0.3684,
      "step": 1563
    },
    {
      "epoch": 2.3732928679817906,
      "grad_norm": 0.7139670252799988,
      "learning_rate": 2.035280728376328e-06,
      "loss": 0.3905,
      "step": 1564
    },
    {
      "epoch": 2.374810318664643,
      "grad_norm": 0.6903208494186401,
      "learning_rate": 2.033383915022762e-06,
      "loss": 0.3747,
      "step": 1565
    },
    {
      "epoch": 2.3763277693474962,
      "grad_norm": 0.6119199991226196,
      "learning_rate": 2.031487101669196e-06,
      "loss": 0.3989,
      "step": 1566
    },
    {
      "epoch": 2.3778452200303493,
      "grad_norm": 0.777018129825592,
      "learning_rate": 2.02959028831563e-06,
      "loss": 0.4412,
      "step": 1567
    },
    {
      "epoch": 2.379362670713202,
      "grad_norm": 0.7938686013221741,
      "learning_rate": 2.0276934749620638e-06,
      "loss": 0.3837,
      "step": 1568
    },
    {
      "epoch": 2.3808801213960544,
      "grad_norm": 0.7075850963592529,
      "learning_rate": 2.0257966616084977e-06,
      "loss": 0.3701,
      "step": 1569
    },
    {
      "epoch": 2.3823975720789075,
      "grad_norm": 0.7990305423736572,
      "learning_rate": 2.023899848254932e-06,
      "loss": 0.4411,
      "step": 1570
    },
    {
      "epoch": 2.3839150227617605,
      "grad_norm": 0.7088891863822937,
      "learning_rate": 2.022003034901366e-06,
      "loss": 0.3715,
      "step": 1571
    },
    {
      "epoch": 2.385432473444613,
      "grad_norm": 0.7236811518669128,
      "learning_rate": 2.0201062215478e-06,
      "loss": 0.3774,
      "step": 1572
    },
    {
      "epoch": 2.3869499241274656,
      "grad_norm": 0.6993321776390076,
      "learning_rate": 2.018209408194234e-06,
      "loss": 0.3418,
      "step": 1573
    },
    {
      "epoch": 2.3884673748103187,
      "grad_norm": 0.7408146858215332,
      "learning_rate": 2.0163125948406677e-06,
      "loss": 0.325,
      "step": 1574
    },
    {
      "epoch": 2.3899848254931717,
      "grad_norm": 0.674461305141449,
      "learning_rate": 2.0144157814871017e-06,
      "loss": 0.3254,
      "step": 1575
    },
    {
      "epoch": 2.3915022761760243,
      "grad_norm": 0.6192679405212402,
      "learning_rate": 2.012518968133536e-06,
      "loss": 0.4086,
      "step": 1576
    },
    {
      "epoch": 2.393019726858877,
      "grad_norm": 0.6397387981414795,
      "learning_rate": 2.01062215477997e-06,
      "loss": 0.3254,
      "step": 1577
    },
    {
      "epoch": 2.39453717754173,
      "grad_norm": 0.6487363576889038,
      "learning_rate": 2.008725341426404e-06,
      "loss": 0.3636,
      "step": 1578
    },
    {
      "epoch": 2.396054628224583,
      "grad_norm": 0.770656943321228,
      "learning_rate": 2.0068285280728378e-06,
      "loss": 0.4224,
      "step": 1579
    },
    {
      "epoch": 2.3975720789074355,
      "grad_norm": 0.62076735496521,
      "learning_rate": 2.0049317147192717e-06,
      "loss": 0.3994,
      "step": 1580
    },
    {
      "epoch": 2.399089529590288,
      "grad_norm": 0.7110504508018494,
      "learning_rate": 2.0030349013657056e-06,
      "loss": 0.5003,
      "step": 1581
    },
    {
      "epoch": 2.400606980273141,
      "grad_norm": 0.6963090896606445,
      "learning_rate": 2.00113808801214e-06,
      "loss": 0.4209,
      "step": 1582
    },
    {
      "epoch": 2.402124430955994,
      "grad_norm": 0.8980427384376526,
      "learning_rate": 1.999241274658574e-06,
      "loss": 0.4207,
      "step": 1583
    },
    {
      "epoch": 2.4036418816388467,
      "grad_norm": 0.7225091457366943,
      "learning_rate": 1.997344461305008e-06,
      "loss": 0.3876,
      "step": 1584
    },
    {
      "epoch": 2.4051593323216993,
      "grad_norm": 0.7103250622749329,
      "learning_rate": 1.9954476479514418e-06,
      "loss": 0.3151,
      "step": 1585
    },
    {
      "epoch": 2.4066767830045523,
      "grad_norm": 0.7739225625991821,
      "learning_rate": 1.9935508345978757e-06,
      "loss": 0.3883,
      "step": 1586
    },
    {
      "epoch": 2.4081942336874054,
      "grad_norm": 0.7187367081642151,
      "learning_rate": 1.9916540212443096e-06,
      "loss": 0.4086,
      "step": 1587
    },
    {
      "epoch": 2.409711684370258,
      "grad_norm": 0.6677945852279663,
      "learning_rate": 1.9897572078907435e-06,
      "loss": 0.31,
      "step": 1588
    },
    {
      "epoch": 2.4112291350531105,
      "grad_norm": 0.7971494197845459,
      "learning_rate": 1.987860394537178e-06,
      "loss": 0.3718,
      "step": 1589
    },
    {
      "epoch": 2.4127465857359636,
      "grad_norm": 0.6502736210823059,
      "learning_rate": 1.985963581183612e-06,
      "loss": 0.3998,
      "step": 1590
    },
    {
      "epoch": 2.4142640364188166,
      "grad_norm": 0.8061493039131165,
      "learning_rate": 1.9840667678300457e-06,
      "loss": 0.3906,
      "step": 1591
    },
    {
      "epoch": 2.415781487101669,
      "grad_norm": 0.6376038193702698,
      "learning_rate": 1.9821699544764797e-06,
      "loss": 0.355,
      "step": 1592
    },
    {
      "epoch": 2.4172989377845218,
      "grad_norm": 0.645298957824707,
      "learning_rate": 1.9802731411229136e-06,
      "loss": 0.3635,
      "step": 1593
    },
    {
      "epoch": 2.418816388467375,
      "grad_norm": 0.7096434831619263,
      "learning_rate": 1.9783763277693475e-06,
      "loss": 0.4074,
      "step": 1594
    },
    {
      "epoch": 2.420333839150228,
      "grad_norm": 0.6718387007713318,
      "learning_rate": 1.976479514415782e-06,
      "loss": 0.3368,
      "step": 1595
    },
    {
      "epoch": 2.4218512898330804,
      "grad_norm": 0.658530056476593,
      "learning_rate": 1.9745827010622158e-06,
      "loss": 0.298,
      "step": 1596
    },
    {
      "epoch": 2.423368740515933,
      "grad_norm": 0.6732994914054871,
      "learning_rate": 1.9726858877086497e-06,
      "loss": 0.336,
      "step": 1597
    },
    {
      "epoch": 2.424886191198786,
      "grad_norm": 0.920672595500946,
      "learning_rate": 1.9707890743550836e-06,
      "loss": 0.3881,
      "step": 1598
    },
    {
      "epoch": 2.426403641881639,
      "grad_norm": 0.6216220855712891,
      "learning_rate": 1.9688922610015176e-06,
      "loss": 0.473,
      "step": 1599
    },
    {
      "epoch": 2.4279210925644916,
      "grad_norm": 0.6069574356079102,
      "learning_rate": 1.9669954476479515e-06,
      "loss": 0.3719,
      "step": 1600
    },
    {
      "epoch": 2.4294385432473447,
      "grad_norm": 0.726065456867218,
      "learning_rate": 1.965098634294386e-06,
      "loss": 0.3682,
      "step": 1601
    },
    {
      "epoch": 2.4309559939301972,
      "grad_norm": 0.5731699466705322,
      "learning_rate": 1.9632018209408198e-06,
      "loss": 0.4102,
      "step": 1602
    },
    {
      "epoch": 2.4324734446130503,
      "grad_norm": 0.6288011074066162,
      "learning_rate": 1.9613050075872537e-06,
      "loss": 0.4191,
      "step": 1603
    },
    {
      "epoch": 2.433990895295903,
      "grad_norm": 0.7666937708854675,
      "learning_rate": 1.9594081942336876e-06,
      "loss": 0.3508,
      "step": 1604
    },
    {
      "epoch": 2.435508345978756,
      "grad_norm": 0.7706596255302429,
      "learning_rate": 1.9575113808801215e-06,
      "loss": 0.409,
      "step": 1605
    },
    {
      "epoch": 2.4370257966616085,
      "grad_norm": 0.697832465171814,
      "learning_rate": 1.9556145675265555e-06,
      "loss": 0.3758,
      "step": 1606
    },
    {
      "epoch": 2.4385432473444615,
      "grad_norm": 0.6981236934661865,
      "learning_rate": 1.9537177541729894e-06,
      "loss": 0.3513,
      "step": 1607
    },
    {
      "epoch": 2.440060698027314,
      "grad_norm": 0.6132651567459106,
      "learning_rate": 1.9518209408194237e-06,
      "loss": 0.3751,
      "step": 1608
    },
    {
      "epoch": 2.441578148710167,
      "grad_norm": 0.7956572771072388,
      "learning_rate": 1.9499241274658577e-06,
      "loss": 0.3819,
      "step": 1609
    },
    {
      "epoch": 2.4430955993930197,
      "grad_norm": 0.6727116703987122,
      "learning_rate": 1.9480273141122916e-06,
      "loss": 0.4183,
      "step": 1610
    },
    {
      "epoch": 2.4446130500758727,
      "grad_norm": 0.6346088647842407,
      "learning_rate": 1.9461305007587255e-06,
      "loss": 0.3291,
      "step": 1611
    },
    {
      "epoch": 2.4461305007587253,
      "grad_norm": 0.6916813850402832,
      "learning_rate": 1.9442336874051594e-06,
      "loss": 0.38,
      "step": 1612
    },
    {
      "epoch": 2.4476479514415783,
      "grad_norm": 0.7465354204177856,
      "learning_rate": 1.9423368740515934e-06,
      "loss": 0.3112,
      "step": 1613
    },
    {
      "epoch": 2.449165402124431,
      "grad_norm": 0.8816928863525391,
      "learning_rate": 1.9404400606980277e-06,
      "loss": 0.4279,
      "step": 1614
    },
    {
      "epoch": 2.450682852807284,
      "grad_norm": 0.992667555809021,
      "learning_rate": 1.9385432473444616e-06,
      "loss": 0.4124,
      "step": 1615
    },
    {
      "epoch": 2.4522003034901365,
      "grad_norm": 0.6831511855125427,
      "learning_rate": 1.9366464339908956e-06,
      "loss": 0.3362,
      "step": 1616
    },
    {
      "epoch": 2.4537177541729895,
      "grad_norm": 0.691728949546814,
      "learning_rate": 1.9347496206373295e-06,
      "loss": 0.2912,
      "step": 1617
    },
    {
      "epoch": 2.455235204855842,
      "grad_norm": 0.7584739923477173,
      "learning_rate": 1.9328528072837634e-06,
      "loss": 0.3522,
      "step": 1618
    },
    {
      "epoch": 2.456752655538695,
      "grad_norm": 0.598990261554718,
      "learning_rate": 1.9309559939301973e-06,
      "loss": 0.4294,
      "step": 1619
    },
    {
      "epoch": 2.4582701062215477,
      "grad_norm": 0.6324948668479919,
      "learning_rate": 1.9290591805766317e-06,
      "loss": 0.4528,
      "step": 1620
    },
    {
      "epoch": 2.4597875569044008,
      "grad_norm": 0.6811280846595764,
      "learning_rate": 1.9271623672230656e-06,
      "loss": 0.3634,
      "step": 1621
    },
    {
      "epoch": 2.4613050075872533,
      "grad_norm": 0.6694777011871338,
      "learning_rate": 1.9252655538694995e-06,
      "loss": 0.4102,
      "step": 1622
    },
    {
      "epoch": 2.4628224582701064,
      "grad_norm": 0.6555052399635315,
      "learning_rate": 1.9233687405159335e-06,
      "loss": 0.3752,
      "step": 1623
    },
    {
      "epoch": 2.464339908952959,
      "grad_norm": 0.7271721959114075,
      "learning_rate": 1.9214719271623674e-06,
      "loss": 0.3231,
      "step": 1624
    },
    {
      "epoch": 2.465857359635812,
      "grad_norm": 0.6414864659309387,
      "learning_rate": 1.9195751138088013e-06,
      "loss": 0.4075,
      "step": 1625
    },
    {
      "epoch": 2.4673748103186646,
      "grad_norm": 0.6496157646179199,
      "learning_rate": 1.9176783004552357e-06,
      "loss": 0.3831,
      "step": 1626
    },
    {
      "epoch": 2.4688922610015176,
      "grad_norm": 0.6445685625076294,
      "learning_rate": 1.9157814871016696e-06,
      "loss": 0.3404,
      "step": 1627
    },
    {
      "epoch": 2.47040971168437,
      "grad_norm": 0.653614342212677,
      "learning_rate": 1.9138846737481035e-06,
      "loss": 0.3994,
      "step": 1628
    },
    {
      "epoch": 2.471927162367223,
      "grad_norm": 0.7240074872970581,
      "learning_rate": 1.9119878603945374e-06,
      "loss": 0.3881,
      "step": 1629
    },
    {
      "epoch": 2.473444613050076,
      "grad_norm": 0.8382724523544312,
      "learning_rate": 1.9100910470409714e-06,
      "loss": 0.3494,
      "step": 1630
    },
    {
      "epoch": 2.474962063732929,
      "grad_norm": 0.5828434824943542,
      "learning_rate": 1.9081942336874053e-06,
      "loss": 0.3897,
      "step": 1631
    },
    {
      "epoch": 2.4764795144157814,
      "grad_norm": 0.785615861415863,
      "learning_rate": 1.9062974203338394e-06,
      "loss": 0.342,
      "step": 1632
    },
    {
      "epoch": 2.4779969650986344,
      "grad_norm": 0.7442377805709839,
      "learning_rate": 1.9044006069802734e-06,
      "loss": 0.3529,
      "step": 1633
    },
    {
      "epoch": 2.479514415781487,
      "grad_norm": 0.7028018236160278,
      "learning_rate": 1.9025037936267075e-06,
      "loss": 0.3402,
      "step": 1634
    },
    {
      "epoch": 2.48103186646434,
      "grad_norm": 0.6369195580482483,
      "learning_rate": 1.9006069802731414e-06,
      "loss": 0.3077,
      "step": 1635
    },
    {
      "epoch": 2.4825493171471926,
      "grad_norm": 0.7306427359580994,
      "learning_rate": 1.8987101669195753e-06,
      "loss": 0.3861,
      "step": 1636
    },
    {
      "epoch": 2.4840667678300457,
      "grad_norm": 0.5826613306999207,
      "learning_rate": 1.8968133535660093e-06,
      "loss": 0.3747,
      "step": 1637
    },
    {
      "epoch": 2.4855842185128982,
      "grad_norm": 0.8260460495948792,
      "learning_rate": 1.8949165402124434e-06,
      "loss": 0.3351,
      "step": 1638
    },
    {
      "epoch": 2.4871016691957513,
      "grad_norm": 0.7197369933128357,
      "learning_rate": 1.8930197268588773e-06,
      "loss": 0.3391,
      "step": 1639
    },
    {
      "epoch": 2.488619119878604,
      "grad_norm": 0.6915396451950073,
      "learning_rate": 1.8911229135053113e-06,
      "loss": 0.3662,
      "step": 1640
    },
    {
      "epoch": 2.490136570561457,
      "grad_norm": 0.6694978475570679,
      "learning_rate": 1.8892261001517454e-06,
      "loss": 0.4232,
      "step": 1641
    },
    {
      "epoch": 2.4916540212443095,
      "grad_norm": 0.5992876887321472,
      "learning_rate": 1.8873292867981793e-06,
      "loss": 0.2987,
      "step": 1642
    },
    {
      "epoch": 2.4931714719271625,
      "grad_norm": 0.696801483631134,
      "learning_rate": 1.8854324734446132e-06,
      "loss": 0.3694,
      "step": 1643
    },
    {
      "epoch": 2.494688922610015,
      "grad_norm": 0.7615652680397034,
      "learning_rate": 1.8835356600910474e-06,
      "loss": 0.4666,
      "step": 1644
    },
    {
      "epoch": 2.496206373292868,
      "grad_norm": 0.6524940133094788,
      "learning_rate": 1.8816388467374813e-06,
      "loss": 0.3959,
      "step": 1645
    },
    {
      "epoch": 2.4977238239757207,
      "grad_norm": 0.7510964870452881,
      "learning_rate": 1.8797420333839152e-06,
      "loss": 0.3474,
      "step": 1646
    },
    {
      "epoch": 2.4992412746585737,
      "grad_norm": 0.6841166019439697,
      "learning_rate": 1.8778452200303494e-06,
      "loss": 0.3533,
      "step": 1647
    },
    {
      "epoch": 2.5007587253414263,
      "grad_norm": 0.7190189361572266,
      "learning_rate": 1.8759484066767833e-06,
      "loss": 0.3873,
      "step": 1648
    },
    {
      "epoch": 2.5022761760242793,
      "grad_norm": 0.7836026549339294,
      "learning_rate": 1.874051593323217e-06,
      "loss": 0.3892,
      "step": 1649
    },
    {
      "epoch": 2.503793626707132,
      "grad_norm": 0.775191068649292,
      "learning_rate": 1.872154779969651e-06,
      "loss": 0.3606,
      "step": 1650
    },
    {
      "epoch": 2.505311077389985,
      "grad_norm": 0.6825246214866638,
      "learning_rate": 1.870257966616085e-06,
      "loss": 0.31,
      "step": 1651
    },
    {
      "epoch": 2.5068285280728375,
      "grad_norm": 0.6020254492759705,
      "learning_rate": 1.868361153262519e-06,
      "loss": 0.3057,
      "step": 1652
    },
    {
      "epoch": 2.5083459787556905,
      "grad_norm": 0.7434682250022888,
      "learning_rate": 1.866464339908953e-06,
      "loss": 0.4081,
      "step": 1653
    },
    {
      "epoch": 2.509863429438543,
      "grad_norm": 0.7314153909683228,
      "learning_rate": 1.864567526555387e-06,
      "loss": 0.3203,
      "step": 1654
    },
    {
      "epoch": 2.511380880121396,
      "grad_norm": 0.8363596200942993,
      "learning_rate": 1.862670713201821e-06,
      "loss": 0.4194,
      "step": 1655
    },
    {
      "epoch": 2.5128983308042487,
      "grad_norm": 0.6580225229263306,
      "learning_rate": 1.860773899848255e-06,
      "loss": 0.3222,
      "step": 1656
    },
    {
      "epoch": 2.5144157814871018,
      "grad_norm": 0.8571581244468689,
      "learning_rate": 1.858877086494689e-06,
      "loss": 0.337,
      "step": 1657
    },
    {
      "epoch": 2.5159332321699543,
      "grad_norm": 0.6458020806312561,
      "learning_rate": 1.856980273141123e-06,
      "loss": 0.3408,
      "step": 1658
    },
    {
      "epoch": 2.5174506828528074,
      "grad_norm": 0.6953999996185303,
      "learning_rate": 1.855083459787557e-06,
      "loss": 0.3796,
      "step": 1659
    },
    {
      "epoch": 2.51896813353566,
      "grad_norm": 0.8597591519355774,
      "learning_rate": 1.853186646433991e-06,
      "loss": 0.4012,
      "step": 1660
    },
    {
      "epoch": 2.520485584218513,
      "grad_norm": 0.7023506760597229,
      "learning_rate": 1.851289833080425e-06,
      "loss": 0.4452,
      "step": 1661
    },
    {
      "epoch": 2.5220030349013656,
      "grad_norm": 0.6863210797309875,
      "learning_rate": 1.8493930197268589e-06,
      "loss": 0.3845,
      "step": 1662
    },
    {
      "epoch": 2.5235204855842186,
      "grad_norm": 0.6667979955673218,
      "learning_rate": 1.8474962063732928e-06,
      "loss": 0.3367,
      "step": 1663
    },
    {
      "epoch": 2.525037936267071,
      "grad_norm": 0.7293188571929932,
      "learning_rate": 1.845599393019727e-06,
      "loss": 0.3662,
      "step": 1664
    },
    {
      "epoch": 2.526555386949924,
      "grad_norm": 0.6627885103225708,
      "learning_rate": 1.8437025796661609e-06,
      "loss": 0.4439,
      "step": 1665
    },
    {
      "epoch": 2.528072837632777,
      "grad_norm": 0.6444072723388672,
      "learning_rate": 1.8418057663125948e-06,
      "loss": 0.4516,
      "step": 1666
    },
    {
      "epoch": 2.52959028831563,
      "grad_norm": 0.6024429798126221,
      "learning_rate": 1.839908952959029e-06,
      "loss": 0.3911,
      "step": 1667
    },
    {
      "epoch": 2.5311077389984824,
      "grad_norm": 0.7522280812263489,
      "learning_rate": 1.8380121396054629e-06,
      "loss": 0.4121,
      "step": 1668
    },
    {
      "epoch": 2.5326251896813354,
      "grad_norm": 0.5803895592689514,
      "learning_rate": 1.8361153262518968e-06,
      "loss": 0.3826,
      "step": 1669
    },
    {
      "epoch": 2.534142640364188,
      "grad_norm": 0.6847776770591736,
      "learning_rate": 1.834218512898331e-06,
      "loss": 0.336,
      "step": 1670
    },
    {
      "epoch": 2.535660091047041,
      "grad_norm": 0.690240204334259,
      "learning_rate": 1.8323216995447649e-06,
      "loss": 0.3558,
      "step": 1671
    },
    {
      "epoch": 2.5371775417298936,
      "grad_norm": 0.7237403988838196,
      "learning_rate": 1.8304248861911988e-06,
      "loss": 0.3764,
      "step": 1672
    },
    {
      "epoch": 2.5386949924127467,
      "grad_norm": 0.6697278618812561,
      "learning_rate": 1.828528072837633e-06,
      "loss": 0.3822,
      "step": 1673
    },
    {
      "epoch": 2.5402124430955992,
      "grad_norm": 0.5727783441543579,
      "learning_rate": 1.8266312594840668e-06,
      "loss": 0.319,
      "step": 1674
    },
    {
      "epoch": 2.5417298937784523,
      "grad_norm": 0.7101137042045593,
      "learning_rate": 1.8247344461305008e-06,
      "loss": 0.4093,
      "step": 1675
    },
    {
      "epoch": 2.543247344461305,
      "grad_norm": 0.6593617796897888,
      "learning_rate": 1.822837632776935e-06,
      "loss": 0.3461,
      "step": 1676
    },
    {
      "epoch": 2.544764795144158,
      "grad_norm": 0.6296799182891846,
      "learning_rate": 1.8209408194233688e-06,
      "loss": 0.3557,
      "step": 1677
    },
    {
      "epoch": 2.5462822458270105,
      "grad_norm": 0.7526244521141052,
      "learning_rate": 1.8190440060698028e-06,
      "loss": 0.2976,
      "step": 1678
    },
    {
      "epoch": 2.5477996965098635,
      "grad_norm": 0.6190820336341858,
      "learning_rate": 1.8171471927162369e-06,
      "loss": 0.3662,
      "step": 1679
    },
    {
      "epoch": 2.549317147192716,
      "grad_norm": 0.6648781895637512,
      "learning_rate": 1.8152503793626708e-06,
      "loss": 0.3261,
      "step": 1680
    },
    {
      "epoch": 2.550834597875569,
      "grad_norm": 0.695359468460083,
      "learning_rate": 1.8133535660091047e-06,
      "loss": 0.349,
      "step": 1681
    },
    {
      "epoch": 2.552352048558422,
      "grad_norm": 0.656437873840332,
      "learning_rate": 1.8114567526555387e-06,
      "loss": 0.3408,
      "step": 1682
    },
    {
      "epoch": 2.5538694992412747,
      "grad_norm": 0.6945465207099915,
      "learning_rate": 1.8095599393019728e-06,
      "loss": 0.284,
      "step": 1683
    },
    {
      "epoch": 2.5553869499241273,
      "grad_norm": 0.6079171895980835,
      "learning_rate": 1.8076631259484067e-06,
      "loss": 0.3462,
      "step": 1684
    },
    {
      "epoch": 2.5569044006069803,
      "grad_norm": 0.6393222808837891,
      "learning_rate": 1.8057663125948407e-06,
      "loss": 0.364,
      "step": 1685
    },
    {
      "epoch": 2.5584218512898333,
      "grad_norm": 0.6401365995407104,
      "learning_rate": 1.8038694992412748e-06,
      "loss": 0.3193,
      "step": 1686
    },
    {
      "epoch": 2.559939301972686,
      "grad_norm": 0.8641067743301392,
      "learning_rate": 1.8019726858877087e-06,
      "loss": 0.3641,
      "step": 1687
    },
    {
      "epoch": 2.5614567526555385,
      "grad_norm": 0.6671780347824097,
      "learning_rate": 1.8000758725341426e-06,
      "loss": 0.3768,
      "step": 1688
    },
    {
      "epoch": 2.5629742033383915,
      "grad_norm": 0.7557299733161926,
      "learning_rate": 1.7981790591805768e-06,
      "loss": 0.4404,
      "step": 1689
    },
    {
      "epoch": 2.5644916540212446,
      "grad_norm": 0.6501238346099854,
      "learning_rate": 1.7962822458270107e-06,
      "loss": 0.2962,
      "step": 1690
    },
    {
      "epoch": 2.566009104704097,
      "grad_norm": 0.8846094608306885,
      "learning_rate": 1.7943854324734446e-06,
      "loss": 0.3849,
      "step": 1691
    },
    {
      "epoch": 2.5675265553869497,
      "grad_norm": 0.6985528469085693,
      "learning_rate": 1.7924886191198788e-06,
      "loss": 0.3878,
      "step": 1692
    },
    {
      "epoch": 2.5690440060698028,
      "grad_norm": 0.7301976084709167,
      "learning_rate": 1.7905918057663127e-06,
      "loss": 0.4315,
      "step": 1693
    },
    {
      "epoch": 2.570561456752656,
      "grad_norm": 0.6025879979133606,
      "learning_rate": 1.7886949924127466e-06,
      "loss": 0.3088,
      "step": 1694
    },
    {
      "epoch": 2.5720789074355084,
      "grad_norm": 0.6396095156669617,
      "learning_rate": 1.7867981790591808e-06,
      "loss": 0.3739,
      "step": 1695
    },
    {
      "epoch": 2.573596358118361,
      "grad_norm": 0.6291137337684631,
      "learning_rate": 1.7849013657056147e-06,
      "loss": 0.3552,
      "step": 1696
    },
    {
      "epoch": 2.575113808801214,
      "grad_norm": 0.795551598072052,
      "learning_rate": 1.7830045523520486e-06,
      "loss": 0.3274,
      "step": 1697
    },
    {
      "epoch": 2.576631259484067,
      "grad_norm": 0.6366688013076782,
      "learning_rate": 1.7811077389984827e-06,
      "loss": 0.2811,
      "step": 1698
    },
    {
      "epoch": 2.5781487101669196,
      "grad_norm": 0.637273371219635,
      "learning_rate": 1.7792109256449167e-06,
      "loss": 0.2804,
      "step": 1699
    },
    {
      "epoch": 2.579666160849772,
      "grad_norm": 0.605186402797699,
      "learning_rate": 1.7773141122913506e-06,
      "loss": 0.4553,
      "step": 1700
    },
    {
      "epoch": 2.581183611532625,
      "grad_norm": 0.6609706282615662,
      "learning_rate": 1.7754172989377847e-06,
      "loss": 0.3265,
      "step": 1701
    },
    {
      "epoch": 2.5827010622154782,
      "grad_norm": 0.6841104626655579,
      "learning_rate": 1.7735204855842187e-06,
      "loss": 0.3472,
      "step": 1702
    },
    {
      "epoch": 2.584218512898331,
      "grad_norm": 0.6036561131477356,
      "learning_rate": 1.7716236722306526e-06,
      "loss": 0.3642,
      "step": 1703
    },
    {
      "epoch": 2.5857359635811834,
      "grad_norm": 0.656626284122467,
      "learning_rate": 1.7697268588770865e-06,
      "loss": 0.3689,
      "step": 1704
    },
    {
      "epoch": 2.5872534142640364,
      "grad_norm": 0.7049858570098877,
      "learning_rate": 1.7678300455235207e-06,
      "loss": 0.4028,
      "step": 1705
    },
    {
      "epoch": 2.5887708649468895,
      "grad_norm": 0.8517928719520569,
      "learning_rate": 1.7659332321699546e-06,
      "loss": 0.3774,
      "step": 1706
    },
    {
      "epoch": 2.590288315629742,
      "grad_norm": 0.6645584106445312,
      "learning_rate": 1.7640364188163885e-06,
      "loss": 0.3616,
      "step": 1707
    },
    {
      "epoch": 2.5918057663125946,
      "grad_norm": 0.6025153994560242,
      "learning_rate": 1.7621396054628226e-06,
      "loss": 0.3061,
      "step": 1708
    },
    {
      "epoch": 2.5933232169954477,
      "grad_norm": 0.5941515564918518,
      "learning_rate": 1.7602427921092566e-06,
      "loss": 0.3521,
      "step": 1709
    },
    {
      "epoch": 2.5948406676783007,
      "grad_norm": 0.7024884223937988,
      "learning_rate": 1.7583459787556905e-06,
      "loss": 0.3346,
      "step": 1710
    },
    {
      "epoch": 2.5963581183611533,
      "grad_norm": 0.6905301809310913,
      "learning_rate": 1.7564491654021246e-06,
      "loss": 0.314,
      "step": 1711
    },
    {
      "epoch": 2.597875569044006,
      "grad_norm": 0.6113348603248596,
      "learning_rate": 1.7545523520485586e-06,
      "loss": 0.2809,
      "step": 1712
    },
    {
      "epoch": 2.599393019726859,
      "grad_norm": 0.6186642050743103,
      "learning_rate": 1.7526555386949925e-06,
      "loss": 0.3905,
      "step": 1713
    },
    {
      "epoch": 2.600910470409712,
      "grad_norm": 0.7352614402770996,
      "learning_rate": 1.7507587253414266e-06,
      "loss": 0.4111,
      "step": 1714
    },
    {
      "epoch": 2.6024279210925645,
      "grad_norm": 0.6631103157997131,
      "learning_rate": 1.7488619119878605e-06,
      "loss": 0.3314,
      "step": 1715
    },
    {
      "epoch": 2.603945371775417,
      "grad_norm": 0.6490837335586548,
      "learning_rate": 1.7469650986342945e-06,
      "loss": 0.422,
      "step": 1716
    },
    {
      "epoch": 2.60546282245827,
      "grad_norm": 0.6176832318305969,
      "learning_rate": 1.7450682852807286e-06,
      "loss": 0.3535,
      "step": 1717
    },
    {
      "epoch": 2.606980273141123,
      "grad_norm": 0.6532945036888123,
      "learning_rate": 1.7431714719271625e-06,
      "loss": 0.3671,
      "step": 1718
    },
    {
      "epoch": 2.6084977238239757,
      "grad_norm": 0.6633699536323547,
      "learning_rate": 1.7412746585735965e-06,
      "loss": 0.4795,
      "step": 1719
    },
    {
      "epoch": 2.6100151745068283,
      "grad_norm": 0.5758225321769714,
      "learning_rate": 1.7393778452200306e-06,
      "loss": 0.4066,
      "step": 1720
    },
    {
      "epoch": 2.6115326251896813,
      "grad_norm": 0.6800470948219299,
      "learning_rate": 1.7374810318664645e-06,
      "loss": 0.3126,
      "step": 1721
    },
    {
      "epoch": 2.6130500758725344,
      "grad_norm": 0.734075129032135,
      "learning_rate": 1.7355842185128984e-06,
      "loss": 0.3531,
      "step": 1722
    },
    {
      "epoch": 2.614567526555387,
      "grad_norm": 0.6752445697784424,
      "learning_rate": 1.7336874051593326e-06,
      "loss": 0.3707,
      "step": 1723
    },
    {
      "epoch": 2.6160849772382395,
      "grad_norm": 0.5855052471160889,
      "learning_rate": 1.7317905918057665e-06,
      "loss": 0.3622,
      "step": 1724
    },
    {
      "epoch": 2.6176024279210925,
      "grad_norm": 0.6133235096931458,
      "learning_rate": 1.7298937784522004e-06,
      "loss": 0.2615,
      "step": 1725
    },
    {
      "epoch": 2.6191198786039456,
      "grad_norm": 0.6212901473045349,
      "learning_rate": 1.7279969650986344e-06,
      "loss": 0.281,
      "step": 1726
    },
    {
      "epoch": 2.620637329286798,
      "grad_norm": 0.6026617884635925,
      "learning_rate": 1.7261001517450685e-06,
      "loss": 0.3627,
      "step": 1727
    },
    {
      "epoch": 2.6221547799696507,
      "grad_norm": 0.6688107848167419,
      "learning_rate": 1.7242033383915024e-06,
      "loss": 0.33,
      "step": 1728
    },
    {
      "epoch": 2.6236722306525038,
      "grad_norm": 0.7353373169898987,
      "learning_rate": 1.7223065250379363e-06,
      "loss": 0.407,
      "step": 1729
    },
    {
      "epoch": 2.625189681335357,
      "grad_norm": 0.6286540627479553,
      "learning_rate": 1.7204097116843705e-06,
      "loss": 0.3582,
      "step": 1730
    },
    {
      "epoch": 2.6267071320182094,
      "grad_norm": 0.612947940826416,
      "learning_rate": 1.7185128983308044e-06,
      "loss": 0.406,
      "step": 1731
    },
    {
      "epoch": 2.628224582701062,
      "grad_norm": 0.6538927555084229,
      "learning_rate": 1.7166160849772383e-06,
      "loss": 0.3814,
      "step": 1732
    },
    {
      "epoch": 2.629742033383915,
      "grad_norm": 0.7640698552131653,
      "learning_rate": 1.7147192716236725e-06,
      "loss": 0.322,
      "step": 1733
    },
    {
      "epoch": 2.631259484066768,
      "grad_norm": 0.6131340861320496,
      "learning_rate": 1.7128224582701064e-06,
      "loss": 0.4157,
      "step": 1734
    },
    {
      "epoch": 2.6327769347496206,
      "grad_norm": 0.639700710773468,
      "learning_rate": 1.7109256449165403e-06,
      "loss": 0.3804,
      "step": 1735
    },
    {
      "epoch": 2.634294385432473,
      "grad_norm": 0.6656138300895691,
      "learning_rate": 1.7090288315629745e-06,
      "loss": 0.3781,
      "step": 1736
    },
    {
      "epoch": 2.635811836115326,
      "grad_norm": 0.9000221490859985,
      "learning_rate": 1.7071320182094084e-06,
      "loss": 0.395,
      "step": 1737
    },
    {
      "epoch": 2.6373292867981792,
      "grad_norm": 0.7028218507766724,
      "learning_rate": 1.7052352048558423e-06,
      "loss": 0.4051,
      "step": 1738
    },
    {
      "epoch": 2.638846737481032,
      "grad_norm": 0.7116014361381531,
      "learning_rate": 1.7033383915022764e-06,
      "loss": 0.3737,
      "step": 1739
    },
    {
      "epoch": 2.6403641881638844,
      "grad_norm": 0.6468605399131775,
      "learning_rate": 1.7014415781487104e-06,
      "loss": 0.2649,
      "step": 1740
    },
    {
      "epoch": 2.6418816388467374,
      "grad_norm": 0.6802920699119568,
      "learning_rate": 1.6995447647951443e-06,
      "loss": 0.3628,
      "step": 1741
    },
    {
      "epoch": 2.6433990895295905,
      "grad_norm": 0.6482523083686829,
      "learning_rate": 1.6976479514415784e-06,
      "loss": 0.3207,
      "step": 1742
    },
    {
      "epoch": 2.644916540212443,
      "grad_norm": 0.6807390451431274,
      "learning_rate": 1.6957511380880124e-06,
      "loss": 0.3554,
      "step": 1743
    },
    {
      "epoch": 2.6464339908952956,
      "grad_norm": 0.5988059639930725,
      "learning_rate": 1.6938543247344463e-06,
      "loss": 0.3454,
      "step": 1744
    },
    {
      "epoch": 2.6479514415781487,
      "grad_norm": 1.963663935661316,
      "learning_rate": 1.6919575113808804e-06,
      "loss": 0.3788,
      "step": 1745
    },
    {
      "epoch": 2.6494688922610017,
      "grad_norm": 0.6743354797363281,
      "learning_rate": 1.6900606980273143e-06,
      "loss": 0.3507,
      "step": 1746
    },
    {
      "epoch": 2.6509863429438543,
      "grad_norm": 0.7217318415641785,
      "learning_rate": 1.6881638846737483e-06,
      "loss": 0.4142,
      "step": 1747
    },
    {
      "epoch": 2.6525037936267073,
      "grad_norm": 0.679594874382019,
      "learning_rate": 1.6862670713201822e-06,
      "loss": 0.3153,
      "step": 1748
    },
    {
      "epoch": 2.65402124430956,
      "grad_norm": 0.6843754053115845,
      "learning_rate": 1.6843702579666163e-06,
      "loss": 0.3685,
      "step": 1749
    },
    {
      "epoch": 2.655538694992413,
      "grad_norm": 0.6490026712417603,
      "learning_rate": 1.6824734446130503e-06,
      "loss": 0.3683,
      "step": 1750
    },
    {
      "epoch": 2.6570561456752655,
      "grad_norm": 0.548251211643219,
      "learning_rate": 1.6805766312594842e-06,
      "loss": 0.3543,
      "step": 1751
    },
    {
      "epoch": 2.6585735963581185,
      "grad_norm": 0.654862642288208,
      "learning_rate": 1.6786798179059183e-06,
      "loss": 0.4349,
      "step": 1752
    },
    {
      "epoch": 2.660091047040971,
      "grad_norm": 0.7217377424240112,
      "learning_rate": 1.6767830045523523e-06,
      "loss": 0.3835,
      "step": 1753
    },
    {
      "epoch": 2.661608497723824,
      "grad_norm": 0.6544373035430908,
      "learning_rate": 1.6748861911987862e-06,
      "loss": 0.3293,
      "step": 1754
    },
    {
      "epoch": 2.6631259484066767,
      "grad_norm": 0.5657952427864075,
      "learning_rate": 1.6729893778452203e-06,
      "loss": 0.3607,
      "step": 1755
    },
    {
      "epoch": 2.6646433990895297,
      "grad_norm": 0.719062328338623,
      "learning_rate": 1.6710925644916542e-06,
      "loss": 0.4196,
      "step": 1756
    },
    {
      "epoch": 2.6661608497723823,
      "grad_norm": 0.6253043413162231,
      "learning_rate": 1.6691957511380882e-06,
      "loss": 0.3785,
      "step": 1757
    },
    {
      "epoch": 2.6676783004552354,
      "grad_norm": 0.6565998792648315,
      "learning_rate": 1.6672989377845223e-06,
      "loss": 0.3276,
      "step": 1758
    },
    {
      "epoch": 2.669195751138088,
      "grad_norm": 0.5897979736328125,
      "learning_rate": 1.6654021244309562e-06,
      "loss": 0.3796,
      "step": 1759
    },
    {
      "epoch": 2.670713201820941,
      "grad_norm": 0.6696348786354065,
      "learning_rate": 1.6635053110773902e-06,
      "loss": 0.3019,
      "step": 1760
    },
    {
      "epoch": 2.6722306525037935,
      "grad_norm": 0.7977352142333984,
      "learning_rate": 1.6616084977238243e-06,
      "loss": 0.3697,
      "step": 1761
    },
    {
      "epoch": 2.6737481031866466,
      "grad_norm": 0.662097692489624,
      "learning_rate": 1.6597116843702582e-06,
      "loss": 0.4237,
      "step": 1762
    },
    {
      "epoch": 2.675265553869499,
      "grad_norm": 0.626571774482727,
      "learning_rate": 1.6578148710166921e-06,
      "loss": 0.3954,
      "step": 1763
    },
    {
      "epoch": 2.676783004552352,
      "grad_norm": 0.6509573459625244,
      "learning_rate": 1.6559180576631263e-06,
      "loss": 0.3572,
      "step": 1764
    },
    {
      "epoch": 2.6783004552352048,
      "grad_norm": 0.6653605699539185,
      "learning_rate": 1.6540212443095602e-06,
      "loss": 0.3483,
      "step": 1765
    },
    {
      "epoch": 2.679817905918058,
      "grad_norm": 0.7265416383743286,
      "learning_rate": 1.6521244309559941e-06,
      "loss": 0.2838,
      "step": 1766
    },
    {
      "epoch": 2.6813353566009104,
      "grad_norm": 0.7071565389633179,
      "learning_rate": 1.6502276176024283e-06,
      "loss": 0.4081,
      "step": 1767
    },
    {
      "epoch": 2.6828528072837634,
      "grad_norm": 0.6160314083099365,
      "learning_rate": 1.6483308042488622e-06,
      "loss": 0.3632,
      "step": 1768
    },
    {
      "epoch": 2.684370257966616,
      "grad_norm": 0.5698915719985962,
      "learning_rate": 1.6464339908952961e-06,
      "loss": 0.4461,
      "step": 1769
    },
    {
      "epoch": 2.685887708649469,
      "grad_norm": 0.6645826697349548,
      "learning_rate": 1.64453717754173e-06,
      "loss": 0.3577,
      "step": 1770
    },
    {
      "epoch": 2.6874051593323216,
      "grad_norm": 0.6771582961082458,
      "learning_rate": 1.6426403641881642e-06,
      "loss": 0.3465,
      "step": 1771
    },
    {
      "epoch": 2.6889226100151746,
      "grad_norm": 0.7122588157653809,
      "learning_rate": 1.6407435508345981e-06,
      "loss": 0.3986,
      "step": 1772
    },
    {
      "epoch": 2.690440060698027,
      "grad_norm": 0.7681506276130676,
      "learning_rate": 1.638846737481032e-06,
      "loss": 0.3528,
      "step": 1773
    },
    {
      "epoch": 2.6919575113808802,
      "grad_norm": 0.8027291893959045,
      "learning_rate": 1.6369499241274662e-06,
      "loss": 0.4184,
      "step": 1774
    },
    {
      "epoch": 2.693474962063733,
      "grad_norm": 0.6735741496086121,
      "learning_rate": 1.6350531107739e-06,
      "loss": 0.4113,
      "step": 1775
    },
    {
      "epoch": 2.694992412746586,
      "grad_norm": 0.619293749332428,
      "learning_rate": 1.633156297420334e-06,
      "loss": 0.305,
      "step": 1776
    },
    {
      "epoch": 2.6965098634294384,
      "grad_norm": 0.6685989499092102,
      "learning_rate": 1.6312594840667682e-06,
      "loss": 0.3419,
      "step": 1777
    },
    {
      "epoch": 2.6980273141122915,
      "grad_norm": 0.6328955888748169,
      "learning_rate": 1.629362670713202e-06,
      "loss": 0.3857,
      "step": 1778
    },
    {
      "epoch": 2.699544764795144,
      "grad_norm": 0.6682487726211548,
      "learning_rate": 1.627465857359636e-06,
      "loss": 0.3976,
      "step": 1779
    },
    {
      "epoch": 2.701062215477997,
      "grad_norm": 0.7675158381462097,
      "learning_rate": 1.6255690440060701e-06,
      "loss": 0.4196,
      "step": 1780
    },
    {
      "epoch": 2.7025796661608497,
      "grad_norm": 0.6411820650100708,
      "learning_rate": 1.6236722306525039e-06,
      "loss": 0.378,
      "step": 1781
    },
    {
      "epoch": 2.7040971168437027,
      "grad_norm": 0.5899679660797119,
      "learning_rate": 1.6217754172989378e-06,
      "loss": 0.4237,
      "step": 1782
    },
    {
      "epoch": 2.7056145675265553,
      "grad_norm": 0.7980819940567017,
      "learning_rate": 1.6198786039453717e-06,
      "loss": 0.4232,
      "step": 1783
    },
    {
      "epoch": 2.7071320182094083,
      "grad_norm": 0.6791512966156006,
      "learning_rate": 1.6179817905918058e-06,
      "loss": 0.452,
      "step": 1784
    },
    {
      "epoch": 2.708649468892261,
      "grad_norm": 0.6962975263595581,
      "learning_rate": 1.6160849772382398e-06,
      "loss": 0.4013,
      "step": 1785
    },
    {
      "epoch": 2.710166919575114,
      "grad_norm": 0.567054033279419,
      "learning_rate": 1.6141881638846737e-06,
      "loss": 0.4058,
      "step": 1786
    },
    {
      "epoch": 2.7116843702579665,
      "grad_norm": 0.6522906422615051,
      "learning_rate": 1.6122913505311078e-06,
      "loss": 0.3664,
      "step": 1787
    },
    {
      "epoch": 2.7132018209408195,
      "grad_norm": 0.6991718411445618,
      "learning_rate": 1.6103945371775418e-06,
      "loss": 0.4052,
      "step": 1788
    },
    {
      "epoch": 2.714719271623672,
      "grad_norm": 0.6786735653877258,
      "learning_rate": 1.6084977238239757e-06,
      "loss": 0.3916,
      "step": 1789
    },
    {
      "epoch": 2.716236722306525,
      "grad_norm": 0.7207143902778625,
      "learning_rate": 1.6066009104704098e-06,
      "loss": 0.4291,
      "step": 1790
    },
    {
      "epoch": 2.7177541729893777,
      "grad_norm": 0.6754008531570435,
      "learning_rate": 1.6047040971168437e-06,
      "loss": 0.3528,
      "step": 1791
    },
    {
      "epoch": 2.7192716236722307,
      "grad_norm": 0.7028223276138306,
      "learning_rate": 1.6028072837632777e-06,
      "loss": 0.2678,
      "step": 1792
    },
    {
      "epoch": 2.7207890743550833,
      "grad_norm": 0.6120207905769348,
      "learning_rate": 1.6009104704097116e-06,
      "loss": 0.3679,
      "step": 1793
    },
    {
      "epoch": 2.7223065250379364,
      "grad_norm": 0.6651562452316284,
      "learning_rate": 1.5990136570561457e-06,
      "loss": 0.3692,
      "step": 1794
    },
    {
      "epoch": 2.723823975720789,
      "grad_norm": 0.7046253681182861,
      "learning_rate": 1.5971168437025797e-06,
      "loss": 0.3396,
      "step": 1795
    },
    {
      "epoch": 2.725341426403642,
      "grad_norm": 0.6450766921043396,
      "learning_rate": 1.5952200303490136e-06,
      "loss": 0.3311,
      "step": 1796
    },
    {
      "epoch": 2.7268588770864945,
      "grad_norm": 0.6626798510551453,
      "learning_rate": 1.5933232169954477e-06,
      "loss": 0.3663,
      "step": 1797
    },
    {
      "epoch": 2.7283763277693476,
      "grad_norm": 0.7321597933769226,
      "learning_rate": 1.5914264036418817e-06,
      "loss": 0.3498,
      "step": 1798
    },
    {
      "epoch": 2.7298937784522,
      "grad_norm": 0.580435574054718,
      "learning_rate": 1.5895295902883156e-06,
      "loss": 0.3696,
      "step": 1799
    },
    {
      "epoch": 2.731411229135053,
      "grad_norm": 0.632725715637207,
      "learning_rate": 1.5876327769347497e-06,
      "loss": 0.3098,
      "step": 1800
    },
    {
      "epoch": 2.7329286798179058,
      "grad_norm": 0.6230491995811462,
      "learning_rate": 1.5857359635811836e-06,
      "loss": 0.3916,
      "step": 1801
    },
    {
      "epoch": 2.734446130500759,
      "grad_norm": 0.8002034425735474,
      "learning_rate": 1.5838391502276176e-06,
      "loss": 0.4241,
      "step": 1802
    },
    {
      "epoch": 2.7359635811836114,
      "grad_norm": 0.7417864203453064,
      "learning_rate": 1.5819423368740517e-06,
      "loss": 0.3444,
      "step": 1803
    },
    {
      "epoch": 2.7374810318664644,
      "grad_norm": 0.6972910165786743,
      "learning_rate": 1.5800455235204856e-06,
      "loss": 0.309,
      "step": 1804
    },
    {
      "epoch": 2.738998482549317,
      "grad_norm": 0.6718483567237854,
      "learning_rate": 1.5781487101669196e-06,
      "loss": 0.3841,
      "step": 1805
    },
    {
      "epoch": 2.74051593323217,
      "grad_norm": 0.7553343176841736,
      "learning_rate": 1.5762518968133537e-06,
      "loss": 0.4285,
      "step": 1806
    },
    {
      "epoch": 2.7420333839150226,
      "grad_norm": 0.5987138152122498,
      "learning_rate": 1.5743550834597876e-06,
      "loss": 0.4416,
      "step": 1807
    },
    {
      "epoch": 2.7435508345978756,
      "grad_norm": 0.7039605379104614,
      "learning_rate": 1.5724582701062215e-06,
      "loss": 0.3661,
      "step": 1808
    },
    {
      "epoch": 2.745068285280728,
      "grad_norm": 0.6707313656806946,
      "learning_rate": 1.5705614567526557e-06,
      "loss": 0.4023,
      "step": 1809
    },
    {
      "epoch": 2.7465857359635812,
      "grad_norm": 0.6414811611175537,
      "learning_rate": 1.5686646433990896e-06,
      "loss": 0.3337,
      "step": 1810
    },
    {
      "epoch": 2.748103186646434,
      "grad_norm": 0.779700756072998,
      "learning_rate": 1.5667678300455235e-06,
      "loss": 0.4232,
      "step": 1811
    },
    {
      "epoch": 2.749620637329287,
      "grad_norm": 0.6158767342567444,
      "learning_rate": 1.5648710166919577e-06,
      "loss": 0.402,
      "step": 1812
    },
    {
      "epoch": 2.75113808801214,
      "grad_norm": 0.7245985865592957,
      "learning_rate": 1.5629742033383916e-06,
      "loss": 0.3687,
      "step": 1813
    },
    {
      "epoch": 2.7526555386949925,
      "grad_norm": 0.6751134395599365,
      "learning_rate": 1.5610773899848255e-06,
      "loss": 0.3928,
      "step": 1814
    },
    {
      "epoch": 2.754172989377845,
      "grad_norm": 0.6056177616119385,
      "learning_rate": 1.5591805766312594e-06,
      "loss": 0.3917,
      "step": 1815
    },
    {
      "epoch": 2.755690440060698,
      "grad_norm": 0.6398768424987793,
      "learning_rate": 1.5572837632776936e-06,
      "loss": 0.3088,
      "step": 1816
    },
    {
      "epoch": 2.757207890743551,
      "grad_norm": 0.535645067691803,
      "learning_rate": 1.5553869499241275e-06,
      "loss": 0.4786,
      "step": 1817
    },
    {
      "epoch": 2.7587253414264037,
      "grad_norm": 0.7365468144416809,
      "learning_rate": 1.5534901365705614e-06,
      "loss": 0.358,
      "step": 1818
    },
    {
      "epoch": 2.7602427921092563,
      "grad_norm": 0.7026424407958984,
      "learning_rate": 1.5515933232169956e-06,
      "loss": 0.3702,
      "step": 1819
    },
    {
      "epoch": 2.7617602427921093,
      "grad_norm": 0.6894193887710571,
      "learning_rate": 1.5496965098634295e-06,
      "loss": 0.3236,
      "step": 1820
    },
    {
      "epoch": 2.7632776934749623,
      "grad_norm": 0.5896501541137695,
      "learning_rate": 1.5477996965098634e-06,
      "loss": 0.2911,
      "step": 1821
    },
    {
      "epoch": 2.764795144157815,
      "grad_norm": 0.8320409655570984,
      "learning_rate": 1.5459028831562976e-06,
      "loss": 0.4374,
      "step": 1822
    },
    {
      "epoch": 2.7663125948406675,
      "grad_norm": 0.8461946845054626,
      "learning_rate": 1.5440060698027315e-06,
      "loss": 0.39,
      "step": 1823
    },
    {
      "epoch": 2.7678300455235205,
      "grad_norm": 0.5960681438446045,
      "learning_rate": 1.5421092564491654e-06,
      "loss": 0.3868,
      "step": 1824
    },
    {
      "epoch": 2.7693474962063735,
      "grad_norm": 0.6144605875015259,
      "learning_rate": 1.5402124430955995e-06,
      "loss": 0.4136,
      "step": 1825
    },
    {
      "epoch": 2.770864946889226,
      "grad_norm": 0.7409563660621643,
      "learning_rate": 1.5383156297420335e-06,
      "loss": 0.3128,
      "step": 1826
    },
    {
      "epoch": 2.7723823975720787,
      "grad_norm": 0.6251935958862305,
      "learning_rate": 1.5364188163884674e-06,
      "loss": 0.3258,
      "step": 1827
    },
    {
      "epoch": 2.7738998482549317,
      "grad_norm": 0.7098795175552368,
      "learning_rate": 1.5345220030349015e-06,
      "loss": 0.4235,
      "step": 1828
    },
    {
      "epoch": 2.7754172989377848,
      "grad_norm": 0.8179904818534851,
      "learning_rate": 1.5326251896813355e-06,
      "loss": 0.3703,
      "step": 1829
    },
    {
      "epoch": 2.7769347496206374,
      "grad_norm": 0.5812712907791138,
      "learning_rate": 1.5307283763277694e-06,
      "loss": 0.3854,
      "step": 1830
    },
    {
      "epoch": 2.77845220030349,
      "grad_norm": 0.5782586932182312,
      "learning_rate": 1.5288315629742035e-06,
      "loss": 0.3929,
      "step": 1831
    },
    {
      "epoch": 2.779969650986343,
      "grad_norm": 1.8210299015045166,
      "learning_rate": 1.5269347496206374e-06,
      "loss": 0.4322,
      "step": 1832
    },
    {
      "epoch": 2.781487101669196,
      "grad_norm": 0.6893594264984131,
      "learning_rate": 1.5250379362670714e-06,
      "loss": 0.356,
      "step": 1833
    },
    {
      "epoch": 2.7830045523520486,
      "grad_norm": 0.777521014213562,
      "learning_rate": 1.5231411229135055e-06,
      "loss": 0.3949,
      "step": 1834
    },
    {
      "epoch": 2.784522003034901,
      "grad_norm": 0.684745728969574,
      "learning_rate": 1.5212443095599394e-06,
      "loss": 0.3541,
      "step": 1835
    },
    {
      "epoch": 2.786039453717754,
      "grad_norm": 0.70225989818573,
      "learning_rate": 1.5193474962063734e-06,
      "loss": 0.3786,
      "step": 1836
    },
    {
      "epoch": 2.787556904400607,
      "grad_norm": 0.6705806851387024,
      "learning_rate": 1.5174506828528073e-06,
      "loss": 0.3746,
      "step": 1837
    },
    {
      "epoch": 2.78907435508346,
      "grad_norm": 0.7322483062744141,
      "learning_rate": 1.5155538694992414e-06,
      "loss": 0.3729,
      "step": 1838
    },
    {
      "epoch": 2.7905918057663124,
      "grad_norm": 0.628572404384613,
      "learning_rate": 1.5136570561456754e-06,
      "loss": 0.4001,
      "step": 1839
    },
    {
      "epoch": 2.7921092564491654,
      "grad_norm": 0.6924571394920349,
      "learning_rate": 1.5117602427921093e-06,
      "loss": 0.319,
      "step": 1840
    },
    {
      "epoch": 2.7936267071320184,
      "grad_norm": 0.5978741645812988,
      "learning_rate": 1.5098634294385434e-06,
      "loss": 0.3651,
      "step": 1841
    },
    {
      "epoch": 2.795144157814871,
      "grad_norm": 0.6165506839752197,
      "learning_rate": 1.5079666160849773e-06,
      "loss": 0.3553,
      "step": 1842
    },
    {
      "epoch": 2.7966616084977236,
      "grad_norm": 0.7222352027893066,
      "learning_rate": 1.5060698027314113e-06,
      "loss": 0.3805,
      "step": 1843
    },
    {
      "epoch": 2.7981790591805766,
      "grad_norm": 0.975058913230896,
      "learning_rate": 1.5041729893778454e-06,
      "loss": 0.3589,
      "step": 1844
    },
    {
      "epoch": 2.7996965098634297,
      "grad_norm": 0.615162193775177,
      "learning_rate": 1.5022761760242793e-06,
      "loss": 0.3426,
      "step": 1845
    },
    {
      "epoch": 2.8012139605462822,
      "grad_norm": 0.6771577596664429,
      "learning_rate": 1.5003793626707133e-06,
      "loss": 0.334,
      "step": 1846
    },
    {
      "epoch": 2.802731411229135,
      "grad_norm": 0.6939575672149658,
      "learning_rate": 1.4984825493171474e-06,
      "loss": 0.3587,
      "step": 1847
    },
    {
      "epoch": 2.804248861911988,
      "grad_norm": 0.5822486877441406,
      "learning_rate": 1.4965857359635813e-06,
      "loss": 0.3497,
      "step": 1848
    },
    {
      "epoch": 2.805766312594841,
      "grad_norm": 0.6450919508934021,
      "learning_rate": 1.4946889226100152e-06,
      "loss": 0.2921,
      "step": 1849
    },
    {
      "epoch": 2.8072837632776935,
      "grad_norm": 0.7506431937217712,
      "learning_rate": 1.4927921092564494e-06,
      "loss": 0.4198,
      "step": 1850
    },
    {
      "epoch": 2.808801213960546,
      "grad_norm": 0.6762922406196594,
      "learning_rate": 1.4908952959028833e-06,
      "loss": 0.4253,
      "step": 1851
    },
    {
      "epoch": 2.810318664643399,
      "grad_norm": 0.6505826115608215,
      "learning_rate": 1.4889984825493172e-06,
      "loss": 0.2997,
      "step": 1852
    },
    {
      "epoch": 2.811836115326252,
      "grad_norm": 0.6435210704803467,
      "learning_rate": 1.4871016691957514e-06,
      "loss": 0.2993,
      "step": 1853
    },
    {
      "epoch": 2.8133535660091047,
      "grad_norm": 0.6713934540748596,
      "learning_rate": 1.4852048558421853e-06,
      "loss": 0.4148,
      "step": 1854
    },
    {
      "epoch": 2.8148710166919573,
      "grad_norm": 0.7424396872520447,
      "learning_rate": 1.4833080424886192e-06,
      "loss": 0.4072,
      "step": 1855
    },
    {
      "epoch": 2.8163884673748103,
      "grad_norm": 0.6192629933357239,
      "learning_rate": 1.4814112291350534e-06,
      "loss": 0.3692,
      "step": 1856
    },
    {
      "epoch": 2.8179059180576633,
      "grad_norm": 0.7226166725158691,
      "learning_rate": 1.4795144157814873e-06,
      "loss": 0.3589,
      "step": 1857
    },
    {
      "epoch": 2.819423368740516,
      "grad_norm": 0.6807238459587097,
      "learning_rate": 1.4776176024279212e-06,
      "loss": 0.3647,
      "step": 1858
    },
    {
      "epoch": 2.8209408194233685,
      "grad_norm": 0.5891393423080444,
      "learning_rate": 1.4757207890743551e-06,
      "loss": 0.2545,
      "step": 1859
    },
    {
      "epoch": 2.8224582701062215,
      "grad_norm": 0.6901318430900574,
      "learning_rate": 1.4738239757207893e-06,
      "loss": 0.3831,
      "step": 1860
    },
    {
      "epoch": 2.8239757207890746,
      "grad_norm": 0.6548179984092712,
      "learning_rate": 1.4719271623672232e-06,
      "loss": 0.3537,
      "step": 1861
    },
    {
      "epoch": 2.825493171471927,
      "grad_norm": 0.5945833325386047,
      "learning_rate": 1.4700303490136571e-06,
      "loss": 0.3073,
      "step": 1862
    },
    {
      "epoch": 2.8270106221547797,
      "grad_norm": 0.5715867280960083,
      "learning_rate": 1.4681335356600913e-06,
      "loss": 0.2873,
      "step": 1863
    },
    {
      "epoch": 2.8285280728376327,
      "grad_norm": 0.7333325147628784,
      "learning_rate": 1.4662367223065252e-06,
      "loss": 0.372,
      "step": 1864
    },
    {
      "epoch": 2.8300455235204858,
      "grad_norm": 0.668951153755188,
      "learning_rate": 1.4643399089529591e-06,
      "loss": 0.3683,
      "step": 1865
    },
    {
      "epoch": 2.8315629742033384,
      "grad_norm": 0.5879345536231995,
      "learning_rate": 1.4624430955993932e-06,
      "loss": 0.3915,
      "step": 1866
    },
    {
      "epoch": 2.833080424886191,
      "grad_norm": 0.6223472356796265,
      "learning_rate": 1.4605462822458272e-06,
      "loss": 0.3226,
      "step": 1867
    },
    {
      "epoch": 2.834597875569044,
      "grad_norm": 0.5799634456634521,
      "learning_rate": 1.458649468892261e-06,
      "loss": 0.3672,
      "step": 1868
    },
    {
      "epoch": 2.836115326251897,
      "grad_norm": 0.5815610289573669,
      "learning_rate": 1.4567526555386952e-06,
      "loss": 0.3533,
      "step": 1869
    },
    {
      "epoch": 2.8376327769347496,
      "grad_norm": 0.6460240483283997,
      "learning_rate": 1.4548558421851292e-06,
      "loss": 0.3628,
      "step": 1870
    },
    {
      "epoch": 2.839150227617602,
      "grad_norm": 0.826854944229126,
      "learning_rate": 1.452959028831563e-06,
      "loss": 0.2891,
      "step": 1871
    },
    {
      "epoch": 2.840667678300455,
      "grad_norm": 0.616378128528595,
      "learning_rate": 1.4510622154779972e-06,
      "loss": 0.3407,
      "step": 1872
    },
    {
      "epoch": 2.842185128983308,
      "grad_norm": 0.6427915096282959,
      "learning_rate": 1.4491654021244311e-06,
      "loss": 0.3385,
      "step": 1873
    },
    {
      "epoch": 2.843702579666161,
      "grad_norm": 0.7536519169807434,
      "learning_rate": 1.447268588770865e-06,
      "loss": 0.414,
      "step": 1874
    },
    {
      "epoch": 2.8452200303490134,
      "grad_norm": 0.7423998713493347,
      "learning_rate": 1.4453717754172992e-06,
      "loss": 0.3745,
      "step": 1875
    },
    {
      "epoch": 2.8467374810318664,
      "grad_norm": 0.6436678171157837,
      "learning_rate": 1.4434749620637331e-06,
      "loss": 0.3927,
      "step": 1876
    },
    {
      "epoch": 2.8482549317147194,
      "grad_norm": 0.7207207679748535,
      "learning_rate": 1.441578148710167e-06,
      "loss": 0.3665,
      "step": 1877
    },
    {
      "epoch": 2.849772382397572,
      "grad_norm": 0.6031435132026672,
      "learning_rate": 1.4396813353566012e-06,
      "loss": 0.2532,
      "step": 1878
    },
    {
      "epoch": 2.851289833080425,
      "grad_norm": 0.5939061045646667,
      "learning_rate": 1.4377845220030351e-06,
      "loss": 0.2917,
      "step": 1879
    },
    {
      "epoch": 2.8528072837632776,
      "grad_norm": 0.7447800636291504,
      "learning_rate": 1.435887708649469e-06,
      "loss": 0.3661,
      "step": 1880
    },
    {
      "epoch": 2.8543247344461307,
      "grad_norm": 0.6347379684448242,
      "learning_rate": 1.433990895295903e-06,
      "loss": 0.3445,
      "step": 1881
    },
    {
      "epoch": 2.8558421851289832,
      "grad_norm": 0.7258802652359009,
      "learning_rate": 1.4320940819423371e-06,
      "loss": 0.3589,
      "step": 1882
    },
    {
      "epoch": 2.8573596358118363,
      "grad_norm": 0.5890679955482483,
      "learning_rate": 1.430197268588771e-06,
      "loss": 0.2717,
      "step": 1883
    },
    {
      "epoch": 2.858877086494689,
      "grad_norm": 0.7628423571586609,
      "learning_rate": 1.428300455235205e-06,
      "loss": 0.3582,
      "step": 1884
    },
    {
      "epoch": 2.860394537177542,
      "grad_norm": 0.6019598841667175,
      "learning_rate": 1.426403641881639e-06,
      "loss": 0.383,
      "step": 1885
    },
    {
      "epoch": 2.8619119878603945,
      "grad_norm": 0.7229456901550293,
      "learning_rate": 1.424506828528073e-06,
      "loss": 0.4069,
      "step": 1886
    },
    {
      "epoch": 2.8634294385432475,
      "grad_norm": 0.6252616047859192,
      "learning_rate": 1.422610015174507e-06,
      "loss": 0.3144,
      "step": 1887
    },
    {
      "epoch": 2.8649468892261,
      "grad_norm": 0.6460568904876709,
      "learning_rate": 1.420713201820941e-06,
      "loss": 0.3697,
      "step": 1888
    },
    {
      "epoch": 2.866464339908953,
      "grad_norm": 0.6557520031929016,
      "learning_rate": 1.418816388467375e-06,
      "loss": 0.4185,
      "step": 1889
    },
    {
      "epoch": 2.8679817905918057,
      "grad_norm": 0.6504587531089783,
      "learning_rate": 1.416919575113809e-06,
      "loss": 0.3759,
      "step": 1890
    },
    {
      "epoch": 2.8694992412746587,
      "grad_norm": 0.6428312063217163,
      "learning_rate": 1.415022761760243e-06,
      "loss": 0.3129,
      "step": 1891
    },
    {
      "epoch": 2.8710166919575113,
      "grad_norm": 0.6146351099014282,
      "learning_rate": 1.413125948406677e-06,
      "loss": 0.2715,
      "step": 1892
    },
    {
      "epoch": 2.8725341426403643,
      "grad_norm": 0.6026948690414429,
      "learning_rate": 1.411229135053111e-06,
      "loss": 0.3131,
      "step": 1893
    },
    {
      "epoch": 2.874051593323217,
      "grad_norm": 0.5845832824707031,
      "learning_rate": 1.409332321699545e-06,
      "loss": 0.3029,
      "step": 1894
    },
    {
      "epoch": 2.87556904400607,
      "grad_norm": 0.626426100730896,
      "learning_rate": 1.407435508345979e-06,
      "loss": 0.3803,
      "step": 1895
    },
    {
      "epoch": 2.8770864946889225,
      "grad_norm": 0.649156928062439,
      "learning_rate": 1.405538694992413e-06,
      "loss": 0.3709,
      "step": 1896
    },
    {
      "epoch": 2.8786039453717756,
      "grad_norm": 0.5871548652648926,
      "learning_rate": 1.403641881638847e-06,
      "loss": 0.3841,
      "step": 1897
    },
    {
      "epoch": 2.880121396054628,
      "grad_norm": 0.5941438674926758,
      "learning_rate": 1.401745068285281e-06,
      "loss": 0.2753,
      "step": 1898
    },
    {
      "epoch": 2.881638846737481,
      "grad_norm": 0.6084138751029968,
      "learning_rate": 1.399848254931715e-06,
      "loss": 0.4044,
      "step": 1899
    },
    {
      "epoch": 2.8831562974203337,
      "grad_norm": 0.777660071849823,
      "learning_rate": 1.397951441578149e-06,
      "loss": 0.2725,
      "step": 1900
    },
    {
      "epoch": 2.8846737481031868,
      "grad_norm": 0.6335766911506653,
      "learning_rate": 1.396054628224583e-06,
      "loss": 0.3295,
      "step": 1901
    },
    {
      "epoch": 2.8861911987860394,
      "grad_norm": 0.6726309061050415,
      "learning_rate": 1.3941578148710169e-06,
      "loss": 0.4154,
      "step": 1902
    },
    {
      "epoch": 2.8877086494688924,
      "grad_norm": 0.6150451302528381,
      "learning_rate": 1.3922610015174508e-06,
      "loss": 0.3828,
      "step": 1903
    },
    {
      "epoch": 2.889226100151745,
      "grad_norm": 0.5632284879684448,
      "learning_rate": 1.390364188163885e-06,
      "loss": 0.3508,
      "step": 1904
    },
    {
      "epoch": 2.890743550834598,
      "grad_norm": 0.5700386166572571,
      "learning_rate": 1.3884673748103189e-06,
      "loss": 0.2853,
      "step": 1905
    },
    {
      "epoch": 2.8922610015174506,
      "grad_norm": 0.66164630651474,
      "learning_rate": 1.3865705614567528e-06,
      "loss": 0.289,
      "step": 1906
    },
    {
      "epoch": 2.8937784522003036,
      "grad_norm": 0.6218346357345581,
      "learning_rate": 1.384673748103187e-06,
      "loss": 0.3758,
      "step": 1907
    },
    {
      "epoch": 2.895295902883156,
      "grad_norm": 0.7737839221954346,
      "learning_rate": 1.3827769347496209e-06,
      "loss": 0.3866,
      "step": 1908
    },
    {
      "epoch": 2.896813353566009,
      "grad_norm": 0.7670190334320068,
      "learning_rate": 1.3808801213960548e-06,
      "loss": 0.3225,
      "step": 1909
    },
    {
      "epoch": 2.898330804248862,
      "grad_norm": 0.705582857131958,
      "learning_rate": 1.378983308042489e-06,
      "loss": 0.3778,
      "step": 1910
    },
    {
      "epoch": 2.899848254931715,
      "grad_norm": 0.6187530159950256,
      "learning_rate": 1.3770864946889229e-06,
      "loss": 0.3557,
      "step": 1911
    },
    {
      "epoch": 2.9013657056145674,
      "grad_norm": 0.6636040210723877,
      "learning_rate": 1.3751896813353568e-06,
      "loss": 0.3497,
      "step": 1912
    },
    {
      "epoch": 2.9028831562974204,
      "grad_norm": 0.6396477222442627,
      "learning_rate": 1.3732928679817905e-06,
      "loss": 0.3589,
      "step": 1913
    },
    {
      "epoch": 2.904400606980273,
      "grad_norm": 0.6223928928375244,
      "learning_rate": 1.3713960546282246e-06,
      "loss": 0.3385,
      "step": 1914
    },
    {
      "epoch": 2.905918057663126,
      "grad_norm": 0.5656706094741821,
      "learning_rate": 1.3694992412746586e-06,
      "loss": 0.3498,
      "step": 1915
    },
    {
      "epoch": 2.9074355083459786,
      "grad_norm": 0.6303865909576416,
      "learning_rate": 1.3676024279210925e-06,
      "loss": 0.3526,
      "step": 1916
    },
    {
      "epoch": 2.9089529590288317,
      "grad_norm": 0.5790904760360718,
      "learning_rate": 1.3657056145675266e-06,
      "loss": 0.3425,
      "step": 1917
    },
    {
      "epoch": 2.9104704097116842,
      "grad_norm": 0.5929707884788513,
      "learning_rate": 1.3638088012139605e-06,
      "loss": 0.3695,
      "step": 1918
    },
    {
      "epoch": 2.9119878603945373,
      "grad_norm": 0.6258234977722168,
      "learning_rate": 1.3619119878603945e-06,
      "loss": 0.303,
      "step": 1919
    },
    {
      "epoch": 2.91350531107739,
      "grad_norm": 0.8749869465827942,
      "learning_rate": 1.3600151745068286e-06,
      "loss": 0.431,
      "step": 1920
    },
    {
      "epoch": 2.915022761760243,
      "grad_norm": 0.6712672114372253,
      "learning_rate": 1.3581183611532625e-06,
      "loss": 0.4095,
      "step": 1921
    },
    {
      "epoch": 2.9165402124430955,
      "grad_norm": 0.690752387046814,
      "learning_rate": 1.3562215477996965e-06,
      "loss": 0.4017,
      "step": 1922
    },
    {
      "epoch": 2.9180576631259485,
      "grad_norm": 0.5807600021362305,
      "learning_rate": 1.3543247344461306e-06,
      "loss": 0.4034,
      "step": 1923
    },
    {
      "epoch": 2.919575113808801,
      "grad_norm": 0.5803263187408447,
      "learning_rate": 1.3524279210925645e-06,
      "loss": 0.3511,
      "step": 1924
    },
    {
      "epoch": 2.921092564491654,
      "grad_norm": 0.6050631403923035,
      "learning_rate": 1.3505311077389985e-06,
      "loss": 0.354,
      "step": 1925
    },
    {
      "epoch": 2.9226100151745067,
      "grad_norm": 0.8025647401809692,
      "learning_rate": 1.3486342943854324e-06,
      "loss": 0.2874,
      "step": 1926
    },
    {
      "epoch": 2.9241274658573597,
      "grad_norm": 0.581550121307373,
      "learning_rate": 1.3467374810318665e-06,
      "loss": 0.32,
      "step": 1927
    },
    {
      "epoch": 2.9256449165402123,
      "grad_norm": 0.5802764892578125,
      "learning_rate": 1.3448406676783004e-06,
      "loss": 0.3945,
      "step": 1928
    },
    {
      "epoch": 2.9271623672230653,
      "grad_norm": 0.6169623732566833,
      "learning_rate": 1.3429438543247344e-06,
      "loss": 0.3709,
      "step": 1929
    },
    {
      "epoch": 2.928679817905918,
      "grad_norm": 0.6534901261329651,
      "learning_rate": 1.3410470409711685e-06,
      "loss": 0.4284,
      "step": 1930
    },
    {
      "epoch": 2.930197268588771,
      "grad_norm": 0.7688989639282227,
      "learning_rate": 1.3391502276176024e-06,
      "loss": 0.3146,
      "step": 1931
    },
    {
      "epoch": 2.9317147192716235,
      "grad_norm": 0.6298232674598694,
      "learning_rate": 1.3372534142640364e-06,
      "loss": 0.3099,
      "step": 1932
    },
    {
      "epoch": 2.9332321699544766,
      "grad_norm": 0.6544803380966187,
      "learning_rate": 1.3353566009104705e-06,
      "loss": 0.3327,
      "step": 1933
    },
    {
      "epoch": 2.934749620637329,
      "grad_norm": 0.6646388173103333,
      "learning_rate": 1.3334597875569044e-06,
      "loss": 0.3558,
      "step": 1934
    },
    {
      "epoch": 2.936267071320182,
      "grad_norm": 0.6938490867614746,
      "learning_rate": 1.3315629742033383e-06,
      "loss": 0.3512,
      "step": 1935
    },
    {
      "epoch": 2.9377845220030347,
      "grad_norm": 0.5734529495239258,
      "learning_rate": 1.3296661608497725e-06,
      "loss": 0.3569,
      "step": 1936
    },
    {
      "epoch": 2.9393019726858878,
      "grad_norm": 0.6362622976303101,
      "learning_rate": 1.3277693474962064e-06,
      "loss": 0.2682,
      "step": 1937
    },
    {
      "epoch": 2.9408194233687404,
      "grad_norm": 0.583694338798523,
      "learning_rate": 1.3258725341426403e-06,
      "loss": 0.3445,
      "step": 1938
    },
    {
      "epoch": 2.9423368740515934,
      "grad_norm": 0.6461304426193237,
      "learning_rate": 1.3239757207890745e-06,
      "loss": 0.3278,
      "step": 1939
    },
    {
      "epoch": 2.943854324734446,
      "grad_norm": 0.5893151760101318,
      "learning_rate": 1.3220789074355084e-06,
      "loss": 0.3214,
      "step": 1940
    },
    {
      "epoch": 2.945371775417299,
      "grad_norm": 0.6233962178230286,
      "learning_rate": 1.3201820940819423e-06,
      "loss": 0.3824,
      "step": 1941
    },
    {
      "epoch": 2.9468892261001516,
      "grad_norm": 0.6906160712242126,
      "learning_rate": 1.3182852807283765e-06,
      "loss": 0.3458,
      "step": 1942
    },
    {
      "epoch": 2.9484066767830046,
      "grad_norm": 0.6927912831306458,
      "learning_rate": 1.3163884673748104e-06,
      "loss": 0.3622,
      "step": 1943
    },
    {
      "epoch": 2.949924127465857,
      "grad_norm": 0.6091412901878357,
      "learning_rate": 1.3144916540212443e-06,
      "loss": 0.2784,
      "step": 1944
    },
    {
      "epoch": 2.95144157814871,
      "grad_norm": 0.5677565932273865,
      "learning_rate": 1.3125948406676784e-06,
      "loss": 0.3056,
      "step": 1945
    },
    {
      "epoch": 2.952959028831563,
      "grad_norm": 0.7563502788543701,
      "learning_rate": 1.3106980273141124e-06,
      "loss": 0.3427,
      "step": 1946
    },
    {
      "epoch": 2.954476479514416,
      "grad_norm": 0.7701770663261414,
      "learning_rate": 1.3088012139605463e-06,
      "loss": 0.4434,
      "step": 1947
    },
    {
      "epoch": 2.955993930197269,
      "grad_norm": 0.5986878871917725,
      "learning_rate": 1.3069044006069802e-06,
      "loss": 0.3612,
      "step": 1948
    },
    {
      "epoch": 2.9575113808801214,
      "grad_norm": 0.7301535606384277,
      "learning_rate": 1.3050075872534144e-06,
      "loss": 0.4062,
      "step": 1949
    },
    {
      "epoch": 2.959028831562974,
      "grad_norm": 0.6227510571479797,
      "learning_rate": 1.3031107738998483e-06,
      "loss": 0.3112,
      "step": 1950
    },
    {
      "epoch": 2.960546282245827,
      "grad_norm": 0.6458553075790405,
      "learning_rate": 1.3012139605462822e-06,
      "loss": 0.3247,
      "step": 1951
    },
    {
      "epoch": 2.96206373292868,
      "grad_norm": 0.8457050323486328,
      "learning_rate": 1.2993171471927163e-06,
      "loss": 0.3406,
      "step": 1952
    },
    {
      "epoch": 2.9635811836115327,
      "grad_norm": 0.71458899974823,
      "learning_rate": 1.2974203338391503e-06,
      "loss": 0.3197,
      "step": 1953
    },
    {
      "epoch": 2.9650986342943852,
      "grad_norm": 0.5813363790512085,
      "learning_rate": 1.2955235204855842e-06,
      "loss": 0.351,
      "step": 1954
    },
    {
      "epoch": 2.9666160849772383,
      "grad_norm": 0.6075373291969299,
      "learning_rate": 1.2936267071320183e-06,
      "loss": 0.3074,
      "step": 1955
    },
    {
      "epoch": 2.9681335356600913,
      "grad_norm": 0.6078566908836365,
      "learning_rate": 1.2917298937784523e-06,
      "loss": 0.3294,
      "step": 1956
    },
    {
      "epoch": 2.969650986342944,
      "grad_norm": 0.6570368409156799,
      "learning_rate": 1.2898330804248862e-06,
      "loss": 0.3665,
      "step": 1957
    },
    {
      "epoch": 2.9711684370257965,
      "grad_norm": 0.8049388527870178,
      "learning_rate": 1.2879362670713203e-06,
      "loss": 0.3649,
      "step": 1958
    },
    {
      "epoch": 2.9726858877086495,
      "grad_norm": 0.8948584198951721,
      "learning_rate": 1.2860394537177542e-06,
      "loss": 0.2933,
      "step": 1959
    },
    {
      "epoch": 2.9742033383915025,
      "grad_norm": 0.6364725232124329,
      "learning_rate": 1.2841426403641882e-06,
      "loss": 0.3381,
      "step": 1960
    },
    {
      "epoch": 2.975720789074355,
      "grad_norm": 0.6045517325401306,
      "learning_rate": 1.2822458270106223e-06,
      "loss": 0.3367,
      "step": 1961
    },
    {
      "epoch": 2.9772382397572077,
      "grad_norm": 0.6656608581542969,
      "learning_rate": 1.2803490136570562e-06,
      "loss": 0.3174,
      "step": 1962
    },
    {
      "epoch": 2.9787556904400607,
      "grad_norm": 0.7101172208786011,
      "learning_rate": 1.2784522003034902e-06,
      "loss": 0.3096,
      "step": 1963
    },
    {
      "epoch": 2.9802731411229137,
      "grad_norm": 0.6270895004272461,
      "learning_rate": 1.2765553869499243e-06,
      "loss": 0.3643,
      "step": 1964
    },
    {
      "epoch": 2.9817905918057663,
      "grad_norm": 0.8006086945533752,
      "learning_rate": 1.2746585735963582e-06,
      "loss": 0.3267,
      "step": 1965
    },
    {
      "epoch": 2.983308042488619,
      "grad_norm": 0.6553527116775513,
      "learning_rate": 1.2727617602427921e-06,
      "loss": 0.4085,
      "step": 1966
    },
    {
      "epoch": 2.984825493171472,
      "grad_norm": 0.6837402582168579,
      "learning_rate": 1.2708649468892263e-06,
      "loss": 0.2965,
      "step": 1967
    },
    {
      "epoch": 2.986342943854325,
      "grad_norm": 0.5316739678382874,
      "learning_rate": 1.2689681335356602e-06,
      "loss": 0.3428,
      "step": 1968
    },
    {
      "epoch": 2.9878603945371776,
      "grad_norm": 0.9545716047286987,
      "learning_rate": 1.2670713201820941e-06,
      "loss": 0.3737,
      "step": 1969
    },
    {
      "epoch": 2.98937784522003,
      "grad_norm": 0.604522705078125,
      "learning_rate": 1.265174506828528e-06,
      "loss": 0.3921,
      "step": 1970
    },
    {
      "epoch": 2.990895295902883,
      "grad_norm": 0.6412726640701294,
      "learning_rate": 1.2632776934749622e-06,
      "loss": 0.4114,
      "step": 1971
    },
    {
      "epoch": 2.992412746585736,
      "grad_norm": 0.6211541295051575,
      "learning_rate": 1.2613808801213961e-06,
      "loss": 0.3121,
      "step": 1972
    },
    {
      "epoch": 2.9939301972685888,
      "grad_norm": 0.6888012886047363,
      "learning_rate": 1.25948406676783e-06,
      "loss": 0.4005,
      "step": 1973
    },
    {
      "epoch": 2.9954476479514414,
      "grad_norm": 0.6140921711921692,
      "learning_rate": 1.2575872534142642e-06,
      "loss": 0.3061,
      "step": 1974
    },
    {
      "epoch": 2.9969650986342944,
      "grad_norm": 0.6691879630088806,
      "learning_rate": 1.2556904400606981e-06,
      "loss": 0.4086,
      "step": 1975
    },
    {
      "epoch": 2.9984825493171474,
      "grad_norm": 0.7065466642379761,
      "learning_rate": 1.253793626707132e-06,
      "loss": 0.3708,
      "step": 1976
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.8081181049346924,
      "learning_rate": 1.2518968133535662e-06,
      "loss": 0.4144,
      "step": 1977
    },
    {
      "epoch": 3.001517450682853,
      "grad_norm": 0.9149163961410522,
      "learning_rate": 1.25e-06,
      "loss": 0.3722,
      "step": 1978
    },
    {
      "epoch": 3.0030349013657056,
      "grad_norm": 0.5673040151596069,
      "learning_rate": 1.248103186646434e-06,
      "loss": 0.3953,
      "step": 1979
    },
    {
      "epoch": 3.0045523520485586,
      "grad_norm": 0.652927577495575,
      "learning_rate": 1.2462063732928682e-06,
      "loss": 0.3477,
      "step": 1980
    },
    {
      "epoch": 3.0060698027314112,
      "grad_norm": 0.6729360818862915,
      "learning_rate": 1.244309559939302e-06,
      "loss": 0.3459,
      "step": 1981
    },
    {
      "epoch": 3.0075872534142643,
      "grad_norm": 0.6646113991737366,
      "learning_rate": 1.242412746585736e-06,
      "loss": 0.3306,
      "step": 1982
    },
    {
      "epoch": 3.009104704097117,
      "grad_norm": 0.6604402661323547,
      "learning_rate": 1.2405159332321702e-06,
      "loss": 0.3362,
      "step": 1983
    },
    {
      "epoch": 3.01062215477997,
      "grad_norm": 0.6607497334480286,
      "learning_rate": 1.238619119878604e-06,
      "loss": 0.3248,
      "step": 1984
    },
    {
      "epoch": 3.0121396054628224,
      "grad_norm": 0.703497588634491,
      "learning_rate": 1.236722306525038e-06,
      "loss": 0.3145,
      "step": 1985
    },
    {
      "epoch": 3.0136570561456755,
      "grad_norm": 0.683040201663971,
      "learning_rate": 1.2348254931714721e-06,
      "loss": 0.3605,
      "step": 1986
    },
    {
      "epoch": 3.015174506828528,
      "grad_norm": 0.5839719772338867,
      "learning_rate": 1.232928679817906e-06,
      "loss": 0.3447,
      "step": 1987
    },
    {
      "epoch": 3.016691957511381,
      "grad_norm": 0.6196966767311096,
      "learning_rate": 1.23103186646434e-06,
      "loss": 0.3343,
      "step": 1988
    },
    {
      "epoch": 3.0182094081942337,
      "grad_norm": 0.6303812861442566,
      "learning_rate": 1.229135053110774e-06,
      "loss": 0.3616,
      "step": 1989
    },
    {
      "epoch": 3.0197268588770867,
      "grad_norm": 0.6556636691093445,
      "learning_rate": 1.227238239757208e-06,
      "loss": 0.2853,
      "step": 1990
    },
    {
      "epoch": 3.0212443095599393,
      "grad_norm": 0.6869394183158875,
      "learning_rate": 1.225341426403642e-06,
      "loss": 0.3797,
      "step": 1991
    },
    {
      "epoch": 3.0227617602427923,
      "grad_norm": 0.6345367431640625,
      "learning_rate": 1.223444613050076e-06,
      "loss": 0.3789,
      "step": 1992
    },
    {
      "epoch": 3.024279210925645,
      "grad_norm": 0.7084026336669922,
      "learning_rate": 1.22154779969651e-06,
      "loss": 0.3897,
      "step": 1993
    },
    {
      "epoch": 3.025796661608498,
      "grad_norm": 0.7656722664833069,
      "learning_rate": 1.219650986342944e-06,
      "loss": 0.36,
      "step": 1994
    },
    {
      "epoch": 3.0273141122913505,
      "grad_norm": 0.638256847858429,
      "learning_rate": 1.217754172989378e-06,
      "loss": 0.3063,
      "step": 1995
    },
    {
      "epoch": 3.0288315629742035,
      "grad_norm": 0.6503910422325134,
      "learning_rate": 1.215857359635812e-06,
      "loss": 0.3582,
      "step": 1996
    },
    {
      "epoch": 3.030349013657056,
      "grad_norm": 0.5465824604034424,
      "learning_rate": 1.213960546282246e-06,
      "loss": 0.3234,
      "step": 1997
    },
    {
      "epoch": 3.031866464339909,
      "grad_norm": 0.6470696330070496,
      "learning_rate": 1.2120637329286799e-06,
      "loss": 0.3771,
      "step": 1998
    },
    {
      "epoch": 3.0333839150227617,
      "grad_norm": 0.5914654731750488,
      "learning_rate": 1.210166919575114e-06,
      "loss": 0.302,
      "step": 1999
    },
    {
      "epoch": 3.0349013657056148,
      "grad_norm": 0.6126951575279236,
      "learning_rate": 1.208270106221548e-06,
      "loss": 0.3413,
      "step": 2000
    },
    {
      "epoch": 3.0364188163884673,
      "grad_norm": 0.752871036529541,
      "learning_rate": 1.2063732928679819e-06,
      "loss": 0.4137,
      "step": 2001
    },
    {
      "epoch": 3.0379362670713204,
      "grad_norm": 0.7481623888015747,
      "learning_rate": 1.204476479514416e-06,
      "loss": 0.3682,
      "step": 2002
    },
    {
      "epoch": 3.039453717754173,
      "grad_norm": 0.5857976078987122,
      "learning_rate": 1.20257966616085e-06,
      "loss": 0.4253,
      "step": 2003
    },
    {
      "epoch": 3.040971168437026,
      "grad_norm": 0.6127939224243164,
      "learning_rate": 1.2006828528072839e-06,
      "loss": 0.353,
      "step": 2004
    },
    {
      "epoch": 3.0424886191198786,
      "grad_norm": 0.5919327139854431,
      "learning_rate": 1.198786039453718e-06,
      "loss": 0.3406,
      "step": 2005
    },
    {
      "epoch": 3.0440060698027316,
      "grad_norm": 0.7866817116737366,
      "learning_rate": 1.196889226100152e-06,
      "loss": 0.3713,
      "step": 2006
    },
    {
      "epoch": 3.045523520485584,
      "grad_norm": 0.6647273898124695,
      "learning_rate": 1.1949924127465858e-06,
      "loss": 0.3614,
      "step": 2007
    },
    {
      "epoch": 3.047040971168437,
      "grad_norm": 0.6354853510856628,
      "learning_rate": 1.19309559939302e-06,
      "loss": 0.3013,
      "step": 2008
    },
    {
      "epoch": 3.04855842185129,
      "grad_norm": 0.5950138568878174,
      "learning_rate": 1.191198786039454e-06,
      "loss": 0.4,
      "step": 2009
    },
    {
      "epoch": 3.050075872534143,
      "grad_norm": 0.6366077065467834,
      "learning_rate": 1.1893019726858878e-06,
      "loss": 0.3267,
      "step": 2010
    },
    {
      "epoch": 3.0515933232169954,
      "grad_norm": 0.5523114800453186,
      "learning_rate": 1.1874051593323218e-06,
      "loss": 0.4062,
      "step": 2011
    },
    {
      "epoch": 3.0531107738998484,
      "grad_norm": 0.7005218267440796,
      "learning_rate": 1.1855083459787557e-06,
      "loss": 0.3626,
      "step": 2012
    },
    {
      "epoch": 3.054628224582701,
      "grad_norm": 0.5871080756187439,
      "learning_rate": 1.1836115326251896e-06,
      "loss": 0.3379,
      "step": 2013
    },
    {
      "epoch": 3.056145675265554,
      "grad_norm": 0.659699022769928,
      "learning_rate": 1.1817147192716237e-06,
      "loss": 0.301,
      "step": 2014
    },
    {
      "epoch": 3.0576631259484066,
      "grad_norm": 0.6232814192771912,
      "learning_rate": 1.1798179059180577e-06,
      "loss": 0.3925,
      "step": 2015
    },
    {
      "epoch": 3.0591805766312596,
      "grad_norm": 0.5807701945304871,
      "learning_rate": 1.1779210925644916e-06,
      "loss": 0.3731,
      "step": 2016
    },
    {
      "epoch": 3.0606980273141122,
      "grad_norm": 0.6723079085350037,
      "learning_rate": 1.1760242792109257e-06,
      "loss": 0.3473,
      "step": 2017
    },
    {
      "epoch": 3.0622154779969653,
      "grad_norm": 0.6673790216445923,
      "learning_rate": 1.1741274658573597e-06,
      "loss": 0.3516,
      "step": 2018
    },
    {
      "epoch": 3.063732928679818,
      "grad_norm": 0.6229419708251953,
      "learning_rate": 1.1722306525037936e-06,
      "loss": 0.3321,
      "step": 2019
    },
    {
      "epoch": 3.065250379362671,
      "grad_norm": 0.6254156827926636,
      "learning_rate": 1.1703338391502277e-06,
      "loss": 0.4349,
      "step": 2020
    },
    {
      "epoch": 3.0667678300455234,
      "grad_norm": 0.6795393228530884,
      "learning_rate": 1.1684370257966617e-06,
      "loss": 0.3964,
      "step": 2021
    },
    {
      "epoch": 3.0682852807283765,
      "grad_norm": 0.7111584544181824,
      "learning_rate": 1.1665402124430956e-06,
      "loss": 0.3901,
      "step": 2022
    },
    {
      "epoch": 3.069802731411229,
      "grad_norm": 0.6218881011009216,
      "learning_rate": 1.1646433990895297e-06,
      "loss": 0.3418,
      "step": 2023
    },
    {
      "epoch": 3.071320182094082,
      "grad_norm": 0.7061963081359863,
      "learning_rate": 1.1627465857359636e-06,
      "loss": 0.3531,
      "step": 2024
    },
    {
      "epoch": 3.0728376327769347,
      "grad_norm": 0.70872563123703,
      "learning_rate": 1.1608497723823976e-06,
      "loss": 0.3211,
      "step": 2025
    },
    {
      "epoch": 3.0743550834597877,
      "grad_norm": 0.6100803017616272,
      "learning_rate": 1.1589529590288317e-06,
      "loss": 0.3647,
      "step": 2026
    },
    {
      "epoch": 3.0758725341426403,
      "grad_norm": 0.5826167464256287,
      "learning_rate": 1.1570561456752656e-06,
      "loss": 0.3165,
      "step": 2027
    },
    {
      "epoch": 3.0773899848254933,
      "grad_norm": 0.7182222008705139,
      "learning_rate": 1.1551593323216996e-06,
      "loss": 0.3456,
      "step": 2028
    },
    {
      "epoch": 3.078907435508346,
      "grad_norm": 0.7117849588394165,
      "learning_rate": 1.1532625189681337e-06,
      "loss": 0.3221,
      "step": 2029
    },
    {
      "epoch": 3.080424886191199,
      "grad_norm": 0.5934374928474426,
      "learning_rate": 1.1513657056145676e-06,
      "loss": 0.3049,
      "step": 2030
    },
    {
      "epoch": 3.0819423368740515,
      "grad_norm": 0.5854859948158264,
      "learning_rate": 1.1494688922610015e-06,
      "loss": 0.3142,
      "step": 2031
    },
    {
      "epoch": 3.0834597875569045,
      "grad_norm": 0.6589733958244324,
      "learning_rate": 1.1475720789074357e-06,
      "loss": 0.27,
      "step": 2032
    },
    {
      "epoch": 3.084977238239757,
      "grad_norm": 0.7374523878097534,
      "learning_rate": 1.1456752655538696e-06,
      "loss": 0.3074,
      "step": 2033
    },
    {
      "epoch": 3.08649468892261,
      "grad_norm": 0.6261499524116516,
      "learning_rate": 1.1437784522003035e-06,
      "loss": 0.3467,
      "step": 2034
    },
    {
      "epoch": 3.0880121396054627,
      "grad_norm": 0.6742152571678162,
      "learning_rate": 1.1418816388467375e-06,
      "loss": 0.379,
      "step": 2035
    },
    {
      "epoch": 3.0895295902883158,
      "grad_norm": 0.6112627387046814,
      "learning_rate": 1.1399848254931716e-06,
      "loss": 0.3556,
      "step": 2036
    },
    {
      "epoch": 3.0910470409711683,
      "grad_norm": 0.6689536571502686,
      "learning_rate": 1.1380880121396055e-06,
      "loss": 0.417,
      "step": 2037
    },
    {
      "epoch": 3.0925644916540214,
      "grad_norm": 0.6150298714637756,
      "learning_rate": 1.1361911987860394e-06,
      "loss": 0.3166,
      "step": 2038
    },
    {
      "epoch": 3.094081942336874,
      "grad_norm": 0.66230708360672,
      "learning_rate": 1.1342943854324736e-06,
      "loss": 0.3452,
      "step": 2039
    },
    {
      "epoch": 3.095599393019727,
      "grad_norm": 0.6647049784660339,
      "learning_rate": 1.1323975720789075e-06,
      "loss": 0.3976,
      "step": 2040
    },
    {
      "epoch": 3.0971168437025796,
      "grad_norm": 0.5933053493499756,
      "learning_rate": 1.1305007587253414e-06,
      "loss": 0.3761,
      "step": 2041
    },
    {
      "epoch": 3.0986342943854326,
      "grad_norm": 0.6740632653236389,
      "learning_rate": 1.1286039453717756e-06,
      "loss": 0.3243,
      "step": 2042
    },
    {
      "epoch": 3.100151745068285,
      "grad_norm": 0.6354049444198608,
      "learning_rate": 1.1267071320182095e-06,
      "loss": 0.3044,
      "step": 2043
    },
    {
      "epoch": 3.101669195751138,
      "grad_norm": 0.5931351184844971,
      "learning_rate": 1.1248103186646434e-06,
      "loss": 0.3695,
      "step": 2044
    },
    {
      "epoch": 3.103186646433991,
      "grad_norm": 0.5974281430244446,
      "learning_rate": 1.1229135053110776e-06,
      "loss": 0.3025,
      "step": 2045
    },
    {
      "epoch": 3.104704097116844,
      "grad_norm": 0.6460341215133667,
      "learning_rate": 1.1210166919575115e-06,
      "loss": 0.3319,
      "step": 2046
    },
    {
      "epoch": 3.1062215477996964,
      "grad_norm": 0.6082801818847656,
      "learning_rate": 1.1191198786039454e-06,
      "loss": 0.3173,
      "step": 2047
    },
    {
      "epoch": 3.1077389984825494,
      "grad_norm": 1.4554401636123657,
      "learning_rate": 1.1172230652503795e-06,
      "loss": 0.349,
      "step": 2048
    },
    {
      "epoch": 3.109256449165402,
      "grad_norm": 0.595453143119812,
      "learning_rate": 1.1153262518968135e-06,
      "loss": 0.3419,
      "step": 2049
    },
    {
      "epoch": 3.110773899848255,
      "grad_norm": 0.6636209487915039,
      "learning_rate": 1.1134294385432474e-06,
      "loss": 0.3497,
      "step": 2050
    },
    {
      "epoch": 3.1122913505311076,
      "grad_norm": 0.8119252324104309,
      "learning_rate": 1.1115326251896815e-06,
      "loss": 0.3358,
      "step": 2051
    },
    {
      "epoch": 3.1138088012139606,
      "grad_norm": 0.6563758254051208,
      "learning_rate": 1.1096358118361155e-06,
      "loss": 0.3212,
      "step": 2052
    },
    {
      "epoch": 3.1153262518968132,
      "grad_norm": 0.6610084772109985,
      "learning_rate": 1.1077389984825494e-06,
      "loss": 0.2691,
      "step": 2053
    },
    {
      "epoch": 3.1168437025796663,
      "grad_norm": 0.6523894667625427,
      "learning_rate": 1.1058421851289835e-06,
      "loss": 0.3643,
      "step": 2054
    },
    {
      "epoch": 3.118361153262519,
      "grad_norm": 0.6158684492111206,
      "learning_rate": 1.1039453717754174e-06,
      "loss": 0.2657,
      "step": 2055
    },
    {
      "epoch": 3.119878603945372,
      "grad_norm": 0.6931095719337463,
      "learning_rate": 1.1020485584218514e-06,
      "loss": 0.3641,
      "step": 2056
    },
    {
      "epoch": 3.1213960546282244,
      "grad_norm": 0.6624704599380493,
      "learning_rate": 1.1001517450682853e-06,
      "loss": 0.2957,
      "step": 2057
    },
    {
      "epoch": 3.1229135053110775,
      "grad_norm": 0.6536478996276855,
      "learning_rate": 1.0982549317147194e-06,
      "loss": 0.4191,
      "step": 2058
    },
    {
      "epoch": 3.12443095599393,
      "grad_norm": 0.5408284068107605,
      "learning_rate": 1.0963581183611534e-06,
      "loss": 0.3563,
      "step": 2059
    },
    {
      "epoch": 3.125948406676783,
      "grad_norm": 0.6267191767692566,
      "learning_rate": 1.0944613050075873e-06,
      "loss": 0.411,
      "step": 2060
    },
    {
      "epoch": 3.1274658573596357,
      "grad_norm": 0.6606989502906799,
      "learning_rate": 1.0925644916540214e-06,
      "loss": 0.3684,
      "step": 2061
    },
    {
      "epoch": 3.1289833080424887,
      "grad_norm": 0.7381962537765503,
      "learning_rate": 1.0906676783004554e-06,
      "loss": 0.3383,
      "step": 2062
    },
    {
      "epoch": 3.1305007587253413,
      "grad_norm": 0.8962039351463318,
      "learning_rate": 1.0887708649468893e-06,
      "loss": 0.4646,
      "step": 2063
    },
    {
      "epoch": 3.1320182094081943,
      "grad_norm": 0.7277424335479736,
      "learning_rate": 1.0868740515933234e-06,
      "loss": 0.3953,
      "step": 2064
    },
    {
      "epoch": 3.133535660091047,
      "grad_norm": 0.627643883228302,
      "learning_rate": 1.0849772382397573e-06,
      "loss": 0.3345,
      "step": 2065
    },
    {
      "epoch": 3.1350531107739,
      "grad_norm": 0.689551591873169,
      "learning_rate": 1.0830804248861913e-06,
      "loss": 0.3392,
      "step": 2066
    },
    {
      "epoch": 3.1365705614567525,
      "grad_norm": 0.639731228351593,
      "learning_rate": 1.0811836115326254e-06,
      "loss": 0.3628,
      "step": 2067
    },
    {
      "epoch": 3.1380880121396055,
      "grad_norm": 0.7992318272590637,
      "learning_rate": 1.0792867981790593e-06,
      "loss": 0.369,
      "step": 2068
    },
    {
      "epoch": 3.139605462822458,
      "grad_norm": 0.6562574505805969,
      "learning_rate": 1.0773899848254933e-06,
      "loss": 0.2771,
      "step": 2069
    },
    {
      "epoch": 3.141122913505311,
      "grad_norm": 0.7224963307380676,
      "learning_rate": 1.0754931714719274e-06,
      "loss": 0.3421,
      "step": 2070
    },
    {
      "epoch": 3.1426403641881637,
      "grad_norm": 0.637843132019043,
      "learning_rate": 1.0735963581183613e-06,
      "loss": 0.2837,
      "step": 2071
    },
    {
      "epoch": 3.1441578148710168,
      "grad_norm": 0.6009637117385864,
      "learning_rate": 1.0716995447647952e-06,
      "loss": 0.3992,
      "step": 2072
    },
    {
      "epoch": 3.1456752655538693,
      "grad_norm": 0.6487325429916382,
      "learning_rate": 1.0698027314112294e-06,
      "loss": 0.2898,
      "step": 2073
    },
    {
      "epoch": 3.1471927162367224,
      "grad_norm": 0.6503043174743652,
      "learning_rate": 1.0679059180576633e-06,
      "loss": 0.3929,
      "step": 2074
    },
    {
      "epoch": 3.148710166919575,
      "grad_norm": 0.6676070690155029,
      "learning_rate": 1.0660091047040972e-06,
      "loss": 0.2905,
      "step": 2075
    },
    {
      "epoch": 3.150227617602428,
      "grad_norm": 0.6446376442909241,
      "learning_rate": 1.0641122913505314e-06,
      "loss": 0.3346,
      "step": 2076
    },
    {
      "epoch": 3.1517450682852806,
      "grad_norm": 0.6780725717544556,
      "learning_rate": 1.062215477996965e-06,
      "loss": 0.3011,
      "step": 2077
    },
    {
      "epoch": 3.1532625189681336,
      "grad_norm": 0.8064426183700562,
      "learning_rate": 1.060318664643399e-06,
      "loss": 0.4018,
      "step": 2078
    },
    {
      "epoch": 3.154779969650986,
      "grad_norm": 0.5916526913642883,
      "learning_rate": 1.0584218512898331e-06,
      "loss": 0.3786,
      "step": 2079
    },
    {
      "epoch": 3.156297420333839,
      "grad_norm": 0.7132896780967712,
      "learning_rate": 1.056525037936267e-06,
      "loss": 0.2743,
      "step": 2080
    },
    {
      "epoch": 3.157814871016692,
      "grad_norm": 0.6499634385108948,
      "learning_rate": 1.054628224582701e-06,
      "loss": 0.3116,
      "step": 2081
    },
    {
      "epoch": 3.159332321699545,
      "grad_norm": 0.6197466850280762,
      "learning_rate": 1.0527314112291351e-06,
      "loss": 0.3624,
      "step": 2082
    },
    {
      "epoch": 3.1608497723823974,
      "grad_norm": 0.6426369547843933,
      "learning_rate": 1.050834597875569e-06,
      "loss": 0.3247,
      "step": 2083
    },
    {
      "epoch": 3.1623672230652504,
      "grad_norm": 0.6145386695861816,
      "learning_rate": 1.048937784522003e-06,
      "loss": 0.3444,
      "step": 2084
    },
    {
      "epoch": 3.163884673748103,
      "grad_norm": 0.6359599232673645,
      "learning_rate": 1.0470409711684371e-06,
      "loss": 0.308,
      "step": 2085
    },
    {
      "epoch": 3.165402124430956,
      "grad_norm": 0.7763223648071289,
      "learning_rate": 1.045144157814871e-06,
      "loss": 0.3795,
      "step": 2086
    },
    {
      "epoch": 3.1669195751138086,
      "grad_norm": 0.6405354738235474,
      "learning_rate": 1.043247344461305e-06,
      "loss": 0.3094,
      "step": 2087
    },
    {
      "epoch": 3.1684370257966616,
      "grad_norm": 0.6792430281639099,
      "learning_rate": 1.0413505311077391e-06,
      "loss": 0.3941,
      "step": 2088
    },
    {
      "epoch": 3.1699544764795142,
      "grad_norm": 0.5996597409248352,
      "learning_rate": 1.039453717754173e-06,
      "loss": 0.3012,
      "step": 2089
    },
    {
      "epoch": 3.1714719271623673,
      "grad_norm": 0.6800519824028015,
      "learning_rate": 1.037556904400607e-06,
      "loss": 0.3474,
      "step": 2090
    },
    {
      "epoch": 3.17298937784522,
      "grad_norm": 0.7137957811355591,
      "learning_rate": 1.035660091047041e-06,
      "loss": 0.3438,
      "step": 2091
    },
    {
      "epoch": 3.174506828528073,
      "grad_norm": 0.6409077644348145,
      "learning_rate": 1.033763277693475e-06,
      "loss": 0.3447,
      "step": 2092
    },
    {
      "epoch": 3.1760242792109254,
      "grad_norm": 0.6659610867500305,
      "learning_rate": 1.031866464339909e-06,
      "loss": 0.3498,
      "step": 2093
    },
    {
      "epoch": 3.1775417298937785,
      "grad_norm": 0.6105619072914124,
      "learning_rate": 1.029969650986343e-06,
      "loss": 0.2831,
      "step": 2094
    },
    {
      "epoch": 3.179059180576631,
      "grad_norm": 0.6313537359237671,
      "learning_rate": 1.028072837632777e-06,
      "loss": 0.3758,
      "step": 2095
    },
    {
      "epoch": 3.180576631259484,
      "grad_norm": 0.6041742563247681,
      "learning_rate": 1.026176024279211e-06,
      "loss": 0.346,
      "step": 2096
    },
    {
      "epoch": 3.1820940819423367,
      "grad_norm": 0.6729753017425537,
      "learning_rate": 1.024279210925645e-06,
      "loss": 0.3731,
      "step": 2097
    },
    {
      "epoch": 3.1836115326251897,
      "grad_norm": 0.6130276322364807,
      "learning_rate": 1.022382397572079e-06,
      "loss": 0.2847,
      "step": 2098
    },
    {
      "epoch": 3.1851289833080423,
      "grad_norm": 0.6517850756645203,
      "learning_rate": 1.020485584218513e-06,
      "loss": 0.3365,
      "step": 2099
    },
    {
      "epoch": 3.1866464339908953,
      "grad_norm": 0.7288710474967957,
      "learning_rate": 1.0185887708649468e-06,
      "loss": 0.3072,
      "step": 2100
    },
    {
      "epoch": 3.188163884673748,
      "grad_norm": 0.6913597583770752,
      "learning_rate": 1.016691957511381e-06,
      "loss": 0.3565,
      "step": 2101
    },
    {
      "epoch": 3.189681335356601,
      "grad_norm": 0.5271592140197754,
      "learning_rate": 1.014795144157815e-06,
      "loss": 0.2788,
      "step": 2102
    },
    {
      "epoch": 3.191198786039454,
      "grad_norm": 0.6341276168823242,
      "learning_rate": 1.0128983308042488e-06,
      "loss": 0.3667,
      "step": 2103
    },
    {
      "epoch": 3.1927162367223065,
      "grad_norm": 0.5766339302062988,
      "learning_rate": 1.011001517450683e-06,
      "loss": 0.3861,
      "step": 2104
    },
    {
      "epoch": 3.194233687405159,
      "grad_norm": 0.6356483101844788,
      "learning_rate": 1.009104704097117e-06,
      "loss": 0.3982,
      "step": 2105
    },
    {
      "epoch": 3.195751138088012,
      "grad_norm": 0.6473066806793213,
      "learning_rate": 1.0072078907435508e-06,
      "loss": 0.3322,
      "step": 2106
    },
    {
      "epoch": 3.197268588770865,
      "grad_norm": 0.5950177907943726,
      "learning_rate": 1.005311077389985e-06,
      "loss": 0.3654,
      "step": 2107
    },
    {
      "epoch": 3.1987860394537178,
      "grad_norm": 0.74159175157547,
      "learning_rate": 1.0034142640364189e-06,
      "loss": 0.3924,
      "step": 2108
    },
    {
      "epoch": 3.2003034901365703,
      "grad_norm": 0.6504355669021606,
      "learning_rate": 1.0015174506828528e-06,
      "loss": 0.2853,
      "step": 2109
    },
    {
      "epoch": 3.2018209408194234,
      "grad_norm": 0.6593113541603088,
      "learning_rate": 9.99620637329287e-07,
      "loss": 0.3907,
      "step": 2110
    },
    {
      "epoch": 3.2033383915022764,
      "grad_norm": 0.6069844365119934,
      "learning_rate": 9.977238239757209e-07,
      "loss": 0.3559,
      "step": 2111
    },
    {
      "epoch": 3.204855842185129,
      "grad_norm": 0.6500954627990723,
      "learning_rate": 9.958270106221548e-07,
      "loss": 0.3368,
      "step": 2112
    },
    {
      "epoch": 3.2063732928679816,
      "grad_norm": 0.7618291974067688,
      "learning_rate": 9.93930197268589e-07,
      "loss": 0.3548,
      "step": 2113
    },
    {
      "epoch": 3.2078907435508346,
      "grad_norm": 0.6565290093421936,
      "learning_rate": 9.920333839150229e-07,
      "loss": 0.3757,
      "step": 2114
    },
    {
      "epoch": 3.2094081942336876,
      "grad_norm": 0.5831916928291321,
      "learning_rate": 9.901365705614568e-07,
      "loss": 0.3265,
      "step": 2115
    },
    {
      "epoch": 3.21092564491654,
      "grad_norm": 0.7075350880622864,
      "learning_rate": 9.88239757207891e-07,
      "loss": 0.4182,
      "step": 2116
    },
    {
      "epoch": 3.212443095599393,
      "grad_norm": 0.597551703453064,
      "learning_rate": 9.863429438543249e-07,
      "loss": 0.4094,
      "step": 2117
    },
    {
      "epoch": 3.213960546282246,
      "grad_norm": 0.7592738270759583,
      "learning_rate": 9.844461305007588e-07,
      "loss": 0.3457,
      "step": 2118
    },
    {
      "epoch": 3.215477996965099,
      "grad_norm": 0.7039574980735779,
      "learning_rate": 9.82549317147193e-07,
      "loss": 0.3944,
      "step": 2119
    },
    {
      "epoch": 3.2169954476479514,
      "grad_norm": 0.610087513923645,
      "learning_rate": 9.806525037936268e-07,
      "loss": 0.3626,
      "step": 2120
    },
    {
      "epoch": 3.2185128983308045,
      "grad_norm": 0.6516669392585754,
      "learning_rate": 9.787556904400608e-07,
      "loss": 0.3195,
      "step": 2121
    },
    {
      "epoch": 3.220030349013657,
      "grad_norm": 0.6529734134674072,
      "learning_rate": 9.768588770864947e-07,
      "loss": 0.3965,
      "step": 2122
    },
    {
      "epoch": 3.22154779969651,
      "grad_norm": 0.5873971581459045,
      "learning_rate": 9.749620637329288e-07,
      "loss": 0.3328,
      "step": 2123
    },
    {
      "epoch": 3.2230652503793626,
      "grad_norm": 0.7414702773094177,
      "learning_rate": 9.730652503793628e-07,
      "loss": 0.3344,
      "step": 2124
    },
    {
      "epoch": 3.2245827010622157,
      "grad_norm": 0.5598865747451782,
      "learning_rate": 9.711684370257967e-07,
      "loss": 0.3665,
      "step": 2125
    },
    {
      "epoch": 3.2261001517450683,
      "grad_norm": 0.6230593919754028,
      "learning_rate": 9.692716236722308e-07,
      "loss": 0.3204,
      "step": 2126
    },
    {
      "epoch": 3.2276176024279213,
      "grad_norm": 0.6894228458404541,
      "learning_rate": 9.673748103186647e-07,
      "loss": 0.2342,
      "step": 2127
    },
    {
      "epoch": 3.229135053110774,
      "grad_norm": 0.7303526997566223,
      "learning_rate": 9.654779969650987e-07,
      "loss": 0.4382,
      "step": 2128
    },
    {
      "epoch": 3.230652503793627,
      "grad_norm": 0.6739724278450012,
      "learning_rate": 9.635811836115328e-07,
      "loss": 0.3705,
      "step": 2129
    },
    {
      "epoch": 3.2321699544764795,
      "grad_norm": 0.6051362752914429,
      "learning_rate": 9.616843702579667e-07,
      "loss": 0.3704,
      "step": 2130
    },
    {
      "epoch": 3.2336874051593325,
      "grad_norm": 0.6053832173347473,
      "learning_rate": 9.597875569044007e-07,
      "loss": 0.3489,
      "step": 2131
    },
    {
      "epoch": 3.235204855842185,
      "grad_norm": 0.5789797902107239,
      "learning_rate": 9.578907435508348e-07,
      "loss": 0.3514,
      "step": 2132
    },
    {
      "epoch": 3.236722306525038,
      "grad_norm": 0.6248273253440857,
      "learning_rate": 9.559939301972687e-07,
      "loss": 0.3577,
      "step": 2133
    },
    {
      "epoch": 3.2382397572078907,
      "grad_norm": 0.6984635591506958,
      "learning_rate": 9.540971168437026e-07,
      "loss": 0.377,
      "step": 2134
    },
    {
      "epoch": 3.2397572078907437,
      "grad_norm": 0.5696229934692383,
      "learning_rate": 9.522003034901367e-07,
      "loss": 0.2579,
      "step": 2135
    },
    {
      "epoch": 3.2412746585735963,
      "grad_norm": 0.5828537940979004,
      "learning_rate": 9.503034901365707e-07,
      "loss": 0.2927,
      "step": 2136
    },
    {
      "epoch": 3.2427921092564493,
      "grad_norm": 0.8539310097694397,
      "learning_rate": 9.484066767830046e-07,
      "loss": 0.4195,
      "step": 2137
    },
    {
      "epoch": 3.244309559939302,
      "grad_norm": 0.8148998618125916,
      "learning_rate": 9.465098634294387e-07,
      "loss": 0.3499,
      "step": 2138
    },
    {
      "epoch": 3.245827010622155,
      "grad_norm": 0.6821956634521484,
      "learning_rate": 9.446130500758727e-07,
      "loss": 0.3291,
      "step": 2139
    },
    {
      "epoch": 3.2473444613050075,
      "grad_norm": 0.6036953330039978,
      "learning_rate": 9.427162367223066e-07,
      "loss": 0.3058,
      "step": 2140
    },
    {
      "epoch": 3.2488619119878606,
      "grad_norm": 0.6346547603607178,
      "learning_rate": 9.408194233687407e-07,
      "loss": 0.3654,
      "step": 2141
    },
    {
      "epoch": 3.250379362670713,
      "grad_norm": 0.7061196565628052,
      "learning_rate": 9.389226100151747e-07,
      "loss": 0.2835,
      "step": 2142
    },
    {
      "epoch": 3.251896813353566,
      "grad_norm": 0.6270520687103271,
      "learning_rate": 9.370257966616085e-07,
      "loss": 0.3276,
      "step": 2143
    },
    {
      "epoch": 3.2534142640364188,
      "grad_norm": 0.6616700291633606,
      "learning_rate": 9.351289833080425e-07,
      "loss": 0.366,
      "step": 2144
    },
    {
      "epoch": 3.254931714719272,
      "grad_norm": 0.5514993071556091,
      "learning_rate": 9.332321699544765e-07,
      "loss": 0.3326,
      "step": 2145
    },
    {
      "epoch": 3.2564491654021244,
      "grad_norm": 0.7440006136894226,
      "learning_rate": 9.313353566009105e-07,
      "loss": 0.2942,
      "step": 2146
    },
    {
      "epoch": 3.2579666160849774,
      "grad_norm": 0.5905061364173889,
      "learning_rate": 9.294385432473445e-07,
      "loss": 0.36,
      "step": 2147
    },
    {
      "epoch": 3.25948406676783,
      "grad_norm": 0.7302789092063904,
      "learning_rate": 9.275417298937785e-07,
      "loss": 0.3853,
      "step": 2148
    },
    {
      "epoch": 3.261001517450683,
      "grad_norm": 0.7090259790420532,
      "learning_rate": 9.256449165402125e-07,
      "loss": 0.3935,
      "step": 2149
    },
    {
      "epoch": 3.2625189681335356,
      "grad_norm": 0.6707515120506287,
      "learning_rate": 9.237481031866464e-07,
      "loss": 0.3561,
      "step": 2150
    },
    {
      "epoch": 3.2640364188163886,
      "grad_norm": 0.6452406644821167,
      "learning_rate": 9.218512898330804e-07,
      "loss": 0.393,
      "step": 2151
    },
    {
      "epoch": 3.265553869499241,
      "grad_norm": 0.5004937648773193,
      "learning_rate": 9.199544764795145e-07,
      "loss": 0.3712,
      "step": 2152
    },
    {
      "epoch": 3.2670713201820942,
      "grad_norm": 0.5828874111175537,
      "learning_rate": 9.180576631259484e-07,
      "loss": 0.4118,
      "step": 2153
    },
    {
      "epoch": 3.268588770864947,
      "grad_norm": 0.5933344960212708,
      "learning_rate": 9.161608497723824e-07,
      "loss": 0.3485,
      "step": 2154
    },
    {
      "epoch": 3.2701062215478,
      "grad_norm": 0.6115604639053345,
      "learning_rate": 9.142640364188165e-07,
      "loss": 0.3519,
      "step": 2155
    },
    {
      "epoch": 3.2716236722306524,
      "grad_norm": 0.6447845697402954,
      "learning_rate": 9.123672230652504e-07,
      "loss": 0.3481,
      "step": 2156
    },
    {
      "epoch": 3.2731411229135055,
      "grad_norm": 0.7332358956336975,
      "learning_rate": 9.104704097116844e-07,
      "loss": 0.3402,
      "step": 2157
    },
    {
      "epoch": 3.274658573596358,
      "grad_norm": 0.7615525126457214,
      "learning_rate": 9.085735963581184e-07,
      "loss": 0.3035,
      "step": 2158
    },
    {
      "epoch": 3.276176024279211,
      "grad_norm": 0.7563300132751465,
      "learning_rate": 9.066767830045524e-07,
      "loss": 0.352,
      "step": 2159
    },
    {
      "epoch": 3.2776934749620636,
      "grad_norm": 0.57706218957901,
      "learning_rate": 9.047799696509864e-07,
      "loss": 0.3456,
      "step": 2160
    },
    {
      "epoch": 3.2792109256449167,
      "grad_norm": 0.7504711151123047,
      "learning_rate": 9.028831562974203e-07,
      "loss": 0.3033,
      "step": 2161
    },
    {
      "epoch": 3.2807283763277693,
      "grad_norm": 0.5102882981300354,
      "learning_rate": 9.009863429438544e-07,
      "loss": 0.3047,
      "step": 2162
    },
    {
      "epoch": 3.2822458270106223,
      "grad_norm": 0.6929329633712769,
      "learning_rate": 8.990895295902884e-07,
      "loss": 0.3166,
      "step": 2163
    },
    {
      "epoch": 3.283763277693475,
      "grad_norm": 0.6261880993843079,
      "learning_rate": 8.971927162367223e-07,
      "loss": 0.3494,
      "step": 2164
    },
    {
      "epoch": 3.285280728376328,
      "grad_norm": 0.4809262454509735,
      "learning_rate": 8.952959028831563e-07,
      "loss": 0.3796,
      "step": 2165
    },
    {
      "epoch": 3.2867981790591805,
      "grad_norm": 0.7684910893440247,
      "learning_rate": 8.933990895295904e-07,
      "loss": 0.3226,
      "step": 2166
    },
    {
      "epoch": 3.2883156297420335,
      "grad_norm": 0.5689930319786072,
      "learning_rate": 8.915022761760243e-07,
      "loss": 0.3538,
      "step": 2167
    },
    {
      "epoch": 3.289833080424886,
      "grad_norm": 0.6821784973144531,
      "learning_rate": 8.896054628224583e-07,
      "loss": 0.3012,
      "step": 2168
    },
    {
      "epoch": 3.291350531107739,
      "grad_norm": 0.6546950340270996,
      "learning_rate": 8.877086494688924e-07,
      "loss": 0.3062,
      "step": 2169
    },
    {
      "epoch": 3.2928679817905917,
      "grad_norm": 0.6331755518913269,
      "learning_rate": 8.858118361153263e-07,
      "loss": 0.3314,
      "step": 2170
    },
    {
      "epoch": 3.2943854324734447,
      "grad_norm": 0.6493279933929443,
      "learning_rate": 8.839150227617603e-07,
      "loss": 0.3095,
      "step": 2171
    },
    {
      "epoch": 3.2959028831562973,
      "grad_norm": 0.7067837715148926,
      "learning_rate": 8.820182094081943e-07,
      "loss": 0.3797,
      "step": 2172
    },
    {
      "epoch": 3.2974203338391503,
      "grad_norm": 0.6444612145423889,
      "learning_rate": 8.801213960546283e-07,
      "loss": 0.2573,
      "step": 2173
    },
    {
      "epoch": 3.298937784522003,
      "grad_norm": 0.5623477697372437,
      "learning_rate": 8.782245827010623e-07,
      "loss": 0.4232,
      "step": 2174
    },
    {
      "epoch": 3.300455235204856,
      "grad_norm": 0.6505182981491089,
      "learning_rate": 8.763277693474962e-07,
      "loss": 0.3193,
      "step": 2175
    },
    {
      "epoch": 3.3019726858877085,
      "grad_norm": 0.6064900159835815,
      "learning_rate": 8.744309559939303e-07,
      "loss": 0.3301,
      "step": 2176
    },
    {
      "epoch": 3.3034901365705616,
      "grad_norm": 0.6472567319869995,
      "learning_rate": 8.725341426403643e-07,
      "loss": 0.3402,
      "step": 2177
    },
    {
      "epoch": 3.305007587253414,
      "grad_norm": 0.6507107019424438,
      "learning_rate": 8.706373292867982e-07,
      "loss": 0.3328,
      "step": 2178
    },
    {
      "epoch": 3.306525037936267,
      "grad_norm": 0.6680933237075806,
      "learning_rate": 8.687405159332323e-07,
      "loss": 0.3215,
      "step": 2179
    },
    {
      "epoch": 3.3080424886191198,
      "grad_norm": 0.7236153483390808,
      "learning_rate": 8.668437025796663e-07,
      "loss": 0.3152,
      "step": 2180
    },
    {
      "epoch": 3.309559939301973,
      "grad_norm": 0.6704484820365906,
      "learning_rate": 8.649468892261002e-07,
      "loss": 0.312,
      "step": 2181
    },
    {
      "epoch": 3.3110773899848254,
      "grad_norm": 0.8655254244804382,
      "learning_rate": 8.630500758725342e-07,
      "loss": 0.4163,
      "step": 2182
    },
    {
      "epoch": 3.3125948406676784,
      "grad_norm": 1.3203459978103638,
      "learning_rate": 8.611532625189682e-07,
      "loss": 0.3854,
      "step": 2183
    },
    {
      "epoch": 3.314112291350531,
      "grad_norm": 0.7113330960273743,
      "learning_rate": 8.592564491654022e-07,
      "loss": 0.3462,
      "step": 2184
    },
    {
      "epoch": 3.315629742033384,
      "grad_norm": 0.5722606182098389,
      "learning_rate": 8.573596358118362e-07,
      "loss": 0.3646,
      "step": 2185
    },
    {
      "epoch": 3.3171471927162366,
      "grad_norm": 0.624113917350769,
      "learning_rate": 8.554628224582702e-07,
      "loss": 0.3493,
      "step": 2186
    },
    {
      "epoch": 3.3186646433990896,
      "grad_norm": 0.596530020236969,
      "learning_rate": 8.535660091047042e-07,
      "loss": 0.3061,
      "step": 2187
    },
    {
      "epoch": 3.320182094081942,
      "grad_norm": 0.6654078960418701,
      "learning_rate": 8.516691957511382e-07,
      "loss": 0.3292,
      "step": 2188
    },
    {
      "epoch": 3.3216995447647952,
      "grad_norm": 0.7291204929351807,
      "learning_rate": 8.497723823975721e-07,
      "loss": 0.3097,
      "step": 2189
    },
    {
      "epoch": 3.323216995447648,
      "grad_norm": 0.6091313362121582,
      "learning_rate": 8.478755690440062e-07,
      "loss": 0.3853,
      "step": 2190
    },
    {
      "epoch": 3.324734446130501,
      "grad_norm": 0.7277981638908386,
      "learning_rate": 8.459787556904402e-07,
      "loss": 0.405,
      "step": 2191
    },
    {
      "epoch": 3.3262518968133534,
      "grad_norm": 1.5999644994735718,
      "learning_rate": 8.440819423368741e-07,
      "loss": 0.3614,
      "step": 2192
    },
    {
      "epoch": 3.3277693474962065,
      "grad_norm": 0.5827577710151672,
      "learning_rate": 8.421851289833082e-07,
      "loss": 0.3407,
      "step": 2193
    },
    {
      "epoch": 3.329286798179059,
      "grad_norm": 0.6241956949234009,
      "learning_rate": 8.402883156297421e-07,
      "loss": 0.3296,
      "step": 2194
    },
    {
      "epoch": 3.330804248861912,
      "grad_norm": 0.5538023114204407,
      "learning_rate": 8.383915022761761e-07,
      "loss": 0.3027,
      "step": 2195
    },
    {
      "epoch": 3.3323216995447646,
      "grad_norm": 0.6567160487174988,
      "learning_rate": 8.364946889226102e-07,
      "loss": 0.2924,
      "step": 2196
    },
    {
      "epoch": 3.3338391502276177,
      "grad_norm": 0.781559944152832,
      "learning_rate": 8.345978755690441e-07,
      "loss": 0.3439,
      "step": 2197
    },
    {
      "epoch": 3.3353566009104703,
      "grad_norm": 0.6585545539855957,
      "learning_rate": 8.327010622154781e-07,
      "loss": 0.3583,
      "step": 2198
    },
    {
      "epoch": 3.3368740515933233,
      "grad_norm": 0.5576002597808838,
      "learning_rate": 8.308042488619121e-07,
      "loss": 0.2779,
      "step": 2199
    },
    {
      "epoch": 3.338391502276176,
      "grad_norm": 0.6462175250053406,
      "learning_rate": 8.289074355083461e-07,
      "loss": 0.3217,
      "step": 2200
    },
    {
      "epoch": 3.339908952959029,
      "grad_norm": 0.5613349676132202,
      "learning_rate": 8.270106221547801e-07,
      "loss": 0.3192,
      "step": 2201
    },
    {
      "epoch": 3.3414264036418815,
      "grad_norm": 0.6302552819252014,
      "learning_rate": 8.251138088012141e-07,
      "loss": 0.2778,
      "step": 2202
    },
    {
      "epoch": 3.3429438543247345,
      "grad_norm": 0.6668598651885986,
      "learning_rate": 8.232169954476481e-07,
      "loss": 0.4122,
      "step": 2203
    },
    {
      "epoch": 3.344461305007587,
      "grad_norm": 0.607858419418335,
      "learning_rate": 8.213201820940821e-07,
      "loss": 0.3507,
      "step": 2204
    },
    {
      "epoch": 3.34597875569044,
      "grad_norm": 0.6727869510650635,
      "learning_rate": 8.19423368740516e-07,
      "loss": 0.3083,
      "step": 2205
    },
    {
      "epoch": 3.3474962063732927,
      "grad_norm": 0.6404129266738892,
      "learning_rate": 8.1752655538695e-07,
      "loss": 0.2446,
      "step": 2206
    },
    {
      "epoch": 3.3490136570561457,
      "grad_norm": 0.670918345451355,
      "learning_rate": 8.156297420333841e-07,
      "loss": 0.3188,
      "step": 2207
    },
    {
      "epoch": 3.3505311077389983,
      "grad_norm": 0.6273123621940613,
      "learning_rate": 8.13732928679818e-07,
      "loss": 0.2909,
      "step": 2208
    },
    {
      "epoch": 3.3520485584218513,
      "grad_norm": 0.7398996353149414,
      "learning_rate": 8.118361153262519e-07,
      "loss": 0.3362,
      "step": 2209
    },
    {
      "epoch": 3.353566009104704,
      "grad_norm": 0.7048208117485046,
      "learning_rate": 8.099393019726859e-07,
      "loss": 0.3933,
      "step": 2210
    },
    {
      "epoch": 3.355083459787557,
      "grad_norm": 0.5868785381317139,
      "learning_rate": 8.080424886191199e-07,
      "loss": 0.3228,
      "step": 2211
    },
    {
      "epoch": 3.3566009104704095,
      "grad_norm": 0.6020663976669312,
      "learning_rate": 8.061456752655539e-07,
      "loss": 0.3685,
      "step": 2212
    },
    {
      "epoch": 3.3581183611532626,
      "grad_norm": 0.5726162195205688,
      "learning_rate": 8.042488619119878e-07,
      "loss": 0.3227,
      "step": 2213
    },
    {
      "epoch": 3.359635811836115,
      "grad_norm": 0.6419351100921631,
      "learning_rate": 8.023520485584219e-07,
      "loss": 0.3107,
      "step": 2214
    },
    {
      "epoch": 3.361153262518968,
      "grad_norm": 0.6874028444290161,
      "learning_rate": 8.004552352048558e-07,
      "loss": 0.3322,
      "step": 2215
    },
    {
      "epoch": 3.3626707132018208,
      "grad_norm": 0.5998178124427795,
      "learning_rate": 7.985584218512898e-07,
      "loss": 0.3477,
      "step": 2216
    },
    {
      "epoch": 3.364188163884674,
      "grad_norm": 0.5933347344398499,
      "learning_rate": 7.966616084977239e-07,
      "loss": 0.2869,
      "step": 2217
    },
    {
      "epoch": 3.3657056145675264,
      "grad_norm": 0.7356338500976562,
      "learning_rate": 7.947647951441578e-07,
      "loss": 0.3148,
      "step": 2218
    },
    {
      "epoch": 3.3672230652503794,
      "grad_norm": 0.5393637418746948,
      "learning_rate": 7.928679817905918e-07,
      "loss": 0.3402,
      "step": 2219
    },
    {
      "epoch": 3.368740515933232,
      "grad_norm": 0.7825514674186707,
      "learning_rate": 7.909711684370259e-07,
      "loss": 0.353,
      "step": 2220
    },
    {
      "epoch": 3.370257966616085,
      "grad_norm": 0.6167687177658081,
      "learning_rate": 7.890743550834598e-07,
      "loss": 0.3505,
      "step": 2221
    },
    {
      "epoch": 3.3717754172989376,
      "grad_norm": 0.6133418083190918,
      "learning_rate": 7.871775417298938e-07,
      "loss": 0.2924,
      "step": 2222
    },
    {
      "epoch": 3.3732928679817906,
      "grad_norm": 0.5984098315238953,
      "learning_rate": 7.852807283763278e-07,
      "loss": 0.356,
      "step": 2223
    },
    {
      "epoch": 3.374810318664643,
      "grad_norm": 0.5939614772796631,
      "learning_rate": 7.833839150227618e-07,
      "loss": 0.3067,
      "step": 2224
    },
    {
      "epoch": 3.3763277693474962,
      "grad_norm": 2.7804577350616455,
      "learning_rate": 7.814871016691958e-07,
      "loss": 0.3212,
      "step": 2225
    },
    {
      "epoch": 3.3778452200303493,
      "grad_norm": 0.686745822429657,
      "learning_rate": 7.795902883156297e-07,
      "loss": 0.3575,
      "step": 2226
    },
    {
      "epoch": 3.379362670713202,
      "grad_norm": 0.6935586929321289,
      "learning_rate": 7.776934749620638e-07,
      "loss": 0.3632,
      "step": 2227
    },
    {
      "epoch": 3.3808801213960544,
      "grad_norm": 0.6179876923561096,
      "learning_rate": 7.757966616084978e-07,
      "loss": 0.3061,
      "step": 2228
    },
    {
      "epoch": 3.3823975720789075,
      "grad_norm": 0.605924129486084,
      "learning_rate": 7.738998482549317e-07,
      "loss": 0.3384,
      "step": 2229
    },
    {
      "epoch": 3.3839150227617605,
      "grad_norm": 0.6008278727531433,
      "learning_rate": 7.720030349013657e-07,
      "loss": 0.321,
      "step": 2230
    },
    {
      "epoch": 3.385432473444613,
      "grad_norm": 0.6699774265289307,
      "learning_rate": 7.701062215477998e-07,
      "loss": 0.3073,
      "step": 2231
    },
    {
      "epoch": 3.3869499241274656,
      "grad_norm": 0.6781790852546692,
      "learning_rate": 7.682094081942337e-07,
      "loss": 0.3324,
      "step": 2232
    },
    {
      "epoch": 3.3884673748103187,
      "grad_norm": 0.6192862391471863,
      "learning_rate": 7.663125948406677e-07,
      "loss": 0.3329,
      "step": 2233
    },
    {
      "epoch": 3.3899848254931717,
      "grad_norm": 0.6531421542167664,
      "learning_rate": 7.644157814871018e-07,
      "loss": 0.3566,
      "step": 2234
    },
    {
      "epoch": 3.3915022761760243,
      "grad_norm": 0.7061408758163452,
      "learning_rate": 7.625189681335357e-07,
      "loss": 0.2595,
      "step": 2235
    },
    {
      "epoch": 3.393019726858877,
      "grad_norm": 0.5415230989456177,
      "learning_rate": 7.606221547799697e-07,
      "loss": 0.2918,
      "step": 2236
    },
    {
      "epoch": 3.39453717754173,
      "grad_norm": 0.7719334959983826,
      "learning_rate": 7.587253414264036e-07,
      "loss": 0.4218,
      "step": 2237
    },
    {
      "epoch": 3.396054628224583,
      "grad_norm": 0.6347887516021729,
      "learning_rate": 7.568285280728377e-07,
      "loss": 0.377,
      "step": 2238
    },
    {
      "epoch": 3.3975720789074355,
      "grad_norm": 0.6374194025993347,
      "learning_rate": 7.549317147192717e-07,
      "loss": 0.4004,
      "step": 2239
    },
    {
      "epoch": 3.399089529590288,
      "grad_norm": 0.5968208909034729,
      "learning_rate": 7.530349013657056e-07,
      "loss": 0.3978,
      "step": 2240
    },
    {
      "epoch": 3.400606980273141,
      "grad_norm": 0.6257688999176025,
      "learning_rate": 7.511380880121397e-07,
      "loss": 0.4251,
      "step": 2241
    },
    {
      "epoch": 3.402124430955994,
      "grad_norm": 0.6495656967163086,
      "learning_rate": 7.492412746585737e-07,
      "loss": 0.3602,
      "step": 2242
    },
    {
      "epoch": 3.4036418816388467,
      "grad_norm": 0.7504865527153015,
      "learning_rate": 7.473444613050076e-07,
      "loss": 0.3345,
      "step": 2243
    },
    {
      "epoch": 3.4051593323216993,
      "grad_norm": 0.637151300907135,
      "learning_rate": 7.454476479514417e-07,
      "loss": 0.3488,
      "step": 2244
    },
    {
      "epoch": 3.4066767830045523,
      "grad_norm": 0.6567947864532471,
      "learning_rate": 7.435508345978757e-07,
      "loss": 0.3072,
      "step": 2245
    },
    {
      "epoch": 3.4081942336874054,
      "grad_norm": 0.6070346236228943,
      "learning_rate": 7.416540212443096e-07,
      "loss": 0.3956,
      "step": 2246
    },
    {
      "epoch": 3.409711684370258,
      "grad_norm": 0.5834759473800659,
      "learning_rate": 7.397572078907436e-07,
      "loss": 0.3525,
      "step": 2247
    },
    {
      "epoch": 3.4112291350531105,
      "grad_norm": 0.5550823211669922,
      "learning_rate": 7.378603945371776e-07,
      "loss": 0.3094,
      "step": 2248
    },
    {
      "epoch": 3.4127465857359636,
      "grad_norm": 0.6091250777244568,
      "learning_rate": 7.359635811836116e-07,
      "loss": 0.3312,
      "step": 2249
    },
    {
      "epoch": 3.4142640364188166,
      "grad_norm": 0.6505414843559265,
      "learning_rate": 7.340667678300456e-07,
      "loss": 0.3653,
      "step": 2250
    },
    {
      "epoch": 3.415781487101669,
      "grad_norm": 0.767755925655365,
      "learning_rate": 7.321699544764796e-07,
      "loss": 0.4443,
      "step": 2251
    },
    {
      "epoch": 3.4172989377845218,
      "grad_norm": 0.6390799283981323,
      "learning_rate": 7.302731411229136e-07,
      "loss": 0.3051,
      "step": 2252
    },
    {
      "epoch": 3.418816388467375,
      "grad_norm": 0.5783661007881165,
      "learning_rate": 7.283763277693476e-07,
      "loss": 0.4306,
      "step": 2253
    },
    {
      "epoch": 3.420333839150228,
      "grad_norm": 0.7088921666145325,
      "learning_rate": 7.264795144157815e-07,
      "loss": 0.3672,
      "step": 2254
    },
    {
      "epoch": 3.4218512898330804,
      "grad_norm": 0.6138518452644348,
      "learning_rate": 7.245827010622156e-07,
      "loss": 0.2861,
      "step": 2255
    },
    {
      "epoch": 3.423368740515933,
      "grad_norm": 0.6258915066719055,
      "learning_rate": 7.226858877086496e-07,
      "loss": 0.3901,
      "step": 2256
    },
    {
      "epoch": 3.424886191198786,
      "grad_norm": 0.5565832257270813,
      "learning_rate": 7.207890743550835e-07,
      "loss": 0.3925,
      "step": 2257
    },
    {
      "epoch": 3.426403641881639,
      "grad_norm": 0.6229720711708069,
      "learning_rate": 7.188922610015176e-07,
      "loss": 0.3415,
      "step": 2258
    },
    {
      "epoch": 3.4279210925644916,
      "grad_norm": 0.6908206939697266,
      "learning_rate": 7.169954476479515e-07,
      "loss": 0.3108,
      "step": 2259
    },
    {
      "epoch": 3.4294385432473447,
      "grad_norm": 0.7144551873207092,
      "learning_rate": 7.150986342943855e-07,
      "loss": 0.3145,
      "step": 2260
    },
    {
      "epoch": 3.4309559939301972,
      "grad_norm": 0.6030591130256653,
      "learning_rate": 7.132018209408196e-07,
      "loss": 0.2965,
      "step": 2261
    },
    {
      "epoch": 3.4324734446130503,
      "grad_norm": 0.7172723412513733,
      "learning_rate": 7.113050075872535e-07,
      "loss": 0.3606,
      "step": 2262
    },
    {
      "epoch": 3.433990895295903,
      "grad_norm": 0.6178188323974609,
      "learning_rate": 7.094081942336875e-07,
      "loss": 0.335,
      "step": 2263
    },
    {
      "epoch": 3.435508345978756,
      "grad_norm": 0.6682829260826111,
      "learning_rate": 7.075113808801215e-07,
      "loss": 0.3647,
      "step": 2264
    },
    {
      "epoch": 3.4370257966616085,
      "grad_norm": 0.6961460113525391,
      "learning_rate": 7.056145675265555e-07,
      "loss": 0.3738,
      "step": 2265
    },
    {
      "epoch": 3.4385432473444615,
      "grad_norm": 0.7495805621147156,
      "learning_rate": 7.037177541729895e-07,
      "loss": 0.4321,
      "step": 2266
    },
    {
      "epoch": 3.440060698027314,
      "grad_norm": 0.6202492713928223,
      "learning_rate": 7.018209408194235e-07,
      "loss": 0.2304,
      "step": 2267
    },
    {
      "epoch": 3.441578148710167,
      "grad_norm": 0.603070855140686,
      "learning_rate": 6.999241274658575e-07,
      "loss": 0.393,
      "step": 2268
    },
    {
      "epoch": 3.4430955993930197,
      "grad_norm": 0.6156257390975952,
      "learning_rate": 6.980273141122915e-07,
      "loss": 0.3841,
      "step": 2269
    },
    {
      "epoch": 3.4446130500758727,
      "grad_norm": 0.6258379220962524,
      "learning_rate": 6.961305007587254e-07,
      "loss": 0.3325,
      "step": 2270
    },
    {
      "epoch": 3.4461305007587253,
      "grad_norm": 0.5208572149276733,
      "learning_rate": 6.942336874051594e-07,
      "loss": 0.3227,
      "step": 2271
    },
    {
      "epoch": 3.4476479514415783,
      "grad_norm": 0.7712974548339844,
      "learning_rate": 6.923368740515935e-07,
      "loss": 0.3889,
      "step": 2272
    },
    {
      "epoch": 3.449165402124431,
      "grad_norm": 0.601897656917572,
      "learning_rate": 6.904400606980274e-07,
      "loss": 0.3956,
      "step": 2273
    },
    {
      "epoch": 3.450682852807284,
      "grad_norm": 0.711835503578186,
      "learning_rate": 6.885432473444614e-07,
      "loss": 0.3537,
      "step": 2274
    },
    {
      "epoch": 3.4522003034901365,
      "grad_norm": 0.5811355113983154,
      "learning_rate": 6.866464339908952e-07,
      "loss": 0.3931,
      "step": 2275
    },
    {
      "epoch": 3.4537177541729895,
      "grad_norm": 0.6029402613639832,
      "learning_rate": 6.847496206373293e-07,
      "loss": 0.3439,
      "step": 2276
    },
    {
      "epoch": 3.455235204855842,
      "grad_norm": 0.5746676921844482,
      "learning_rate": 6.828528072837633e-07,
      "loss": 0.2894,
      "step": 2277
    },
    {
      "epoch": 3.456752655538695,
      "grad_norm": 0.7062175273895264,
      "learning_rate": 6.809559939301972e-07,
      "loss": 0.3107,
      "step": 2278
    },
    {
      "epoch": 3.4582701062215477,
      "grad_norm": 0.5237852931022644,
      "learning_rate": 6.790591805766313e-07,
      "loss": 0.3378,
      "step": 2279
    },
    {
      "epoch": 3.4597875569044008,
      "grad_norm": 0.6600927710533142,
      "learning_rate": 6.771623672230653e-07,
      "loss": 0.3598,
      "step": 2280
    },
    {
      "epoch": 3.4613050075872533,
      "grad_norm": 0.5994452238082886,
      "learning_rate": 6.752655538694992e-07,
      "loss": 0.2753,
      "step": 2281
    },
    {
      "epoch": 3.4628224582701064,
      "grad_norm": 0.7123321294784546,
      "learning_rate": 6.733687405159333e-07,
      "loss": 0.3401,
      "step": 2282
    },
    {
      "epoch": 3.464339908952959,
      "grad_norm": 0.6180089712142944,
      "learning_rate": 6.714719271623672e-07,
      "loss": 0.2733,
      "step": 2283
    },
    {
      "epoch": 3.465857359635812,
      "grad_norm": 0.6662874817848206,
      "learning_rate": 6.695751138088012e-07,
      "loss": 0.3026,
      "step": 2284
    },
    {
      "epoch": 3.4673748103186646,
      "grad_norm": 0.6501687169075012,
      "learning_rate": 6.676783004552352e-07,
      "loss": 0.2913,
      "step": 2285
    },
    {
      "epoch": 3.4688922610015176,
      "grad_norm": 0.5811793208122253,
      "learning_rate": 6.657814871016692e-07,
      "loss": 0.3146,
      "step": 2286
    },
    {
      "epoch": 3.47040971168437,
      "grad_norm": 0.6527822017669678,
      "learning_rate": 6.638846737481032e-07,
      "loss": 0.4131,
      "step": 2287
    },
    {
      "epoch": 3.471927162367223,
      "grad_norm": 0.5709004402160645,
      "learning_rate": 6.619878603945372e-07,
      "loss": 0.3589,
      "step": 2288
    },
    {
      "epoch": 3.473444613050076,
      "grad_norm": 0.5912383794784546,
      "learning_rate": 6.600910470409712e-07,
      "loss": 0.2622,
      "step": 2289
    },
    {
      "epoch": 3.474962063732929,
      "grad_norm": 0.7783474922180176,
      "learning_rate": 6.581942336874052e-07,
      "loss": 0.3685,
      "step": 2290
    },
    {
      "epoch": 3.4764795144157814,
      "grad_norm": 0.656514048576355,
      "learning_rate": 6.562974203338392e-07,
      "loss": 0.3066,
      "step": 2291
    },
    {
      "epoch": 3.4779969650986344,
      "grad_norm": 0.6617339253425598,
      "learning_rate": 6.544006069802731e-07,
      "loss": 0.282,
      "step": 2292
    },
    {
      "epoch": 3.479514415781487,
      "grad_norm": 0.6277039647102356,
      "learning_rate": 6.525037936267072e-07,
      "loss": 0.3811,
      "step": 2293
    },
    {
      "epoch": 3.48103186646434,
      "grad_norm": 0.9712435603141785,
      "learning_rate": 6.506069802731411e-07,
      "loss": 0.3483,
      "step": 2294
    },
    {
      "epoch": 3.4825493171471926,
      "grad_norm": 0.5675109028816223,
      "learning_rate": 6.487101669195751e-07,
      "loss": 0.3316,
      "step": 2295
    },
    {
      "epoch": 3.4840667678300457,
      "grad_norm": 0.6103448867797852,
      "learning_rate": 6.468133535660092e-07,
      "loss": 0.3825,
      "step": 2296
    },
    {
      "epoch": 3.4855842185128982,
      "grad_norm": 0.5558280348777771,
      "learning_rate": 6.449165402124431e-07,
      "loss": 0.2165,
      "step": 2297
    },
    {
      "epoch": 3.4871016691957513,
      "grad_norm": 0.6630460619926453,
      "learning_rate": 6.430197268588771e-07,
      "loss": 0.3928,
      "step": 2298
    },
    {
      "epoch": 3.488619119878604,
      "grad_norm": 0.6002416014671326,
      "learning_rate": 6.411229135053112e-07,
      "loss": 0.3751,
      "step": 2299
    },
    {
      "epoch": 3.490136570561457,
      "grad_norm": 0.6367866396903992,
      "learning_rate": 6.392261001517451e-07,
      "loss": 0.369,
      "step": 2300
    },
    {
      "epoch": 3.4916540212443095,
      "grad_norm": 0.5926533937454224,
      "learning_rate": 6.373292867981791e-07,
      "loss": 0.3568,
      "step": 2301
    },
    {
      "epoch": 3.4931714719271625,
      "grad_norm": 0.7147424221038818,
      "learning_rate": 6.354324734446131e-07,
      "loss": 0.3754,
      "step": 2302
    },
    {
      "epoch": 3.494688922610015,
      "grad_norm": 0.6162973642349243,
      "learning_rate": 6.335356600910471e-07,
      "loss": 0.351,
      "step": 2303
    },
    {
      "epoch": 3.496206373292868,
      "grad_norm": 0.5610958337783813,
      "learning_rate": 6.316388467374811e-07,
      "loss": 0.3923,
      "step": 2304
    },
    {
      "epoch": 3.4977238239757207,
      "grad_norm": 0.5678163170814514,
      "learning_rate": 6.29742033383915e-07,
      "loss": 0.2933,
      "step": 2305
    },
    {
      "epoch": 3.4992412746585737,
      "grad_norm": 0.624110996723175,
      "learning_rate": 6.278452200303491e-07,
      "loss": 0.3321,
      "step": 2306
    },
    {
      "epoch": 3.5007587253414263,
      "grad_norm": 0.6844291687011719,
      "learning_rate": 6.259484066767831e-07,
      "loss": 0.4154,
      "step": 2307
    },
    {
      "epoch": 3.5022761760242793,
      "grad_norm": 0.6206443905830383,
      "learning_rate": 6.24051593323217e-07,
      "loss": 0.4211,
      "step": 2308
    },
    {
      "epoch": 3.503793626707132,
      "grad_norm": 0.6074811220169067,
      "learning_rate": 6.22154779969651e-07,
      "loss": 0.3624,
      "step": 2309
    },
    {
      "epoch": 3.505311077389985,
      "grad_norm": 0.7240820527076721,
      "learning_rate": 6.202579666160851e-07,
      "loss": 0.4137,
      "step": 2310
    },
    {
      "epoch": 3.5068285280728375,
      "grad_norm": 0.5787087082862854,
      "learning_rate": 6.18361153262519e-07,
      "loss": 0.2987,
      "step": 2311
    },
    {
      "epoch": 3.5083459787556905,
      "grad_norm": 0.5873554348945618,
      "learning_rate": 6.16464339908953e-07,
      "loss": 0.3651,
      "step": 2312
    },
    {
      "epoch": 3.509863429438543,
      "grad_norm": 0.740679144859314,
      "learning_rate": 6.14567526555387e-07,
      "loss": 0.3166,
      "step": 2313
    },
    {
      "epoch": 3.511380880121396,
      "grad_norm": 0.6940522789955139,
      "learning_rate": 6.12670713201821e-07,
      "loss": 0.3412,
      "step": 2314
    },
    {
      "epoch": 3.5128983308042487,
      "grad_norm": 0.613487184047699,
      "learning_rate": 6.10773899848255e-07,
      "loss": 0.3636,
      "step": 2315
    },
    {
      "epoch": 3.5144157814871018,
      "grad_norm": 0.5400553941726685,
      "learning_rate": 6.08877086494689e-07,
      "loss": 0.2922,
      "step": 2316
    },
    {
      "epoch": 3.5159332321699543,
      "grad_norm": 0.8118513822555542,
      "learning_rate": 6.06980273141123e-07,
      "loss": 0.3534,
      "step": 2317
    },
    {
      "epoch": 3.5174506828528074,
      "grad_norm": 0.6571717858314514,
      "learning_rate": 6.05083459787557e-07,
      "loss": 0.3129,
      "step": 2318
    },
    {
      "epoch": 3.51896813353566,
      "grad_norm": 0.637413740158081,
      "learning_rate": 6.031866464339909e-07,
      "loss": 0.3461,
      "step": 2319
    },
    {
      "epoch": 3.520485584218513,
      "grad_norm": 0.6343108415603638,
      "learning_rate": 6.01289833080425e-07,
      "loss": 0.2999,
      "step": 2320
    },
    {
      "epoch": 3.5220030349013656,
      "grad_norm": 0.6274487376213074,
      "learning_rate": 5.99393019726859e-07,
      "loss": 0.3824,
      "step": 2321
    },
    {
      "epoch": 3.5235204855842186,
      "grad_norm": 0.6943455338478088,
      "learning_rate": 5.974962063732929e-07,
      "loss": 0.341,
      "step": 2322
    },
    {
      "epoch": 3.525037936267071,
      "grad_norm": 0.7420991659164429,
      "learning_rate": 5.95599393019727e-07,
      "loss": 0.3979,
      "step": 2323
    },
    {
      "epoch": 3.526555386949924,
      "grad_norm": 0.5803298354148865,
      "learning_rate": 5.937025796661609e-07,
      "loss": 0.374,
      "step": 2324
    },
    {
      "epoch": 3.528072837632777,
      "grad_norm": 0.9926204085350037,
      "learning_rate": 5.918057663125948e-07,
      "loss": 0.3615,
      "step": 2325
    },
    {
      "epoch": 3.52959028831563,
      "grad_norm": 0.5878685712814331,
      "learning_rate": 5.899089529590288e-07,
      "loss": 0.3051,
      "step": 2326
    },
    {
      "epoch": 3.5311077389984824,
      "grad_norm": 0.5886387228965759,
      "learning_rate": 5.880121396054629e-07,
      "loss": 0.3914,
      "step": 2327
    },
    {
      "epoch": 3.5326251896813354,
      "grad_norm": 0.6380818486213684,
      "learning_rate": 5.861153262518968e-07,
      "loss": 0.2518,
      "step": 2328
    },
    {
      "epoch": 3.534142640364188,
      "grad_norm": 0.7320091724395752,
      "learning_rate": 5.842185128983308e-07,
      "loss": 0.359,
      "step": 2329
    },
    {
      "epoch": 3.535660091047041,
      "grad_norm": 0.773711085319519,
      "learning_rate": 5.823216995447649e-07,
      "loss": 0.3368,
      "step": 2330
    },
    {
      "epoch": 3.5371775417298936,
      "grad_norm": 0.8801909685134888,
      "learning_rate": 5.804248861911988e-07,
      "loss": 0.4165,
      "step": 2331
    },
    {
      "epoch": 3.5386949924127467,
      "grad_norm": 0.7256240248680115,
      "learning_rate": 5.785280728376328e-07,
      "loss": 0.2983,
      "step": 2332
    },
    {
      "epoch": 3.5402124430955992,
      "grad_norm": 0.6893038749694824,
      "learning_rate": 5.766312594840668e-07,
      "loss": 0.337,
      "step": 2333
    },
    {
      "epoch": 3.5417298937784523,
      "grad_norm": 0.6691523790359497,
      "learning_rate": 5.747344461305008e-07,
      "loss": 0.3675,
      "step": 2334
    },
    {
      "epoch": 3.543247344461305,
      "grad_norm": 0.6596639752388,
      "learning_rate": 5.728376327769348e-07,
      "loss": 0.2752,
      "step": 2335
    },
    {
      "epoch": 3.544764795144158,
      "grad_norm": 0.5568361878395081,
      "learning_rate": 5.709408194233687e-07,
      "loss": 0.374,
      "step": 2336
    },
    {
      "epoch": 3.5462822458270105,
      "grad_norm": 0.6529479622840881,
      "learning_rate": 5.690440060698028e-07,
      "loss": 0.3448,
      "step": 2337
    },
    {
      "epoch": 3.5477996965098635,
      "grad_norm": 0.6757881045341492,
      "learning_rate": 5.671471927162368e-07,
      "loss": 0.3211,
      "step": 2338
    },
    {
      "epoch": 3.549317147192716,
      "grad_norm": 0.6826539039611816,
      "learning_rate": 5.652503793626707e-07,
      "loss": 0.4568,
      "step": 2339
    },
    {
      "epoch": 3.550834597875569,
      "grad_norm": 0.8158725500106812,
      "learning_rate": 5.633535660091047e-07,
      "loss": 0.2937,
      "step": 2340
    },
    {
      "epoch": 3.552352048558422,
      "grad_norm": 0.7242905497550964,
      "learning_rate": 5.614567526555388e-07,
      "loss": 0.3373,
      "step": 2341
    },
    {
      "epoch": 3.5538694992412747,
      "grad_norm": 0.6392512321472168,
      "learning_rate": 5.595599393019727e-07,
      "loss": 0.3405,
      "step": 2342
    },
    {
      "epoch": 3.5553869499241273,
      "grad_norm": 0.6056243777275085,
      "learning_rate": 5.576631259484067e-07,
      "loss": 0.323,
      "step": 2343
    },
    {
      "epoch": 3.5569044006069803,
      "grad_norm": 0.6317049264907837,
      "learning_rate": 5.557663125948408e-07,
      "loss": 0.3703,
      "step": 2344
    },
    {
      "epoch": 3.5584218512898333,
      "grad_norm": 0.667340099811554,
      "learning_rate": 5.538694992412747e-07,
      "loss": 0.3184,
      "step": 2345
    },
    {
      "epoch": 3.559939301972686,
      "grad_norm": 0.6276607513427734,
      "learning_rate": 5.519726858877087e-07,
      "loss": 0.2863,
      "step": 2346
    },
    {
      "epoch": 3.5614567526555385,
      "grad_norm": 0.6502708792686462,
      "learning_rate": 5.500758725341426e-07,
      "loss": 0.3092,
      "step": 2347
    },
    {
      "epoch": 3.5629742033383915,
      "grad_norm": 0.6717259883880615,
      "learning_rate": 5.481790591805767e-07,
      "loss": 0.3741,
      "step": 2348
    },
    {
      "epoch": 3.5644916540212446,
      "grad_norm": 0.6327048540115356,
      "learning_rate": 5.462822458270107e-07,
      "loss": 0.3387,
      "step": 2349
    },
    {
      "epoch": 3.566009104704097,
      "grad_norm": 0.5876423120498657,
      "learning_rate": 5.443854324734446e-07,
      "loss": 0.3451,
      "step": 2350
    },
    {
      "epoch": 3.5675265553869497,
      "grad_norm": 0.5136117935180664,
      "learning_rate": 5.424886191198787e-07,
      "loss": 0.3304,
      "step": 2351
    },
    {
      "epoch": 3.5690440060698028,
      "grad_norm": 0.7096386551856995,
      "learning_rate": 5.405918057663127e-07,
      "loss": 0.3774,
      "step": 2352
    },
    {
      "epoch": 3.570561456752656,
      "grad_norm": 0.6763873100280762,
      "learning_rate": 5.386949924127466e-07,
      "loss": 0.3715,
      "step": 2353
    },
    {
      "epoch": 3.5720789074355084,
      "grad_norm": 0.6862262487411499,
      "learning_rate": 5.367981790591807e-07,
      "loss": 0.4767,
      "step": 2354
    },
    {
      "epoch": 3.573596358118361,
      "grad_norm": 0.5673750638961792,
      "learning_rate": 5.349013657056147e-07,
      "loss": 0.313,
      "step": 2355
    },
    {
      "epoch": 3.575113808801214,
      "grad_norm": 0.6657471060752869,
      "learning_rate": 5.330045523520486e-07,
      "loss": 0.3418,
      "step": 2356
    },
    {
      "epoch": 3.576631259484067,
      "grad_norm": 0.7538619041442871,
      "learning_rate": 5.311077389984825e-07,
      "loss": 0.2693,
      "step": 2357
    },
    {
      "epoch": 3.5781487101669196,
      "grad_norm": 0.5666702389717102,
      "learning_rate": 5.292109256449166e-07,
      "loss": 0.4101,
      "step": 2358
    },
    {
      "epoch": 3.579666160849772,
      "grad_norm": 0.560494601726532,
      "learning_rate": 5.273141122913505e-07,
      "loss": 0.3065,
      "step": 2359
    },
    {
      "epoch": 3.581183611532625,
      "grad_norm": 0.6337800025939941,
      "learning_rate": 5.254172989377845e-07,
      "loss": 0.3718,
      "step": 2360
    },
    {
      "epoch": 3.5827010622154782,
      "grad_norm": 0.6559855341911316,
      "learning_rate": 5.235204855842186e-07,
      "loss": 0.3734,
      "step": 2361
    },
    {
      "epoch": 3.584218512898331,
      "grad_norm": 0.9065050482749939,
      "learning_rate": 5.216236722306525e-07,
      "loss": 0.327,
      "step": 2362
    },
    {
      "epoch": 3.5857359635811834,
      "grad_norm": 0.5989809036254883,
      "learning_rate": 5.197268588770865e-07,
      "loss": 0.3291,
      "step": 2363
    },
    {
      "epoch": 3.5872534142640364,
      "grad_norm": 0.7078510522842407,
      "learning_rate": 5.178300455235205e-07,
      "loss": 0.3367,
      "step": 2364
    },
    {
      "epoch": 3.5887708649468895,
      "grad_norm": 0.7283576130867004,
      "learning_rate": 5.159332321699545e-07,
      "loss": 0.3956,
      "step": 2365
    },
    {
      "epoch": 3.590288315629742,
      "grad_norm": 0.5997415781021118,
      "learning_rate": 5.140364188163885e-07,
      "loss": 0.3649,
      "step": 2366
    },
    {
      "epoch": 3.5918057663125946,
      "grad_norm": 0.6563153266906738,
      "learning_rate": 5.121396054628225e-07,
      "loss": 0.3303,
      "step": 2367
    },
    {
      "epoch": 3.5933232169954477,
      "grad_norm": 0.6184437870979309,
      "learning_rate": 5.102427921092565e-07,
      "loss": 0.3932,
      "step": 2368
    },
    {
      "epoch": 3.5948406676783007,
      "grad_norm": 0.6772330403327942,
      "learning_rate": 5.083459787556905e-07,
      "loss": 0.3854,
      "step": 2369
    },
    {
      "epoch": 3.5963581183611533,
      "grad_norm": 0.6675716042518616,
      "learning_rate": 5.064491654021244e-07,
      "loss": 0.4398,
      "step": 2370
    },
    {
      "epoch": 3.597875569044006,
      "grad_norm": 0.7234470248222351,
      "learning_rate": 5.045523520485585e-07,
      "loss": 0.2833,
      "step": 2371
    },
    {
      "epoch": 3.599393019726859,
      "grad_norm": 0.6062527298927307,
      "learning_rate": 5.026555386949925e-07,
      "loss": 0.2888,
      "step": 2372
    },
    {
      "epoch": 3.600910470409712,
      "grad_norm": 0.5722903609275818,
      "learning_rate": 5.007587253414264e-07,
      "loss": 0.2827,
      "step": 2373
    },
    {
      "epoch": 3.6024279210925645,
      "grad_norm": 0.5780583620071411,
      "learning_rate": 4.988619119878604e-07,
      "loss": 0.3406,
      "step": 2374
    },
    {
      "epoch": 3.603945371775417,
      "grad_norm": 0.6167804002761841,
      "learning_rate": 4.969650986342945e-07,
      "loss": 0.3196,
      "step": 2375
    },
    {
      "epoch": 3.60546282245827,
      "grad_norm": 0.7840381860733032,
      "learning_rate": 4.950682852807284e-07,
      "loss": 0.4336,
      "step": 2376
    },
    {
      "epoch": 3.606980273141123,
      "grad_norm": 0.6221323013305664,
      "learning_rate": 4.931714719271624e-07,
      "loss": 0.268,
      "step": 2377
    },
    {
      "epoch": 3.6084977238239757,
      "grad_norm": 0.6449162364006042,
      "learning_rate": 4.912746585735965e-07,
      "loss": 0.3206,
      "step": 2378
    },
    {
      "epoch": 3.6100151745068283,
      "grad_norm": 0.6984840631484985,
      "learning_rate": 4.893778452200304e-07,
      "loss": 0.2816,
      "step": 2379
    },
    {
      "epoch": 3.6115326251896813,
      "grad_norm": 0.7745941281318665,
      "learning_rate": 4.874810318664644e-07,
      "loss": 0.3142,
      "step": 2380
    },
    {
      "epoch": 3.6130500758725344,
      "grad_norm": 0.5799615979194641,
      "learning_rate": 4.855842185128983e-07,
      "loss": 0.3545,
      "step": 2381
    },
    {
      "epoch": 3.614567526555387,
      "grad_norm": 0.750431478023529,
      "learning_rate": 4.836874051593324e-07,
      "loss": 0.428,
      "step": 2382
    },
    {
      "epoch": 3.6160849772382395,
      "grad_norm": 0.6619032025337219,
      "learning_rate": 4.817905918057664e-07,
      "loss": 0.3184,
      "step": 2383
    },
    {
      "epoch": 3.6176024279210925,
      "grad_norm": 0.6438280344009399,
      "learning_rate": 4.798937784522003e-07,
      "loss": 0.3495,
      "step": 2384
    },
    {
      "epoch": 3.6191198786039456,
      "grad_norm": 0.5742233991622925,
      "learning_rate": 4.779969650986344e-07,
      "loss": 0.3665,
      "step": 2385
    },
    {
      "epoch": 3.620637329286798,
      "grad_norm": 0.6783165335655212,
      "learning_rate": 4.7610015174506834e-07,
      "loss": 0.3597,
      "step": 2386
    },
    {
      "epoch": 3.6221547799696507,
      "grad_norm": 0.6460316181182861,
      "learning_rate": 4.742033383915023e-07,
      "loss": 0.3517,
      "step": 2387
    },
    {
      "epoch": 3.6236722306525038,
      "grad_norm": 0.6136041283607483,
      "learning_rate": 4.7230652503793635e-07,
      "loss": 0.3094,
      "step": 2388
    },
    {
      "epoch": 3.625189681335357,
      "grad_norm": 0.5962646007537842,
      "learning_rate": 4.7040971168437033e-07,
      "loss": 0.3168,
      "step": 2389
    },
    {
      "epoch": 3.6267071320182094,
      "grad_norm": 0.6175129413604736,
      "learning_rate": 4.6851289833080425e-07,
      "loss": 0.3198,
      "step": 2390
    },
    {
      "epoch": 3.628224582701062,
      "grad_norm": 0.6178389191627502,
      "learning_rate": 4.6661608497723823e-07,
      "loss": 0.323,
      "step": 2391
    },
    {
      "epoch": 3.629742033383915,
      "grad_norm": 0.6163281798362732,
      "learning_rate": 4.6471927162367226e-07,
      "loss": 0.3368,
      "step": 2392
    },
    {
      "epoch": 3.631259484066768,
      "grad_norm": 0.6296706795692444,
      "learning_rate": 4.6282245827010624e-07,
      "loss": 0.312,
      "step": 2393
    },
    {
      "epoch": 3.6327769347496206,
      "grad_norm": 0.6998986005783081,
      "learning_rate": 4.609256449165402e-07,
      "loss": 0.4142,
      "step": 2394
    },
    {
      "epoch": 3.634294385432473,
      "grad_norm": 0.6463344693183899,
      "learning_rate": 4.590288315629742e-07,
      "loss": 0.2661,
      "step": 2395
    },
    {
      "epoch": 3.635811836115326,
      "grad_norm": 0.6966448426246643,
      "learning_rate": 4.5713201820940823e-07,
      "loss": 0.4054,
      "step": 2396
    },
    {
      "epoch": 3.6373292867981792,
      "grad_norm": 0.7277569770812988,
      "learning_rate": 4.552352048558422e-07,
      "loss": 0.3019,
      "step": 2397
    },
    {
      "epoch": 3.638846737481032,
      "grad_norm": 0.7082681059837341,
      "learning_rate": 4.533383915022762e-07,
      "loss": 0.4321,
      "step": 2398
    },
    {
      "epoch": 3.6403641881638844,
      "grad_norm": 0.7556372284889221,
      "learning_rate": 4.5144157814871016e-07,
      "loss": 0.3302,
      "step": 2399
    },
    {
      "epoch": 3.6418816388467374,
      "grad_norm": 0.5756751894950867,
      "learning_rate": 4.495447647951442e-07,
      "loss": 0.3528,
      "step": 2400
    },
    {
      "epoch": 3.6433990895295905,
      "grad_norm": 0.551807165145874,
      "learning_rate": 4.476479514415782e-07,
      "loss": 0.3485,
      "step": 2401
    },
    {
      "epoch": 3.644916540212443,
      "grad_norm": 0.6240176558494568,
      "learning_rate": 4.4575113808801215e-07,
      "loss": 0.3145,
      "step": 2402
    },
    {
      "epoch": 3.6464339908952956,
      "grad_norm": 0.770011842250824,
      "learning_rate": 4.438543247344462e-07,
      "loss": 0.3647,
      "step": 2403
    },
    {
      "epoch": 3.6479514415781487,
      "grad_norm": 0.628086507320404,
      "learning_rate": 4.4195751138088016e-07,
      "loss": 0.3996,
      "step": 2404
    },
    {
      "epoch": 3.6494688922610017,
      "grad_norm": 0.6251811385154724,
      "learning_rate": 4.4006069802731414e-07,
      "loss": 0.3512,
      "step": 2405
    },
    {
      "epoch": 3.6509863429438543,
      "grad_norm": 0.722964346408844,
      "learning_rate": 4.381638846737481e-07,
      "loss": 0.3018,
      "step": 2406
    },
    {
      "epoch": 3.6525037936267073,
      "grad_norm": 0.7136405110359192,
      "learning_rate": 4.3626707132018215e-07,
      "loss": 0.3175,
      "step": 2407
    },
    {
      "epoch": 3.65402124430956,
      "grad_norm": 0.8333045840263367,
      "learning_rate": 4.3437025796661613e-07,
      "loss": 0.3742,
      "step": 2408
    },
    {
      "epoch": 3.655538694992413,
      "grad_norm": 0.628524124622345,
      "learning_rate": 4.324734446130501e-07,
      "loss": 0.3357,
      "step": 2409
    },
    {
      "epoch": 3.6570561456752655,
      "grad_norm": 0.758547842502594,
      "learning_rate": 4.305766312594841e-07,
      "loss": 0.3193,
      "step": 2410
    },
    {
      "epoch": 3.6585735963581185,
      "grad_norm": 0.7336419224739075,
      "learning_rate": 4.286798179059181e-07,
      "loss": 0.292,
      "step": 2411
    },
    {
      "epoch": 3.660091047040971,
      "grad_norm": 0.7635446190834045,
      "learning_rate": 4.267830045523521e-07,
      "loss": 0.3563,
      "step": 2412
    },
    {
      "epoch": 3.661608497723824,
      "grad_norm": 0.5642763376235962,
      "learning_rate": 4.248861911987861e-07,
      "loss": 0.236,
      "step": 2413
    },
    {
      "epoch": 3.6631259484066767,
      "grad_norm": 0.5928298830986023,
      "learning_rate": 4.229893778452201e-07,
      "loss": 0.3223,
      "step": 2414
    },
    {
      "epoch": 3.6646433990895297,
      "grad_norm": 0.6118178963661194,
      "learning_rate": 4.210925644916541e-07,
      "loss": 0.3903,
      "step": 2415
    },
    {
      "epoch": 3.6661608497723823,
      "grad_norm": 0.8389796018600464,
      "learning_rate": 4.1919575113808806e-07,
      "loss": 0.3712,
      "step": 2416
    },
    {
      "epoch": 3.6676783004552354,
      "grad_norm": 0.677452802658081,
      "learning_rate": 4.1729893778452204e-07,
      "loss": 0.2647,
      "step": 2417
    },
    {
      "epoch": 3.669195751138088,
      "grad_norm": 0.5835546851158142,
      "learning_rate": 4.1540212443095607e-07,
      "loss": 0.3659,
      "step": 2418
    },
    {
      "epoch": 3.670713201820941,
      "grad_norm": 0.6807457804679871,
      "learning_rate": 4.1350531107739005e-07,
      "loss": 0.3317,
      "step": 2419
    },
    {
      "epoch": 3.6722306525037935,
      "grad_norm": 0.581176221370697,
      "learning_rate": 4.1160849772382403e-07,
      "loss": 0.3069,
      "step": 2420
    },
    {
      "epoch": 3.6737481031866466,
      "grad_norm": 0.7147262692451477,
      "learning_rate": 4.09711684370258e-07,
      "loss": 0.3848,
      "step": 2421
    },
    {
      "epoch": 3.675265553869499,
      "grad_norm": 0.6001118421554565,
      "learning_rate": 4.0781487101669204e-07,
      "loss": 0.3827,
      "step": 2422
    },
    {
      "epoch": 3.676783004552352,
      "grad_norm": 0.5866952538490295,
      "learning_rate": 4.0591805766312596e-07,
      "loss": 0.3402,
      "step": 2423
    },
    {
      "epoch": 3.6783004552352048,
      "grad_norm": 0.579597532749176,
      "learning_rate": 4.0402124430955994e-07,
      "loss": 0.2587,
      "step": 2424
    },
    {
      "epoch": 3.679817905918058,
      "grad_norm": 0.6684035062789917,
      "learning_rate": 4.021244309559939e-07,
      "loss": 0.3009,
      "step": 2425
    },
    {
      "epoch": 3.6813353566009104,
      "grad_norm": 0.591463565826416,
      "learning_rate": 4.002276176024279e-07,
      "loss": 0.339,
      "step": 2426
    },
    {
      "epoch": 3.6828528072837634,
      "grad_norm": 0.7106773257255554,
      "learning_rate": 3.9833080424886193e-07,
      "loss": 0.4179,
      "step": 2427
    },
    {
      "epoch": 3.684370257966616,
      "grad_norm": 0.7123122215270996,
      "learning_rate": 3.964339908952959e-07,
      "loss": 0.4341,
      "step": 2428
    },
    {
      "epoch": 3.685887708649469,
      "grad_norm": 0.5599395036697388,
      "learning_rate": 3.945371775417299e-07,
      "loss": 0.2722,
      "step": 2429
    },
    {
      "epoch": 3.6874051593323216,
      "grad_norm": 0.6097015738487244,
      "learning_rate": 3.926403641881639e-07,
      "loss": 0.3247,
      "step": 2430
    },
    {
      "epoch": 3.6889226100151746,
      "grad_norm": 0.6014378666877747,
      "learning_rate": 3.907435508345979e-07,
      "loss": 0.3467,
      "step": 2431
    },
    {
      "epoch": 3.690440060698027,
      "grad_norm": 0.6626063585281372,
      "learning_rate": 3.888467374810319e-07,
      "loss": 0.2915,
      "step": 2432
    },
    {
      "epoch": 3.6919575113808802,
      "grad_norm": 0.6915204524993896,
      "learning_rate": 3.8694992412746586e-07,
      "loss": 0.389,
      "step": 2433
    },
    {
      "epoch": 3.693474962063733,
      "grad_norm": 0.689159095287323,
      "learning_rate": 3.850531107738999e-07,
      "loss": 0.3196,
      "step": 2434
    },
    {
      "epoch": 3.694992412746586,
      "grad_norm": 0.570745587348938,
      "learning_rate": 3.8315629742033387e-07,
      "loss": 0.2627,
      "step": 2435
    },
    {
      "epoch": 3.6965098634294384,
      "grad_norm": 0.598243236541748,
      "learning_rate": 3.8125948406676784e-07,
      "loss": 0.3273,
      "step": 2436
    },
    {
      "epoch": 3.6980273141122915,
      "grad_norm": 0.595473051071167,
      "learning_rate": 3.793626707132018e-07,
      "loss": 0.3744,
      "step": 2437
    },
    {
      "epoch": 3.699544764795144,
      "grad_norm": 0.6967988610267639,
      "learning_rate": 3.7746585735963585e-07,
      "loss": 0.4138,
      "step": 2438
    },
    {
      "epoch": 3.701062215477997,
      "grad_norm": 0.6809980869293213,
      "learning_rate": 3.7556904400606983e-07,
      "loss": 0.312,
      "step": 2439
    },
    {
      "epoch": 3.7025796661608497,
      "grad_norm": 0.5899787545204163,
      "learning_rate": 3.736722306525038e-07,
      "loss": 0.3815,
      "step": 2440
    },
    {
      "epoch": 3.7040971168437027,
      "grad_norm": 0.8788357973098755,
      "learning_rate": 3.7177541729893784e-07,
      "loss": 0.3851,
      "step": 2441
    },
    {
      "epoch": 3.7056145675265553,
      "grad_norm": 0.5919912457466125,
      "learning_rate": 3.698786039453718e-07,
      "loss": 0.2825,
      "step": 2442
    },
    {
      "epoch": 3.7071320182094083,
      "grad_norm": 0.8493963479995728,
      "learning_rate": 3.679817905918058e-07,
      "loss": 0.3378,
      "step": 2443
    },
    {
      "epoch": 3.708649468892261,
      "grad_norm": 0.5830265283584595,
      "learning_rate": 3.660849772382398e-07,
      "loss": 0.2671,
      "step": 2444
    },
    {
      "epoch": 3.710166919575114,
      "grad_norm": 0.733627200126648,
      "learning_rate": 3.641881638846738e-07,
      "loss": 0.3109,
      "step": 2445
    },
    {
      "epoch": 3.7116843702579665,
      "grad_norm": 0.6346585750579834,
      "learning_rate": 3.622913505311078e-07,
      "loss": 0.3602,
      "step": 2446
    },
    {
      "epoch": 3.7132018209408195,
      "grad_norm": 0.5815078616142273,
      "learning_rate": 3.6039453717754177e-07,
      "loss": 0.4231,
      "step": 2447
    },
    {
      "epoch": 3.714719271623672,
      "grad_norm": 0.7819437980651855,
      "learning_rate": 3.5849772382397574e-07,
      "loss": 0.446,
      "step": 2448
    },
    {
      "epoch": 3.716236722306525,
      "grad_norm": 0.6514448523521423,
      "learning_rate": 3.566009104704098e-07,
      "loss": 0.3249,
      "step": 2449
    },
    {
      "epoch": 3.7177541729893777,
      "grad_norm": 0.54094398021698,
      "learning_rate": 3.5470409711684375e-07,
      "loss": 0.3765,
      "step": 2450
    },
    {
      "epoch": 3.7192716236722307,
      "grad_norm": 0.6057764291763306,
      "learning_rate": 3.5280728376327773e-07,
      "loss": 0.3788,
      "step": 2451
    },
    {
      "epoch": 3.7207890743550833,
      "grad_norm": 0.6359969973564148,
      "learning_rate": 3.5091047040971176e-07,
      "loss": 0.2628,
      "step": 2452
    },
    {
      "epoch": 3.7223065250379364,
      "grad_norm": 0.6957624554634094,
      "learning_rate": 3.4901365705614574e-07,
      "loss": 0.3838,
      "step": 2453
    },
    {
      "epoch": 3.723823975720789,
      "grad_norm": 0.5460019111633301,
      "learning_rate": 3.471168437025797e-07,
      "loss": 0.3143,
      "step": 2454
    },
    {
      "epoch": 3.725341426403642,
      "grad_norm": 0.5958351492881775,
      "learning_rate": 3.452200303490137e-07,
      "loss": 0.3684,
      "step": 2455
    },
    {
      "epoch": 3.7268588770864945,
      "grad_norm": 0.545854926109314,
      "learning_rate": 3.433232169954476e-07,
      "loss": 0.2901,
      "step": 2456
    },
    {
      "epoch": 3.7283763277693476,
      "grad_norm": 0.5286843180656433,
      "learning_rate": 3.4142640364188166e-07,
      "loss": 0.3351,
      "step": 2457
    },
    {
      "epoch": 3.7298937784522,
      "grad_norm": 0.612864077091217,
      "learning_rate": 3.3952959028831563e-07,
      "loss": 0.3094,
      "step": 2458
    },
    {
      "epoch": 3.731411229135053,
      "grad_norm": 0.6443967819213867,
      "learning_rate": 3.376327769347496e-07,
      "loss": 0.3053,
      "step": 2459
    },
    {
      "epoch": 3.7329286798179058,
      "grad_norm": 0.5787863731384277,
      "learning_rate": 3.357359635811836e-07,
      "loss": 0.273,
      "step": 2460
    },
    {
      "epoch": 3.734446130500759,
      "grad_norm": 0.5643014311790466,
      "learning_rate": 3.338391502276176e-07,
      "loss": 0.3353,
      "step": 2461
    },
    {
      "epoch": 3.7359635811836114,
      "grad_norm": 0.6442890763282776,
      "learning_rate": 3.319423368740516e-07,
      "loss": 0.3188,
      "step": 2462
    },
    {
      "epoch": 3.7374810318664644,
      "grad_norm": 0.8085268139839172,
      "learning_rate": 3.300455235204856e-07,
      "loss": 0.3939,
      "step": 2463
    },
    {
      "epoch": 3.738998482549317,
      "grad_norm": 0.6660759449005127,
      "learning_rate": 3.281487101669196e-07,
      "loss": 0.2781,
      "step": 2464
    },
    {
      "epoch": 3.74051593323217,
      "grad_norm": 1.0660322904586792,
      "learning_rate": 3.262518968133536e-07,
      "loss": 0.2759,
      "step": 2465
    },
    {
      "epoch": 3.7420333839150226,
      "grad_norm": 0.5911706686019897,
      "learning_rate": 3.2435508345978757e-07,
      "loss": 0.3003,
      "step": 2466
    },
    {
      "epoch": 3.7435508345978756,
      "grad_norm": 0.5545117259025574,
      "learning_rate": 3.2245827010622155e-07,
      "loss": 0.4028,
      "step": 2467
    },
    {
      "epoch": 3.745068285280728,
      "grad_norm": 0.696492612361908,
      "learning_rate": 3.205614567526556e-07,
      "loss": 0.3779,
      "step": 2468
    },
    {
      "epoch": 3.7465857359635812,
      "grad_norm": 0.6628065705299377,
      "learning_rate": 3.1866464339908956e-07,
      "loss": 0.2969,
      "step": 2469
    },
    {
      "epoch": 3.748103186646434,
      "grad_norm": 0.5882891416549683,
      "learning_rate": 3.1676783004552353e-07,
      "loss": 0.2938,
      "step": 2470
    },
    {
      "epoch": 3.749620637329287,
      "grad_norm": 0.6231839656829834,
      "learning_rate": 3.148710166919575e-07,
      "loss": 0.3821,
      "step": 2471
    },
    {
      "epoch": 3.75113808801214,
      "grad_norm": 0.6501815915107727,
      "learning_rate": 3.1297420333839154e-07,
      "loss": 0.3371,
      "step": 2472
    },
    {
      "epoch": 3.7526555386949925,
      "grad_norm": 0.5889323353767395,
      "learning_rate": 3.110773899848255e-07,
      "loss": 0.3337,
      "step": 2473
    },
    {
      "epoch": 3.754172989377845,
      "grad_norm": 0.6021686792373657,
      "learning_rate": 3.091805766312595e-07,
      "loss": 0.2605,
      "step": 2474
    },
    {
      "epoch": 3.755690440060698,
      "grad_norm": 0.6960484981536865,
      "learning_rate": 3.072837632776935e-07,
      "loss": 0.3451,
      "step": 2475
    },
    {
      "epoch": 3.757207890743551,
      "grad_norm": 0.5698781609535217,
      "learning_rate": 3.053869499241275e-07,
      "loss": 0.3463,
      "step": 2476
    },
    {
      "epoch": 3.7587253414264037,
      "grad_norm": 0.690875768661499,
      "learning_rate": 3.034901365705615e-07,
      "loss": 0.3515,
      "step": 2477
    },
    {
      "epoch": 3.7602427921092563,
      "grad_norm": 0.7112894654273987,
      "learning_rate": 3.0159332321699547e-07,
      "loss": 0.3944,
      "step": 2478
    },
    {
      "epoch": 3.7617602427921093,
      "grad_norm": 0.6028826236724854,
      "learning_rate": 2.996965098634295e-07,
      "loss": 0.3214,
      "step": 2479
    },
    {
      "epoch": 3.7632776934749623,
      "grad_norm": 0.705826461315155,
      "learning_rate": 2.977996965098635e-07,
      "loss": 0.259,
      "step": 2480
    },
    {
      "epoch": 3.764795144157815,
      "grad_norm": 0.6545286178588867,
      "learning_rate": 2.959028831562974e-07,
      "loss": 0.3097,
      "step": 2481
    },
    {
      "epoch": 3.7663125948406675,
      "grad_norm": 0.7292355895042419,
      "learning_rate": 2.9400606980273143e-07,
      "loss": 0.4588,
      "step": 2482
    },
    {
      "epoch": 3.7678300455235205,
      "grad_norm": 0.5956377387046814,
      "learning_rate": 2.921092564491654e-07,
      "loss": 0.3304,
      "step": 2483
    },
    {
      "epoch": 3.7693474962063735,
      "grad_norm": 0.5802574753761292,
      "learning_rate": 2.902124430955994e-07,
      "loss": 0.4147,
      "step": 2484
    },
    {
      "epoch": 3.770864946889226,
      "grad_norm": 0.608919620513916,
      "learning_rate": 2.883156297420334e-07,
      "loss": 0.37,
      "step": 2485
    },
    {
      "epoch": 3.7723823975720787,
      "grad_norm": 0.6155621409416199,
      "learning_rate": 2.864188163884674e-07,
      "loss": 0.3431,
      "step": 2486
    },
    {
      "epoch": 3.7738998482549317,
      "grad_norm": 0.7036159038543701,
      "learning_rate": 2.845220030349014e-07,
      "loss": 0.3562,
      "step": 2487
    },
    {
      "epoch": 3.7754172989377848,
      "grad_norm": 0.5886510610580444,
      "learning_rate": 2.8262518968133536e-07,
      "loss": 0.3052,
      "step": 2488
    },
    {
      "epoch": 3.7769347496206374,
      "grad_norm": 0.7603327631950378,
      "learning_rate": 2.807283763277694e-07,
      "loss": 0.2818,
      "step": 2489
    },
    {
      "epoch": 3.77845220030349,
      "grad_norm": 0.651705801486969,
      "learning_rate": 2.7883156297420337e-07,
      "loss": 0.3644,
      "step": 2490
    },
    {
      "epoch": 3.779969650986343,
      "grad_norm": 0.7074829936027527,
      "learning_rate": 2.7693474962063735e-07,
      "loss": 0.3365,
      "step": 2491
    },
    {
      "epoch": 3.781487101669196,
      "grad_norm": 0.6385526657104492,
      "learning_rate": 2.750379362670713e-07,
      "loss": 0.3396,
      "step": 2492
    },
    {
      "epoch": 3.7830045523520486,
      "grad_norm": 0.6627464294433594,
      "learning_rate": 2.7314112291350536e-07,
      "loss": 0.3676,
      "step": 2493
    },
    {
      "epoch": 3.784522003034901,
      "grad_norm": 0.6730060577392578,
      "learning_rate": 2.7124430955993933e-07,
      "loss": 0.3486,
      "step": 2494
    },
    {
      "epoch": 3.786039453717754,
      "grad_norm": 0.6002183556556702,
      "learning_rate": 2.693474962063733e-07,
      "loss": 0.3013,
      "step": 2495
    },
    {
      "epoch": 3.787556904400607,
      "grad_norm": 0.7390202283859253,
      "learning_rate": 2.6745068285280734e-07,
      "loss": 0.3994,
      "step": 2496
    },
    {
      "epoch": 3.78907435508346,
      "grad_norm": 0.6078596711158752,
      "learning_rate": 2.6555386949924127e-07,
      "loss": 0.3768,
      "step": 2497
    },
    {
      "epoch": 3.7905918057663124,
      "grad_norm": 0.6079313158988953,
      "learning_rate": 2.6365705614567525e-07,
      "loss": 0.3723,
      "step": 2498
    },
    {
      "epoch": 3.7921092564491654,
      "grad_norm": 0.5703608393669128,
      "learning_rate": 2.617602427921093e-07,
      "loss": 0.3322,
      "step": 2499
    },
    {
      "epoch": 3.7936267071320184,
      "grad_norm": 0.5461058616638184,
      "learning_rate": 2.5986342943854326e-07,
      "loss": 0.3542,
      "step": 2500
    }
  ],
  "logging_steps": 1,
  "max_steps": 2636,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.7220952236032e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
