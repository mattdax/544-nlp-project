{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.3218770654329148,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0006609385327164573,
      "grad_norm": 1.4005711078643799,
      "learning_rate": 4.999173826834105e-06,
      "loss": 3.079,
      "step": 1
    },
    {
      "epoch": 0.0013218770654329147,
      "grad_norm": 1.997644066810608,
      "learning_rate": 4.998347653668209e-06,
      "loss": 3.1937,
      "step": 2
    },
    {
      "epoch": 0.0019828155981493722,
      "grad_norm": 1.4142615795135498,
      "learning_rate": 4.997521480502313e-06,
      "loss": 3.0553,
      "step": 3
    },
    {
      "epoch": 0.0026437541308658294,
      "grad_norm": 1.4031556844711304,
      "learning_rate": 4.996695307336419e-06,
      "loss": 3.0516,
      "step": 4
    },
    {
      "epoch": 0.003304692663582287,
      "grad_norm": 1.5555206537246704,
      "learning_rate": 4.995869134170522e-06,
      "loss": 2.9373,
      "step": 5
    },
    {
      "epoch": 0.0039656311962987445,
      "grad_norm": 2.288196086883545,
      "learning_rate": 4.995042961004627e-06,
      "loss": 3.1206,
      "step": 6
    },
    {
      "epoch": 0.004626569729015202,
      "grad_norm": 1.5020673274993896,
      "learning_rate": 4.994216787838732e-06,
      "loss": 3.1639,
      "step": 7
    },
    {
      "epoch": 0.005287508261731659,
      "grad_norm": 1.4617024660110474,
      "learning_rate": 4.993390614672836e-06,
      "loss": 3.0603,
      "step": 8
    },
    {
      "epoch": 0.005948446794448116,
      "grad_norm": 1.5572799444198608,
      "learning_rate": 4.99256444150694e-06,
      "loss": 3.209,
      "step": 9
    },
    {
      "epoch": 0.006609385327164574,
      "grad_norm": 1.4235516786575317,
      "learning_rate": 4.991738268341045e-06,
      "loss": 2.9904,
      "step": 10
    },
    {
      "epoch": 0.007270323859881031,
      "grad_norm": 1.42690908908844,
      "learning_rate": 4.990912095175149e-06,
      "loss": 3.0097,
      "step": 11
    },
    {
      "epoch": 0.007931262392597489,
      "grad_norm": 1.5257008075714111,
      "learning_rate": 4.990085922009253e-06,
      "loss": 3.0696,
      "step": 12
    },
    {
      "epoch": 0.008592200925313946,
      "grad_norm": 1.5257415771484375,
      "learning_rate": 4.9892597488433584e-06,
      "loss": 3.0184,
      "step": 13
    },
    {
      "epoch": 0.009253139458030404,
      "grad_norm": 1.528669834136963,
      "learning_rate": 4.988433575677463e-06,
      "loss": 3.0577,
      "step": 14
    },
    {
      "epoch": 0.00991407799074686,
      "grad_norm": 1.5488026142120361,
      "learning_rate": 4.987607402511567e-06,
      "loss": 3.0926,
      "step": 15
    },
    {
      "epoch": 0.010575016523463317,
      "grad_norm": 1.5138288736343384,
      "learning_rate": 4.9867812293456714e-06,
      "loss": 3.0774,
      "step": 16
    },
    {
      "epoch": 0.011235955056179775,
      "grad_norm": 1.5463603734970093,
      "learning_rate": 4.985955056179776e-06,
      "loss": 3.0162,
      "step": 17
    },
    {
      "epoch": 0.011896893588896233,
      "grad_norm": 1.5815919637680054,
      "learning_rate": 4.98512888301388e-06,
      "loss": 3.0619,
      "step": 18
    },
    {
      "epoch": 0.01255783212161269,
      "grad_norm": 1.5388901233673096,
      "learning_rate": 4.9843027098479844e-06,
      "loss": 3.0147,
      "step": 19
    },
    {
      "epoch": 0.013218770654329148,
      "grad_norm": 1.64370858669281,
      "learning_rate": 4.98347653668209e-06,
      "loss": 3.0803,
      "step": 20
    },
    {
      "epoch": 0.013879709187045605,
      "grad_norm": 1.516729712486267,
      "learning_rate": 4.982650363516193e-06,
      "loss": 2.9248,
      "step": 21
    },
    {
      "epoch": 0.014540647719762063,
      "grad_norm": 1.995240330696106,
      "learning_rate": 4.9818241903502974e-06,
      "loss": 3.0045,
      "step": 22
    },
    {
      "epoch": 0.01520158625247852,
      "grad_norm": 1.7044057846069336,
      "learning_rate": 4.980998017184403e-06,
      "loss": 3.0098,
      "step": 23
    },
    {
      "epoch": 0.015862524785194978,
      "grad_norm": 1.6248955726623535,
      "learning_rate": 4.980171844018506e-06,
      "loss": 3.0345,
      "step": 24
    },
    {
      "epoch": 0.016523463317911435,
      "grad_norm": 1.6041439771652222,
      "learning_rate": 4.979345670852611e-06,
      "loss": 3.0119,
      "step": 25
    },
    {
      "epoch": 0.017184401850627893,
      "grad_norm": 1.715948462486267,
      "learning_rate": 4.978519497686716e-06,
      "loss": 3.109,
      "step": 26
    },
    {
      "epoch": 0.01784534038334435,
      "grad_norm": 1.6810417175292969,
      "learning_rate": 4.97769332452082e-06,
      "loss": 3.0654,
      "step": 27
    },
    {
      "epoch": 0.018506278916060808,
      "grad_norm": 1.8283010721206665,
      "learning_rate": 4.976867151354924e-06,
      "loss": 2.9511,
      "step": 28
    },
    {
      "epoch": 0.019167217448777262,
      "grad_norm": 1.473180890083313,
      "learning_rate": 4.976040978189029e-06,
      "loss": 2.8042,
      "step": 29
    },
    {
      "epoch": 0.01982815598149372,
      "grad_norm": 1.663628339767456,
      "learning_rate": 4.975214805023133e-06,
      "loss": 3.0318,
      "step": 30
    },
    {
      "epoch": 0.020489094514210177,
      "grad_norm": 1.6431506872177124,
      "learning_rate": 4.974388631857237e-06,
      "loss": 3.011,
      "step": 31
    },
    {
      "epoch": 0.021150033046926635,
      "grad_norm": 1.6622884273529053,
      "learning_rate": 4.9735624586913425e-06,
      "loss": 2.9919,
      "step": 32
    },
    {
      "epoch": 0.021810971579643092,
      "grad_norm": 1.7212798595428467,
      "learning_rate": 4.972736285525446e-06,
      "loss": 3.0223,
      "step": 33
    },
    {
      "epoch": 0.02247191011235955,
      "grad_norm": 1.7496756315231323,
      "learning_rate": 4.971910112359551e-06,
      "loss": 2.9482,
      "step": 34
    },
    {
      "epoch": 0.023132848645076007,
      "grad_norm": 1.6948050260543823,
      "learning_rate": 4.9710839391936555e-06,
      "loss": 2.888,
      "step": 35
    },
    {
      "epoch": 0.023793787177792465,
      "grad_norm": 1.6244698762893677,
      "learning_rate": 4.97025776602776e-06,
      "loss": 2.8699,
      "step": 36
    },
    {
      "epoch": 0.024454725710508923,
      "grad_norm": 1.691523790359497,
      "learning_rate": 4.969431592861864e-06,
      "loss": 2.9483,
      "step": 37
    },
    {
      "epoch": 0.02511566424322538,
      "grad_norm": 1.7434515953063965,
      "learning_rate": 4.9686054196959685e-06,
      "loss": 2.8664,
      "step": 38
    },
    {
      "epoch": 0.025776602775941838,
      "grad_norm": 1.680916666984558,
      "learning_rate": 4.967779246530074e-06,
      "loss": 2.887,
      "step": 39
    },
    {
      "epoch": 0.026437541308658295,
      "grad_norm": 2.301673412322998,
      "learning_rate": 4.966953073364177e-06,
      "loss": 2.9429,
      "step": 40
    },
    {
      "epoch": 0.027098479841374753,
      "grad_norm": 1.751227855682373,
      "learning_rate": 4.966126900198282e-06,
      "loss": 2.9333,
      "step": 41
    },
    {
      "epoch": 0.02775941837409121,
      "grad_norm": 1.7651689052581787,
      "learning_rate": 4.965300727032387e-06,
      "loss": 2.8815,
      "step": 42
    },
    {
      "epoch": 0.028420356906807668,
      "grad_norm": 2.3545968532562256,
      "learning_rate": 4.964474553866491e-06,
      "loss": 2.9269,
      "step": 43
    },
    {
      "epoch": 0.029081295439524125,
      "grad_norm": 1.8220851421356201,
      "learning_rate": 4.963648380700595e-06,
      "loss": 2.9436,
      "step": 44
    },
    {
      "epoch": 0.029742233972240583,
      "grad_norm": 1.7581517696380615,
      "learning_rate": 4.9628222075347e-06,
      "loss": 2.8995,
      "step": 45
    },
    {
      "epoch": 0.03040317250495704,
      "grad_norm": 1.7981290817260742,
      "learning_rate": 4.961996034368804e-06,
      "loss": 2.8687,
      "step": 46
    },
    {
      "epoch": 0.031064111037673495,
      "grad_norm": 1.891503930091858,
      "learning_rate": 4.961169861202908e-06,
      "loss": 2.9218,
      "step": 47
    },
    {
      "epoch": 0.031725049570389956,
      "grad_norm": 1.8256828784942627,
      "learning_rate": 4.9603436880370135e-06,
      "loss": 2.9185,
      "step": 48
    },
    {
      "epoch": 0.03238598810310641,
      "grad_norm": 1.8332817554473877,
      "learning_rate": 4.959517514871117e-06,
      "loss": 2.8672,
      "step": 49
    },
    {
      "epoch": 0.03304692663582287,
      "grad_norm": 1.8252222537994385,
      "learning_rate": 4.958691341705222e-06,
      "loss": 2.8694,
      "step": 50
    },
    {
      "epoch": 0.033707865168539325,
      "grad_norm": 1.7973591089248657,
      "learning_rate": 4.9578651685393265e-06,
      "loss": 2.8691,
      "step": 51
    },
    {
      "epoch": 0.034368803701255786,
      "grad_norm": 1.8755789995193481,
      "learning_rate": 4.957038995373431e-06,
      "loss": 2.8542,
      "step": 52
    },
    {
      "epoch": 0.03502974223397224,
      "grad_norm": 1.8552229404449463,
      "learning_rate": 4.956212822207535e-06,
      "loss": 2.9254,
      "step": 53
    },
    {
      "epoch": 0.0356906807666887,
      "grad_norm": 1.8232207298278809,
      "learning_rate": 4.9553866490416395e-06,
      "loss": 2.8441,
      "step": 54
    },
    {
      "epoch": 0.036351619299405155,
      "grad_norm": 1.8204385042190552,
      "learning_rate": 4.954560475875744e-06,
      "loss": 2.849,
      "step": 55
    },
    {
      "epoch": 0.037012557832121616,
      "grad_norm": 1.8583855628967285,
      "learning_rate": 4.953734302709848e-06,
      "loss": 2.906,
      "step": 56
    },
    {
      "epoch": 0.03767349636483807,
      "grad_norm": 1.9378479719161987,
      "learning_rate": 4.952908129543953e-06,
      "loss": 2.8428,
      "step": 57
    },
    {
      "epoch": 0.038334434897554524,
      "grad_norm": 1.8995692729949951,
      "learning_rate": 4.952081956378057e-06,
      "loss": 2.81,
      "step": 58
    },
    {
      "epoch": 0.038995373430270985,
      "grad_norm": 1.897080421447754,
      "learning_rate": 4.951255783212162e-06,
      "loss": 2.8116,
      "step": 59
    },
    {
      "epoch": 0.03965631196298744,
      "grad_norm": 1.9231780767440796,
      "learning_rate": 4.950429610046266e-06,
      "loss": 2.8508,
      "step": 60
    },
    {
      "epoch": 0.0403172504957039,
      "grad_norm": 1.9701685905456543,
      "learning_rate": 4.949603436880371e-06,
      "loss": 2.8156,
      "step": 61
    },
    {
      "epoch": 0.040978189028420355,
      "grad_norm": 1.9705959558486938,
      "learning_rate": 4.948777263714475e-06,
      "loss": 2.8342,
      "step": 62
    },
    {
      "epoch": 0.041639127561136816,
      "grad_norm": 2.526210069656372,
      "learning_rate": 4.947951090548579e-06,
      "loss": 2.7088,
      "step": 63
    },
    {
      "epoch": 0.04230006609385327,
      "grad_norm": 1.9214668273925781,
      "learning_rate": 4.947124917382684e-06,
      "loss": 2.7686,
      "step": 64
    },
    {
      "epoch": 0.04296100462656973,
      "grad_norm": 1.9305036067962646,
      "learning_rate": 4.946298744216788e-06,
      "loss": 2.8184,
      "step": 65
    },
    {
      "epoch": 0.043621943159286185,
      "grad_norm": 1.9191267490386963,
      "learning_rate": 4.945472571050892e-06,
      "loss": 2.7738,
      "step": 66
    },
    {
      "epoch": 0.044282881692002646,
      "grad_norm": 1.9538345336914062,
      "learning_rate": 4.9446463978849975e-06,
      "loss": 2.8035,
      "step": 67
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 1.9249752759933472,
      "learning_rate": 4.943820224719101e-06,
      "loss": 2.7776,
      "step": 68
    },
    {
      "epoch": 0.04560475875743556,
      "grad_norm": 2.077332019805908,
      "learning_rate": 4.942994051553206e-06,
      "loss": 2.8514,
      "step": 69
    },
    {
      "epoch": 0.046265697290152015,
      "grad_norm": 1.991405963897705,
      "learning_rate": 4.9421678783873105e-06,
      "loss": 2.7658,
      "step": 70
    },
    {
      "epoch": 0.046926635822868476,
      "grad_norm": 2.1819472312927246,
      "learning_rate": 4.941341705221415e-06,
      "loss": 2.7924,
      "step": 71
    },
    {
      "epoch": 0.04758757435558493,
      "grad_norm": 1.929642677307129,
      "learning_rate": 4.940515532055519e-06,
      "loss": 2.7518,
      "step": 72
    },
    {
      "epoch": 0.04824851288830139,
      "grad_norm": 1.9053728580474854,
      "learning_rate": 4.9396893588896235e-06,
      "loss": 2.6744,
      "step": 73
    },
    {
      "epoch": 0.048909451421017845,
      "grad_norm": 2.0295252799987793,
      "learning_rate": 4.938863185723728e-06,
      "loss": 2.7892,
      "step": 74
    },
    {
      "epoch": 0.0495703899537343,
      "grad_norm": 2.0262107849121094,
      "learning_rate": 4.938037012557832e-06,
      "loss": 2.8067,
      "step": 75
    },
    {
      "epoch": 0.05023132848645076,
      "grad_norm": 2.105534791946411,
      "learning_rate": 4.937210839391937e-06,
      "loss": 2.7072,
      "step": 76
    },
    {
      "epoch": 0.050892267019167214,
      "grad_norm": 2.0825114250183105,
      "learning_rate": 4.936384666226041e-06,
      "loss": 2.7566,
      "step": 77
    },
    {
      "epoch": 0.051553205551883675,
      "grad_norm": 1.9831475019454956,
      "learning_rate": 4.935558493060146e-06,
      "loss": 2.6796,
      "step": 78
    },
    {
      "epoch": 0.05221414408460013,
      "grad_norm": 1.842248558998108,
      "learning_rate": 4.93473231989425e-06,
      "loss": 2.5953,
      "step": 79
    },
    {
      "epoch": 0.05287508261731659,
      "grad_norm": 1.987701416015625,
      "learning_rate": 4.933906146728355e-06,
      "loss": 2.64,
      "step": 80
    },
    {
      "epoch": 0.053536021150033045,
      "grad_norm": 1.9849730730056763,
      "learning_rate": 4.933079973562459e-06,
      "loss": 2.726,
      "step": 81
    },
    {
      "epoch": 0.054196959682749506,
      "grad_norm": 2.0428640842437744,
      "learning_rate": 4.932253800396563e-06,
      "loss": 2.7592,
      "step": 82
    },
    {
      "epoch": 0.05485789821546596,
      "grad_norm": 2.019193410873413,
      "learning_rate": 4.931427627230668e-06,
      "loss": 2.6631,
      "step": 83
    },
    {
      "epoch": 0.05551883674818242,
      "grad_norm": 1.9742246866226196,
      "learning_rate": 4.930601454064772e-06,
      "loss": 2.684,
      "step": 84
    },
    {
      "epoch": 0.056179775280898875,
      "grad_norm": 1.945424199104309,
      "learning_rate": 4.929775280898877e-06,
      "loss": 2.5833,
      "step": 85
    },
    {
      "epoch": 0.056840713813615336,
      "grad_norm": 2.5429794788360596,
      "learning_rate": 4.928949107732981e-06,
      "loss": 2.6708,
      "step": 86
    },
    {
      "epoch": 0.05750165234633179,
      "grad_norm": 2.1376447677612305,
      "learning_rate": 4.928122934567086e-06,
      "loss": 2.762,
      "step": 87
    },
    {
      "epoch": 0.05816259087904825,
      "grad_norm": 2.0151970386505127,
      "learning_rate": 4.92729676140119e-06,
      "loss": 2.6284,
      "step": 88
    },
    {
      "epoch": 0.058823529411764705,
      "grad_norm": 2.057809352874756,
      "learning_rate": 4.9264705882352945e-06,
      "loss": 2.6069,
      "step": 89
    },
    {
      "epoch": 0.059484467944481166,
      "grad_norm": 2.343465805053711,
      "learning_rate": 4.925644415069399e-06,
      "loss": 2.6489,
      "step": 90
    },
    {
      "epoch": 0.06014540647719762,
      "grad_norm": 1.9881248474121094,
      "learning_rate": 4.924818241903503e-06,
      "loss": 2.5915,
      "step": 91
    },
    {
      "epoch": 0.06080634500991408,
      "grad_norm": 1.9702702760696411,
      "learning_rate": 4.923992068737608e-06,
      "loss": 2.5542,
      "step": 92
    },
    {
      "epoch": 0.061467283542630535,
      "grad_norm": 2.101930618286133,
      "learning_rate": 4.923165895571712e-06,
      "loss": 2.6214,
      "step": 93
    },
    {
      "epoch": 0.06212822207534699,
      "grad_norm": 2.1357686519622803,
      "learning_rate": 4.922339722405817e-06,
      "loss": 2.5965,
      "step": 94
    },
    {
      "epoch": 0.06278916060806344,
      "grad_norm": 1.8995221853256226,
      "learning_rate": 4.921513549239921e-06,
      "loss": 2.4656,
      "step": 95
    },
    {
      "epoch": 0.06345009914077991,
      "grad_norm": 2.0224416255950928,
      "learning_rate": 4.920687376074026e-06,
      "loss": 2.5832,
      "step": 96
    },
    {
      "epoch": 0.06411103767349637,
      "grad_norm": 2.014092206954956,
      "learning_rate": 4.91986120290813e-06,
      "loss": 2.5897,
      "step": 97
    },
    {
      "epoch": 0.06477197620621282,
      "grad_norm": 1.9652372598648071,
      "learning_rate": 4.919035029742234e-06,
      "loss": 2.565,
      "step": 98
    },
    {
      "epoch": 0.06543291473892927,
      "grad_norm": 2.0571329593658447,
      "learning_rate": 4.918208856576339e-06,
      "loss": 2.5763,
      "step": 99
    },
    {
      "epoch": 0.06609385327164574,
      "grad_norm": 2.028899669647217,
      "learning_rate": 4.917382683410443e-06,
      "loss": 2.5452,
      "step": 100
    },
    {
      "epoch": 0.0667547918043622,
      "grad_norm": 1.9691815376281738,
      "learning_rate": 4.916556510244548e-06,
      "loss": 2.5037,
      "step": 101
    },
    {
      "epoch": 0.06741573033707865,
      "grad_norm": 2.0027546882629395,
      "learning_rate": 4.915730337078652e-06,
      "loss": 2.5681,
      "step": 102
    },
    {
      "epoch": 0.0680766688697951,
      "grad_norm": 2.0147252082824707,
      "learning_rate": 4.914904163912757e-06,
      "loss": 2.5518,
      "step": 103
    },
    {
      "epoch": 0.06873760740251157,
      "grad_norm": 1.8276948928833008,
      "learning_rate": 4.914077990746861e-06,
      "loss": 2.4068,
      "step": 104
    },
    {
      "epoch": 0.06939854593522803,
      "grad_norm": 2.1989026069641113,
      "learning_rate": 4.9132518175809656e-06,
      "loss": 2.4986,
      "step": 105
    },
    {
      "epoch": 0.07005948446794448,
      "grad_norm": 2.0116665363311768,
      "learning_rate": 4.91242564441507e-06,
      "loss": 2.5325,
      "step": 106
    },
    {
      "epoch": 0.07072042300066093,
      "grad_norm": 1.8622798919677734,
      "learning_rate": 4.911599471249174e-06,
      "loss": 2.4672,
      "step": 107
    },
    {
      "epoch": 0.0713813615333774,
      "grad_norm": 2.086193084716797,
      "learning_rate": 4.9107732980832786e-06,
      "loss": 2.6025,
      "step": 108
    },
    {
      "epoch": 0.07204230006609386,
      "grad_norm": 1.994432806968689,
      "learning_rate": 4.909947124917383e-06,
      "loss": 2.4888,
      "step": 109
    },
    {
      "epoch": 0.07270323859881031,
      "grad_norm": 1.8918005228042603,
      "learning_rate": 4.909120951751487e-06,
      "loss": 2.4598,
      "step": 110
    },
    {
      "epoch": 0.07336417713152676,
      "grad_norm": 1.9245164394378662,
      "learning_rate": 4.9082947785855916e-06,
      "loss": 2.4229,
      "step": 111
    },
    {
      "epoch": 0.07402511566424323,
      "grad_norm": 1.8435667753219604,
      "learning_rate": 4.907468605419696e-06,
      "loss": 2.4064,
      "step": 112
    },
    {
      "epoch": 0.07468605419695969,
      "grad_norm": 1.8779711723327637,
      "learning_rate": 4.906642432253801e-06,
      "loss": 2.4068,
      "step": 113
    },
    {
      "epoch": 0.07534699272967614,
      "grad_norm": 1.8902631998062134,
      "learning_rate": 4.905816259087905e-06,
      "loss": 2.3717,
      "step": 114
    },
    {
      "epoch": 0.0760079312623926,
      "grad_norm": 1.8585724830627441,
      "learning_rate": 4.90499008592201e-06,
      "loss": 2.4084,
      "step": 115
    },
    {
      "epoch": 0.07666886979510905,
      "grad_norm": 2.0681371688842773,
      "learning_rate": 4.904163912756114e-06,
      "loss": 2.4637,
      "step": 116
    },
    {
      "epoch": 0.07732980832782552,
      "grad_norm": 2.0200300216674805,
      "learning_rate": 4.903337739590218e-06,
      "loss": 2.4391,
      "step": 117
    },
    {
      "epoch": 0.07799074686054197,
      "grad_norm": 1.9250898361206055,
      "learning_rate": 4.902511566424323e-06,
      "loss": 2.3676,
      "step": 118
    },
    {
      "epoch": 0.07865168539325842,
      "grad_norm": 1.8594087362289429,
      "learning_rate": 4.901685393258427e-06,
      "loss": 2.366,
      "step": 119
    },
    {
      "epoch": 0.07931262392597488,
      "grad_norm": 1.8311253786087036,
      "learning_rate": 4.900859220092532e-06,
      "loss": 2.2878,
      "step": 120
    },
    {
      "epoch": 0.07997356245869135,
      "grad_norm": 2.098421573638916,
      "learning_rate": 4.900033046926636e-06,
      "loss": 2.4245,
      "step": 121
    },
    {
      "epoch": 0.0806345009914078,
      "grad_norm": 1.886752724647522,
      "learning_rate": 4.899206873760741e-06,
      "loss": 2.4277,
      "step": 122
    },
    {
      "epoch": 0.08129543952412426,
      "grad_norm": 1.7490092515945435,
      "learning_rate": 4.898380700594845e-06,
      "loss": 2.2863,
      "step": 123
    },
    {
      "epoch": 0.08195637805684071,
      "grad_norm": 1.8936113119125366,
      "learning_rate": 4.89755452742895e-06,
      "loss": 2.3823,
      "step": 124
    },
    {
      "epoch": 0.08261731658955718,
      "grad_norm": 1.8998316526412964,
      "learning_rate": 4.896728354263054e-06,
      "loss": 2.4038,
      "step": 125
    },
    {
      "epoch": 0.08327825512227363,
      "grad_norm": 1.796960711479187,
      "learning_rate": 4.895902181097158e-06,
      "loss": 2.326,
      "step": 126
    },
    {
      "epoch": 0.08393919365499009,
      "grad_norm": 1.894325852394104,
      "learning_rate": 4.895076007931263e-06,
      "loss": 2.3928,
      "step": 127
    },
    {
      "epoch": 0.08460013218770654,
      "grad_norm": 1.8418384790420532,
      "learning_rate": 4.894249834765367e-06,
      "loss": 2.3256,
      "step": 128
    },
    {
      "epoch": 0.08526107072042301,
      "grad_norm": 1.8537873029708862,
      "learning_rate": 4.893423661599472e-06,
      "loss": 2.3528,
      "step": 129
    },
    {
      "epoch": 0.08592200925313946,
      "grad_norm": 1.9295352697372437,
      "learning_rate": 4.892597488433576e-06,
      "loss": 2.359,
      "step": 130
    },
    {
      "epoch": 0.08658294778585592,
      "grad_norm": 1.7552698850631714,
      "learning_rate": 4.891771315267681e-06,
      "loss": 2.2839,
      "step": 131
    },
    {
      "epoch": 0.08724388631857237,
      "grad_norm": 1.7339398860931396,
      "learning_rate": 4.890945142101785e-06,
      "loss": 2.269,
      "step": 132
    },
    {
      "epoch": 0.08790482485128882,
      "grad_norm": 1.701088547706604,
      "learning_rate": 4.8901189689358894e-06,
      "loss": 2.2583,
      "step": 133
    },
    {
      "epoch": 0.08856576338400529,
      "grad_norm": 1.6418612003326416,
      "learning_rate": 4.889292795769994e-06,
      "loss": 2.1974,
      "step": 134
    },
    {
      "epoch": 0.08922670191672175,
      "grad_norm": 1.7571924924850464,
      "learning_rate": 4.888466622604098e-06,
      "loss": 2.2496,
      "step": 135
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 1.7543162107467651,
      "learning_rate": 4.8876404494382024e-06,
      "loss": 2.1916,
      "step": 136
    },
    {
      "epoch": 0.09054857898215465,
      "grad_norm": 1.7041460275650024,
      "learning_rate": 4.886814276272307e-06,
      "loss": 2.234,
      "step": 137
    },
    {
      "epoch": 0.09120951751487112,
      "grad_norm": 1.6668424606323242,
      "learning_rate": 4.885988103106412e-06,
      "loss": 2.1959,
      "step": 138
    },
    {
      "epoch": 0.09187045604758758,
      "grad_norm": 1.7996070384979248,
      "learning_rate": 4.885161929940516e-06,
      "loss": 2.2303,
      "step": 139
    },
    {
      "epoch": 0.09253139458030403,
      "grad_norm": 1.6736938953399658,
      "learning_rate": 4.884335756774621e-06,
      "loss": 2.2041,
      "step": 140
    },
    {
      "epoch": 0.09319233311302048,
      "grad_norm": 1.6325297355651855,
      "learning_rate": 4.883509583608725e-06,
      "loss": 2.1797,
      "step": 141
    },
    {
      "epoch": 0.09385327164573695,
      "grad_norm": 1.7144660949707031,
      "learning_rate": 4.882683410442829e-06,
      "loss": 2.2399,
      "step": 142
    },
    {
      "epoch": 0.0945142101784534,
      "grad_norm": 1.6747082471847534,
      "learning_rate": 4.881857237276934e-06,
      "loss": 2.1639,
      "step": 143
    },
    {
      "epoch": 0.09517514871116986,
      "grad_norm": 1.72825288772583,
      "learning_rate": 4.881031064111038e-06,
      "loss": 2.2496,
      "step": 144
    },
    {
      "epoch": 0.09583608724388631,
      "grad_norm": 1.5909136533737183,
      "learning_rate": 4.880204890945143e-06,
      "loss": 2.1746,
      "step": 145
    },
    {
      "epoch": 0.09649702577660278,
      "grad_norm": 1.644136667251587,
      "learning_rate": 4.879378717779247e-06,
      "loss": 2.2066,
      "step": 146
    },
    {
      "epoch": 0.09715796430931924,
      "grad_norm": 1.6308926343917847,
      "learning_rate": 4.878552544613352e-06,
      "loss": 2.1508,
      "step": 147
    },
    {
      "epoch": 0.09781890284203569,
      "grad_norm": 1.7100327014923096,
      "learning_rate": 4.877726371447456e-06,
      "loss": 2.2113,
      "step": 148
    },
    {
      "epoch": 0.09847984137475214,
      "grad_norm": 1.6442875862121582,
      "learning_rate": 4.87690019828156e-06,
      "loss": 2.1677,
      "step": 149
    },
    {
      "epoch": 0.0991407799074686,
      "grad_norm": 1.578938364982605,
      "learning_rate": 4.876074025115665e-06,
      "loss": 2.145,
      "step": 150
    },
    {
      "epoch": 0.09980171844018507,
      "grad_norm": 1.7589948177337646,
      "learning_rate": 4.875247851949769e-06,
      "loss": 2.2289,
      "step": 151
    },
    {
      "epoch": 0.10046265697290152,
      "grad_norm": 1.5449533462524414,
      "learning_rate": 4.8744216787838735e-06,
      "loss": 2.1098,
      "step": 152
    },
    {
      "epoch": 0.10112359550561797,
      "grad_norm": 1.6386624574661255,
      "learning_rate": 4.873595505617978e-06,
      "loss": 2.1254,
      "step": 153
    },
    {
      "epoch": 0.10178453403833443,
      "grad_norm": 1.5473750829696655,
      "learning_rate": 4.872769332452082e-06,
      "loss": 2.155,
      "step": 154
    },
    {
      "epoch": 0.1024454725710509,
      "grad_norm": 1.4907310009002686,
      "learning_rate": 4.8719431592861865e-06,
      "loss": 2.096,
      "step": 155
    },
    {
      "epoch": 0.10310641110376735,
      "grad_norm": 1.614349603652954,
      "learning_rate": 4.871116986120291e-06,
      "loss": 2.139,
      "step": 156
    },
    {
      "epoch": 0.1037673496364838,
      "grad_norm": 1.4470409154891968,
      "learning_rate": 4.870290812954396e-06,
      "loss": 2.0472,
      "step": 157
    },
    {
      "epoch": 0.10442828816920026,
      "grad_norm": 1.4880987405776978,
      "learning_rate": 4.8694646397884995e-06,
      "loss": 2.0707,
      "step": 158
    },
    {
      "epoch": 0.10508922670191673,
      "grad_norm": 1.5199474096298218,
      "learning_rate": 4.868638466622605e-06,
      "loss": 2.1099,
      "step": 159
    },
    {
      "epoch": 0.10575016523463318,
      "grad_norm": 1.5216703414916992,
      "learning_rate": 4.867812293456709e-06,
      "loss": 2.0682,
      "step": 160
    },
    {
      "epoch": 0.10641110376734964,
      "grad_norm": 1.498556137084961,
      "learning_rate": 4.866986120290813e-06,
      "loss": 2.0581,
      "step": 161
    },
    {
      "epoch": 0.10707204230006609,
      "grad_norm": 1.497844934463501,
      "learning_rate": 4.866159947124918e-06,
      "loss": 2.1107,
      "step": 162
    },
    {
      "epoch": 0.10773298083278256,
      "grad_norm": 1.4581305980682373,
      "learning_rate": 4.865333773959022e-06,
      "loss": 2.0411,
      "step": 163
    },
    {
      "epoch": 0.10839391936549901,
      "grad_norm": 1.4937142133712769,
      "learning_rate": 4.864507600793127e-06,
      "loss": 2.0583,
      "step": 164
    },
    {
      "epoch": 0.10905485789821547,
      "grad_norm": 1.881799340248108,
      "learning_rate": 4.863681427627231e-06,
      "loss": 2.0636,
      "step": 165
    },
    {
      "epoch": 0.10971579643093192,
      "grad_norm": 1.4578202962875366,
      "learning_rate": 4.862855254461336e-06,
      "loss": 2.0635,
      "step": 166
    },
    {
      "epoch": 0.11037673496364839,
      "grad_norm": 1.3715510368347168,
      "learning_rate": 4.86202908129544e-06,
      "loss": 2.0324,
      "step": 167
    },
    {
      "epoch": 0.11103767349636484,
      "grad_norm": 1.5240275859832764,
      "learning_rate": 4.8612029081295445e-06,
      "loss": 2.0595,
      "step": 168
    },
    {
      "epoch": 0.1116986120290813,
      "grad_norm": 1.5044569969177246,
      "learning_rate": 4.860376734963649e-06,
      "loss": 2.028,
      "step": 169
    },
    {
      "epoch": 0.11235955056179775,
      "grad_norm": 1.3468403816223145,
      "learning_rate": 4.859550561797753e-06,
      "loss": 1.9583,
      "step": 170
    },
    {
      "epoch": 0.1130204890945142,
      "grad_norm": 1.4361428022384644,
      "learning_rate": 4.8587243886318575e-06,
      "loss": 2.0498,
      "step": 171
    },
    {
      "epoch": 0.11368142762723067,
      "grad_norm": 1.4250413179397583,
      "learning_rate": 4.857898215465962e-06,
      "loss": 2.0204,
      "step": 172
    },
    {
      "epoch": 0.11434236615994713,
      "grad_norm": 1.3667571544647217,
      "learning_rate": 4.857072042300067e-06,
      "loss": 1.9573,
      "step": 173
    },
    {
      "epoch": 0.11500330469266358,
      "grad_norm": 1.3945454359054565,
      "learning_rate": 4.8562458691341705e-06,
      "loss": 2.0041,
      "step": 174
    },
    {
      "epoch": 0.11566424322538003,
      "grad_norm": 1.3278405666351318,
      "learning_rate": 4.855419695968276e-06,
      "loss": 1.9686,
      "step": 175
    },
    {
      "epoch": 0.1163251817580965,
      "grad_norm": 1.4718647003173828,
      "learning_rate": 4.85459352280238e-06,
      "loss": 2.0465,
      "step": 176
    },
    {
      "epoch": 0.11698612029081296,
      "grad_norm": 1.3239259719848633,
      "learning_rate": 4.853767349636484e-06,
      "loss": 2.0086,
      "step": 177
    },
    {
      "epoch": 0.11764705882352941,
      "grad_norm": 1.3745903968811035,
      "learning_rate": 4.852941176470589e-06,
      "loss": 1.9957,
      "step": 178
    },
    {
      "epoch": 0.11830799735624586,
      "grad_norm": 1.350940465927124,
      "learning_rate": 4.852115003304693e-06,
      "loss": 2.0123,
      "step": 179
    },
    {
      "epoch": 0.11896893588896233,
      "grad_norm": 1.3964428901672363,
      "learning_rate": 4.851288830138797e-06,
      "loss": 2.0053,
      "step": 180
    },
    {
      "epoch": 0.11962987442167879,
      "grad_norm": 1.2997570037841797,
      "learning_rate": 4.850462656972902e-06,
      "loss": 1.9431,
      "step": 181
    },
    {
      "epoch": 0.12029081295439524,
      "grad_norm": 1.254032015800476,
      "learning_rate": 4.849636483807007e-06,
      "loss": 1.9274,
      "step": 182
    },
    {
      "epoch": 0.1209517514871117,
      "grad_norm": 1.2745285034179688,
      "learning_rate": 4.84881031064111e-06,
      "loss": 1.9442,
      "step": 183
    },
    {
      "epoch": 0.12161269001982816,
      "grad_norm": 1.1882996559143066,
      "learning_rate": 4.8479841374752155e-06,
      "loss": 1.9221,
      "step": 184
    },
    {
      "epoch": 0.12227362855254462,
      "grad_norm": 1.3012981414794922,
      "learning_rate": 4.84715796430932e-06,
      "loss": 1.9903,
      "step": 185
    },
    {
      "epoch": 0.12293456708526107,
      "grad_norm": 1.2033417224884033,
      "learning_rate": 4.846331791143424e-06,
      "loss": 1.8915,
      "step": 186
    },
    {
      "epoch": 0.12359550561797752,
      "grad_norm": 1.1683975458145142,
      "learning_rate": 4.8455056179775285e-06,
      "loss": 1.8746,
      "step": 187
    },
    {
      "epoch": 0.12425644415069398,
      "grad_norm": 1.1429494619369507,
      "learning_rate": 4.844679444811633e-06,
      "loss": 1.8602,
      "step": 188
    },
    {
      "epoch": 0.12491738268341045,
      "grad_norm": 1.1203409433364868,
      "learning_rate": 4.843853271645738e-06,
      "loss": 1.8691,
      "step": 189
    },
    {
      "epoch": 0.1255783212161269,
      "grad_norm": 1.2294234037399292,
      "learning_rate": 4.8430270984798415e-06,
      "loss": 1.9256,
      "step": 190
    },
    {
      "epoch": 0.12623925974884337,
      "grad_norm": 1.1472210884094238,
      "learning_rate": 4.842200925313946e-06,
      "loss": 1.9068,
      "step": 191
    },
    {
      "epoch": 0.12690019828155982,
      "grad_norm": 1.109458088874817,
      "learning_rate": 4.841374752148051e-06,
      "loss": 1.85,
      "step": 192
    },
    {
      "epoch": 0.12756113681427628,
      "grad_norm": 1.12019681930542,
      "learning_rate": 4.8405485789821545e-06,
      "loss": 1.8949,
      "step": 193
    },
    {
      "epoch": 0.12822207534699273,
      "grad_norm": 1.1329843997955322,
      "learning_rate": 4.83972240581626e-06,
      "loss": 1.8634,
      "step": 194
    },
    {
      "epoch": 0.12888301387970919,
      "grad_norm": 1.1294711828231812,
      "learning_rate": 4.838896232650364e-06,
      "loss": 1.883,
      "step": 195
    },
    {
      "epoch": 0.12954395241242564,
      "grad_norm": 1.1064971685409546,
      "learning_rate": 4.838070059484468e-06,
      "loss": 1.9044,
      "step": 196
    },
    {
      "epoch": 0.1302048909451421,
      "grad_norm": 1.0697661638259888,
      "learning_rate": 4.837243886318573e-06,
      "loss": 1.8397,
      "step": 197
    },
    {
      "epoch": 0.13086582947785855,
      "grad_norm": 1.0116523504257202,
      "learning_rate": 4.836417713152677e-06,
      "loss": 1.8444,
      "step": 198
    },
    {
      "epoch": 0.13152676801057503,
      "grad_norm": 1.0384225845336914,
      "learning_rate": 4.835591539986781e-06,
      "loss": 1.8832,
      "step": 199
    },
    {
      "epoch": 0.13218770654329148,
      "grad_norm": 1.0505363941192627,
      "learning_rate": 4.834765366820886e-06,
      "loss": 1.8603,
      "step": 200
    },
    {
      "epoch": 0.13284864507600794,
      "grad_norm": 1.0147335529327393,
      "learning_rate": 4.833939193654991e-06,
      "loss": 1.8591,
      "step": 201
    },
    {
      "epoch": 0.1335095836087244,
      "grad_norm": 0.9970289468765259,
      "learning_rate": 4.833113020489094e-06,
      "loss": 1.8265,
      "step": 202
    },
    {
      "epoch": 0.13417052214144085,
      "grad_norm": 1.0131603479385376,
      "learning_rate": 4.8322868473231995e-06,
      "loss": 1.8741,
      "step": 203
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 0.9613272547721863,
      "learning_rate": 4.831460674157304e-06,
      "loss": 1.8052,
      "step": 204
    },
    {
      "epoch": 0.13549239920687375,
      "grad_norm": 0.953173041343689,
      "learning_rate": 4.830634500991408e-06,
      "loss": 1.8633,
      "step": 205
    },
    {
      "epoch": 0.1361533377395902,
      "grad_norm": 1.005715012550354,
      "learning_rate": 4.8298083278255125e-06,
      "loss": 1.8253,
      "step": 206
    },
    {
      "epoch": 0.13681427627230666,
      "grad_norm": 0.9633169174194336,
      "learning_rate": 4.828982154659617e-06,
      "loss": 1.8114,
      "step": 207
    },
    {
      "epoch": 0.13747521480502314,
      "grad_norm": 0.9481495022773743,
      "learning_rate": 4.828155981493721e-06,
      "loss": 1.8323,
      "step": 208
    },
    {
      "epoch": 0.1381361533377396,
      "grad_norm": 0.9382897615432739,
      "learning_rate": 4.8273298083278255e-06,
      "loss": 1.7986,
      "step": 209
    },
    {
      "epoch": 0.13879709187045605,
      "grad_norm": 0.9237919449806213,
      "learning_rate": 4.826503635161931e-06,
      "loss": 1.7813,
      "step": 210
    },
    {
      "epoch": 0.1394580304031725,
      "grad_norm": 0.8938630819320679,
      "learning_rate": 4.825677461996035e-06,
      "loss": 1.801,
      "step": 211
    },
    {
      "epoch": 0.14011896893588896,
      "grad_norm": 0.9717701077461243,
      "learning_rate": 4.824851288830139e-06,
      "loss": 1.8389,
      "step": 212
    },
    {
      "epoch": 0.14077990746860541,
      "grad_norm": 0.9634246826171875,
      "learning_rate": 4.824025115664244e-06,
      "loss": 1.831,
      "step": 213
    },
    {
      "epoch": 0.14144084600132187,
      "grad_norm": 0.884535014629364,
      "learning_rate": 4.823198942498348e-06,
      "loss": 1.8278,
      "step": 214
    },
    {
      "epoch": 0.14210178453403832,
      "grad_norm": 0.8974631428718567,
      "learning_rate": 4.822372769332452e-06,
      "loss": 1.7912,
      "step": 215
    },
    {
      "epoch": 0.1427627230667548,
      "grad_norm": 0.8783655166625977,
      "learning_rate": 4.821546596166557e-06,
      "loss": 1.8129,
      "step": 216
    },
    {
      "epoch": 0.14342366159947126,
      "grad_norm": 0.8732411861419678,
      "learning_rate": 4.820720423000662e-06,
      "loss": 1.8026,
      "step": 217
    },
    {
      "epoch": 0.1440846001321877,
      "grad_norm": 0.8177760243415833,
      "learning_rate": 4.819894249834765e-06,
      "loss": 1.755,
      "step": 218
    },
    {
      "epoch": 0.14474553866490417,
      "grad_norm": 0.7962428331375122,
      "learning_rate": 4.8190680766688706e-06,
      "loss": 1.7216,
      "step": 219
    },
    {
      "epoch": 0.14540647719762062,
      "grad_norm": 0.7743921875953674,
      "learning_rate": 4.818241903502975e-06,
      "loss": 1.7437,
      "step": 220
    },
    {
      "epoch": 0.14606741573033707,
      "grad_norm": 0.8640263080596924,
      "learning_rate": 4.817415730337079e-06,
      "loss": 1.8122,
      "step": 221
    },
    {
      "epoch": 0.14672835426305353,
      "grad_norm": 0.8053755760192871,
      "learning_rate": 4.8165895571711836e-06,
      "loss": 1.7812,
      "step": 222
    },
    {
      "epoch": 0.14738929279576998,
      "grad_norm": 0.8231702446937561,
      "learning_rate": 4.815763384005288e-06,
      "loss": 1.7718,
      "step": 223
    },
    {
      "epoch": 0.14805023132848646,
      "grad_norm": 0.8155174851417542,
      "learning_rate": 4.814937210839392e-06,
      "loss": 1.8038,
      "step": 224
    },
    {
      "epoch": 0.14871116986120292,
      "grad_norm": 0.7690178751945496,
      "learning_rate": 4.8141110376734966e-06,
      "loss": 1.7515,
      "step": 225
    },
    {
      "epoch": 0.14937210839391937,
      "grad_norm": 0.7656160593032837,
      "learning_rate": 4.813284864507602e-06,
      "loss": 1.7597,
      "step": 226
    },
    {
      "epoch": 0.15003304692663583,
      "grad_norm": 0.8190918564796448,
      "learning_rate": 4.812458691341705e-06,
      "loss": 1.7624,
      "step": 227
    },
    {
      "epoch": 0.15069398545935228,
      "grad_norm": 0.7669620513916016,
      "learning_rate": 4.81163251817581e-06,
      "loss": 1.7301,
      "step": 228
    },
    {
      "epoch": 0.15135492399206874,
      "grad_norm": 0.8108041286468506,
      "learning_rate": 4.810806345009915e-06,
      "loss": 1.8031,
      "step": 229
    },
    {
      "epoch": 0.1520158625247852,
      "grad_norm": 0.7350674271583557,
      "learning_rate": 4.809980171844019e-06,
      "loss": 1.7158,
      "step": 230
    },
    {
      "epoch": 0.15267680105750164,
      "grad_norm": 0.7357922792434692,
      "learning_rate": 4.809153998678123e-06,
      "loss": 1.708,
      "step": 231
    },
    {
      "epoch": 0.1533377395902181,
      "grad_norm": 0.7706838846206665,
      "learning_rate": 4.808327825512228e-06,
      "loss": 1.7378,
      "step": 232
    },
    {
      "epoch": 0.15399867812293458,
      "grad_norm": 0.75755774974823,
      "learning_rate": 4.807501652346332e-06,
      "loss": 1.7159,
      "step": 233
    },
    {
      "epoch": 0.15465961665565103,
      "grad_norm": 0.747462809085846,
      "learning_rate": 4.806675479180436e-06,
      "loss": 1.7264,
      "step": 234
    },
    {
      "epoch": 0.1553205551883675,
      "grad_norm": 0.7428946495056152,
      "learning_rate": 4.805849306014541e-06,
      "loss": 1.7466,
      "step": 235
    },
    {
      "epoch": 0.15598149372108394,
      "grad_norm": 0.749506413936615,
      "learning_rate": 4.805023132848646e-06,
      "loss": 1.7383,
      "step": 236
    },
    {
      "epoch": 0.1566424322538004,
      "grad_norm": 0.715894341468811,
      "learning_rate": 4.804196959682749e-06,
      "loss": 1.6935,
      "step": 237
    },
    {
      "epoch": 0.15730337078651685,
      "grad_norm": 0.7881640791893005,
      "learning_rate": 4.803370786516855e-06,
      "loss": 1.74,
      "step": 238
    },
    {
      "epoch": 0.1579643093192333,
      "grad_norm": 0.7608830332756042,
      "learning_rate": 4.802544613350959e-06,
      "loss": 1.78,
      "step": 239
    },
    {
      "epoch": 0.15862524785194976,
      "grad_norm": 0.723659098148346,
      "learning_rate": 4.801718440185063e-06,
      "loss": 1.6814,
      "step": 240
    },
    {
      "epoch": 0.15928618638466624,
      "grad_norm": 0.7504878044128418,
      "learning_rate": 4.800892267019168e-06,
      "loss": 1.7454,
      "step": 241
    },
    {
      "epoch": 0.1599471249173827,
      "grad_norm": 0.7431653738021851,
      "learning_rate": 4.800066093853272e-06,
      "loss": 1.7543,
      "step": 242
    },
    {
      "epoch": 0.16060806345009915,
      "grad_norm": 0.7890812754631042,
      "learning_rate": 4.799239920687376e-06,
      "loss": 1.7512,
      "step": 243
    },
    {
      "epoch": 0.1612690019828156,
      "grad_norm": 0.7260445952415466,
      "learning_rate": 4.798413747521481e-06,
      "loss": 1.7425,
      "step": 244
    },
    {
      "epoch": 0.16192994051553206,
      "grad_norm": 0.685817539691925,
      "learning_rate": 4.797587574355586e-06,
      "loss": 1.699,
      "step": 245
    },
    {
      "epoch": 0.1625908790482485,
      "grad_norm": 0.7333817481994629,
      "learning_rate": 4.796761401189689e-06,
      "loss": 1.725,
      "step": 246
    },
    {
      "epoch": 0.16325181758096496,
      "grad_norm": 0.7115589380264282,
      "learning_rate": 4.795935228023794e-06,
      "loss": 1.7148,
      "step": 247
    },
    {
      "epoch": 0.16391275611368142,
      "grad_norm": 0.7053766250610352,
      "learning_rate": 4.795109054857899e-06,
      "loss": 1.7128,
      "step": 248
    },
    {
      "epoch": 0.16457369464639787,
      "grad_norm": 0.6866073608398438,
      "learning_rate": 4.794282881692003e-06,
      "loss": 1.6904,
      "step": 249
    },
    {
      "epoch": 0.16523463317911435,
      "grad_norm": 0.7028011679649353,
      "learning_rate": 4.793456708526107e-06,
      "loss": 1.7024,
      "step": 250
    },
    {
      "epoch": 0.1658955717118308,
      "grad_norm": 0.7517967820167542,
      "learning_rate": 4.792630535360212e-06,
      "loss": 1.7663,
      "step": 251
    },
    {
      "epoch": 0.16655651024454726,
      "grad_norm": 0.6993275284767151,
      "learning_rate": 4.791804362194316e-06,
      "loss": 1.6882,
      "step": 252
    },
    {
      "epoch": 0.16721744877726372,
      "grad_norm": 0.7388395667076111,
      "learning_rate": 4.79097818902842e-06,
      "loss": 1.7196,
      "step": 253
    },
    {
      "epoch": 0.16787838730998017,
      "grad_norm": 0.713239312171936,
      "learning_rate": 4.790152015862526e-06,
      "loss": 1.6958,
      "step": 254
    },
    {
      "epoch": 0.16853932584269662,
      "grad_norm": 0.7079726457595825,
      "learning_rate": 4.789325842696629e-06,
      "loss": 1.6962,
      "step": 255
    },
    {
      "epoch": 0.16920026437541308,
      "grad_norm": 0.6829885840415955,
      "learning_rate": 4.788499669530734e-06,
      "loss": 1.6835,
      "step": 256
    },
    {
      "epoch": 0.16986120290812953,
      "grad_norm": 0.646965742111206,
      "learning_rate": 4.787673496364839e-06,
      "loss": 1.6469,
      "step": 257
    },
    {
      "epoch": 0.17052214144084601,
      "grad_norm": 0.7043466567993164,
      "learning_rate": 4.786847323198943e-06,
      "loss": 1.7033,
      "step": 258
    },
    {
      "epoch": 0.17118307997356247,
      "grad_norm": 0.6721484661102295,
      "learning_rate": 4.786021150033047e-06,
      "loss": 1.6848,
      "step": 259
    },
    {
      "epoch": 0.17184401850627892,
      "grad_norm": 0.6758043766021729,
      "learning_rate": 4.785194976867152e-06,
      "loss": 1.6848,
      "step": 260
    },
    {
      "epoch": 0.17250495703899538,
      "grad_norm": 0.6988973617553711,
      "learning_rate": 4.784368803701257e-06,
      "loss": 1.6983,
      "step": 261
    },
    {
      "epoch": 0.17316589557171183,
      "grad_norm": 0.7473607063293457,
      "learning_rate": 4.78354263053536e-06,
      "loss": 1.7155,
      "step": 262
    },
    {
      "epoch": 0.17382683410442828,
      "grad_norm": 0.6588588356971741,
      "learning_rate": 4.7827164573694654e-06,
      "loss": 1.6669,
      "step": 263
    },
    {
      "epoch": 0.17448777263714474,
      "grad_norm": 0.6576295495033264,
      "learning_rate": 4.78189028420357e-06,
      "loss": 1.6123,
      "step": 264
    },
    {
      "epoch": 0.1751487111698612,
      "grad_norm": 0.6610793471336365,
      "learning_rate": 4.781064111037674e-06,
      "loss": 1.63,
      "step": 265
    },
    {
      "epoch": 0.17580964970257765,
      "grad_norm": 0.6962478756904602,
      "learning_rate": 4.7802379378717784e-06,
      "loss": 1.6783,
      "step": 266
    },
    {
      "epoch": 0.17647058823529413,
      "grad_norm": 0.6635796427726746,
      "learning_rate": 4.779411764705883e-06,
      "loss": 1.658,
      "step": 267
    },
    {
      "epoch": 0.17713152676801058,
      "grad_norm": 0.6749326586723328,
      "learning_rate": 4.778585591539987e-06,
      "loss": 1.6776,
      "step": 268
    },
    {
      "epoch": 0.17779246530072704,
      "grad_norm": 0.6881639361381531,
      "learning_rate": 4.7777594183740914e-06,
      "loss": 1.7022,
      "step": 269
    },
    {
      "epoch": 0.1784534038334435,
      "grad_norm": 0.6901903748512268,
      "learning_rate": 4.776933245208197e-06,
      "loss": 1.6754,
      "step": 270
    },
    {
      "epoch": 0.17911434236615995,
      "grad_norm": 0.6456502676010132,
      "learning_rate": 4.7761070720423e-06,
      "loss": 1.6999,
      "step": 271
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.6914335489273071,
      "learning_rate": 4.775280898876405e-06,
      "loss": 1.6747,
      "step": 272
    },
    {
      "epoch": 0.18043621943159285,
      "grad_norm": 0.6819585561752319,
      "learning_rate": 4.77445472571051e-06,
      "loss": 1.6752,
      "step": 273
    },
    {
      "epoch": 0.1810971579643093,
      "grad_norm": 0.6639529466629028,
      "learning_rate": 4.773628552544614e-06,
      "loss": 1.6604,
      "step": 274
    },
    {
      "epoch": 0.1817580964970258,
      "grad_norm": 0.6613438129425049,
      "learning_rate": 4.772802379378718e-06,
      "loss": 1.672,
      "step": 275
    },
    {
      "epoch": 0.18241903502974224,
      "grad_norm": 0.7109504342079163,
      "learning_rate": 4.771976206212823e-06,
      "loss": 1.6448,
      "step": 276
    },
    {
      "epoch": 0.1830799735624587,
      "grad_norm": 0.6522674560546875,
      "learning_rate": 4.771150033046927e-06,
      "loss": 1.6334,
      "step": 277
    },
    {
      "epoch": 0.18374091209517515,
      "grad_norm": 0.7025291323661804,
      "learning_rate": 4.770323859881031e-06,
      "loss": 1.6592,
      "step": 278
    },
    {
      "epoch": 0.1844018506278916,
      "grad_norm": 0.6573469638824463,
      "learning_rate": 4.769497686715136e-06,
      "loss": 1.6506,
      "step": 279
    },
    {
      "epoch": 0.18506278916060806,
      "grad_norm": 0.6497788429260254,
      "learning_rate": 4.76867151354924e-06,
      "loss": 1.6478,
      "step": 280
    },
    {
      "epoch": 0.18572372769332451,
      "grad_norm": 0.7029447555541992,
      "learning_rate": 4.767845340383344e-06,
      "loss": 1.6498,
      "step": 281
    },
    {
      "epoch": 0.18638466622604097,
      "grad_norm": 0.6877869963645935,
      "learning_rate": 4.7670191672174495e-06,
      "loss": 1.6737,
      "step": 282
    },
    {
      "epoch": 0.18704560475875742,
      "grad_norm": 0.6587092876434326,
      "learning_rate": 4.766192994051553e-06,
      "loss": 1.6393,
      "step": 283
    },
    {
      "epoch": 0.1877065432914739,
      "grad_norm": 0.6654231548309326,
      "learning_rate": 4.765366820885658e-06,
      "loss": 1.6464,
      "step": 284
    },
    {
      "epoch": 0.18836748182419036,
      "grad_norm": 0.6454508900642395,
      "learning_rate": 4.7645406477197625e-06,
      "loss": 1.643,
      "step": 285
    },
    {
      "epoch": 0.1890284203569068,
      "grad_norm": 0.6129191517829895,
      "learning_rate": 4.763714474553867e-06,
      "loss": 1.5976,
      "step": 286
    },
    {
      "epoch": 0.18968935888962327,
      "grad_norm": 0.6613320112228394,
      "learning_rate": 4.762888301387971e-06,
      "loss": 1.637,
      "step": 287
    },
    {
      "epoch": 0.19035029742233972,
      "grad_norm": 0.6557473540306091,
      "learning_rate": 4.7620621282220755e-06,
      "loss": 1.6393,
      "step": 288
    },
    {
      "epoch": 0.19101123595505617,
      "grad_norm": 0.6342883706092834,
      "learning_rate": 4.761235955056181e-06,
      "loss": 1.5915,
      "step": 289
    },
    {
      "epoch": 0.19167217448777263,
      "grad_norm": 0.6208983659744263,
      "learning_rate": 4.760409781890284e-06,
      "loss": 1.5731,
      "step": 290
    },
    {
      "epoch": 0.19233311302048908,
      "grad_norm": 0.6282642483711243,
      "learning_rate": 4.759583608724389e-06,
      "loss": 1.5964,
      "step": 291
    },
    {
      "epoch": 0.19299405155320556,
      "grad_norm": 0.6759480834007263,
      "learning_rate": 4.758757435558494e-06,
      "loss": 1.6466,
      "step": 292
    },
    {
      "epoch": 0.19365499008592202,
      "grad_norm": 0.6564729809761047,
      "learning_rate": 4.757931262392598e-06,
      "loss": 1.6329,
      "step": 293
    },
    {
      "epoch": 0.19431592861863847,
      "grad_norm": 0.6525057554244995,
      "learning_rate": 4.757105089226702e-06,
      "loss": 1.6127,
      "step": 294
    },
    {
      "epoch": 0.19497686715135493,
      "grad_norm": 0.6290599703788757,
      "learning_rate": 4.756278916060807e-06,
      "loss": 1.5914,
      "step": 295
    },
    {
      "epoch": 0.19563780568407138,
      "grad_norm": 0.621023416519165,
      "learning_rate": 4.755452742894911e-06,
      "loss": 1.5912,
      "step": 296
    },
    {
      "epoch": 0.19629874421678783,
      "grad_norm": 0.6274163126945496,
      "learning_rate": 4.754626569729015e-06,
      "loss": 1.5913,
      "step": 297
    },
    {
      "epoch": 0.1969596827495043,
      "grad_norm": 0.6616371273994446,
      "learning_rate": 4.7538003965631205e-06,
      "loss": 1.6298,
      "step": 298
    },
    {
      "epoch": 0.19762062128222074,
      "grad_norm": 0.6473385095596313,
      "learning_rate": 4.752974223397224e-06,
      "loss": 1.6157,
      "step": 299
    },
    {
      "epoch": 0.1982815598149372,
      "grad_norm": 0.6527983546257019,
      "learning_rate": 4.752148050231329e-06,
      "loss": 1.6047,
      "step": 300
    },
    {
      "epoch": 0.19894249834765368,
      "grad_norm": 0.6250231862068176,
      "learning_rate": 4.7513218770654335e-06,
      "loss": 1.6097,
      "step": 301
    },
    {
      "epoch": 0.19960343688037013,
      "grad_norm": 0.6404376029968262,
      "learning_rate": 4.750495703899538e-06,
      "loss": 1.6282,
      "step": 302
    },
    {
      "epoch": 0.2002643754130866,
      "grad_norm": 0.6277452111244202,
      "learning_rate": 4.749669530733642e-06,
      "loss": 1.585,
      "step": 303
    },
    {
      "epoch": 0.20092531394580304,
      "grad_norm": 0.6569641828536987,
      "learning_rate": 4.7488433575677465e-06,
      "loss": 1.5614,
      "step": 304
    },
    {
      "epoch": 0.2015862524785195,
      "grad_norm": 0.6301534175872803,
      "learning_rate": 4.748017184401851e-06,
      "loss": 1.5609,
      "step": 305
    },
    {
      "epoch": 0.20224719101123595,
      "grad_norm": 0.6570590734481812,
      "learning_rate": 4.747191011235955e-06,
      "loss": 1.6326,
      "step": 306
    },
    {
      "epoch": 0.2029081295439524,
      "grad_norm": 0.6520040035247803,
      "learning_rate": 4.74636483807006e-06,
      "loss": 1.5999,
      "step": 307
    },
    {
      "epoch": 0.20356906807666886,
      "grad_norm": 0.6304307579994202,
      "learning_rate": 4.745538664904164e-06,
      "loss": 1.5635,
      "step": 308
    },
    {
      "epoch": 0.20423000660938534,
      "grad_norm": 0.6877005100250244,
      "learning_rate": 4.744712491738269e-06,
      "loss": 1.5724,
      "step": 309
    },
    {
      "epoch": 0.2048909451421018,
      "grad_norm": 0.633468508720398,
      "learning_rate": 4.743886318572373e-06,
      "loss": 1.5526,
      "step": 310
    },
    {
      "epoch": 0.20555188367481825,
      "grad_norm": 0.6402007341384888,
      "learning_rate": 4.743060145406478e-06,
      "loss": 1.5687,
      "step": 311
    },
    {
      "epoch": 0.2062128222075347,
      "grad_norm": 0.6112467646598816,
      "learning_rate": 4.742233972240582e-06,
      "loss": 1.5715,
      "step": 312
    },
    {
      "epoch": 0.20687376074025116,
      "grad_norm": 0.6443524956703186,
      "learning_rate": 4.741407799074686e-06,
      "loss": 1.5615,
      "step": 313
    },
    {
      "epoch": 0.2075346992729676,
      "grad_norm": 0.6672149896621704,
      "learning_rate": 4.7405816259087915e-06,
      "loss": 1.6016,
      "step": 314
    },
    {
      "epoch": 0.20819563780568406,
      "grad_norm": 0.6271277070045471,
      "learning_rate": 4.739755452742895e-06,
      "loss": 1.5258,
      "step": 315
    },
    {
      "epoch": 0.20885657633840052,
      "grad_norm": 0.6355605125427246,
      "learning_rate": 4.738929279577e-06,
      "loss": 1.5594,
      "step": 316
    },
    {
      "epoch": 0.209517514871117,
      "grad_norm": 0.6255838871002197,
      "learning_rate": 4.7381031064111045e-06,
      "loss": 1.5624,
      "step": 317
    },
    {
      "epoch": 0.21017845340383345,
      "grad_norm": 0.6511525511741638,
      "learning_rate": 4.737276933245208e-06,
      "loss": 1.5667,
      "step": 318
    },
    {
      "epoch": 0.2108393919365499,
      "grad_norm": 0.6522502899169922,
      "learning_rate": 4.736450760079313e-06,
      "loss": 1.5778,
      "step": 319
    },
    {
      "epoch": 0.21150033046926636,
      "grad_norm": 0.6371287107467651,
      "learning_rate": 4.7356245869134175e-06,
      "loss": 1.5751,
      "step": 320
    },
    {
      "epoch": 0.21216126900198282,
      "grad_norm": 0.6240413188934326,
      "learning_rate": 4.734798413747522e-06,
      "loss": 1.5478,
      "step": 321
    },
    {
      "epoch": 0.21282220753469927,
      "grad_norm": 0.6426659822463989,
      "learning_rate": 4.733972240581626e-06,
      "loss": 1.5431,
      "step": 322
    },
    {
      "epoch": 0.21348314606741572,
      "grad_norm": 0.6060061454772949,
      "learning_rate": 4.7331460674157305e-06,
      "loss": 1.526,
      "step": 323
    },
    {
      "epoch": 0.21414408460013218,
      "grad_norm": 0.6456289291381836,
      "learning_rate": 4.732319894249835e-06,
      "loss": 1.531,
      "step": 324
    },
    {
      "epoch": 0.21480502313284863,
      "grad_norm": 0.8162108063697815,
      "learning_rate": 4.731493721083939e-06,
      "loss": 1.5067,
      "step": 325
    },
    {
      "epoch": 0.21546596166556511,
      "grad_norm": 0.6325536370277405,
      "learning_rate": 4.730667547918044e-06,
      "loss": 1.5262,
      "step": 326
    },
    {
      "epoch": 0.21612690019828157,
      "grad_norm": 0.6340579986572266,
      "learning_rate": 4.729841374752148e-06,
      "loss": 1.5132,
      "step": 327
    },
    {
      "epoch": 0.21678783873099802,
      "grad_norm": 0.6414518356323242,
      "learning_rate": 4.729015201586253e-06,
      "loss": 1.5451,
      "step": 328
    },
    {
      "epoch": 0.21744877726371448,
      "grad_norm": 0.6480995416641235,
      "learning_rate": 4.728189028420357e-06,
      "loss": 1.5634,
      "step": 329
    },
    {
      "epoch": 0.21810971579643093,
      "grad_norm": 0.626893162727356,
      "learning_rate": 4.727362855254462e-06,
      "loss": 1.5251,
      "step": 330
    },
    {
      "epoch": 0.21877065432914738,
      "grad_norm": 0.7340523600578308,
      "learning_rate": 4.726536682088566e-06,
      "loss": 1.5312,
      "step": 331
    },
    {
      "epoch": 0.21943159286186384,
      "grad_norm": 0.6222277879714966,
      "learning_rate": 4.72571050892267e-06,
      "loss": 1.5266,
      "step": 332
    },
    {
      "epoch": 0.2200925313945803,
      "grad_norm": 0.6204467415809631,
      "learning_rate": 4.724884335756775e-06,
      "loss": 1.5538,
      "step": 333
    },
    {
      "epoch": 0.22075346992729677,
      "grad_norm": 0.6108472347259521,
      "learning_rate": 4.724058162590879e-06,
      "loss": 1.5179,
      "step": 334
    },
    {
      "epoch": 0.22141440846001323,
      "grad_norm": 0.6559787392616272,
      "learning_rate": 4.723231989424984e-06,
      "loss": 1.5543,
      "step": 335
    },
    {
      "epoch": 0.22207534699272968,
      "grad_norm": 0.6365138292312622,
      "learning_rate": 4.7224058162590885e-06,
      "loss": 1.5331,
      "step": 336
    },
    {
      "epoch": 0.22273628552544614,
      "grad_norm": 0.6676861047744751,
      "learning_rate": 4.721579643093193e-06,
      "loss": 1.4964,
      "step": 337
    },
    {
      "epoch": 0.2233972240581626,
      "grad_norm": 0.5976248979568481,
      "learning_rate": 4.720753469927297e-06,
      "loss": 1.4667,
      "step": 338
    },
    {
      "epoch": 0.22405816259087905,
      "grad_norm": 0.6277708411216736,
      "learning_rate": 4.7199272967614015e-06,
      "loss": 1.4855,
      "step": 339
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 0.6116730570793152,
      "learning_rate": 4.719101123595506e-06,
      "loss": 1.4826,
      "step": 340
    },
    {
      "epoch": 0.22538003965631195,
      "grad_norm": 0.6163004040718079,
      "learning_rate": 4.71827495042961e-06,
      "loss": 1.5299,
      "step": 341
    },
    {
      "epoch": 0.2260409781890284,
      "grad_norm": 0.6201580762863159,
      "learning_rate": 4.717448777263715e-06,
      "loss": 1.4923,
      "step": 342
    },
    {
      "epoch": 0.2267019167217449,
      "grad_norm": 0.649380624294281,
      "learning_rate": 4.716622604097819e-06,
      "loss": 1.5277,
      "step": 343
    },
    {
      "epoch": 0.22736285525446134,
      "grad_norm": 0.664179265499115,
      "learning_rate": 4.715796430931924e-06,
      "loss": 1.5278,
      "step": 344
    },
    {
      "epoch": 0.2280237937871778,
      "grad_norm": 0.6334760785102844,
      "learning_rate": 4.714970257766028e-06,
      "loss": 1.4874,
      "step": 345
    },
    {
      "epoch": 0.22868473231989425,
      "grad_norm": 0.6023991107940674,
      "learning_rate": 4.714144084600133e-06,
      "loss": 1.4855,
      "step": 346
    },
    {
      "epoch": 0.2293456708526107,
      "grad_norm": 0.6342647075653076,
      "learning_rate": 4.713317911434237e-06,
      "loss": 1.4974,
      "step": 347
    },
    {
      "epoch": 0.23000660938532716,
      "grad_norm": 0.6045287251472473,
      "learning_rate": 4.712491738268341e-06,
      "loss": 1.4867,
      "step": 348
    },
    {
      "epoch": 0.23066754791804361,
      "grad_norm": 0.6297218799591064,
      "learning_rate": 4.711665565102446e-06,
      "loss": 1.5082,
      "step": 349
    },
    {
      "epoch": 0.23132848645076007,
      "grad_norm": 0.6317171454429626,
      "learning_rate": 4.71083939193655e-06,
      "loss": 1.4932,
      "step": 350
    },
    {
      "epoch": 0.23198942498347655,
      "grad_norm": 0.6126062870025635,
      "learning_rate": 4.710013218770655e-06,
      "loss": 1.4925,
      "step": 351
    },
    {
      "epoch": 0.232650363516193,
      "grad_norm": 0.6576836109161377,
      "learning_rate": 4.709187045604759e-06,
      "loss": 1.5325,
      "step": 352
    },
    {
      "epoch": 0.23331130204890946,
      "grad_norm": 0.6390630602836609,
      "learning_rate": 4.708360872438864e-06,
      "loss": 1.5115,
      "step": 353
    },
    {
      "epoch": 0.2339722405816259,
      "grad_norm": 0.6287491321563721,
      "learning_rate": 4.707534699272968e-06,
      "loss": 1.4639,
      "step": 354
    },
    {
      "epoch": 0.23463317911434237,
      "grad_norm": 0.620930552482605,
      "learning_rate": 4.7067085261070726e-06,
      "loss": 1.4758,
      "step": 355
    },
    {
      "epoch": 0.23529411764705882,
      "grad_norm": 0.635141134262085,
      "learning_rate": 4.705882352941177e-06,
      "loss": 1.4954,
      "step": 356
    },
    {
      "epoch": 0.23595505617977527,
      "grad_norm": 0.632436215877533,
      "learning_rate": 4.705056179775281e-06,
      "loss": 1.4983,
      "step": 357
    },
    {
      "epoch": 0.23661599471249173,
      "grad_norm": 0.5959930419921875,
      "learning_rate": 4.7042300066093856e-06,
      "loss": 1.4454,
      "step": 358
    },
    {
      "epoch": 0.23727693324520818,
      "grad_norm": 0.633516788482666,
      "learning_rate": 4.70340383344349e-06,
      "loss": 1.4847,
      "step": 359
    },
    {
      "epoch": 0.23793787177792466,
      "grad_norm": 0.7788264155387878,
      "learning_rate": 4.702577660277594e-06,
      "loss": 1.5316,
      "step": 360
    },
    {
      "epoch": 0.23859881031064112,
      "grad_norm": 0.6976113319396973,
      "learning_rate": 4.701751487111699e-06,
      "loss": 1.4986,
      "step": 361
    },
    {
      "epoch": 0.23925974884335757,
      "grad_norm": 0.6636934876441956,
      "learning_rate": 4.700925313945803e-06,
      "loss": 1.4847,
      "step": 362
    },
    {
      "epoch": 0.23992068737607403,
      "grad_norm": 0.6217162609100342,
      "learning_rate": 4.700099140779908e-06,
      "loss": 1.4453,
      "step": 363
    },
    {
      "epoch": 0.24058162590879048,
      "grad_norm": 0.6179846525192261,
      "learning_rate": 4.699272967614012e-06,
      "loss": 1.455,
      "step": 364
    },
    {
      "epoch": 0.24124256444150693,
      "grad_norm": 0.5991073250770569,
      "learning_rate": 4.698446794448117e-06,
      "loss": 1.4364,
      "step": 365
    },
    {
      "epoch": 0.2419035029742234,
      "grad_norm": 0.645834743976593,
      "learning_rate": 4.697620621282221e-06,
      "loss": 1.4701,
      "step": 366
    },
    {
      "epoch": 0.24256444150693984,
      "grad_norm": 0.6670123338699341,
      "learning_rate": 4.696794448116325e-06,
      "loss": 1.4824,
      "step": 367
    },
    {
      "epoch": 0.24322538003965632,
      "grad_norm": 0.6091025471687317,
      "learning_rate": 4.69596827495043e-06,
      "loss": 1.4279,
      "step": 368
    },
    {
      "epoch": 0.24388631857237278,
      "grad_norm": 0.6289503574371338,
      "learning_rate": 4.695142101784534e-06,
      "loss": 1.4449,
      "step": 369
    },
    {
      "epoch": 0.24454725710508923,
      "grad_norm": 0.6521211266517639,
      "learning_rate": 4.694315928618639e-06,
      "loss": 1.4477,
      "step": 370
    },
    {
      "epoch": 0.2452081956378057,
      "grad_norm": 0.6435553431510925,
      "learning_rate": 4.693489755452743e-06,
      "loss": 1.4503,
      "step": 371
    },
    {
      "epoch": 0.24586913417052214,
      "grad_norm": 0.6452319622039795,
      "learning_rate": 4.692663582286848e-06,
      "loss": 1.4813,
      "step": 372
    },
    {
      "epoch": 0.2465300727032386,
      "grad_norm": 0.7305469512939453,
      "learning_rate": 4.691837409120952e-06,
      "loss": 1.4458,
      "step": 373
    },
    {
      "epoch": 0.24719101123595505,
      "grad_norm": 0.6365989446640015,
      "learning_rate": 4.691011235955057e-06,
      "loss": 1.4502,
      "step": 374
    },
    {
      "epoch": 0.2478519497686715,
      "grad_norm": 0.628042459487915,
      "learning_rate": 4.690185062789161e-06,
      "loss": 1.4191,
      "step": 375
    },
    {
      "epoch": 0.24851288830138796,
      "grad_norm": 0.624040961265564,
      "learning_rate": 4.689358889623265e-06,
      "loss": 1.4106,
      "step": 376
    },
    {
      "epoch": 0.24917382683410444,
      "grad_norm": 0.639431357383728,
      "learning_rate": 4.68853271645737e-06,
      "loss": 1.4356,
      "step": 377
    },
    {
      "epoch": 0.2498347653668209,
      "grad_norm": 0.6465407609939575,
      "learning_rate": 4.687706543291474e-06,
      "loss": 1.4456,
      "step": 378
    },
    {
      "epoch": 0.25049570389953735,
      "grad_norm": 0.6447776556015015,
      "learning_rate": 4.686880370125579e-06,
      "loss": 1.4523,
      "step": 379
    },
    {
      "epoch": 0.2511566424322538,
      "grad_norm": 0.6018092632293701,
      "learning_rate": 4.686054196959683e-06,
      "loss": 1.4046,
      "step": 380
    },
    {
      "epoch": 0.25181758096497026,
      "grad_norm": 0.6299067139625549,
      "learning_rate": 4.685228023793788e-06,
      "loss": 1.4354,
      "step": 381
    },
    {
      "epoch": 0.25247851949768674,
      "grad_norm": 0.6267932057380676,
      "learning_rate": 4.684401850627892e-06,
      "loss": 1.4521,
      "step": 382
    },
    {
      "epoch": 0.25313945803040316,
      "grad_norm": 0.6537691354751587,
      "learning_rate": 4.6835756774619964e-06,
      "loss": 1.455,
      "step": 383
    },
    {
      "epoch": 0.25380039656311965,
      "grad_norm": 0.6184224486351013,
      "learning_rate": 4.682749504296101e-06,
      "loss": 1.4024,
      "step": 384
    },
    {
      "epoch": 0.25446133509583607,
      "grad_norm": 0.6505248546600342,
      "learning_rate": 4.681923331130205e-06,
      "loss": 1.433,
      "step": 385
    },
    {
      "epoch": 0.25512227362855255,
      "grad_norm": 0.6513524055480957,
      "learning_rate": 4.68109715796431e-06,
      "loss": 1.4428,
      "step": 386
    },
    {
      "epoch": 0.255783212161269,
      "grad_norm": 0.6380325555801392,
      "learning_rate": 4.680270984798414e-06,
      "loss": 1.4479,
      "step": 387
    },
    {
      "epoch": 0.25644415069398546,
      "grad_norm": 0.6341189742088318,
      "learning_rate": 4.679444811632519e-06,
      "loss": 1.4308,
      "step": 388
    },
    {
      "epoch": 0.25710508922670194,
      "grad_norm": 0.6527215242385864,
      "learning_rate": 4.678618638466623e-06,
      "loss": 1.4521,
      "step": 389
    },
    {
      "epoch": 0.25776602775941837,
      "grad_norm": 0.6520373225212097,
      "learning_rate": 4.677792465300728e-06,
      "loss": 1.4108,
      "step": 390
    },
    {
      "epoch": 0.25842696629213485,
      "grad_norm": 0.6354460120201111,
      "learning_rate": 4.676966292134832e-06,
      "loss": 1.4057,
      "step": 391
    },
    {
      "epoch": 0.2590879048248513,
      "grad_norm": 0.6237524151802063,
      "learning_rate": 4.676140118968936e-06,
      "loss": 1.4058,
      "step": 392
    },
    {
      "epoch": 0.25974884335756776,
      "grad_norm": 0.6338370442390442,
      "learning_rate": 4.675313945803041e-06,
      "loss": 1.4055,
      "step": 393
    },
    {
      "epoch": 0.2604097818902842,
      "grad_norm": 0.6288467049598694,
      "learning_rate": 4.674487772637145e-06,
      "loss": 1.4186,
      "step": 394
    },
    {
      "epoch": 0.26107072042300067,
      "grad_norm": 0.6641553044319153,
      "learning_rate": 4.67366159947125e-06,
      "loss": 1.4315,
      "step": 395
    },
    {
      "epoch": 0.2617316589557171,
      "grad_norm": 0.6001843214035034,
      "learning_rate": 4.672835426305354e-06,
      "loss": 1.361,
      "step": 396
    },
    {
      "epoch": 0.2623925974884336,
      "grad_norm": 0.6648199558258057,
      "learning_rate": 4.672009253139459e-06,
      "loss": 1.414,
      "step": 397
    },
    {
      "epoch": 0.26305353602115006,
      "grad_norm": 0.6670883893966675,
      "learning_rate": 4.671183079973563e-06,
      "loss": 1.4132,
      "step": 398
    },
    {
      "epoch": 0.2637144745538665,
      "grad_norm": 0.6805518865585327,
      "learning_rate": 4.6703569068076675e-06,
      "loss": 1.4327,
      "step": 399
    },
    {
      "epoch": 0.26437541308658297,
      "grad_norm": 0.6743345856666565,
      "learning_rate": 4.669530733641772e-06,
      "loss": 1.4093,
      "step": 400
    },
    {
      "epoch": 0.2650363516192994,
      "grad_norm": 0.6170269846916199,
      "learning_rate": 4.668704560475876e-06,
      "loss": 1.3965,
      "step": 401
    },
    {
      "epoch": 0.2656972901520159,
      "grad_norm": 0.6585536599159241,
      "learning_rate": 4.6678783873099805e-06,
      "loss": 1.4127,
      "step": 402
    },
    {
      "epoch": 0.2663582286847323,
      "grad_norm": 0.6513043642044067,
      "learning_rate": 4.667052214144085e-06,
      "loss": 1.3943,
      "step": 403
    },
    {
      "epoch": 0.2670191672174488,
      "grad_norm": 0.7528220415115356,
      "learning_rate": 4.666226040978189e-06,
      "loss": 1.4438,
      "step": 404
    },
    {
      "epoch": 0.2676801057501652,
      "grad_norm": 0.6663563847541809,
      "learning_rate": 4.6653998678122935e-06,
      "loss": 1.4064,
      "step": 405
    },
    {
      "epoch": 0.2683410442828817,
      "grad_norm": 0.6721760034561157,
      "learning_rate": 4.664573694646398e-06,
      "loss": 1.4095,
      "step": 406
    },
    {
      "epoch": 0.2690019828155982,
      "grad_norm": 0.6516403555870056,
      "learning_rate": 4.663747521480503e-06,
      "loss": 1.3677,
      "step": 407
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.6783421635627747,
      "learning_rate": 4.6629213483146065e-06,
      "loss": 1.3644,
      "step": 408
    },
    {
      "epoch": 0.2703238598810311,
      "grad_norm": 0.6485779881477356,
      "learning_rate": 4.662095175148712e-06,
      "loss": 1.3905,
      "step": 409
    },
    {
      "epoch": 0.2709847984137475,
      "grad_norm": 0.6777647137641907,
      "learning_rate": 4.661269001982816e-06,
      "loss": 1.4011,
      "step": 410
    },
    {
      "epoch": 0.271645736946464,
      "grad_norm": 0.6386076807975769,
      "learning_rate": 4.66044282881692e-06,
      "loss": 1.3447,
      "step": 411
    },
    {
      "epoch": 0.2723066754791804,
      "grad_norm": 0.663454532623291,
      "learning_rate": 4.659616655651025e-06,
      "loss": 1.3813,
      "step": 412
    },
    {
      "epoch": 0.2729676140118969,
      "grad_norm": 0.6810556054115295,
      "learning_rate": 4.658790482485129e-06,
      "loss": 1.3951,
      "step": 413
    },
    {
      "epoch": 0.2736285525446133,
      "grad_norm": 0.6886003613471985,
      "learning_rate": 4.657964309319234e-06,
      "loss": 1.4059,
      "step": 414
    },
    {
      "epoch": 0.2742894910773298,
      "grad_norm": 0.6471579670906067,
      "learning_rate": 4.657138136153338e-06,
      "loss": 1.3364,
      "step": 415
    },
    {
      "epoch": 0.2749504296100463,
      "grad_norm": 0.6472222208976746,
      "learning_rate": 4.656311962987443e-06,
      "loss": 1.3568,
      "step": 416
    },
    {
      "epoch": 0.2756113681427627,
      "grad_norm": 0.6683050394058228,
      "learning_rate": 4.655485789821547e-06,
      "loss": 1.3819,
      "step": 417
    },
    {
      "epoch": 0.2762723066754792,
      "grad_norm": 0.6815972924232483,
      "learning_rate": 4.6546596166556515e-06,
      "loss": 1.3897,
      "step": 418
    },
    {
      "epoch": 0.2769332452081956,
      "grad_norm": 0.7043532729148865,
      "learning_rate": 4.653833443489756e-06,
      "loss": 1.3722,
      "step": 419
    },
    {
      "epoch": 0.2775941837409121,
      "grad_norm": 0.6711735129356384,
      "learning_rate": 4.65300727032386e-06,
      "loss": 1.3639,
      "step": 420
    },
    {
      "epoch": 0.27825512227362853,
      "grad_norm": 0.657962441444397,
      "learning_rate": 4.6521810971579645e-06,
      "loss": 1.327,
      "step": 421
    },
    {
      "epoch": 0.278916060806345,
      "grad_norm": 0.7258066534996033,
      "learning_rate": 4.651354923992069e-06,
      "loss": 1.3772,
      "step": 422
    },
    {
      "epoch": 0.2795769993390615,
      "grad_norm": 0.6584302186965942,
      "learning_rate": 4.650528750826174e-06,
      "loss": 1.3598,
      "step": 423
    },
    {
      "epoch": 0.2802379378717779,
      "grad_norm": 0.7080212235450745,
      "learning_rate": 4.6497025776602775e-06,
      "loss": 1.3609,
      "step": 424
    },
    {
      "epoch": 0.2808988764044944,
      "grad_norm": 0.6976220011711121,
      "learning_rate": 4.648876404494383e-06,
      "loss": 1.3585,
      "step": 425
    },
    {
      "epoch": 0.28155981493721083,
      "grad_norm": 0.6777753233909607,
      "learning_rate": 4.648050231328487e-06,
      "loss": 1.3232,
      "step": 426
    },
    {
      "epoch": 0.2822207534699273,
      "grad_norm": 0.6777904629707336,
      "learning_rate": 4.647224058162591e-06,
      "loss": 1.3845,
      "step": 427
    },
    {
      "epoch": 0.28288169200264374,
      "grad_norm": 0.6635798811912537,
      "learning_rate": 4.646397884996696e-06,
      "loss": 1.3521,
      "step": 428
    },
    {
      "epoch": 0.2835426305353602,
      "grad_norm": 0.6586585640907288,
      "learning_rate": 4.6455717118308e-06,
      "loss": 1.3379,
      "step": 429
    },
    {
      "epoch": 0.28420356906807664,
      "grad_norm": 0.7473470568656921,
      "learning_rate": 4.644745538664904e-06,
      "loss": 1.3657,
      "step": 430
    },
    {
      "epoch": 0.2848645076007931,
      "grad_norm": 0.7119421362876892,
      "learning_rate": 4.643919365499009e-06,
      "loss": 1.3534,
      "step": 431
    },
    {
      "epoch": 0.2855254461335096,
      "grad_norm": 0.6887909770011902,
      "learning_rate": 4.643093192333114e-06,
      "loss": 1.3553,
      "step": 432
    },
    {
      "epoch": 0.28618638466622603,
      "grad_norm": 0.720491886138916,
      "learning_rate": 4.642267019167217e-06,
      "loss": 1.3405,
      "step": 433
    },
    {
      "epoch": 0.2868473231989425,
      "grad_norm": 0.9239579439163208,
      "learning_rate": 4.6414408460013225e-06,
      "loss": 1.3273,
      "step": 434
    },
    {
      "epoch": 0.28750826173165894,
      "grad_norm": 0.6744029521942139,
      "learning_rate": 4.640614672835427e-06,
      "loss": 1.3158,
      "step": 435
    },
    {
      "epoch": 0.2881692002643754,
      "grad_norm": 0.7229343056678772,
      "learning_rate": 4.639788499669531e-06,
      "loss": 1.3747,
      "step": 436
    },
    {
      "epoch": 0.28883013879709185,
      "grad_norm": 0.6719276309013367,
      "learning_rate": 4.6389623265036355e-06,
      "loss": 1.3147,
      "step": 437
    },
    {
      "epoch": 0.28949107732980833,
      "grad_norm": 0.6760846972465515,
      "learning_rate": 4.63813615333774e-06,
      "loss": 1.3284,
      "step": 438
    },
    {
      "epoch": 0.29015201586252476,
      "grad_norm": 0.6968767642974854,
      "learning_rate": 4.637309980171845e-06,
      "loss": 1.3318,
      "step": 439
    },
    {
      "epoch": 0.29081295439524124,
      "grad_norm": 0.6886916756629944,
      "learning_rate": 4.6364838070059485e-06,
      "loss": 1.3219,
      "step": 440
    },
    {
      "epoch": 0.2914738929279577,
      "grad_norm": 0.6890798807144165,
      "learning_rate": 4.635657633840054e-06,
      "loss": 1.3458,
      "step": 441
    },
    {
      "epoch": 0.29213483146067415,
      "grad_norm": 0.6729485392570496,
      "learning_rate": 4.634831460674158e-06,
      "loss": 1.32,
      "step": 442
    },
    {
      "epoch": 0.29279576999339063,
      "grad_norm": 0.7197085618972778,
      "learning_rate": 4.634005287508262e-06,
      "loss": 1.3464,
      "step": 443
    },
    {
      "epoch": 0.29345670852610706,
      "grad_norm": 0.704325258731842,
      "learning_rate": 4.633179114342367e-06,
      "loss": 1.3427,
      "step": 444
    },
    {
      "epoch": 0.29411764705882354,
      "grad_norm": 0.6713944673538208,
      "learning_rate": 4.632352941176471e-06,
      "loss": 1.3223,
      "step": 445
    },
    {
      "epoch": 0.29477858559153997,
      "grad_norm": 0.6687025427818298,
      "learning_rate": 4.631526768010575e-06,
      "loss": 1.3138,
      "step": 446
    },
    {
      "epoch": 0.29543952412425645,
      "grad_norm": 0.6778646111488342,
      "learning_rate": 4.63070059484468e-06,
      "loss": 1.2859,
      "step": 447
    },
    {
      "epoch": 0.29610046265697293,
      "grad_norm": 0.7081811428070068,
      "learning_rate": 4.629874421678784e-06,
      "loss": 1.3137,
      "step": 448
    },
    {
      "epoch": 0.29676140118968936,
      "grad_norm": 0.7103601694107056,
      "learning_rate": 4.629048248512888e-06,
      "loss": 1.3306,
      "step": 449
    },
    {
      "epoch": 0.29742233972240584,
      "grad_norm": 0.7768099904060364,
      "learning_rate": 4.628222075346993e-06,
      "loss": 1.3185,
      "step": 450
    },
    {
      "epoch": 0.29808327825512226,
      "grad_norm": 0.6896178126335144,
      "learning_rate": 4.627395902181098e-06,
      "loss": 1.2747,
      "step": 451
    },
    {
      "epoch": 0.29874421678783875,
      "grad_norm": 0.7135205864906311,
      "learning_rate": 4.626569729015201e-06,
      "loss": 1.3056,
      "step": 452
    },
    {
      "epoch": 0.29940515532055517,
      "grad_norm": 0.6952946186065674,
      "learning_rate": 4.6257435558493065e-06,
      "loss": 1.2879,
      "step": 453
    },
    {
      "epoch": 0.30006609385327165,
      "grad_norm": 0.6698349714279175,
      "learning_rate": 4.624917382683411e-06,
      "loss": 1.3053,
      "step": 454
    },
    {
      "epoch": 0.3007270323859881,
      "grad_norm": 0.6532787680625916,
      "learning_rate": 4.624091209517515e-06,
      "loss": 1.244,
      "step": 455
    },
    {
      "epoch": 0.30138797091870456,
      "grad_norm": 0.7068110108375549,
      "learning_rate": 4.6232650363516195e-06,
      "loss": 1.2763,
      "step": 456
    },
    {
      "epoch": 0.30204890945142104,
      "grad_norm": 0.6785716414451599,
      "learning_rate": 4.622438863185724e-06,
      "loss": 1.2261,
      "step": 457
    },
    {
      "epoch": 0.30270984798413747,
      "grad_norm": 0.6854847073554993,
      "learning_rate": 4.621612690019828e-06,
      "loss": 1.2737,
      "step": 458
    },
    {
      "epoch": 0.30337078651685395,
      "grad_norm": 0.7109431028366089,
      "learning_rate": 4.6207865168539325e-06,
      "loss": 1.2709,
      "step": 459
    },
    {
      "epoch": 0.3040317250495704,
      "grad_norm": 0.7603332996368408,
      "learning_rate": 4.619960343688038e-06,
      "loss": 1.3079,
      "step": 460
    },
    {
      "epoch": 0.30469266358228686,
      "grad_norm": 0.7187338471412659,
      "learning_rate": 4.619134170522142e-06,
      "loss": 1.2872,
      "step": 461
    },
    {
      "epoch": 0.3053536021150033,
      "grad_norm": 0.7550373077392578,
      "learning_rate": 4.618307997356246e-06,
      "loss": 1.2886,
      "step": 462
    },
    {
      "epoch": 0.30601454064771977,
      "grad_norm": 0.7333846688270569,
      "learning_rate": 4.617481824190351e-06,
      "loss": 1.2889,
      "step": 463
    },
    {
      "epoch": 0.3066754791804362,
      "grad_norm": 0.7228555083274841,
      "learning_rate": 4.616655651024455e-06,
      "loss": 1.2861,
      "step": 464
    },
    {
      "epoch": 0.3073364177131527,
      "grad_norm": 0.7109024524688721,
      "learning_rate": 4.615829477858559e-06,
      "loss": 1.2781,
      "step": 465
    },
    {
      "epoch": 0.30799735624586916,
      "grad_norm": 0.72463458776474,
      "learning_rate": 4.615003304692664e-06,
      "loss": 1.2528,
      "step": 466
    },
    {
      "epoch": 0.3086582947785856,
      "grad_norm": 0.7328885793685913,
      "learning_rate": 4.614177131526769e-06,
      "loss": 1.3125,
      "step": 467
    },
    {
      "epoch": 0.30931923331130207,
      "grad_norm": 0.7283593416213989,
      "learning_rate": 4.613350958360872e-06,
      "loss": 1.2634,
      "step": 468
    },
    {
      "epoch": 0.3099801718440185,
      "grad_norm": 0.7586998343467712,
      "learning_rate": 4.6125247851949776e-06,
      "loss": 1.2926,
      "step": 469
    },
    {
      "epoch": 0.310641110376735,
      "grad_norm": 0.7389402389526367,
      "learning_rate": 4.611698612029082e-06,
      "loss": 1.2608,
      "step": 470
    },
    {
      "epoch": 0.3113020489094514,
      "grad_norm": 0.8389947414398193,
      "learning_rate": 4.610872438863186e-06,
      "loss": 1.2519,
      "step": 471
    },
    {
      "epoch": 0.3119629874421679,
      "grad_norm": 0.7469790577888489,
      "learning_rate": 4.6100462656972906e-06,
      "loss": 1.2765,
      "step": 472
    },
    {
      "epoch": 0.3126239259748843,
      "grad_norm": 0.7024245858192444,
      "learning_rate": 4.609220092531395e-06,
      "loss": 1.2598,
      "step": 473
    },
    {
      "epoch": 0.3132848645076008,
      "grad_norm": 0.8093885779380798,
      "learning_rate": 4.608393919365499e-06,
      "loss": 1.2384,
      "step": 474
    },
    {
      "epoch": 0.3139458030403173,
      "grad_norm": 0.7988040447235107,
      "learning_rate": 4.6075677461996036e-06,
      "loss": 1.2648,
      "step": 475
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.7054316401481628,
      "learning_rate": 4.606741573033709e-06,
      "loss": 1.1921,
      "step": 476
    },
    {
      "epoch": 0.3152676801057502,
      "grad_norm": 0.7841094732284546,
      "learning_rate": 4.605915399867812e-06,
      "loss": 1.2794,
      "step": 477
    },
    {
      "epoch": 0.3159286186384666,
      "grad_norm": 0.74278324842453,
      "learning_rate": 4.605089226701917e-06,
      "loss": 1.2474,
      "step": 478
    },
    {
      "epoch": 0.3165895571711831,
      "grad_norm": 0.7672340869903564,
      "learning_rate": 4.604263053536022e-06,
      "loss": 1.2591,
      "step": 479
    },
    {
      "epoch": 0.3172504957038995,
      "grad_norm": 0.7445765733718872,
      "learning_rate": 4.603436880370126e-06,
      "loss": 1.2235,
      "step": 480
    },
    {
      "epoch": 0.317911434236616,
      "grad_norm": 0.7230504751205444,
      "learning_rate": 4.60261070720423e-06,
      "loss": 1.2423,
      "step": 481
    },
    {
      "epoch": 0.3185723727693325,
      "grad_norm": 0.7318434119224548,
      "learning_rate": 4.601784534038335e-06,
      "loss": 1.2202,
      "step": 482
    },
    {
      "epoch": 0.3192333113020489,
      "grad_norm": 0.7021795511245728,
      "learning_rate": 4.600958360872439e-06,
      "loss": 1.2277,
      "step": 483
    },
    {
      "epoch": 0.3198942498347654,
      "grad_norm": 0.7214084267616272,
      "learning_rate": 4.600132187706543e-06,
      "loss": 1.211,
      "step": 484
    },
    {
      "epoch": 0.3205551883674818,
      "grad_norm": 0.7902666330337524,
      "learning_rate": 4.599306014540649e-06,
      "loss": 1.2354,
      "step": 485
    },
    {
      "epoch": 0.3212161269001983,
      "grad_norm": 0.8082709908485413,
      "learning_rate": 4.598479841374753e-06,
      "loss": 1.2263,
      "step": 486
    },
    {
      "epoch": 0.3218770654329147,
      "grad_norm": 0.7769575715065002,
      "learning_rate": 4.597653668208857e-06,
      "loss": 1.2537,
      "step": 487
    },
    {
      "epoch": 0.3225380039656312,
      "grad_norm": 0.7548536062240601,
      "learning_rate": 4.596827495042962e-06,
      "loss": 1.2351,
      "step": 488
    },
    {
      "epoch": 0.32319894249834763,
      "grad_norm": 0.7306246757507324,
      "learning_rate": 4.596001321877066e-06,
      "loss": 1.2196,
      "step": 489
    },
    {
      "epoch": 0.3238598810310641,
      "grad_norm": 0.7513923645019531,
      "learning_rate": 4.59517514871117e-06,
      "loss": 1.2088,
      "step": 490
    },
    {
      "epoch": 0.3245208195637806,
      "grad_norm": 0.7328814268112183,
      "learning_rate": 4.594348975545275e-06,
      "loss": 1.1903,
      "step": 491
    },
    {
      "epoch": 0.325181758096497,
      "grad_norm": 0.7202123403549194,
      "learning_rate": 4.593522802379379e-06,
      "loss": 1.207,
      "step": 492
    },
    {
      "epoch": 0.3258426966292135,
      "grad_norm": 0.7893062829971313,
      "learning_rate": 4.592696629213483e-06,
      "loss": 1.2077,
      "step": 493
    },
    {
      "epoch": 0.32650363516192993,
      "grad_norm": 0.7540025115013123,
      "learning_rate": 4.591870456047588e-06,
      "loss": 1.1926,
      "step": 494
    },
    {
      "epoch": 0.3271645736946464,
      "grad_norm": 0.8283161520957947,
      "learning_rate": 4.591044282881693e-06,
      "loss": 1.1965,
      "step": 495
    },
    {
      "epoch": 0.32782551222736284,
      "grad_norm": 0.7561062574386597,
      "learning_rate": 4.590218109715796e-06,
      "loss": 1.217,
      "step": 496
    },
    {
      "epoch": 0.3284864507600793,
      "grad_norm": 0.7434338331222534,
      "learning_rate": 4.5893919365499014e-06,
      "loss": 1.1933,
      "step": 497
    },
    {
      "epoch": 0.32914738929279574,
      "grad_norm": 0.7423408031463623,
      "learning_rate": 4.588565763384006e-06,
      "loss": 1.1938,
      "step": 498
    },
    {
      "epoch": 0.3298083278255122,
      "grad_norm": 0.7948976755142212,
      "learning_rate": 4.58773959021811e-06,
      "loss": 1.1949,
      "step": 499
    },
    {
      "epoch": 0.3304692663582287,
      "grad_norm": 0.8040528893470764,
      "learning_rate": 4.5869134170522144e-06,
      "loss": 1.1854,
      "step": 500
    },
    {
      "epoch": 0.33113020489094513,
      "grad_norm": 0.7554665803909302,
      "learning_rate": 4.586087243886319e-06,
      "loss": 1.1785,
      "step": 501
    },
    {
      "epoch": 0.3317911434236616,
      "grad_norm": 0.7594969868659973,
      "learning_rate": 4.585261070720423e-06,
      "loss": 1.1979,
      "step": 502
    },
    {
      "epoch": 0.33245208195637804,
      "grad_norm": 0.7560259103775024,
      "learning_rate": 4.5844348975545274e-06,
      "loss": 1.1715,
      "step": 503
    },
    {
      "epoch": 0.3331130204890945,
      "grad_norm": 0.7650278806686401,
      "learning_rate": 4.583608724388633e-06,
      "loss": 1.1732,
      "step": 504
    },
    {
      "epoch": 0.33377395902181095,
      "grad_norm": 0.7817701697349548,
      "learning_rate": 4.582782551222736e-06,
      "loss": 1.1652,
      "step": 505
    },
    {
      "epoch": 0.33443489755452743,
      "grad_norm": 0.7586098909378052,
      "learning_rate": 4.581956378056841e-06,
      "loss": 1.1773,
      "step": 506
    },
    {
      "epoch": 0.33509583608724386,
      "grad_norm": 0.7648153901100159,
      "learning_rate": 4.581130204890946e-06,
      "loss": 1.1673,
      "step": 507
    },
    {
      "epoch": 0.33575677461996034,
      "grad_norm": 0.7612197399139404,
      "learning_rate": 4.58030403172505e-06,
      "loss": 1.1811,
      "step": 508
    },
    {
      "epoch": 0.3364177131526768,
      "grad_norm": 0.7810595035552979,
      "learning_rate": 4.579477858559154e-06,
      "loss": 1.1604,
      "step": 509
    },
    {
      "epoch": 0.33707865168539325,
      "grad_norm": 0.8377236723899841,
      "learning_rate": 4.578651685393259e-06,
      "loss": 1.1834,
      "step": 510
    },
    {
      "epoch": 0.33773959021810973,
      "grad_norm": 0.7726429104804993,
      "learning_rate": 4.577825512227364e-06,
      "loss": 1.1382,
      "step": 511
    },
    {
      "epoch": 0.33840052875082616,
      "grad_norm": 0.7460008263587952,
      "learning_rate": 4.576999339061467e-06,
      "loss": 1.141,
      "step": 512
    },
    {
      "epoch": 0.33906146728354264,
      "grad_norm": 0.7964341640472412,
      "learning_rate": 4.5761731658955725e-06,
      "loss": 1.1457,
      "step": 513
    },
    {
      "epoch": 0.33972240581625907,
      "grad_norm": 0.8176025748252869,
      "learning_rate": 4.575346992729677e-06,
      "loss": 1.1368,
      "step": 514
    },
    {
      "epoch": 0.34038334434897555,
      "grad_norm": 1.32077157497406,
      "learning_rate": 4.574520819563781e-06,
      "loss": 1.1563,
      "step": 515
    },
    {
      "epoch": 0.34104428288169203,
      "grad_norm": 0.826379656791687,
      "learning_rate": 4.5736946463978855e-06,
      "loss": 1.1201,
      "step": 516
    },
    {
      "epoch": 0.34170522141440846,
      "grad_norm": 0.831074059009552,
      "learning_rate": 4.57286847323199e-06,
      "loss": 1.1741,
      "step": 517
    },
    {
      "epoch": 0.34236615994712494,
      "grad_norm": 0.7581003308296204,
      "learning_rate": 4.572042300066094e-06,
      "loss": 1.1206,
      "step": 518
    },
    {
      "epoch": 0.34302709847984136,
      "grad_norm": 1.0673390626907349,
      "learning_rate": 4.5712161269001985e-06,
      "loss": 1.1105,
      "step": 519
    },
    {
      "epoch": 0.34368803701255785,
      "grad_norm": 0.8116494417190552,
      "learning_rate": 4.570389953734304e-06,
      "loss": 1.1723,
      "step": 520
    },
    {
      "epoch": 0.34434897554527427,
      "grad_norm": 0.7635611295700073,
      "learning_rate": 4.569563780568407e-06,
      "loss": 1.1286,
      "step": 521
    },
    {
      "epoch": 0.34500991407799075,
      "grad_norm": 0.8755189776420593,
      "learning_rate": 4.568737607402512e-06,
      "loss": 1.1071,
      "step": 522
    },
    {
      "epoch": 0.3456708526107072,
      "grad_norm": 0.7964727282524109,
      "learning_rate": 4.567911434236617e-06,
      "loss": 1.1171,
      "step": 523
    },
    {
      "epoch": 0.34633179114342366,
      "grad_norm": 0.8368403911590576,
      "learning_rate": 4.567085261070721e-06,
      "loss": 1.1233,
      "step": 524
    },
    {
      "epoch": 0.34699272967614014,
      "grad_norm": 0.7703066468238831,
      "learning_rate": 4.566259087904825e-06,
      "loss": 1.1222,
      "step": 525
    },
    {
      "epoch": 0.34765366820885657,
      "grad_norm": 0.8001256585121155,
      "learning_rate": 4.56543291473893e-06,
      "loss": 1.1321,
      "step": 526
    },
    {
      "epoch": 0.34831460674157305,
      "grad_norm": 0.7838549613952637,
      "learning_rate": 4.564606741573034e-06,
      "loss": 1.105,
      "step": 527
    },
    {
      "epoch": 0.3489755452742895,
      "grad_norm": 0.8220099806785583,
      "learning_rate": 4.563780568407138e-06,
      "loss": 1.1269,
      "step": 528
    },
    {
      "epoch": 0.34963648380700596,
      "grad_norm": 0.8365773558616638,
      "learning_rate": 4.562954395241243e-06,
      "loss": 1.1244,
      "step": 529
    },
    {
      "epoch": 0.3502974223397224,
      "grad_norm": 0.8931971788406372,
      "learning_rate": 4.562128222075347e-06,
      "loss": 1.1423,
      "step": 530
    },
    {
      "epoch": 0.35095836087243887,
      "grad_norm": 0.7940479516983032,
      "learning_rate": 4.561302048909451e-06,
      "loss": 1.1166,
      "step": 531
    },
    {
      "epoch": 0.3516192994051553,
      "grad_norm": 0.8224549889564514,
      "learning_rate": 4.5604758757435565e-06,
      "loss": 1.0936,
      "step": 532
    },
    {
      "epoch": 0.3522802379378718,
      "grad_norm": 0.8146182894706726,
      "learning_rate": 4.55964970257766e-06,
      "loss": 1.1213,
      "step": 533
    },
    {
      "epoch": 0.35294117647058826,
      "grad_norm": 0.8909528255462646,
      "learning_rate": 4.558823529411765e-06,
      "loss": 1.0951,
      "step": 534
    },
    {
      "epoch": 0.3536021150033047,
      "grad_norm": 0.835228681564331,
      "learning_rate": 4.5579973562458695e-06,
      "loss": 1.104,
      "step": 535
    },
    {
      "epoch": 0.35426305353602117,
      "grad_norm": 0.8476085066795349,
      "learning_rate": 4.557171183079974e-06,
      "loss": 1.1,
      "step": 536
    },
    {
      "epoch": 0.3549239920687376,
      "grad_norm": 0.968234121799469,
      "learning_rate": 4.556345009914078e-06,
      "loss": 1.1205,
      "step": 537
    },
    {
      "epoch": 0.3555849306014541,
      "grad_norm": 0.9554416537284851,
      "learning_rate": 4.5555188367481825e-06,
      "loss": 1.0672,
      "step": 538
    },
    {
      "epoch": 0.3562458691341705,
      "grad_norm": 0.8837869763374329,
      "learning_rate": 4.554692663582288e-06,
      "loss": 1.0732,
      "step": 539
    },
    {
      "epoch": 0.356906807666887,
      "grad_norm": 0.807288408279419,
      "learning_rate": 4.553866490416391e-06,
      "loss": 1.0461,
      "step": 540
    },
    {
      "epoch": 0.35756774619960346,
      "grad_norm": 1.0876729488372803,
      "learning_rate": 4.553040317250496e-06,
      "loss": 1.0705,
      "step": 541
    },
    {
      "epoch": 0.3582286847323199,
      "grad_norm": 0.8456372022628784,
      "learning_rate": 4.552214144084601e-06,
      "loss": 1.0666,
      "step": 542
    },
    {
      "epoch": 0.3588896232650364,
      "grad_norm": 0.8463603854179382,
      "learning_rate": 4.551387970918705e-06,
      "loss": 1.0604,
      "step": 543
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 0.9144019484519958,
      "learning_rate": 4.550561797752809e-06,
      "loss": 1.052,
      "step": 544
    },
    {
      "epoch": 0.3602115003304693,
      "grad_norm": 0.7965595722198486,
      "learning_rate": 4.549735624586914e-06,
      "loss": 1.0487,
      "step": 545
    },
    {
      "epoch": 0.3608724388631857,
      "grad_norm": 0.7987320423126221,
      "learning_rate": 4.548909451421018e-06,
      "loss": 1.0437,
      "step": 546
    },
    {
      "epoch": 0.3615333773959022,
      "grad_norm": 0.882976233959198,
      "learning_rate": 4.548083278255122e-06,
      "loss": 1.0829,
      "step": 547
    },
    {
      "epoch": 0.3621943159286186,
      "grad_norm": 0.8648003935813904,
      "learning_rate": 4.5472571050892275e-06,
      "loss": 1.0369,
      "step": 548
    },
    {
      "epoch": 0.3628552544613351,
      "grad_norm": 0.867009162902832,
      "learning_rate": 4.546430931923331e-06,
      "loss": 1.0575,
      "step": 549
    },
    {
      "epoch": 0.3635161929940516,
      "grad_norm": 0.9239495396614075,
      "learning_rate": 4.545604758757436e-06,
      "loss": 1.0705,
      "step": 550
    },
    {
      "epoch": 0.364177131526768,
      "grad_norm": 0.901383638381958,
      "learning_rate": 4.5447785855915405e-06,
      "loss": 1.0361,
      "step": 551
    },
    {
      "epoch": 0.3648380700594845,
      "grad_norm": 0.9186726212501526,
      "learning_rate": 4.543952412425645e-06,
      "loss": 1.0661,
      "step": 552
    },
    {
      "epoch": 0.3654990085922009,
      "grad_norm": 1.031648874282837,
      "learning_rate": 4.543126239259749e-06,
      "loss": 1.0047,
      "step": 553
    },
    {
      "epoch": 0.3661599471249174,
      "grad_norm": 0.8641780614852905,
      "learning_rate": 4.5423000660938535e-06,
      "loss": 1.0178,
      "step": 554
    },
    {
      "epoch": 0.3668208856576338,
      "grad_norm": 0.8960682153701782,
      "learning_rate": 4.541473892927958e-06,
      "loss": 1.0163,
      "step": 555
    },
    {
      "epoch": 0.3674818241903503,
      "grad_norm": 0.8570274710655212,
      "learning_rate": 4.540647719762062e-06,
      "loss": 1.0155,
      "step": 556
    },
    {
      "epoch": 0.36814276272306673,
      "grad_norm": 0.8652419447898865,
      "learning_rate": 4.539821546596167e-06,
      "loss": 0.9976,
      "step": 557
    },
    {
      "epoch": 0.3688037012557832,
      "grad_norm": 0.868623673915863,
      "learning_rate": 4.538995373430271e-06,
      "loss": 1.0086,
      "step": 558
    },
    {
      "epoch": 0.3694646397884997,
      "grad_norm": 0.8798169493675232,
      "learning_rate": 4.538169200264376e-06,
      "loss": 1.0172,
      "step": 559
    },
    {
      "epoch": 0.3701255783212161,
      "grad_norm": 0.8691268563270569,
      "learning_rate": 4.53734302709848e-06,
      "loss": 1.0007,
      "step": 560
    },
    {
      "epoch": 0.3707865168539326,
      "grad_norm": 1.017903208732605,
      "learning_rate": 4.536516853932585e-06,
      "loss": 1.0099,
      "step": 561
    },
    {
      "epoch": 0.37144745538664903,
      "grad_norm": 0.8979422450065613,
      "learning_rate": 4.535690680766689e-06,
      "loss": 1.0098,
      "step": 562
    },
    {
      "epoch": 0.3721083939193655,
      "grad_norm": 0.8901090621948242,
      "learning_rate": 4.534864507600793e-06,
      "loss": 0.9979,
      "step": 563
    },
    {
      "epoch": 0.37276933245208194,
      "grad_norm": 0.9620102643966675,
      "learning_rate": 4.5340383344348985e-06,
      "loss": 1.0294,
      "step": 564
    },
    {
      "epoch": 0.3734302709847984,
      "grad_norm": 0.9268177151679993,
      "learning_rate": 4.533212161269002e-06,
      "loss": 1.009,
      "step": 565
    },
    {
      "epoch": 0.37409120951751484,
      "grad_norm": 0.8919501304626465,
      "learning_rate": 4.532385988103107e-06,
      "loss": 0.9869,
      "step": 566
    },
    {
      "epoch": 0.3747521480502313,
      "grad_norm": 0.9811770915985107,
      "learning_rate": 4.5315598149372115e-06,
      "loss": 0.9802,
      "step": 567
    },
    {
      "epoch": 0.3754130865829478,
      "grad_norm": 0.9237381219863892,
      "learning_rate": 4.530733641771316e-06,
      "loss": 0.9888,
      "step": 568
    },
    {
      "epoch": 0.37607402511566423,
      "grad_norm": 0.9346895813941956,
      "learning_rate": 4.52990746860542e-06,
      "loss": 0.991,
      "step": 569
    },
    {
      "epoch": 0.3767349636483807,
      "grad_norm": 0.8949970006942749,
      "learning_rate": 4.5290812954395245e-06,
      "loss": 0.991,
      "step": 570
    },
    {
      "epoch": 0.37739590218109714,
      "grad_norm": 0.8854564428329468,
      "learning_rate": 4.528255122273629e-06,
      "loss": 0.9819,
      "step": 571
    },
    {
      "epoch": 0.3780568407138136,
      "grad_norm": 0.9117364287376404,
      "learning_rate": 4.527428949107733e-06,
      "loss": 0.9598,
      "step": 572
    },
    {
      "epoch": 0.37871777924653005,
      "grad_norm": 0.9327212572097778,
      "learning_rate": 4.5266027759418375e-06,
      "loss": 0.9534,
      "step": 573
    },
    {
      "epoch": 0.37937871777924653,
      "grad_norm": 0.9243253469467163,
      "learning_rate": 4.525776602775942e-06,
      "loss": 0.9834,
      "step": 574
    },
    {
      "epoch": 0.380039656311963,
      "grad_norm": 0.8967118263244629,
      "learning_rate": 4.524950429610046e-06,
      "loss": 0.9532,
      "step": 575
    },
    {
      "epoch": 0.38070059484467944,
      "grad_norm": 0.9892927408218384,
      "learning_rate": 4.524124256444151e-06,
      "loss": 0.9695,
      "step": 576
    },
    {
      "epoch": 0.3813615333773959,
      "grad_norm": 0.9515790939331055,
      "learning_rate": 4.523298083278255e-06,
      "loss": 0.9467,
      "step": 577
    },
    {
      "epoch": 0.38202247191011235,
      "grad_norm": 0.9445192813873291,
      "learning_rate": 4.52247191011236e-06,
      "loss": 0.9559,
      "step": 578
    },
    {
      "epoch": 0.38268341044282883,
      "grad_norm": 0.9452953934669495,
      "learning_rate": 4.521645736946464e-06,
      "loss": 0.9566,
      "step": 579
    },
    {
      "epoch": 0.38334434897554526,
      "grad_norm": 0.9399541616439819,
      "learning_rate": 4.520819563780569e-06,
      "loss": 0.9334,
      "step": 580
    },
    {
      "epoch": 0.38400528750826174,
      "grad_norm": 0.943397045135498,
      "learning_rate": 4.519993390614673e-06,
      "loss": 0.9328,
      "step": 581
    },
    {
      "epoch": 0.38466622604097817,
      "grad_norm": 0.977137565612793,
      "learning_rate": 4.519167217448777e-06,
      "loss": 0.95,
      "step": 582
    },
    {
      "epoch": 0.38532716457369465,
      "grad_norm": 0.9543893337249756,
      "learning_rate": 4.5183410442828826e-06,
      "loss": 0.9513,
      "step": 583
    },
    {
      "epoch": 0.38598810310641113,
      "grad_norm": 1.0054658651351929,
      "learning_rate": 4.517514871116986e-06,
      "loss": 0.9407,
      "step": 584
    },
    {
      "epoch": 0.38664904163912756,
      "grad_norm": 0.9030207991600037,
      "learning_rate": 4.516688697951091e-06,
      "loss": 0.9281,
      "step": 585
    },
    {
      "epoch": 0.38730998017184404,
      "grad_norm": 0.920081377029419,
      "learning_rate": 4.5158625247851956e-06,
      "loss": 0.9388,
      "step": 586
    },
    {
      "epoch": 0.38797091870456046,
      "grad_norm": 0.9310010671615601,
      "learning_rate": 4.5150363516193e-06,
      "loss": 0.9194,
      "step": 587
    },
    {
      "epoch": 0.38863185723727695,
      "grad_norm": 0.9635581374168396,
      "learning_rate": 4.514210178453404e-06,
      "loss": 0.9504,
      "step": 588
    },
    {
      "epoch": 0.38929279576999337,
      "grad_norm": 0.9271895289421082,
      "learning_rate": 4.5133840052875086e-06,
      "loss": 0.9253,
      "step": 589
    },
    {
      "epoch": 0.38995373430270985,
      "grad_norm": 0.9639081358909607,
      "learning_rate": 4.512557832121613e-06,
      "loss": 0.9244,
      "step": 590
    },
    {
      "epoch": 0.3906146728354263,
      "grad_norm": 0.8861356973648071,
      "learning_rate": 4.511731658955717e-06,
      "loss": 0.8902,
      "step": 591
    },
    {
      "epoch": 0.39127561136814276,
      "grad_norm": 0.9426878690719604,
      "learning_rate": 4.510905485789822e-06,
      "loss": 0.9019,
      "step": 592
    },
    {
      "epoch": 0.39193654990085924,
      "grad_norm": 0.9550070762634277,
      "learning_rate": 4.510079312623926e-06,
      "loss": 0.9282,
      "step": 593
    },
    {
      "epoch": 0.39259748843357567,
      "grad_norm": 0.9771260619163513,
      "learning_rate": 4.509253139458031e-06,
      "loss": 0.8856,
      "step": 594
    },
    {
      "epoch": 0.39325842696629215,
      "grad_norm": 0.9905484914779663,
      "learning_rate": 4.508426966292135e-06,
      "loss": 0.8679,
      "step": 595
    },
    {
      "epoch": 0.3939193654990086,
      "grad_norm": 0.9551529288291931,
      "learning_rate": 4.50760079312624e-06,
      "loss": 0.9029,
      "step": 596
    },
    {
      "epoch": 0.39458030403172506,
      "grad_norm": 0.9491757154464722,
      "learning_rate": 4.506774619960344e-06,
      "loss": 0.9039,
      "step": 597
    },
    {
      "epoch": 0.3952412425644415,
      "grad_norm": 0.9670857787132263,
      "learning_rate": 4.505948446794448e-06,
      "loss": 0.9074,
      "step": 598
    },
    {
      "epoch": 0.39590218109715797,
      "grad_norm": 0.9455589652061462,
      "learning_rate": 4.505122273628553e-06,
      "loss": 0.901,
      "step": 599
    },
    {
      "epoch": 0.3965631196298744,
      "grad_norm": 1.0740381479263306,
      "learning_rate": 4.504296100462657e-06,
      "loss": 0.8988,
      "step": 600
    },
    {
      "epoch": 0.3972240581625909,
      "grad_norm": 0.9345666766166687,
      "learning_rate": 4.503469927296762e-06,
      "loss": 0.8993,
      "step": 601
    },
    {
      "epoch": 0.39788499669530736,
      "grad_norm": 0.9177737236022949,
      "learning_rate": 4.502643754130866e-06,
      "loss": 0.8603,
      "step": 602
    },
    {
      "epoch": 0.3985459352280238,
      "grad_norm": 0.9389392137527466,
      "learning_rate": 4.501817580964971e-06,
      "loss": 0.8767,
      "step": 603
    },
    {
      "epoch": 0.39920687376074027,
      "grad_norm": 0.9349985718727112,
      "learning_rate": 4.500991407799075e-06,
      "loss": 0.8611,
      "step": 604
    },
    {
      "epoch": 0.3998678122934567,
      "grad_norm": 0.9191200137138367,
      "learning_rate": 4.50016523463318e-06,
      "loss": 0.8758,
      "step": 605
    },
    {
      "epoch": 0.4005287508261732,
      "grad_norm": 1.0325877666473389,
      "learning_rate": 4.499339061467284e-06,
      "loss": 0.8499,
      "step": 606
    },
    {
      "epoch": 0.4011896893588896,
      "grad_norm": 0.9631955027580261,
      "learning_rate": 4.498512888301388e-06,
      "loss": 0.8398,
      "step": 607
    },
    {
      "epoch": 0.4018506278916061,
      "grad_norm": 0.9574329257011414,
      "learning_rate": 4.4976867151354934e-06,
      "loss": 0.8483,
      "step": 608
    },
    {
      "epoch": 0.40251156642432256,
      "grad_norm": 0.942704439163208,
      "learning_rate": 4.496860541969597e-06,
      "loss": 0.8281,
      "step": 609
    },
    {
      "epoch": 0.403172504957039,
      "grad_norm": 1.1169970035552979,
      "learning_rate": 4.496034368803702e-06,
      "loss": 0.8547,
      "step": 610
    },
    {
      "epoch": 0.4038334434897555,
      "grad_norm": 0.9750238656997681,
      "learning_rate": 4.4952081956378064e-06,
      "loss": 0.8474,
      "step": 611
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 0.9411843419075012,
      "learning_rate": 4.494382022471911e-06,
      "loss": 0.7907,
      "step": 612
    },
    {
      "epoch": 0.4051553205551884,
      "grad_norm": 0.914838433265686,
      "learning_rate": 4.493555849306015e-06,
      "loss": 0.8419,
      "step": 613
    },
    {
      "epoch": 0.4058162590879048,
      "grad_norm": 0.9379400610923767,
      "learning_rate": 4.4927296761401194e-06,
      "loss": 0.8347,
      "step": 614
    },
    {
      "epoch": 0.4064771976206213,
      "grad_norm": 0.9576043486595154,
      "learning_rate": 4.491903502974224e-06,
      "loss": 0.8044,
      "step": 615
    },
    {
      "epoch": 0.4071381361533377,
      "grad_norm": 0.9870913028717041,
      "learning_rate": 4.491077329808328e-06,
      "loss": 0.8136,
      "step": 616
    },
    {
      "epoch": 0.4077990746860542,
      "grad_norm": 0.9599229097366333,
      "learning_rate": 4.4902511566424324e-06,
      "loss": 0.8139,
      "step": 617
    },
    {
      "epoch": 0.4084600132187707,
      "grad_norm": 0.9212406873703003,
      "learning_rate": 4.489424983476537e-06,
      "loss": 0.8038,
      "step": 618
    },
    {
      "epoch": 0.4091209517514871,
      "grad_norm": 1.095212697982788,
      "learning_rate": 4.488598810310641e-06,
      "loss": 0.8108,
      "step": 619
    },
    {
      "epoch": 0.4097818902842036,
      "grad_norm": 0.9919085502624512,
      "learning_rate": 4.487772637144746e-06,
      "loss": 0.8309,
      "step": 620
    },
    {
      "epoch": 0.41044282881692,
      "grad_norm": 0.9965183138847351,
      "learning_rate": 4.48694646397885e-06,
      "loss": 0.8393,
      "step": 621
    },
    {
      "epoch": 0.4111037673496365,
      "grad_norm": 1.048288106918335,
      "learning_rate": 4.486120290812955e-06,
      "loss": 0.8235,
      "step": 622
    },
    {
      "epoch": 0.4117647058823529,
      "grad_norm": 0.9807974100112915,
      "learning_rate": 4.485294117647059e-06,
      "loss": 0.7873,
      "step": 623
    },
    {
      "epoch": 0.4124256444150694,
      "grad_norm": 0.9420766234397888,
      "learning_rate": 4.484467944481164e-06,
      "loss": 0.796,
      "step": 624
    },
    {
      "epoch": 0.41308658294778583,
      "grad_norm": 0.9757975935935974,
      "learning_rate": 4.483641771315268e-06,
      "loss": 0.7991,
      "step": 625
    },
    {
      "epoch": 0.4137475214805023,
      "grad_norm": 0.9585786461830139,
      "learning_rate": 4.482815598149372e-06,
      "loss": 0.8015,
      "step": 626
    },
    {
      "epoch": 0.4144084600132188,
      "grad_norm": 0.9663804173469543,
      "learning_rate": 4.481989424983477e-06,
      "loss": 0.807,
      "step": 627
    },
    {
      "epoch": 0.4150693985459352,
      "grad_norm": 1.0949870347976685,
      "learning_rate": 4.481163251817581e-06,
      "loss": 0.8219,
      "step": 628
    },
    {
      "epoch": 0.4157303370786517,
      "grad_norm": 0.9516918659210205,
      "learning_rate": 4.480337078651686e-06,
      "loss": 0.7841,
      "step": 629
    },
    {
      "epoch": 0.41639127561136813,
      "grad_norm": 0.9703711867332458,
      "learning_rate": 4.47951090548579e-06,
      "loss": 0.809,
      "step": 630
    },
    {
      "epoch": 0.4170522141440846,
      "grad_norm": 0.9720208048820496,
      "learning_rate": 4.478684732319895e-06,
      "loss": 0.7729,
      "step": 631
    },
    {
      "epoch": 0.41771315267680104,
      "grad_norm": 1.021195650100708,
      "learning_rate": 4.477858559153999e-06,
      "loss": 0.7717,
      "step": 632
    },
    {
      "epoch": 0.4183740912095175,
      "grad_norm": 1.0282799005508423,
      "learning_rate": 4.4770323859881035e-06,
      "loss": 0.7956,
      "step": 633
    },
    {
      "epoch": 0.419035029742234,
      "grad_norm": 1.0088402032852173,
      "learning_rate": 4.476206212822208e-06,
      "loss": 0.7764,
      "step": 634
    },
    {
      "epoch": 0.4196959682749504,
      "grad_norm": 1.0235365629196167,
      "learning_rate": 4.475380039656312e-06,
      "loss": 0.78,
      "step": 635
    },
    {
      "epoch": 0.4203569068076669,
      "grad_norm": 0.9551931619644165,
      "learning_rate": 4.474553866490417e-06,
      "loss": 0.777,
      "step": 636
    },
    {
      "epoch": 0.42101784534038333,
      "grad_norm": 1.0705862045288086,
      "learning_rate": 4.473727693324521e-06,
      "loss": 0.7639,
      "step": 637
    },
    {
      "epoch": 0.4216787838730998,
      "grad_norm": 0.9812426567077637,
      "learning_rate": 4.472901520158626e-06,
      "loss": 0.7545,
      "step": 638
    },
    {
      "epoch": 0.42233972240581624,
      "grad_norm": 0.9774298667907715,
      "learning_rate": 4.47207534699273e-06,
      "loss": 0.7619,
      "step": 639
    },
    {
      "epoch": 0.4230006609385327,
      "grad_norm": 0.9920323491096497,
      "learning_rate": 4.471249173826835e-06,
      "loss": 0.7762,
      "step": 640
    },
    {
      "epoch": 0.42366159947124915,
      "grad_norm": 1.026538372039795,
      "learning_rate": 4.470423000660939e-06,
      "loss": 0.7437,
      "step": 641
    },
    {
      "epoch": 0.42432253800396563,
      "grad_norm": 1.013596773147583,
      "learning_rate": 4.469596827495043e-06,
      "loss": 0.7327,
      "step": 642
    },
    {
      "epoch": 0.4249834765366821,
      "grad_norm": 1.0262476205825806,
      "learning_rate": 4.468770654329148e-06,
      "loss": 0.75,
      "step": 643
    },
    {
      "epoch": 0.42564441506939854,
      "grad_norm": 1.0106909275054932,
      "learning_rate": 4.467944481163252e-06,
      "loss": 0.7629,
      "step": 644
    },
    {
      "epoch": 0.426305353602115,
      "grad_norm": 1.0528424978256226,
      "learning_rate": 4.467118307997357e-06,
      "loss": 0.7506,
      "step": 645
    },
    {
      "epoch": 0.42696629213483145,
      "grad_norm": 0.9906139373779297,
      "learning_rate": 4.466292134831461e-06,
      "loss": 0.7493,
      "step": 646
    },
    {
      "epoch": 0.42762723066754793,
      "grad_norm": 1.1150749921798706,
      "learning_rate": 4.465465961665566e-06,
      "loss": 0.7453,
      "step": 647
    },
    {
      "epoch": 0.42828816920026436,
      "grad_norm": 1.1215875148773193,
      "learning_rate": 4.46463978849967e-06,
      "loss": 0.7595,
      "step": 648
    },
    {
      "epoch": 0.42894910773298084,
      "grad_norm": 1.0311689376831055,
      "learning_rate": 4.4638136153337745e-06,
      "loss": 0.7129,
      "step": 649
    },
    {
      "epoch": 0.42961004626569727,
      "grad_norm": 1.0246862173080444,
      "learning_rate": 4.462987442167879e-06,
      "loss": 0.7355,
      "step": 650
    },
    {
      "epoch": 0.43027098479841375,
      "grad_norm": 0.996644139289856,
      "learning_rate": 4.462161269001983e-06,
      "loss": 0.7323,
      "step": 651
    },
    {
      "epoch": 0.43093192333113023,
      "grad_norm": 1.083788275718689,
      "learning_rate": 4.4613350958360875e-06,
      "loss": 0.7194,
      "step": 652
    },
    {
      "epoch": 0.43159286186384666,
      "grad_norm": 1.0477689504623413,
      "learning_rate": 4.460508922670192e-06,
      "loss": 0.7346,
      "step": 653
    },
    {
      "epoch": 0.43225380039656314,
      "grad_norm": 1.013001799583435,
      "learning_rate": 4.459682749504297e-06,
      "loss": 0.6912,
      "step": 654
    },
    {
      "epoch": 0.43291473892927956,
      "grad_norm": 0.9578079581260681,
      "learning_rate": 4.4588565763384005e-06,
      "loss": 0.7073,
      "step": 655
    },
    {
      "epoch": 0.43357567746199605,
      "grad_norm": 1.071426272392273,
      "learning_rate": 4.458030403172506e-06,
      "loss": 0.6689,
      "step": 656
    },
    {
      "epoch": 0.43423661599471247,
      "grad_norm": 1.0251870155334473,
      "learning_rate": 4.45720423000661e-06,
      "loss": 0.703,
      "step": 657
    },
    {
      "epoch": 0.43489755452742895,
      "grad_norm": 1.0540878772735596,
      "learning_rate": 4.456378056840714e-06,
      "loss": 0.6879,
      "step": 658
    },
    {
      "epoch": 0.4355584930601454,
      "grad_norm": 1.009677767753601,
      "learning_rate": 4.455551883674819e-06,
      "loss": 0.721,
      "step": 659
    },
    {
      "epoch": 0.43621943159286186,
      "grad_norm": 1.0416098833084106,
      "learning_rate": 4.454725710508923e-06,
      "loss": 0.7088,
      "step": 660
    },
    {
      "epoch": 0.43688037012557834,
      "grad_norm": 0.9847232103347778,
      "learning_rate": 4.453899537343027e-06,
      "loss": 0.7012,
      "step": 661
    },
    {
      "epoch": 0.43754130865829477,
      "grad_norm": 1.1227941513061523,
      "learning_rate": 4.453073364177132e-06,
      "loss": 0.666,
      "step": 662
    },
    {
      "epoch": 0.43820224719101125,
      "grad_norm": 1.06183922290802,
      "learning_rate": 4.452247191011236e-06,
      "loss": 0.7003,
      "step": 663
    },
    {
      "epoch": 0.4388631857237277,
      "grad_norm": 1.0691806077957153,
      "learning_rate": 4.451421017845341e-06,
      "loss": 0.676,
      "step": 664
    },
    {
      "epoch": 0.43952412425644416,
      "grad_norm": 1.0473062992095947,
      "learning_rate": 4.450594844679445e-06,
      "loss": 0.6991,
      "step": 665
    },
    {
      "epoch": 0.4401850627891606,
      "grad_norm": 0.9436799883842468,
      "learning_rate": 4.44976867151355e-06,
      "loss": 0.6707,
      "step": 666
    },
    {
      "epoch": 0.44084600132187707,
      "grad_norm": 0.9684655666351318,
      "learning_rate": 4.448942498347654e-06,
      "loss": 0.6735,
      "step": 667
    },
    {
      "epoch": 0.44150693985459355,
      "grad_norm": 1.0152767896652222,
      "learning_rate": 4.4481163251817585e-06,
      "loss": 0.6495,
      "step": 668
    },
    {
      "epoch": 0.44216787838731,
      "grad_norm": 0.9926691055297852,
      "learning_rate": 4.447290152015863e-06,
      "loss": 0.6474,
      "step": 669
    },
    {
      "epoch": 0.44282881692002646,
      "grad_norm": 0.9405991435050964,
      "learning_rate": 4.446463978849967e-06,
      "loss": 0.6401,
      "step": 670
    },
    {
      "epoch": 0.4434897554527429,
      "grad_norm": 0.982231080532074,
      "learning_rate": 4.4456378056840715e-06,
      "loss": 0.6579,
      "step": 671
    },
    {
      "epoch": 0.44415069398545937,
      "grad_norm": 1.1427491903305054,
      "learning_rate": 4.444811632518176e-06,
      "loss": 0.6684,
      "step": 672
    },
    {
      "epoch": 0.4448116325181758,
      "grad_norm": 1.0067946910858154,
      "learning_rate": 4.443985459352281e-06,
      "loss": 0.6434,
      "step": 673
    },
    {
      "epoch": 0.4454725710508923,
      "grad_norm": 0.9481509923934937,
      "learning_rate": 4.4431592861863845e-06,
      "loss": 0.6477,
      "step": 674
    },
    {
      "epoch": 0.4461335095836087,
      "grad_norm": 0.9823086857795715,
      "learning_rate": 4.44233311302049e-06,
      "loss": 0.6691,
      "step": 675
    },
    {
      "epoch": 0.4467944481163252,
      "grad_norm": 1.0078306198120117,
      "learning_rate": 4.441506939854594e-06,
      "loss": 0.6658,
      "step": 676
    },
    {
      "epoch": 0.44745538664904166,
      "grad_norm": 0.8971468210220337,
      "learning_rate": 4.440680766688698e-06,
      "loss": 0.6681,
      "step": 677
    },
    {
      "epoch": 0.4481163251817581,
      "grad_norm": 0.9477197527885437,
      "learning_rate": 4.439854593522803e-06,
      "loss": 0.683,
      "step": 678
    },
    {
      "epoch": 0.4487772637144746,
      "grad_norm": 0.9859177470207214,
      "learning_rate": 4.439028420356907e-06,
      "loss": 0.6722,
      "step": 679
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.9678669571876526,
      "learning_rate": 4.438202247191011e-06,
      "loss": 0.6853,
      "step": 680
    },
    {
      "epoch": 0.4500991407799075,
      "grad_norm": 1.0018529891967773,
      "learning_rate": 4.437376074025116e-06,
      "loss": 0.6949,
      "step": 681
    },
    {
      "epoch": 0.4507600793126239,
      "grad_norm": 0.965299665927887,
      "learning_rate": 4.436549900859221e-06,
      "loss": 0.6569,
      "step": 682
    },
    {
      "epoch": 0.4514210178453404,
      "grad_norm": 0.9112321734428406,
      "learning_rate": 4.435723727693325e-06,
      "loss": 0.651,
      "step": 683
    },
    {
      "epoch": 0.4520819563780568,
      "grad_norm": 0.972880482673645,
      "learning_rate": 4.4348975545274295e-06,
      "loss": 0.6715,
      "step": 684
    },
    {
      "epoch": 0.4527428949107733,
      "grad_norm": 0.955851137638092,
      "learning_rate": 4.434071381361534e-06,
      "loss": 0.7001,
      "step": 685
    },
    {
      "epoch": 0.4534038334434898,
      "grad_norm": 0.9458754062652588,
      "learning_rate": 4.433245208195638e-06,
      "loss": 0.6437,
      "step": 686
    },
    {
      "epoch": 0.4540647719762062,
      "grad_norm": 0.9125726819038391,
      "learning_rate": 4.4324190350297425e-06,
      "loss": 0.6487,
      "step": 687
    },
    {
      "epoch": 0.4547257105089227,
      "grad_norm": 0.910071074962616,
      "learning_rate": 4.431592861863847e-06,
      "loss": 0.6323,
      "step": 688
    },
    {
      "epoch": 0.4553866490416391,
      "grad_norm": 1.8423514366149902,
      "learning_rate": 4.430766688697952e-06,
      "loss": 0.6654,
      "step": 689
    },
    {
      "epoch": 0.4560475875743556,
      "grad_norm": 0.931227445602417,
      "learning_rate": 4.4299405155320555e-06,
      "loss": 0.6425,
      "step": 690
    },
    {
      "epoch": 0.456708526107072,
      "grad_norm": 0.8792334794998169,
      "learning_rate": 4.429114342366161e-06,
      "loss": 0.5997,
      "step": 691
    },
    {
      "epoch": 0.4573694646397885,
      "grad_norm": 0.9393466114997864,
      "learning_rate": 4.428288169200265e-06,
      "loss": 0.6202,
      "step": 692
    },
    {
      "epoch": 0.45803040317250493,
      "grad_norm": 1.005448341369629,
      "learning_rate": 4.427461996034369e-06,
      "loss": 0.6438,
      "step": 693
    },
    {
      "epoch": 0.4586913417052214,
      "grad_norm": 0.9729195237159729,
      "learning_rate": 4.426635822868474e-06,
      "loss": 0.6503,
      "step": 694
    },
    {
      "epoch": 0.4593522802379379,
      "grad_norm": 0.8786792755126953,
      "learning_rate": 4.425809649702578e-06,
      "loss": 0.5777,
      "step": 695
    },
    {
      "epoch": 0.4600132187706543,
      "grad_norm": 0.9933199286460876,
      "learning_rate": 4.424983476536682e-06,
      "loss": 0.6562,
      "step": 696
    },
    {
      "epoch": 0.4606741573033708,
      "grad_norm": 0.9864950180053711,
      "learning_rate": 4.424157303370787e-06,
      "loss": 0.6275,
      "step": 697
    },
    {
      "epoch": 0.46133509583608723,
      "grad_norm": 0.8841434717178345,
      "learning_rate": 4.423331130204891e-06,
      "loss": 0.596,
      "step": 698
    },
    {
      "epoch": 0.4619960343688037,
      "grad_norm": 0.9945974946022034,
      "learning_rate": 4.422504957038995e-06,
      "loss": 0.6008,
      "step": 699
    },
    {
      "epoch": 0.46265697290152014,
      "grad_norm": 0.9498652815818787,
      "learning_rate": 4.4216787838731e-06,
      "loss": 0.629,
      "step": 700
    },
    {
      "epoch": 0.4633179114342366,
      "grad_norm": 0.9972216486930847,
      "learning_rate": 4.420852610707205e-06,
      "loss": 0.6537,
      "step": 701
    },
    {
      "epoch": 0.4639788499669531,
      "grad_norm": 1.0332010984420776,
      "learning_rate": 4.420026437541308e-06,
      "loss": 0.6365,
      "step": 702
    },
    {
      "epoch": 0.4646397884996695,
      "grad_norm": 1.0389437675476074,
      "learning_rate": 4.4192002643754136e-06,
      "loss": 0.6354,
      "step": 703
    },
    {
      "epoch": 0.465300727032386,
      "grad_norm": 0.9040376543998718,
      "learning_rate": 4.418374091209518e-06,
      "loss": 0.6373,
      "step": 704
    },
    {
      "epoch": 0.46596166556510243,
      "grad_norm": 0.890993595123291,
      "learning_rate": 4.417547918043622e-06,
      "loss": 0.5787,
      "step": 705
    },
    {
      "epoch": 0.4666226040978189,
      "grad_norm": 1.0467143058776855,
      "learning_rate": 4.4167217448777266e-06,
      "loss": 0.6188,
      "step": 706
    },
    {
      "epoch": 0.46728354263053534,
      "grad_norm": 0.9213902950286865,
      "learning_rate": 4.415895571711831e-06,
      "loss": 0.6005,
      "step": 707
    },
    {
      "epoch": 0.4679444811632518,
      "grad_norm": 1.006403923034668,
      "learning_rate": 4.415069398545936e-06,
      "loss": 0.6208,
      "step": 708
    },
    {
      "epoch": 0.46860541969596825,
      "grad_norm": 0.9269090294837952,
      "learning_rate": 4.4142432253800396e-06,
      "loss": 0.6014,
      "step": 709
    },
    {
      "epoch": 0.46926635822868473,
      "grad_norm": 1.0160166025161743,
      "learning_rate": 4.413417052214145e-06,
      "loss": 0.576,
      "step": 710
    },
    {
      "epoch": 0.4699272967614012,
      "grad_norm": 1.0175862312316895,
      "learning_rate": 4.412590879048249e-06,
      "loss": 0.6342,
      "step": 711
    },
    {
      "epoch": 0.47058823529411764,
      "grad_norm": 0.8750340938568115,
      "learning_rate": 4.411764705882353e-06,
      "loss": 0.5939,
      "step": 712
    },
    {
      "epoch": 0.4712491738268341,
      "grad_norm": 0.8257645964622498,
      "learning_rate": 4.410938532716458e-06,
      "loss": 0.5859,
      "step": 713
    },
    {
      "epoch": 0.47191011235955055,
      "grad_norm": 0.9220126271247864,
      "learning_rate": 4.410112359550562e-06,
      "loss": 0.609,
      "step": 714
    },
    {
      "epoch": 0.47257105089226703,
      "grad_norm": 0.8548728823661804,
      "learning_rate": 4.409286186384666e-06,
      "loss": 0.5718,
      "step": 715
    },
    {
      "epoch": 0.47323198942498346,
      "grad_norm": 0.9171983599662781,
      "learning_rate": 4.408460013218771e-06,
      "loss": 0.5658,
      "step": 716
    },
    {
      "epoch": 0.47389292795769994,
      "grad_norm": 0.9487581849098206,
      "learning_rate": 4.407633840052876e-06,
      "loss": 0.6171,
      "step": 717
    },
    {
      "epoch": 0.47455386649041637,
      "grad_norm": 0.9129846096038818,
      "learning_rate": 4.406807666886979e-06,
      "loss": 0.5657,
      "step": 718
    },
    {
      "epoch": 0.47521480502313285,
      "grad_norm": 0.9220523238182068,
      "learning_rate": 4.405981493721085e-06,
      "loss": 0.6147,
      "step": 719
    },
    {
      "epoch": 0.47587574355584933,
      "grad_norm": 0.8714309930801392,
      "learning_rate": 4.405155320555189e-06,
      "loss": 0.5624,
      "step": 720
    },
    {
      "epoch": 0.47653668208856576,
      "grad_norm": 0.9205124378204346,
      "learning_rate": 4.404329147389293e-06,
      "loss": 0.5902,
      "step": 721
    },
    {
      "epoch": 0.47719762062128224,
      "grad_norm": 0.8410819172859192,
      "learning_rate": 4.403502974223398e-06,
      "loss": 0.5523,
      "step": 722
    },
    {
      "epoch": 0.47785855915399866,
      "grad_norm": 0.8918874263763428,
      "learning_rate": 4.402676801057502e-06,
      "loss": 0.5684,
      "step": 723
    },
    {
      "epoch": 0.47851949768671515,
      "grad_norm": 0.9524827599525452,
      "learning_rate": 4.401850627891606e-06,
      "loss": 0.5527,
      "step": 724
    },
    {
      "epoch": 0.47918043621943157,
      "grad_norm": 0.9410390257835388,
      "learning_rate": 4.401024454725711e-06,
      "loss": 0.5861,
      "step": 725
    },
    {
      "epoch": 0.47984137475214805,
      "grad_norm": 0.8498568534851074,
      "learning_rate": 4.400198281559816e-06,
      "loss": 0.5358,
      "step": 726
    },
    {
      "epoch": 0.48050231328486454,
      "grad_norm": 0.942970871925354,
      "learning_rate": 4.399372108393919e-06,
      "loss": 0.5698,
      "step": 727
    },
    {
      "epoch": 0.48116325181758096,
      "grad_norm": 0.9539231657981873,
      "learning_rate": 4.3985459352280244e-06,
      "loss": 0.6097,
      "step": 728
    },
    {
      "epoch": 0.48182419035029744,
      "grad_norm": 0.8809593319892883,
      "learning_rate": 4.397719762062129e-06,
      "loss": 0.5653,
      "step": 729
    },
    {
      "epoch": 0.48248512888301387,
      "grad_norm": 1.062846302986145,
      "learning_rate": 4.396893588896233e-06,
      "loss": 0.5529,
      "step": 730
    },
    {
      "epoch": 0.48314606741573035,
      "grad_norm": 0.9190112948417664,
      "learning_rate": 4.3960674157303374e-06,
      "loss": 0.5881,
      "step": 731
    },
    {
      "epoch": 0.4838070059484468,
      "grad_norm": 0.8223477602005005,
      "learning_rate": 4.395241242564442e-06,
      "loss": 0.5668,
      "step": 732
    },
    {
      "epoch": 0.48446794448116326,
      "grad_norm": 0.8982295393943787,
      "learning_rate": 4.394415069398547e-06,
      "loss": 0.5277,
      "step": 733
    },
    {
      "epoch": 0.4851288830138797,
      "grad_norm": 0.9239581823348999,
      "learning_rate": 4.3935888962326504e-06,
      "loss": 0.5575,
      "step": 734
    },
    {
      "epoch": 0.48578982154659617,
      "grad_norm": 0.9291678071022034,
      "learning_rate": 4.392762723066756e-06,
      "loss": 0.5941,
      "step": 735
    },
    {
      "epoch": 0.48645076007931265,
      "grad_norm": 0.9447208642959595,
      "learning_rate": 4.39193654990086e-06,
      "loss": 0.5456,
      "step": 736
    },
    {
      "epoch": 0.4871116986120291,
      "grad_norm": 0.8848518133163452,
      "learning_rate": 4.391110376734964e-06,
      "loss": 0.5581,
      "step": 737
    },
    {
      "epoch": 0.48777263714474556,
      "grad_norm": 0.8725208044052124,
      "learning_rate": 4.390284203569069e-06,
      "loss": 0.5577,
      "step": 738
    },
    {
      "epoch": 0.488433575677462,
      "grad_norm": 1.00883948802948,
      "learning_rate": 4.389458030403173e-06,
      "loss": 0.5633,
      "step": 739
    },
    {
      "epoch": 0.48909451421017847,
      "grad_norm": 0.9494480490684509,
      "learning_rate": 4.388631857237277e-06,
      "loss": 0.5776,
      "step": 740
    },
    {
      "epoch": 0.4897554527428949,
      "grad_norm": 0.8802131414413452,
      "learning_rate": 4.387805684071382e-06,
      "loss": 0.5489,
      "step": 741
    },
    {
      "epoch": 0.4904163912756114,
      "grad_norm": 0.8615452647209167,
      "learning_rate": 4.386979510905486e-06,
      "loss": 0.5591,
      "step": 742
    },
    {
      "epoch": 0.4910773298083278,
      "grad_norm": 0.8569892644882202,
      "learning_rate": 4.38615333773959e-06,
      "loss": 0.5159,
      "step": 743
    },
    {
      "epoch": 0.4917382683410443,
      "grad_norm": 0.8743109107017517,
      "learning_rate": 4.385327164573695e-06,
      "loss": 0.5194,
      "step": 744
    },
    {
      "epoch": 0.49239920687376076,
      "grad_norm": 0.9719794392585754,
      "learning_rate": 4.3845009914078e-06,
      "loss": 0.5614,
      "step": 745
    },
    {
      "epoch": 0.4930601454064772,
      "grad_norm": 1.2270972728729248,
      "learning_rate": 4.383674818241903e-06,
      "loss": 0.5535,
      "step": 746
    },
    {
      "epoch": 0.4937210839391937,
      "grad_norm": 0.9095900654792786,
      "learning_rate": 4.3828486450760085e-06,
      "loss": 0.5569,
      "step": 747
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.9536663293838501,
      "learning_rate": 4.382022471910113e-06,
      "loss": 0.5298,
      "step": 748
    },
    {
      "epoch": 0.4950429610046266,
      "grad_norm": 0.9227783679962158,
      "learning_rate": 4.381196298744217e-06,
      "loss": 0.5264,
      "step": 749
    },
    {
      "epoch": 0.495703899537343,
      "grad_norm": 1.00017511844635,
      "learning_rate": 4.3803701255783215e-06,
      "loss": 0.5079,
      "step": 750
    },
    {
      "epoch": 0.4963648380700595,
      "grad_norm": 0.9464336633682251,
      "learning_rate": 4.379543952412426e-06,
      "loss": 0.5403,
      "step": 751
    },
    {
      "epoch": 0.4970257766027759,
      "grad_norm": 0.9157587289810181,
      "learning_rate": 4.37871777924653e-06,
      "loss": 0.519,
      "step": 752
    },
    {
      "epoch": 0.4976867151354924,
      "grad_norm": 0.876078724861145,
      "learning_rate": 4.3778916060806345e-06,
      "loss": 0.4977,
      "step": 753
    },
    {
      "epoch": 0.4983476536682089,
      "grad_norm": 0.8904042840003967,
      "learning_rate": 4.37706543291474e-06,
      "loss": 0.5062,
      "step": 754
    },
    {
      "epoch": 0.4990085922009253,
      "grad_norm": 0.8670127391815186,
      "learning_rate": 4.376239259748843e-06,
      "loss": 0.51,
      "step": 755
    },
    {
      "epoch": 0.4996695307336418,
      "grad_norm": 0.897854208946228,
      "learning_rate": 4.375413086582948e-06,
      "loss": 0.56,
      "step": 756
    },
    {
      "epoch": 0.5003304692663583,
      "grad_norm": 0.8977994322776794,
      "learning_rate": 4.374586913417053e-06,
      "loss": 0.5126,
      "step": 757
    },
    {
      "epoch": 0.5009914077990747,
      "grad_norm": 0.9357699155807495,
      "learning_rate": 4.373760740251157e-06,
      "loss": 0.5145,
      "step": 758
    },
    {
      "epoch": 0.5016523463317911,
      "grad_norm": 0.9028556942939758,
      "learning_rate": 4.372934567085261e-06,
      "loss": 0.5088,
      "step": 759
    },
    {
      "epoch": 0.5023132848645075,
      "grad_norm": 0.9204849004745483,
      "learning_rate": 4.372108393919366e-06,
      "loss": 0.5253,
      "step": 760
    },
    {
      "epoch": 0.5029742233972241,
      "grad_norm": 0.892850399017334,
      "learning_rate": 4.371282220753471e-06,
      "loss": 0.541,
      "step": 761
    },
    {
      "epoch": 0.5036351619299405,
      "grad_norm": 0.8567554950714111,
      "learning_rate": 4.370456047587574e-06,
      "loss": 0.5441,
      "step": 762
    },
    {
      "epoch": 0.5042961004626569,
      "grad_norm": 0.9289616346359253,
      "learning_rate": 4.3696298744216795e-06,
      "loss": 0.4955,
      "step": 763
    },
    {
      "epoch": 0.5049570389953735,
      "grad_norm": 0.9803995490074158,
      "learning_rate": 4.368803701255784e-06,
      "loss": 0.5165,
      "step": 764
    },
    {
      "epoch": 0.5056179775280899,
      "grad_norm": 0.8589466214179993,
      "learning_rate": 4.367977528089888e-06,
      "loss": 0.5139,
      "step": 765
    },
    {
      "epoch": 0.5062789160608063,
      "grad_norm": 0.8960080146789551,
      "learning_rate": 4.3671513549239925e-06,
      "loss": 0.5079,
      "step": 766
    },
    {
      "epoch": 0.5069398545935228,
      "grad_norm": 0.9936041235923767,
      "learning_rate": 4.366325181758097e-06,
      "loss": 0.5477,
      "step": 767
    },
    {
      "epoch": 0.5076007931262393,
      "grad_norm": 0.9483379125595093,
      "learning_rate": 4.365499008592201e-06,
      "loss": 0.5104,
      "step": 768
    },
    {
      "epoch": 0.5082617316589557,
      "grad_norm": 0.8633374571800232,
      "learning_rate": 4.3646728354263055e-06,
      "loss": 0.4936,
      "step": 769
    },
    {
      "epoch": 0.5089226701916721,
      "grad_norm": 0.9699112176895142,
      "learning_rate": 4.363846662260411e-06,
      "loss": 0.5228,
      "step": 770
    },
    {
      "epoch": 0.5095836087243887,
      "grad_norm": 0.8892650604248047,
      "learning_rate": 4.363020489094514e-06,
      "loss": 0.487,
      "step": 771
    },
    {
      "epoch": 0.5102445472571051,
      "grad_norm": 0.8536967635154724,
      "learning_rate": 4.362194315928619e-06,
      "loss": 0.5128,
      "step": 772
    },
    {
      "epoch": 0.5109054857898215,
      "grad_norm": 0.9570530652999878,
      "learning_rate": 4.361368142762724e-06,
      "loss": 0.5181,
      "step": 773
    },
    {
      "epoch": 0.511566424322538,
      "grad_norm": 0.9089099168777466,
      "learning_rate": 4.360541969596828e-06,
      "loss": 0.4871,
      "step": 774
    },
    {
      "epoch": 0.5122273628552545,
      "grad_norm": 0.846722424030304,
      "learning_rate": 4.359715796430932e-06,
      "loss": 0.4933,
      "step": 775
    },
    {
      "epoch": 0.5128883013879709,
      "grad_norm": 0.851162850856781,
      "learning_rate": 4.358889623265037e-06,
      "loss": 0.4895,
      "step": 776
    },
    {
      "epoch": 0.5135492399206874,
      "grad_norm": 0.8736350536346436,
      "learning_rate": 4.358063450099141e-06,
      "loss": 0.4631,
      "step": 777
    },
    {
      "epoch": 0.5142101784534039,
      "grad_norm": 0.8759934902191162,
      "learning_rate": 4.357237276933245e-06,
      "loss": 0.4828,
      "step": 778
    },
    {
      "epoch": 0.5148711169861203,
      "grad_norm": 0.8546406030654907,
      "learning_rate": 4.3564111037673505e-06,
      "loss": 0.4629,
      "step": 779
    },
    {
      "epoch": 0.5155320555188367,
      "grad_norm": 0.8127352595329285,
      "learning_rate": 4.355584930601454e-06,
      "loss": 0.5127,
      "step": 780
    },
    {
      "epoch": 0.5161929940515532,
      "grad_norm": 0.9172448515892029,
      "learning_rate": 4.354758757435559e-06,
      "loss": 0.4799,
      "step": 781
    },
    {
      "epoch": 0.5168539325842697,
      "grad_norm": 0.8830775618553162,
      "learning_rate": 4.3539325842696635e-06,
      "loss": 0.4747,
      "step": 782
    },
    {
      "epoch": 0.5175148711169861,
      "grad_norm": 0.9659636616706848,
      "learning_rate": 4.353106411103768e-06,
      "loss": 0.5055,
      "step": 783
    },
    {
      "epoch": 0.5181758096497026,
      "grad_norm": 0.9145908951759338,
      "learning_rate": 4.352280237937872e-06,
      "loss": 0.521,
      "step": 784
    },
    {
      "epoch": 0.518836748182419,
      "grad_norm": 0.8721673488616943,
      "learning_rate": 4.3514540647719765e-06,
      "loss": 0.4814,
      "step": 785
    },
    {
      "epoch": 0.5194976867151355,
      "grad_norm": 0.870853066444397,
      "learning_rate": 4.350627891606081e-06,
      "loss": 0.489,
      "step": 786
    },
    {
      "epoch": 0.520158625247852,
      "grad_norm": 0.8997185826301575,
      "learning_rate": 4.349801718440185e-06,
      "loss": 0.4781,
      "step": 787
    },
    {
      "epoch": 0.5208195637805684,
      "grad_norm": 0.8588562607765198,
      "learning_rate": 4.3489755452742895e-06,
      "loss": 0.4714,
      "step": 788
    },
    {
      "epoch": 0.5214805023132849,
      "grad_norm": 0.8553215265274048,
      "learning_rate": 4.348149372108395e-06,
      "loss": 0.467,
      "step": 789
    },
    {
      "epoch": 0.5221414408460013,
      "grad_norm": 0.8633610606193542,
      "learning_rate": 4.347323198942498e-06,
      "loss": 0.4785,
      "step": 790
    },
    {
      "epoch": 0.5228023793787178,
      "grad_norm": 0.873223602771759,
      "learning_rate": 4.346497025776603e-06,
      "loss": 0.4496,
      "step": 791
    },
    {
      "epoch": 0.5234633179114342,
      "grad_norm": 0.8928974270820618,
      "learning_rate": 4.345670852610708e-06,
      "loss": 0.4823,
      "step": 792
    },
    {
      "epoch": 0.5241242564441507,
      "grad_norm": 0.8744205832481384,
      "learning_rate": 4.344844679444812e-06,
      "loss": 0.4562,
      "step": 793
    },
    {
      "epoch": 0.5247851949768672,
      "grad_norm": 0.8779988884925842,
      "learning_rate": 4.344018506278916e-06,
      "loss": 0.4989,
      "step": 794
    },
    {
      "epoch": 0.5254461335095836,
      "grad_norm": 0.8930135369300842,
      "learning_rate": 4.343192333113021e-06,
      "loss": 0.4872,
      "step": 795
    },
    {
      "epoch": 0.5261070720423001,
      "grad_norm": 0.8460457921028137,
      "learning_rate": 4.342366159947125e-06,
      "loss": 0.4293,
      "step": 796
    },
    {
      "epoch": 0.5267680105750165,
      "grad_norm": 0.9138777852058411,
      "learning_rate": 4.341539986781229e-06,
      "loss": 0.4961,
      "step": 797
    },
    {
      "epoch": 0.527428949107733,
      "grad_norm": 1.498815894126892,
      "learning_rate": 4.3407138136153345e-06,
      "loss": 0.4794,
      "step": 798
    },
    {
      "epoch": 0.5280898876404494,
      "grad_norm": 0.9048401713371277,
      "learning_rate": 4.339887640449438e-06,
      "loss": 0.461,
      "step": 799
    },
    {
      "epoch": 0.5287508261731659,
      "grad_norm": 0.8191498517990112,
      "learning_rate": 4.339061467283543e-06,
      "loss": 0.4501,
      "step": 800
    },
    {
      "epoch": 0.5294117647058824,
      "grad_norm": 0.8901243805885315,
      "learning_rate": 4.3382352941176475e-06,
      "loss": 0.5157,
      "step": 801
    },
    {
      "epoch": 0.5300727032385988,
      "grad_norm": 0.9150500297546387,
      "learning_rate": 4.337409120951752e-06,
      "loss": 0.487,
      "step": 802
    },
    {
      "epoch": 0.5307336417713153,
      "grad_norm": 0.9238541722297668,
      "learning_rate": 4.336582947785856e-06,
      "loss": 0.4713,
      "step": 803
    },
    {
      "epoch": 0.5313945803040317,
      "grad_norm": 0.9021601676940918,
      "learning_rate": 4.3357567746199605e-06,
      "loss": 0.4288,
      "step": 804
    },
    {
      "epoch": 0.5320555188367482,
      "grad_norm": 0.8870630264282227,
      "learning_rate": 4.334930601454065e-06,
      "loss": 0.442,
      "step": 805
    },
    {
      "epoch": 0.5327164573694646,
      "grad_norm": 0.8588947057723999,
      "learning_rate": 4.334104428288169e-06,
      "loss": 0.475,
      "step": 806
    },
    {
      "epoch": 0.5333773959021811,
      "grad_norm": 0.8299189805984497,
      "learning_rate": 4.333278255122274e-06,
      "loss": 0.4643,
      "step": 807
    },
    {
      "epoch": 0.5340383344348976,
      "grad_norm": 0.8217135667800903,
      "learning_rate": 4.332452081956379e-06,
      "loss": 0.4702,
      "step": 808
    },
    {
      "epoch": 0.534699272967614,
      "grad_norm": 0.875540554523468,
      "learning_rate": 4.331625908790483e-06,
      "loss": 0.4426,
      "step": 809
    },
    {
      "epoch": 0.5353602115003304,
      "grad_norm": 0.8795346617698669,
      "learning_rate": 4.330799735624587e-06,
      "loss": 0.465,
      "step": 810
    },
    {
      "epoch": 0.536021150033047,
      "grad_norm": 0.943850576877594,
      "learning_rate": 4.329973562458692e-06,
      "loss": 0.4482,
      "step": 811
    },
    {
      "epoch": 0.5366820885657634,
      "grad_norm": 0.8006826639175415,
      "learning_rate": 4.329147389292796e-06,
      "loss": 0.4615,
      "step": 812
    },
    {
      "epoch": 0.5373430270984798,
      "grad_norm": 0.8432038426399231,
      "learning_rate": 4.3283212161269e-06,
      "loss": 0.4458,
      "step": 813
    },
    {
      "epoch": 0.5380039656311963,
      "grad_norm": 0.8439184427261353,
      "learning_rate": 4.3274950429610056e-06,
      "loss": 0.4359,
      "step": 814
    },
    {
      "epoch": 0.5386649041639128,
      "grad_norm": 0.8270184397697449,
      "learning_rate": 4.326668869795109e-06,
      "loss": 0.4083,
      "step": 815
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.9928416609764099,
      "learning_rate": 4.325842696629214e-06,
      "loss": 0.483,
      "step": 816
    },
    {
      "epoch": 0.5399867812293456,
      "grad_norm": 0.8449748754501343,
      "learning_rate": 4.3250165234633186e-06,
      "loss": 0.4178,
      "step": 817
    },
    {
      "epoch": 0.5406477197620622,
      "grad_norm": 0.8416401147842407,
      "learning_rate": 4.324190350297423e-06,
      "loss": 0.466,
      "step": 818
    },
    {
      "epoch": 0.5413086582947786,
      "grad_norm": 0.8925421237945557,
      "learning_rate": 4.323364177131527e-06,
      "loss": 0.4507,
      "step": 819
    },
    {
      "epoch": 0.541969596827495,
      "grad_norm": 0.775690495967865,
      "learning_rate": 4.3225380039656316e-06,
      "loss": 0.4512,
      "step": 820
    },
    {
      "epoch": 0.5426305353602116,
      "grad_norm": 0.8494747281074524,
      "learning_rate": 4.321711830799736e-06,
      "loss": 0.4487,
      "step": 821
    },
    {
      "epoch": 0.543291473892928,
      "grad_norm": 0.8675339818000793,
      "learning_rate": 4.32088565763384e-06,
      "loss": 0.4458,
      "step": 822
    },
    {
      "epoch": 0.5439524124256444,
      "grad_norm": 0.9173070788383484,
      "learning_rate": 4.320059484467945e-06,
      "loss": 0.4368,
      "step": 823
    },
    {
      "epoch": 0.5446133509583608,
      "grad_norm": 0.792540431022644,
      "learning_rate": 4.319233311302049e-06,
      "loss": 0.44,
      "step": 824
    },
    {
      "epoch": 0.5452742894910774,
      "grad_norm": 0.7810364961624146,
      "learning_rate": 4.318407138136154e-06,
      "loss": 0.4452,
      "step": 825
    },
    {
      "epoch": 0.5459352280237938,
      "grad_norm": 0.8693791031837463,
      "learning_rate": 4.317580964970258e-06,
      "loss": 0.4301,
      "step": 826
    },
    {
      "epoch": 0.5465961665565102,
      "grad_norm": 0.8860304355621338,
      "learning_rate": 4.316754791804362e-06,
      "loss": 0.4265,
      "step": 827
    },
    {
      "epoch": 0.5472571050892266,
      "grad_norm": 0.841106653213501,
      "learning_rate": 4.315928618638467e-06,
      "loss": 0.4381,
      "step": 828
    },
    {
      "epoch": 0.5479180436219432,
      "grad_norm": 0.8397200107574463,
      "learning_rate": 4.315102445472571e-06,
      "loss": 0.417,
      "step": 829
    },
    {
      "epoch": 0.5485789821546596,
      "grad_norm": 0.8386351466178894,
      "learning_rate": 4.314276272306676e-06,
      "loss": 0.4094,
      "step": 830
    },
    {
      "epoch": 0.549239920687376,
      "grad_norm": 0.8535388708114624,
      "learning_rate": 4.31345009914078e-06,
      "loss": 0.4192,
      "step": 831
    },
    {
      "epoch": 0.5499008592200926,
      "grad_norm": 0.7942900657653809,
      "learning_rate": 4.312623925974884e-06,
      "loss": 0.4345,
      "step": 832
    },
    {
      "epoch": 0.550561797752809,
      "grad_norm": 0.8469151258468628,
      "learning_rate": 4.3117977528089896e-06,
      "loss": 0.4274,
      "step": 833
    },
    {
      "epoch": 0.5512227362855254,
      "grad_norm": 0.7776496410369873,
      "learning_rate": 4.310971579643093e-06,
      "loss": 0.4072,
      "step": 834
    },
    {
      "epoch": 0.5518836748182419,
      "grad_norm": 0.8327667117118835,
      "learning_rate": 4.310145406477198e-06,
      "loss": 0.4106,
      "step": 835
    },
    {
      "epoch": 0.5525446133509584,
      "grad_norm": 0.8583775162696838,
      "learning_rate": 4.3093192333113026e-06,
      "loss": 0.4032,
      "step": 836
    },
    {
      "epoch": 0.5532055518836748,
      "grad_norm": 0.8385676145553589,
      "learning_rate": 4.308493060145407e-06,
      "loss": 0.4543,
      "step": 837
    },
    {
      "epoch": 0.5538664904163912,
      "grad_norm": 0.8982905745506287,
      "learning_rate": 4.307666886979511e-06,
      "loss": 0.4481,
      "step": 838
    },
    {
      "epoch": 0.5545274289491078,
      "grad_norm": 0.8513773679733276,
      "learning_rate": 4.3068407138136156e-06,
      "loss": 0.4152,
      "step": 839
    },
    {
      "epoch": 0.5551883674818242,
      "grad_norm": 0.7556638717651367,
      "learning_rate": 4.30601454064772e-06,
      "loss": 0.3727,
      "step": 840
    },
    {
      "epoch": 0.5558493060145406,
      "grad_norm": 0.8029106855392456,
      "learning_rate": 4.305188367481824e-06,
      "loss": 0.4267,
      "step": 841
    },
    {
      "epoch": 0.5565102445472571,
      "grad_norm": 0.8795979022979736,
      "learning_rate": 4.304362194315929e-06,
      "loss": 0.4367,
      "step": 842
    },
    {
      "epoch": 0.5571711830799736,
      "grad_norm": 0.9119191765785217,
      "learning_rate": 4.303536021150033e-06,
      "loss": 0.437,
      "step": 843
    },
    {
      "epoch": 0.55783212161269,
      "grad_norm": 0.7931537628173828,
      "learning_rate": 4.302709847984138e-06,
      "loss": 0.4118,
      "step": 844
    },
    {
      "epoch": 0.5584930601454065,
      "grad_norm": 0.8579287528991699,
      "learning_rate": 4.301883674818242e-06,
      "loss": 0.4551,
      "step": 845
    },
    {
      "epoch": 0.559153998678123,
      "grad_norm": 0.788446843624115,
      "learning_rate": 4.301057501652347e-06,
      "loss": 0.4306,
      "step": 846
    },
    {
      "epoch": 0.5598149372108394,
      "grad_norm": 0.8165716528892517,
      "learning_rate": 4.300231328486451e-06,
      "loss": 0.3944,
      "step": 847
    },
    {
      "epoch": 0.5604758757435558,
      "grad_norm": 0.7962663769721985,
      "learning_rate": 4.299405155320555e-06,
      "loss": 0.4414,
      "step": 848
    },
    {
      "epoch": 0.5611368142762723,
      "grad_norm": 0.8241642713546753,
      "learning_rate": 4.29857898215466e-06,
      "loss": 0.4074,
      "step": 849
    },
    {
      "epoch": 0.5617977528089888,
      "grad_norm": 0.872893750667572,
      "learning_rate": 4.297752808988764e-06,
      "loss": 0.4666,
      "step": 850
    },
    {
      "epoch": 0.5624586913417052,
      "grad_norm": 0.8210150003433228,
      "learning_rate": 4.296926635822869e-06,
      "loss": 0.4352,
      "step": 851
    },
    {
      "epoch": 0.5631196298744217,
      "grad_norm": 0.8420312404632568,
      "learning_rate": 4.296100462656973e-06,
      "loss": 0.3888,
      "step": 852
    },
    {
      "epoch": 0.5637805684071381,
      "grad_norm": 0.7983489632606506,
      "learning_rate": 4.295274289491078e-06,
      "loss": 0.4027,
      "step": 853
    },
    {
      "epoch": 0.5644415069398546,
      "grad_norm": 0.8228299021720886,
      "learning_rate": 4.294448116325182e-06,
      "loss": 0.4096,
      "step": 854
    },
    {
      "epoch": 0.565102445472571,
      "grad_norm": 0.829628050327301,
      "learning_rate": 4.293621943159287e-06,
      "loss": 0.4324,
      "step": 855
    },
    {
      "epoch": 0.5657633840052875,
      "grad_norm": 0.858195960521698,
      "learning_rate": 4.292795769993391e-06,
      "loss": 0.395,
      "step": 856
    },
    {
      "epoch": 0.566424322538004,
      "grad_norm": 0.840943455696106,
      "learning_rate": 4.291969596827495e-06,
      "loss": 0.4562,
      "step": 857
    },
    {
      "epoch": 0.5670852610707204,
      "grad_norm": 0.8128605484962463,
      "learning_rate": 4.2911434236616004e-06,
      "loss": 0.4058,
      "step": 858
    },
    {
      "epoch": 0.5677461996034369,
      "grad_norm": 0.9038912653923035,
      "learning_rate": 4.290317250495704e-06,
      "loss": 0.4041,
      "step": 859
    },
    {
      "epoch": 0.5684071381361533,
      "grad_norm": 0.7835940718650818,
      "learning_rate": 4.289491077329809e-06,
      "loss": 0.3861,
      "step": 860
    },
    {
      "epoch": 0.5690680766688698,
      "grad_norm": 0.8057880401611328,
      "learning_rate": 4.2886649041639134e-06,
      "loss": 0.4188,
      "step": 861
    },
    {
      "epoch": 0.5697290152015863,
      "grad_norm": 0.8528323173522949,
      "learning_rate": 4.287838730998018e-06,
      "loss": 0.3859,
      "step": 862
    },
    {
      "epoch": 0.5703899537343027,
      "grad_norm": 0.7850666642189026,
      "learning_rate": 4.287012557832122e-06,
      "loss": 0.3928,
      "step": 863
    },
    {
      "epoch": 0.5710508922670192,
      "grad_norm": 0.7876420617103577,
      "learning_rate": 4.2861863846662264e-06,
      "loss": 0.3992,
      "step": 864
    },
    {
      "epoch": 0.5717118307997356,
      "grad_norm": 0.785914421081543,
      "learning_rate": 4.285360211500331e-06,
      "loss": 0.3879,
      "step": 865
    },
    {
      "epoch": 0.5723727693324521,
      "grad_norm": 0.7907494902610779,
      "learning_rate": 4.284534038334435e-06,
      "loss": 0.4301,
      "step": 866
    },
    {
      "epoch": 0.5730337078651685,
      "grad_norm": 0.7678529620170593,
      "learning_rate": 4.28370786516854e-06,
      "loss": 0.402,
      "step": 867
    },
    {
      "epoch": 0.573694646397885,
      "grad_norm": 0.8049115538597107,
      "learning_rate": 4.282881692002644e-06,
      "loss": 0.3705,
      "step": 868
    },
    {
      "epoch": 0.5743555849306015,
      "grad_norm": 0.7592657804489136,
      "learning_rate": 4.282055518836748e-06,
      "loss": 0.3914,
      "step": 869
    },
    {
      "epoch": 0.5750165234633179,
      "grad_norm": 0.7765206098556519,
      "learning_rate": 4.281229345670853e-06,
      "loss": 0.403,
      "step": 870
    },
    {
      "epoch": 0.5756774619960344,
      "grad_norm": 0.8469984531402588,
      "learning_rate": 4.280403172504957e-06,
      "loss": 0.3813,
      "step": 871
    },
    {
      "epoch": 0.5763384005287508,
      "grad_norm": 0.8983191847801208,
      "learning_rate": 4.279576999339062e-06,
      "loss": 0.3879,
      "step": 872
    },
    {
      "epoch": 0.5769993390614673,
      "grad_norm": 0.7843501567840576,
      "learning_rate": 4.278750826173166e-06,
      "loss": 0.412,
      "step": 873
    },
    {
      "epoch": 0.5776602775941837,
      "grad_norm": 0.8130376935005188,
      "learning_rate": 4.277924653007271e-06,
      "loss": 0.4314,
      "step": 874
    },
    {
      "epoch": 0.5783212161269002,
      "grad_norm": 0.7664197087287903,
      "learning_rate": 4.277098479841375e-06,
      "loss": 0.3712,
      "step": 875
    },
    {
      "epoch": 0.5789821546596167,
      "grad_norm": 0.8128633499145508,
      "learning_rate": 4.276272306675479e-06,
      "loss": 0.3825,
      "step": 876
    },
    {
      "epoch": 0.5796430931923331,
      "grad_norm": 0.7827865481376648,
      "learning_rate": 4.275446133509584e-06,
      "loss": 0.4136,
      "step": 877
    },
    {
      "epoch": 0.5803040317250495,
      "grad_norm": 0.8105295300483704,
      "learning_rate": 4.274619960343688e-06,
      "loss": 0.4074,
      "step": 878
    },
    {
      "epoch": 0.5809649702577661,
      "grad_norm": 0.7820135354995728,
      "learning_rate": 4.273793787177793e-06,
      "loss": 0.3921,
      "step": 879
    },
    {
      "epoch": 0.5816259087904825,
      "grad_norm": 0.784250795841217,
      "learning_rate": 4.272967614011897e-06,
      "loss": 0.3965,
      "step": 880
    },
    {
      "epoch": 0.5822868473231989,
      "grad_norm": 0.8048874139785767,
      "learning_rate": 4.272141440846002e-06,
      "loss": 0.3803,
      "step": 881
    },
    {
      "epoch": 0.5829477858559154,
      "grad_norm": 0.81029212474823,
      "learning_rate": 4.271315267680106e-06,
      "loss": 0.3398,
      "step": 882
    },
    {
      "epoch": 0.5836087243886319,
      "grad_norm": 0.751998782157898,
      "learning_rate": 4.2704890945142105e-06,
      "loss": 0.3704,
      "step": 883
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.8133649826049805,
      "learning_rate": 4.269662921348315e-06,
      "loss": 0.3818,
      "step": 884
    },
    {
      "epoch": 0.5849306014540647,
      "grad_norm": 0.8040895462036133,
      "learning_rate": 4.268836748182419e-06,
      "loss": 0.3884,
      "step": 885
    },
    {
      "epoch": 0.5855915399867813,
      "grad_norm": 0.9517098069190979,
      "learning_rate": 4.268010575016524e-06,
      "loss": 0.4125,
      "step": 886
    },
    {
      "epoch": 0.5862524785194977,
      "grad_norm": 0.8012877106666565,
      "learning_rate": 4.267184401850628e-06,
      "loss": 0.3596,
      "step": 887
    },
    {
      "epoch": 0.5869134170522141,
      "grad_norm": 0.7902956008911133,
      "learning_rate": 4.266358228684733e-06,
      "loss": 0.4039,
      "step": 888
    },
    {
      "epoch": 0.5875743555849307,
      "grad_norm": 0.7991634011268616,
      "learning_rate": 4.265532055518837e-06,
      "loss": 0.3803,
      "step": 889
    },
    {
      "epoch": 0.5882352941176471,
      "grad_norm": 0.7724401950836182,
      "learning_rate": 4.264705882352942e-06,
      "loss": 0.3747,
      "step": 890
    },
    {
      "epoch": 0.5888962326503635,
      "grad_norm": 0.7845816612243652,
      "learning_rate": 4.263879709187046e-06,
      "loss": 0.4234,
      "step": 891
    },
    {
      "epoch": 0.5895571711830799,
      "grad_norm": 0.8918713927268982,
      "learning_rate": 4.26305353602115e-06,
      "loss": 0.4073,
      "step": 892
    },
    {
      "epoch": 0.5902181097157965,
      "grad_norm": 0.7674925923347473,
      "learning_rate": 4.262227362855255e-06,
      "loss": 0.4064,
      "step": 893
    },
    {
      "epoch": 0.5908790482485129,
      "grad_norm": 0.9848127365112305,
      "learning_rate": 4.261401189689359e-06,
      "loss": 0.3724,
      "step": 894
    },
    {
      "epoch": 0.5915399867812293,
      "grad_norm": 0.766975998878479,
      "learning_rate": 4.260575016523464e-06,
      "loss": 0.3755,
      "step": 895
    },
    {
      "epoch": 0.5922009253139459,
      "grad_norm": 0.8502320647239685,
      "learning_rate": 4.259748843357568e-06,
      "loss": 0.393,
      "step": 896
    },
    {
      "epoch": 0.5928618638466623,
      "grad_norm": 0.7678463459014893,
      "learning_rate": 4.258922670191673e-06,
      "loss": 0.3981,
      "step": 897
    },
    {
      "epoch": 0.5935228023793787,
      "grad_norm": 0.8085315823554993,
      "learning_rate": 4.258096497025777e-06,
      "loss": 0.4215,
      "step": 898
    },
    {
      "epoch": 0.5941837409120951,
      "grad_norm": 0.777773916721344,
      "learning_rate": 4.2572703238598815e-06,
      "loss": 0.3665,
      "step": 899
    },
    {
      "epoch": 0.5948446794448117,
      "grad_norm": 0.7130144238471985,
      "learning_rate": 4.256444150693986e-06,
      "loss": 0.3436,
      "step": 900
    },
    {
      "epoch": 0.5955056179775281,
      "grad_norm": 0.7766662240028381,
      "learning_rate": 4.25561797752809e-06,
      "loss": 0.3641,
      "step": 901
    },
    {
      "epoch": 0.5961665565102445,
      "grad_norm": 0.7328522801399231,
      "learning_rate": 4.2547918043621945e-06,
      "loss": 0.3472,
      "step": 902
    },
    {
      "epoch": 0.596827495042961,
      "grad_norm": 0.8303283452987671,
      "learning_rate": 4.253965631196299e-06,
      "loss": 0.4006,
      "step": 903
    },
    {
      "epoch": 0.5974884335756775,
      "grad_norm": 0.9153186082839966,
      "learning_rate": 4.253139458030404e-06,
      "loss": 0.3709,
      "step": 904
    },
    {
      "epoch": 0.5981493721083939,
      "grad_norm": 0.8434338569641113,
      "learning_rate": 4.2523132848645075e-06,
      "loss": 0.4112,
      "step": 905
    },
    {
      "epoch": 0.5988103106411103,
      "grad_norm": 0.8252372145652771,
      "learning_rate": 4.251487111698613e-06,
      "loss": 0.3684,
      "step": 906
    },
    {
      "epoch": 0.5994712491738269,
      "grad_norm": 0.8066956400871277,
      "learning_rate": 4.250660938532717e-06,
      "loss": 0.3455,
      "step": 907
    },
    {
      "epoch": 0.6001321877065433,
      "grad_norm": 0.7911277413368225,
      "learning_rate": 4.249834765366821e-06,
      "loss": 0.3798,
      "step": 908
    },
    {
      "epoch": 0.6007931262392597,
      "grad_norm": 3.294790267944336,
      "learning_rate": 4.249008592200926e-06,
      "loss": 0.4677,
      "step": 909
    },
    {
      "epoch": 0.6014540647719762,
      "grad_norm": 0.7450324892997742,
      "learning_rate": 4.24818241903503e-06,
      "loss": 0.3855,
      "step": 910
    },
    {
      "epoch": 0.6021150033046927,
      "grad_norm": 0.7565584778785706,
      "learning_rate": 4.247356245869134e-06,
      "loss": 0.3758,
      "step": 911
    },
    {
      "epoch": 0.6027759418374091,
      "grad_norm": 0.7709395289421082,
      "learning_rate": 4.246530072703239e-06,
      "loss": 0.3839,
      "step": 912
    },
    {
      "epoch": 0.6034368803701256,
      "grad_norm": 0.8137488961219788,
      "learning_rate": 4.245703899537343e-06,
      "loss": 0.3785,
      "step": 913
    },
    {
      "epoch": 0.6040978189028421,
      "grad_norm": 1.6916083097457886,
      "learning_rate": 4.244877726371448e-06,
      "loss": 0.4109,
      "step": 914
    },
    {
      "epoch": 0.6047587574355585,
      "grad_norm": 0.7493830919265747,
      "learning_rate": 4.244051553205552e-06,
      "loss": 0.3475,
      "step": 915
    },
    {
      "epoch": 0.6054196959682749,
      "grad_norm": 0.7795392274856567,
      "learning_rate": 4.243225380039657e-06,
      "loss": 0.3677,
      "step": 916
    },
    {
      "epoch": 0.6060806345009914,
      "grad_norm": 0.8038662672042847,
      "learning_rate": 4.242399206873761e-06,
      "loss": 0.3754,
      "step": 917
    },
    {
      "epoch": 0.6067415730337079,
      "grad_norm": 0.7479028105735779,
      "learning_rate": 4.2415730337078655e-06,
      "loss": 0.3631,
      "step": 918
    },
    {
      "epoch": 0.6074025115664243,
      "grad_norm": 0.7731584310531616,
      "learning_rate": 4.24074686054197e-06,
      "loss": 0.3713,
      "step": 919
    },
    {
      "epoch": 0.6080634500991408,
      "grad_norm": 0.8010244369506836,
      "learning_rate": 4.239920687376074e-06,
      "loss": 0.4144,
      "step": 920
    },
    {
      "epoch": 0.6087243886318572,
      "grad_norm": 0.7939297556877136,
      "learning_rate": 4.2390945142101785e-06,
      "loss": 0.3753,
      "step": 921
    },
    {
      "epoch": 0.6093853271645737,
      "grad_norm": 0.7345120906829834,
      "learning_rate": 4.238268341044283e-06,
      "loss": 0.3443,
      "step": 922
    },
    {
      "epoch": 0.6100462656972901,
      "grad_norm": 0.7627056837081909,
      "learning_rate": 4.237442167878388e-06,
      "loss": 0.3608,
      "step": 923
    },
    {
      "epoch": 0.6107072042300066,
      "grad_norm": 0.7458444237709045,
      "learning_rate": 4.2366159947124915e-06,
      "loss": 0.4014,
      "step": 924
    },
    {
      "epoch": 0.6113681427627231,
      "grad_norm": 0.7980412244796753,
      "learning_rate": 4.235789821546597e-06,
      "loss": 0.3624,
      "step": 925
    },
    {
      "epoch": 0.6120290812954395,
      "grad_norm": 0.7972511649131775,
      "learning_rate": 4.234963648380701e-06,
      "loss": 0.3362,
      "step": 926
    },
    {
      "epoch": 0.612690019828156,
      "grad_norm": 0.7805209755897522,
      "learning_rate": 4.234137475214805e-06,
      "loss": 0.3876,
      "step": 927
    },
    {
      "epoch": 0.6133509583608724,
      "grad_norm": 0.7871447801589966,
      "learning_rate": 4.23331130204891e-06,
      "loss": 0.3445,
      "step": 928
    },
    {
      "epoch": 0.6140118968935889,
      "grad_norm": 0.7749810218811035,
      "learning_rate": 4.232485128883014e-06,
      "loss": 0.3929,
      "step": 929
    },
    {
      "epoch": 0.6146728354263054,
      "grad_norm": 0.7741055488586426,
      "learning_rate": 4.231658955717118e-06,
      "loss": 0.3282,
      "step": 930
    },
    {
      "epoch": 0.6153337739590218,
      "grad_norm": 0.7858315706253052,
      "learning_rate": 4.230832782551223e-06,
      "loss": 0.3693,
      "step": 931
    },
    {
      "epoch": 0.6159947124917383,
      "grad_norm": 0.748212456703186,
      "learning_rate": 4.230006609385328e-06,
      "loss": 0.372,
      "step": 932
    },
    {
      "epoch": 0.6166556510244547,
      "grad_norm": 0.7662058472633362,
      "learning_rate": 4.229180436219432e-06,
      "loss": 0.3397,
      "step": 933
    },
    {
      "epoch": 0.6173165895571712,
      "grad_norm": 0.7605568170547485,
      "learning_rate": 4.2283542630535365e-06,
      "loss": 0.3806,
      "step": 934
    },
    {
      "epoch": 0.6179775280898876,
      "grad_norm": 0.7444822192192078,
      "learning_rate": 4.227528089887641e-06,
      "loss": 0.3633,
      "step": 935
    },
    {
      "epoch": 0.6186384666226041,
      "grad_norm": 0.7674415111541748,
      "learning_rate": 4.226701916721745e-06,
      "loss": 0.3507,
      "step": 936
    },
    {
      "epoch": 0.6192994051553206,
      "grad_norm": 0.7823972105979919,
      "learning_rate": 4.2258757435558495e-06,
      "loss": 0.3359,
      "step": 937
    },
    {
      "epoch": 0.619960343688037,
      "grad_norm": 0.6966978907585144,
      "learning_rate": 4.225049570389954e-06,
      "loss": 0.3188,
      "step": 938
    },
    {
      "epoch": 0.6206212822207535,
      "grad_norm": 0.7399783730506897,
      "learning_rate": 4.224223397224059e-06,
      "loss": 0.3397,
      "step": 939
    },
    {
      "epoch": 0.62128222075347,
      "grad_norm": 0.8028866052627563,
      "learning_rate": 4.2233972240581625e-06,
      "loss": 0.3428,
      "step": 940
    },
    {
      "epoch": 0.6219431592861864,
      "grad_norm": 0.7978582382202148,
      "learning_rate": 4.222571050892268e-06,
      "loss": 0.3606,
      "step": 941
    },
    {
      "epoch": 0.6226040978189028,
      "grad_norm": 1.3093215227127075,
      "learning_rate": 4.221744877726372e-06,
      "loss": 0.377,
      "step": 942
    },
    {
      "epoch": 0.6232650363516193,
      "grad_norm": 0.8086025714874268,
      "learning_rate": 4.220918704560476e-06,
      "loss": 0.3645,
      "step": 943
    },
    {
      "epoch": 0.6239259748843358,
      "grad_norm": 0.9031201004981995,
      "learning_rate": 4.220092531394581e-06,
      "loss": 0.3478,
      "step": 944
    },
    {
      "epoch": 0.6245869134170522,
      "grad_norm": 0.7778975963592529,
      "learning_rate": 4.219266358228685e-06,
      "loss": 0.3644,
      "step": 945
    },
    {
      "epoch": 0.6252478519497686,
      "grad_norm": 0.7560508847236633,
      "learning_rate": 4.218440185062789e-06,
      "loss": 0.3319,
      "step": 946
    },
    {
      "epoch": 0.6259087904824852,
      "grad_norm": 0.7894421219825745,
      "learning_rate": 4.217614011896894e-06,
      "loss": 0.3354,
      "step": 947
    },
    {
      "epoch": 0.6265697290152016,
      "grad_norm": 0.7271245718002319,
      "learning_rate": 4.216787838730999e-06,
      "loss": 0.3621,
      "step": 948
    },
    {
      "epoch": 0.627230667547918,
      "grad_norm": 0.7746531367301941,
      "learning_rate": 4.215961665565102e-06,
      "loss": 0.3567,
      "step": 949
    },
    {
      "epoch": 0.6278916060806345,
      "grad_norm": 0.7146746516227722,
      "learning_rate": 4.2151354923992076e-06,
      "loss": 0.3523,
      "step": 950
    },
    {
      "epoch": 0.628552544613351,
      "grad_norm": 0.708782434463501,
      "learning_rate": 4.214309319233312e-06,
      "loss": 0.3723,
      "step": 951
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.7014385461807251,
      "learning_rate": 4.213483146067416e-06,
      "loss": 0.3473,
      "step": 952
    },
    {
      "epoch": 0.6298744216787838,
      "grad_norm": 0.7554508447647095,
      "learning_rate": 4.2126569729015206e-06,
      "loss": 0.3466,
      "step": 953
    },
    {
      "epoch": 0.6305353602115004,
      "grad_norm": 0.7403007745742798,
      "learning_rate": 4.211830799735625e-06,
      "loss": 0.3722,
      "step": 954
    },
    {
      "epoch": 0.6311962987442168,
      "grad_norm": 0.7919089794158936,
      "learning_rate": 4.211004626569729e-06,
      "loss": 0.3616,
      "step": 955
    },
    {
      "epoch": 0.6318572372769332,
      "grad_norm": 0.7609822750091553,
      "learning_rate": 4.2101784534038336e-06,
      "loss": 0.3627,
      "step": 956
    },
    {
      "epoch": 0.6325181758096498,
      "grad_norm": 0.8489881157875061,
      "learning_rate": 4.209352280237938e-06,
      "loss": 0.3623,
      "step": 957
    },
    {
      "epoch": 0.6331791143423662,
      "grad_norm": 0.711240828037262,
      "learning_rate": 4.208526107072043e-06,
      "loss": 0.3561,
      "step": 958
    },
    {
      "epoch": 0.6338400528750826,
      "grad_norm": 0.6993433833122253,
      "learning_rate": 4.2076999339061466e-06,
      "loss": 0.3209,
      "step": 959
    },
    {
      "epoch": 0.634500991407799,
      "grad_norm": 0.7520989775657654,
      "learning_rate": 4.206873760740252e-06,
      "loss": 0.3494,
      "step": 960
    },
    {
      "epoch": 0.6351619299405156,
      "grad_norm": 0.8257452249526978,
      "learning_rate": 4.206047587574356e-06,
      "loss": 0.3518,
      "step": 961
    },
    {
      "epoch": 0.635822868473232,
      "grad_norm": 0.7504831552505493,
      "learning_rate": 4.20522141440846e-06,
      "loss": 0.3467,
      "step": 962
    },
    {
      "epoch": 0.6364838070059484,
      "grad_norm": 0.7904658913612366,
      "learning_rate": 4.204395241242565e-06,
      "loss": 0.3542,
      "step": 963
    },
    {
      "epoch": 0.637144745538665,
      "grad_norm": 0.7545216679573059,
      "learning_rate": 4.203569068076669e-06,
      "loss": 0.3378,
      "step": 964
    },
    {
      "epoch": 0.6378056840713814,
      "grad_norm": 0.6961724758148193,
      "learning_rate": 4.202742894910773e-06,
      "loss": 0.3325,
      "step": 965
    },
    {
      "epoch": 0.6384666226040978,
      "grad_norm": 0.6874046921730042,
      "learning_rate": 4.201916721744878e-06,
      "loss": 0.314,
      "step": 966
    },
    {
      "epoch": 0.6391275611368142,
      "grad_norm": 0.713027834892273,
      "learning_rate": 4.201090548578983e-06,
      "loss": 0.3595,
      "step": 967
    },
    {
      "epoch": 0.6397884996695308,
      "grad_norm": 0.708091139793396,
      "learning_rate": 4.200264375413086e-06,
      "loss": 0.3453,
      "step": 968
    },
    {
      "epoch": 0.6404494382022472,
      "grad_norm": 0.7419936656951904,
      "learning_rate": 4.199438202247192e-06,
      "loss": 0.3247,
      "step": 969
    },
    {
      "epoch": 0.6411103767349636,
      "grad_norm": 0.7264487743377686,
      "learning_rate": 4.198612029081296e-06,
      "loss": 0.3311,
      "step": 970
    },
    {
      "epoch": 0.64177131526768,
      "grad_norm": 0.6766046285629272,
      "learning_rate": 4.1977858559154e-06,
      "loss": 0.3497,
      "step": 971
    },
    {
      "epoch": 0.6424322538003966,
      "grad_norm": 0.7107743620872498,
      "learning_rate": 4.196959682749505e-06,
      "loss": 0.3438,
      "step": 972
    },
    {
      "epoch": 0.643093192333113,
      "grad_norm": 0.7216804027557373,
      "learning_rate": 4.196133509583609e-06,
      "loss": 0.3328,
      "step": 973
    },
    {
      "epoch": 0.6437541308658294,
      "grad_norm": 1.082044243812561,
      "learning_rate": 4.195307336417713e-06,
      "loss": 0.361,
      "step": 974
    },
    {
      "epoch": 0.644415069398546,
      "grad_norm": 0.7689810991287231,
      "learning_rate": 4.194481163251818e-06,
      "loss": 0.3311,
      "step": 975
    },
    {
      "epoch": 0.6450760079312624,
      "grad_norm": 0.6868047118186951,
      "learning_rate": 4.193654990085923e-06,
      "loss": 0.3276,
      "step": 976
    },
    {
      "epoch": 0.6457369464639788,
      "grad_norm": 0.7412847280502319,
      "learning_rate": 4.192828816920026e-06,
      "loss": 0.3062,
      "step": 977
    },
    {
      "epoch": 0.6463978849966953,
      "grad_norm": 0.7939605712890625,
      "learning_rate": 4.1920026437541314e-06,
      "loss": 0.3129,
      "step": 978
    },
    {
      "epoch": 0.6470588235294118,
      "grad_norm": 0.7489936947822571,
      "learning_rate": 4.191176470588236e-06,
      "loss": 0.3478,
      "step": 979
    },
    {
      "epoch": 0.6477197620621282,
      "grad_norm": 0.7250478267669678,
      "learning_rate": 4.19035029742234e-06,
      "loss": 0.3503,
      "step": 980
    },
    {
      "epoch": 0.6483807005948447,
      "grad_norm": 0.7310577034950256,
      "learning_rate": 4.1895241242564444e-06,
      "loss": 0.3161,
      "step": 981
    },
    {
      "epoch": 0.6490416391275612,
      "grad_norm": 0.6519314646720886,
      "learning_rate": 4.188697951090549e-06,
      "loss": 0.3122,
      "step": 982
    },
    {
      "epoch": 0.6497025776602776,
      "grad_norm": 0.7610346674919128,
      "learning_rate": 4.187871777924654e-06,
      "loss": 0.3339,
      "step": 983
    },
    {
      "epoch": 0.650363516192994,
      "grad_norm": 0.7742217183113098,
      "learning_rate": 4.1870456047587574e-06,
      "loss": 0.3286,
      "step": 984
    },
    {
      "epoch": 0.6510244547257105,
      "grad_norm": 0.6963500380516052,
      "learning_rate": 4.186219431592863e-06,
      "loss": 0.335,
      "step": 985
    },
    {
      "epoch": 0.651685393258427,
      "grad_norm": 0.7740925550460815,
      "learning_rate": 4.185393258426967e-06,
      "loss": 0.3468,
      "step": 986
    },
    {
      "epoch": 0.6523463317911434,
      "grad_norm": 0.7525449991226196,
      "learning_rate": 4.184567085261071e-06,
      "loss": 0.3244,
      "step": 987
    },
    {
      "epoch": 0.6530072703238599,
      "grad_norm": 0.7268984913825989,
      "learning_rate": 4.183740912095176e-06,
      "loss": 0.3341,
      "step": 988
    },
    {
      "epoch": 0.6536682088565764,
      "grad_norm": 0.7455431818962097,
      "learning_rate": 4.18291473892928e-06,
      "loss": 0.3273,
      "step": 989
    },
    {
      "epoch": 0.6543291473892928,
      "grad_norm": 0.7277068495750427,
      "learning_rate": 4.182088565763384e-06,
      "loss": 0.3315,
      "step": 990
    },
    {
      "epoch": 0.6549900859220092,
      "grad_norm": 0.6963084936141968,
      "learning_rate": 4.181262392597489e-06,
      "loss": 0.3379,
      "step": 991
    },
    {
      "epoch": 0.6556510244547257,
      "grad_norm": 0.6945335268974304,
      "learning_rate": 4.180436219431594e-06,
      "loss": 0.329,
      "step": 992
    },
    {
      "epoch": 0.6563119629874422,
      "grad_norm": 0.7187401652336121,
      "learning_rate": 4.179610046265697e-06,
      "loss": 0.2969,
      "step": 993
    },
    {
      "epoch": 0.6569729015201586,
      "grad_norm": 0.7350297570228577,
      "learning_rate": 4.1787838730998025e-06,
      "loss": 0.3523,
      "step": 994
    },
    {
      "epoch": 0.6576338400528751,
      "grad_norm": 0.6784144043922424,
      "learning_rate": 4.177957699933907e-06,
      "loss": 0.298,
      "step": 995
    },
    {
      "epoch": 0.6582947785855915,
      "grad_norm": 0.7114740610122681,
      "learning_rate": 4.17713152676801e-06,
      "loss": 0.336,
      "step": 996
    },
    {
      "epoch": 0.658955717118308,
      "grad_norm": 0.6988235712051392,
      "learning_rate": 4.1763053536021155e-06,
      "loss": 0.3264,
      "step": 997
    },
    {
      "epoch": 0.6596166556510245,
      "grad_norm": 0.7419730424880981,
      "learning_rate": 4.17547918043622e-06,
      "loss": 0.3481,
      "step": 998
    },
    {
      "epoch": 0.6602775941837409,
      "grad_norm": 0.7673406004905701,
      "learning_rate": 4.174653007270324e-06,
      "loss": 0.3306,
      "step": 999
    },
    {
      "epoch": 0.6609385327164574,
      "grad_norm": 0.7392644882202148,
      "learning_rate": 4.1738268341044285e-06,
      "loss": 0.3529,
      "step": 1000
    },
    {
      "epoch": 0.6615994712491738,
      "grad_norm": 0.6985164284706116,
      "learning_rate": 4.173000660938533e-06,
      "loss": 0.3401,
      "step": 1001
    },
    {
      "epoch": 0.6622604097818903,
      "grad_norm": 0.7307760715484619,
      "learning_rate": 4.172174487772637e-06,
      "loss": 0.3111,
      "step": 1002
    },
    {
      "epoch": 0.6629213483146067,
      "grad_norm": 0.7642197012901306,
      "learning_rate": 4.1713483146067415e-06,
      "loss": 0.3386,
      "step": 1003
    },
    {
      "epoch": 0.6635822868473232,
      "grad_norm": 0.6866815686225891,
      "learning_rate": 4.170522141440847e-06,
      "loss": 0.3231,
      "step": 1004
    },
    {
      "epoch": 0.6642432253800397,
      "grad_norm": 0.68461012840271,
      "learning_rate": 4.169695968274951e-06,
      "loss": 0.3228,
      "step": 1005
    },
    {
      "epoch": 0.6649041639127561,
      "grad_norm": 0.755614697933197,
      "learning_rate": 4.168869795109055e-06,
      "loss": 0.3024,
      "step": 1006
    },
    {
      "epoch": 0.6655651024454726,
      "grad_norm": 0.7136640548706055,
      "learning_rate": 4.16804362194316e-06,
      "loss": 0.2947,
      "step": 1007
    },
    {
      "epoch": 0.666226040978189,
      "grad_norm": 0.6782714128494263,
      "learning_rate": 4.167217448777264e-06,
      "loss": 0.3165,
      "step": 1008
    },
    {
      "epoch": 0.6668869795109055,
      "grad_norm": 0.714317262172699,
      "learning_rate": 4.166391275611368e-06,
      "loss": 0.3377,
      "step": 1009
    },
    {
      "epoch": 0.6675479180436219,
      "grad_norm": 0.6500983238220215,
      "learning_rate": 4.165565102445473e-06,
      "loss": 0.3321,
      "step": 1010
    },
    {
      "epoch": 0.6682088565763384,
      "grad_norm": 0.7436153292655945,
      "learning_rate": 4.164738929279578e-06,
      "loss": 0.3146,
      "step": 1011
    },
    {
      "epoch": 0.6688697951090549,
      "grad_norm": 0.716801106929779,
      "learning_rate": 4.163912756113681e-06,
      "loss": 0.3111,
      "step": 1012
    },
    {
      "epoch": 0.6695307336417713,
      "grad_norm": 0.6735310554504395,
      "learning_rate": 4.1630865829477865e-06,
      "loss": 0.3294,
      "step": 1013
    },
    {
      "epoch": 0.6701916721744877,
      "grad_norm": 0.7082980275154114,
      "learning_rate": 4.162260409781891e-06,
      "loss": 0.3723,
      "step": 1014
    },
    {
      "epoch": 0.6708526107072043,
      "grad_norm": 0.6307771801948547,
      "learning_rate": 4.161434236615995e-06,
      "loss": 0.2935,
      "step": 1015
    },
    {
      "epoch": 0.6715135492399207,
      "grad_norm": 0.6810401082038879,
      "learning_rate": 4.1606080634500995e-06,
      "loss": 0.314,
      "step": 1016
    },
    {
      "epoch": 0.6721744877726371,
      "grad_norm": 0.7271692156791687,
      "learning_rate": 4.159781890284204e-06,
      "loss": 0.313,
      "step": 1017
    },
    {
      "epoch": 0.6728354263053536,
      "grad_norm": 0.6840254664421082,
      "learning_rate": 4.158955717118308e-06,
      "loss": 0.3099,
      "step": 1018
    },
    {
      "epoch": 0.6734963648380701,
      "grad_norm": 0.6038527488708496,
      "learning_rate": 4.1581295439524125e-06,
      "loss": 0.3069,
      "step": 1019
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.7281535863876343,
      "learning_rate": 4.157303370786518e-06,
      "loss": 0.2973,
      "step": 1020
    },
    {
      "epoch": 0.6748182419035029,
      "grad_norm": 0.7796501517295837,
      "learning_rate": 4.156477197620621e-06,
      "loss": 0.3205,
      "step": 1021
    },
    {
      "epoch": 0.6754791804362195,
      "grad_norm": 0.6869689226150513,
      "learning_rate": 4.155651024454726e-06,
      "loss": 0.3181,
      "step": 1022
    },
    {
      "epoch": 0.6761401189689359,
      "grad_norm": 0.6553439497947693,
      "learning_rate": 4.154824851288831e-06,
      "loss": 0.3098,
      "step": 1023
    },
    {
      "epoch": 0.6768010575016523,
      "grad_norm": 0.7098472714424133,
      "learning_rate": 4.153998678122935e-06,
      "loss": 0.3094,
      "step": 1024
    },
    {
      "epoch": 0.6774619960343689,
      "grad_norm": 0.6731439232826233,
      "learning_rate": 4.153172504957039e-06,
      "loss": 0.3722,
      "step": 1025
    },
    {
      "epoch": 0.6781229345670853,
      "grad_norm": 0.7014856338500977,
      "learning_rate": 4.152346331791144e-06,
      "loss": 0.3414,
      "step": 1026
    },
    {
      "epoch": 0.6787838730998017,
      "grad_norm": 0.7012448310852051,
      "learning_rate": 4.151520158625248e-06,
      "loss": 0.3131,
      "step": 1027
    },
    {
      "epoch": 0.6794448116325181,
      "grad_norm": 0.6941016912460327,
      "learning_rate": 4.150693985459352e-06,
      "loss": 0.315,
      "step": 1028
    },
    {
      "epoch": 0.6801057501652347,
      "grad_norm": 0.7011038064956665,
      "learning_rate": 4.1498678122934575e-06,
      "loss": 0.318,
      "step": 1029
    },
    {
      "epoch": 0.6807666886979511,
      "grad_norm": 0.6677269339561462,
      "learning_rate": 4.149041639127562e-06,
      "loss": 0.2736,
      "step": 1030
    },
    {
      "epoch": 0.6814276272306675,
      "grad_norm": 0.6949245929718018,
      "learning_rate": 4.148215465961666e-06,
      "loss": 0.319,
      "step": 1031
    },
    {
      "epoch": 0.6820885657633841,
      "grad_norm": 0.6785677671432495,
      "learning_rate": 4.1473892927957705e-06,
      "loss": 0.314,
      "step": 1032
    },
    {
      "epoch": 0.6827495042961005,
      "grad_norm": 0.6936357617378235,
      "learning_rate": 4.146563119629875e-06,
      "loss": 0.2995,
      "step": 1033
    },
    {
      "epoch": 0.6834104428288169,
      "grad_norm": 0.6548665761947632,
      "learning_rate": 4.145736946463979e-06,
      "loss": 0.2644,
      "step": 1034
    },
    {
      "epoch": 0.6840713813615333,
      "grad_norm": 0.6726876497268677,
      "learning_rate": 4.1449107732980835e-06,
      "loss": 0.3284,
      "step": 1035
    },
    {
      "epoch": 0.6847323198942499,
      "grad_norm": 0.718692421913147,
      "learning_rate": 4.144084600132189e-06,
      "loss": 0.2826,
      "step": 1036
    },
    {
      "epoch": 0.6853932584269663,
      "grad_norm": 0.7133889198303223,
      "learning_rate": 4.143258426966292e-06,
      "loss": 0.3227,
      "step": 1037
    },
    {
      "epoch": 0.6860541969596827,
      "grad_norm": 0.7022390365600586,
      "learning_rate": 4.1424322538003965e-06,
      "loss": 0.3054,
      "step": 1038
    },
    {
      "epoch": 0.6867151354923992,
      "grad_norm": 0.663865864276886,
      "learning_rate": 4.141606080634502e-06,
      "loss": 0.2466,
      "step": 1039
    },
    {
      "epoch": 0.6873760740251157,
      "grad_norm": 0.6866430044174194,
      "learning_rate": 4.140779907468605e-06,
      "loss": 0.3094,
      "step": 1040
    },
    {
      "epoch": 0.6880370125578321,
      "grad_norm": 0.6697847843170166,
      "learning_rate": 4.13995373430271e-06,
      "loss": 0.3014,
      "step": 1041
    },
    {
      "epoch": 0.6886979510905485,
      "grad_norm": 0.6638756990432739,
      "learning_rate": 4.139127561136815e-06,
      "loss": 0.3071,
      "step": 1042
    },
    {
      "epoch": 0.6893588896232651,
      "grad_norm": 0.6981351971626282,
      "learning_rate": 4.138301387970919e-06,
      "loss": 0.2896,
      "step": 1043
    },
    {
      "epoch": 0.6900198281559815,
      "grad_norm": 0.7492567300796509,
      "learning_rate": 4.137475214805023e-06,
      "loss": 0.2892,
      "step": 1044
    },
    {
      "epoch": 0.6906807666886979,
      "grad_norm": 0.7107019424438477,
      "learning_rate": 4.136649041639128e-06,
      "loss": 0.3147,
      "step": 1045
    },
    {
      "epoch": 0.6913417052214144,
      "grad_norm": 0.6584558486938477,
      "learning_rate": 4.135822868473232e-06,
      "loss": 0.2805,
      "step": 1046
    },
    {
      "epoch": 0.6920026437541309,
      "grad_norm": 0.6367576718330383,
      "learning_rate": 4.134996695307336e-06,
      "loss": 0.2859,
      "step": 1047
    },
    {
      "epoch": 0.6926635822868473,
      "grad_norm": 0.6811100840568542,
      "learning_rate": 4.1341705221414415e-06,
      "loss": 0.3056,
      "step": 1048
    },
    {
      "epoch": 0.6933245208195637,
      "grad_norm": 0.5975125432014465,
      "learning_rate": 4.133344348975545e-06,
      "loss": 0.2819,
      "step": 1049
    },
    {
      "epoch": 0.6939854593522803,
      "grad_norm": 0.7008915543556213,
      "learning_rate": 4.13251817580965e-06,
      "loss": 0.331,
      "step": 1050
    },
    {
      "epoch": 0.6946463978849967,
      "grad_norm": 0.6137401461601257,
      "learning_rate": 4.1316920026437545e-06,
      "loss": 0.2962,
      "step": 1051
    },
    {
      "epoch": 0.6953073364177131,
      "grad_norm": 0.5809264779090881,
      "learning_rate": 4.130865829477859e-06,
      "loss": 0.3088,
      "step": 1052
    },
    {
      "epoch": 0.6959682749504296,
      "grad_norm": 0.6355889439582825,
      "learning_rate": 4.130039656311963e-06,
      "loss": 0.2933,
      "step": 1053
    },
    {
      "epoch": 0.6966292134831461,
      "grad_norm": 0.6754727363586426,
      "learning_rate": 4.1292134831460675e-06,
      "loss": 0.3064,
      "step": 1054
    },
    {
      "epoch": 0.6972901520158625,
      "grad_norm": 0.6931716799736023,
      "learning_rate": 4.128387309980173e-06,
      "loss": 0.2873,
      "step": 1055
    },
    {
      "epoch": 0.697951090548579,
      "grad_norm": 0.6530923843383789,
      "learning_rate": 4.127561136814276e-06,
      "loss": 0.3263,
      "step": 1056
    },
    {
      "epoch": 0.6986120290812955,
      "grad_norm": 0.7182236313819885,
      "learning_rate": 4.126734963648381e-06,
      "loss": 0.2655,
      "step": 1057
    },
    {
      "epoch": 0.6992729676140119,
      "grad_norm": 0.6417848467826843,
      "learning_rate": 4.125908790482486e-06,
      "loss": 0.2895,
      "step": 1058
    },
    {
      "epoch": 0.6999339061467283,
      "grad_norm": 0.751213550567627,
      "learning_rate": 4.12508261731659e-06,
      "loss": 0.3068,
      "step": 1059
    },
    {
      "epoch": 0.7005948446794448,
      "grad_norm": 0.6993134021759033,
      "learning_rate": 4.124256444150694e-06,
      "loss": 0.327,
      "step": 1060
    },
    {
      "epoch": 0.7012557832121613,
      "grad_norm": 0.6472730040550232,
      "learning_rate": 4.123430270984799e-06,
      "loss": 0.3118,
      "step": 1061
    },
    {
      "epoch": 0.7019167217448777,
      "grad_norm": 0.5659637451171875,
      "learning_rate": 4.122604097818903e-06,
      "loss": 0.2957,
      "step": 1062
    },
    {
      "epoch": 0.7025776602775942,
      "grad_norm": 0.738116979598999,
      "learning_rate": 4.121777924653007e-06,
      "loss": 0.2835,
      "step": 1063
    },
    {
      "epoch": 0.7032385988103106,
      "grad_norm": 0.6721702218055725,
      "learning_rate": 4.1209517514871126e-06,
      "loss": 0.2828,
      "step": 1064
    },
    {
      "epoch": 0.7038995373430271,
      "grad_norm": 0.6634120345115662,
      "learning_rate": 4.120125578321216e-06,
      "loss": 0.3318,
      "step": 1065
    },
    {
      "epoch": 0.7045604758757436,
      "grad_norm": 0.6680054664611816,
      "learning_rate": 4.119299405155321e-06,
      "loss": 0.2976,
      "step": 1066
    },
    {
      "epoch": 0.70522141440846,
      "grad_norm": 0.6942099332809448,
      "learning_rate": 4.1184732319894256e-06,
      "loss": 0.3178,
      "step": 1067
    },
    {
      "epoch": 0.7058823529411765,
      "grad_norm": 0.7023160457611084,
      "learning_rate": 4.11764705882353e-06,
      "loss": 0.3462,
      "step": 1068
    },
    {
      "epoch": 0.7065432914738929,
      "grad_norm": 0.6551231145858765,
      "learning_rate": 4.116820885657634e-06,
      "loss": 0.3367,
      "step": 1069
    },
    {
      "epoch": 0.7072042300066094,
      "grad_norm": 0.6799728274345398,
      "learning_rate": 4.1159947124917386e-06,
      "loss": 0.2887,
      "step": 1070
    },
    {
      "epoch": 0.7078651685393258,
      "grad_norm": 0.6696480512619019,
      "learning_rate": 4.115168539325843e-06,
      "loss": 0.2753,
      "step": 1071
    },
    {
      "epoch": 0.7085261070720423,
      "grad_norm": 0.7013856768608093,
      "learning_rate": 4.114342366159947e-06,
      "loss": 0.2745,
      "step": 1072
    },
    {
      "epoch": 0.7091870456047588,
      "grad_norm": 0.6212797164916992,
      "learning_rate": 4.113516192994052e-06,
      "loss": 0.2899,
      "step": 1073
    },
    {
      "epoch": 0.7098479841374752,
      "grad_norm": 0.6688599586486816,
      "learning_rate": 4.112690019828156e-06,
      "loss": 0.2883,
      "step": 1074
    },
    {
      "epoch": 0.7105089226701917,
      "grad_norm": 0.7144126892089844,
      "learning_rate": 4.111863846662261e-06,
      "loss": 0.2929,
      "step": 1075
    },
    {
      "epoch": 0.7111698612029081,
      "grad_norm": 0.7547051310539246,
      "learning_rate": 4.111037673496365e-06,
      "loss": 0.3218,
      "step": 1076
    },
    {
      "epoch": 0.7118307997356246,
      "grad_norm": 0.6908218860626221,
      "learning_rate": 4.11021150033047e-06,
      "loss": 0.3267,
      "step": 1077
    },
    {
      "epoch": 0.712491738268341,
      "grad_norm": 0.6435956358909607,
      "learning_rate": 4.109385327164574e-06,
      "loss": 0.2791,
      "step": 1078
    },
    {
      "epoch": 0.7131526768010575,
      "grad_norm": 0.6679517030715942,
      "learning_rate": 4.108559153998678e-06,
      "loss": 0.266,
      "step": 1079
    },
    {
      "epoch": 0.713813615333774,
      "grad_norm": 0.6713435649871826,
      "learning_rate": 4.107732980832783e-06,
      "loss": 0.3158,
      "step": 1080
    },
    {
      "epoch": 0.7144745538664904,
      "grad_norm": 0.6543514132499695,
      "learning_rate": 4.106906807666887e-06,
      "loss": 0.2759,
      "step": 1081
    },
    {
      "epoch": 0.7151354923992069,
      "grad_norm": 0.6847873330116272,
      "learning_rate": 4.106080634500991e-06,
      "loss": 0.3121,
      "step": 1082
    },
    {
      "epoch": 0.7157964309319234,
      "grad_norm": 0.6571863293647766,
      "learning_rate": 4.105254461335097e-06,
      "loss": 0.306,
      "step": 1083
    },
    {
      "epoch": 0.7164573694646398,
      "grad_norm": 0.6451383233070374,
      "learning_rate": 4.1044282881692e-06,
      "loss": 0.2645,
      "step": 1084
    },
    {
      "epoch": 0.7171183079973562,
      "grad_norm": 0.6498525142669678,
      "learning_rate": 4.103602115003305e-06,
      "loss": 0.2737,
      "step": 1085
    },
    {
      "epoch": 0.7177792465300727,
      "grad_norm": 0.6865611672401428,
      "learning_rate": 4.10277594183741e-06,
      "loss": 0.3027,
      "step": 1086
    },
    {
      "epoch": 0.7184401850627892,
      "grad_norm": 0.6919524073600769,
      "learning_rate": 4.101949768671514e-06,
      "loss": 0.2991,
      "step": 1087
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.6008554100990295,
      "learning_rate": 4.101123595505618e-06,
      "loss": 0.2852,
      "step": 1088
    },
    {
      "epoch": 0.719762062128222,
      "grad_norm": 0.5823476314544678,
      "learning_rate": 4.100297422339723e-06,
      "loss": 0.2934,
      "step": 1089
    },
    {
      "epoch": 0.7204230006609386,
      "grad_norm": 0.7024294137954712,
      "learning_rate": 4.099471249173827e-06,
      "loss": 0.3159,
      "step": 1090
    },
    {
      "epoch": 0.721083939193655,
      "grad_norm": 0.6631206274032593,
      "learning_rate": 4.098645076007931e-06,
      "loss": 0.3321,
      "step": 1091
    },
    {
      "epoch": 0.7217448777263714,
      "grad_norm": 0.6869503855705261,
      "learning_rate": 4.0978189028420364e-06,
      "loss": 0.2818,
      "step": 1092
    },
    {
      "epoch": 0.722405816259088,
      "grad_norm": 0.6576237082481384,
      "learning_rate": 4.09699272967614e-06,
      "loss": 0.3004,
      "step": 1093
    },
    {
      "epoch": 0.7230667547918044,
      "grad_norm": 0.6663273572921753,
      "learning_rate": 4.096166556510245e-06,
      "loss": 0.3001,
      "step": 1094
    },
    {
      "epoch": 0.7237276933245208,
      "grad_norm": 0.6611816883087158,
      "learning_rate": 4.0953403833443494e-06,
      "loss": 0.277,
      "step": 1095
    },
    {
      "epoch": 0.7243886318572372,
      "grad_norm": 0.6906142234802246,
      "learning_rate": 4.094514210178454e-06,
      "loss": 0.2894,
      "step": 1096
    },
    {
      "epoch": 0.7250495703899538,
      "grad_norm": 0.6940711140632629,
      "learning_rate": 4.093688037012558e-06,
      "loss": 0.2842,
      "step": 1097
    },
    {
      "epoch": 0.7257105089226702,
      "grad_norm": 0.6702866554260254,
      "learning_rate": 4.0928618638466624e-06,
      "loss": 0.3059,
      "step": 1098
    },
    {
      "epoch": 0.7263714474553866,
      "grad_norm": 0.6266857981681824,
      "learning_rate": 4.092035690680767e-06,
      "loss": 0.307,
      "step": 1099
    },
    {
      "epoch": 0.7270323859881032,
      "grad_norm": 0.6958047151565552,
      "learning_rate": 4.091209517514871e-06,
      "loss": 0.3394,
      "step": 1100
    },
    {
      "epoch": 0.7276933245208196,
      "grad_norm": 0.689924955368042,
      "learning_rate": 4.090383344348976e-06,
      "loss": 0.2622,
      "step": 1101
    },
    {
      "epoch": 0.728354263053536,
      "grad_norm": 0.734453558921814,
      "learning_rate": 4.08955717118308e-06,
      "loss": 0.349,
      "step": 1102
    },
    {
      "epoch": 0.7290152015862524,
      "grad_norm": 0.6197458505630493,
      "learning_rate": 4.088730998017185e-06,
      "loss": 0.3009,
      "step": 1103
    },
    {
      "epoch": 0.729676140118969,
      "grad_norm": 0.6585438251495361,
      "learning_rate": 4.087904824851289e-06,
      "loss": 0.3489,
      "step": 1104
    },
    {
      "epoch": 0.7303370786516854,
      "grad_norm": 0.6554515361785889,
      "learning_rate": 4.087078651685394e-06,
      "loss": 0.2556,
      "step": 1105
    },
    {
      "epoch": 0.7309980171844018,
      "grad_norm": 0.6462857723236084,
      "learning_rate": 4.086252478519498e-06,
      "loss": 0.2964,
      "step": 1106
    },
    {
      "epoch": 0.7316589557171183,
      "grad_norm": 0.7260949015617371,
      "learning_rate": 4.085426305353602e-06,
      "loss": 0.3087,
      "step": 1107
    },
    {
      "epoch": 0.7323198942498348,
      "grad_norm": 0.6942178606987,
      "learning_rate": 4.0846001321877075e-06,
      "loss": 0.2887,
      "step": 1108
    },
    {
      "epoch": 0.7329808327825512,
      "grad_norm": 0.6334149837493896,
      "learning_rate": 4.083773959021811e-06,
      "loss": 0.3025,
      "step": 1109
    },
    {
      "epoch": 0.7336417713152676,
      "grad_norm": 0.649965226650238,
      "learning_rate": 4.082947785855916e-06,
      "loss": 0.2864,
      "step": 1110
    },
    {
      "epoch": 0.7343027098479842,
      "grad_norm": 0.6320056915283203,
      "learning_rate": 4.0821216126900205e-06,
      "loss": 0.2903,
      "step": 1111
    },
    {
      "epoch": 0.7349636483807006,
      "grad_norm": 0.651177167892456,
      "learning_rate": 4.081295439524125e-06,
      "loss": 0.3503,
      "step": 1112
    },
    {
      "epoch": 0.735624586913417,
      "grad_norm": 0.6975913643836975,
      "learning_rate": 4.080469266358229e-06,
      "loss": 0.282,
      "step": 1113
    },
    {
      "epoch": 0.7362855254461335,
      "grad_norm": 0.6863639950752258,
      "learning_rate": 4.0796430931923335e-06,
      "loss": 0.2852,
      "step": 1114
    },
    {
      "epoch": 0.73694646397885,
      "grad_norm": 0.5899696350097656,
      "learning_rate": 4.078816920026438e-06,
      "loss": 0.2676,
      "step": 1115
    },
    {
      "epoch": 0.7376074025115664,
      "grad_norm": 0.6491444110870361,
      "learning_rate": 4.077990746860542e-06,
      "loss": 0.2808,
      "step": 1116
    },
    {
      "epoch": 0.7382683410442828,
      "grad_norm": 0.727794885635376,
      "learning_rate": 4.077164573694647e-06,
      "loss": 0.3421,
      "step": 1117
    },
    {
      "epoch": 0.7389292795769994,
      "grad_norm": 0.6954272389411926,
      "learning_rate": 4.076338400528751e-06,
      "loss": 0.3144,
      "step": 1118
    },
    {
      "epoch": 0.7395902181097158,
      "grad_norm": 0.5763495564460754,
      "learning_rate": 4.075512227362856e-06,
      "loss": 0.2745,
      "step": 1119
    },
    {
      "epoch": 0.7402511566424322,
      "grad_norm": 0.6669211983680725,
      "learning_rate": 4.07468605419696e-06,
      "loss": 0.3358,
      "step": 1120
    },
    {
      "epoch": 0.7409120951751487,
      "grad_norm": 0.633937418460846,
      "learning_rate": 4.073859881031065e-06,
      "loss": 0.2855,
      "step": 1121
    },
    {
      "epoch": 0.7415730337078652,
      "grad_norm": 0.646672785282135,
      "learning_rate": 4.073033707865169e-06,
      "loss": 0.3031,
      "step": 1122
    },
    {
      "epoch": 0.7422339722405816,
      "grad_norm": 0.6164218187332153,
      "learning_rate": 4.072207534699273e-06,
      "loss": 0.312,
      "step": 1123
    },
    {
      "epoch": 0.7428949107732981,
      "grad_norm": 0.6886007785797119,
      "learning_rate": 4.071381361533378e-06,
      "loss": 0.3274,
      "step": 1124
    },
    {
      "epoch": 0.7435558493060146,
      "grad_norm": 0.6517882943153381,
      "learning_rate": 4.070555188367482e-06,
      "loss": 0.2873,
      "step": 1125
    },
    {
      "epoch": 0.744216787838731,
      "grad_norm": 0.6029191613197327,
      "learning_rate": 4.069729015201586e-06,
      "loss": 0.2859,
      "step": 1126
    },
    {
      "epoch": 0.7448777263714474,
      "grad_norm": 0.7188848853111267,
      "learning_rate": 4.068902842035691e-06,
      "loss": 0.3126,
      "step": 1127
    },
    {
      "epoch": 0.7455386649041639,
      "grad_norm": 0.6720013618469238,
      "learning_rate": 4.068076668869795e-06,
      "loss": 0.2787,
      "step": 1128
    },
    {
      "epoch": 0.7461996034368804,
      "grad_norm": 0.6468028426170349,
      "learning_rate": 4.0672504957039e-06,
      "loss": 0.2739,
      "step": 1129
    },
    {
      "epoch": 0.7468605419695968,
      "grad_norm": 0.667640209197998,
      "learning_rate": 4.0664243225380045e-06,
      "loss": 0.2986,
      "step": 1130
    },
    {
      "epoch": 0.7475214805023133,
      "grad_norm": 0.5986810326576233,
      "learning_rate": 4.065598149372109e-06,
      "loss": 0.2884,
      "step": 1131
    },
    {
      "epoch": 0.7481824190350297,
      "grad_norm": 0.6912170052528381,
      "learning_rate": 4.064771976206213e-06,
      "loss": 0.2987,
      "step": 1132
    },
    {
      "epoch": 0.7488433575677462,
      "grad_norm": 0.6533598899841309,
      "learning_rate": 4.0639458030403175e-06,
      "loss": 0.2994,
      "step": 1133
    },
    {
      "epoch": 0.7495042961004627,
      "grad_norm": 0.6442549228668213,
      "learning_rate": 4.063119629874422e-06,
      "loss": 0.2686,
      "step": 1134
    },
    {
      "epoch": 0.7501652346331791,
      "grad_norm": 0.6777106523513794,
      "learning_rate": 4.062293456708526e-06,
      "loss": 0.2866,
      "step": 1135
    },
    {
      "epoch": 0.7508261731658956,
      "grad_norm": 0.6813165545463562,
      "learning_rate": 4.061467283542631e-06,
      "loss": 0.308,
      "step": 1136
    },
    {
      "epoch": 0.751487111698612,
      "grad_norm": 0.589013397693634,
      "learning_rate": 4.060641110376735e-06,
      "loss": 0.3112,
      "step": 1137
    },
    {
      "epoch": 0.7521480502313285,
      "grad_norm": 0.6277503371238708,
      "learning_rate": 4.05981493721084e-06,
      "loss": 0.283,
      "step": 1138
    },
    {
      "epoch": 0.7528089887640449,
      "grad_norm": 0.6603242754936218,
      "learning_rate": 4.058988764044944e-06,
      "loss": 0.2987,
      "step": 1139
    },
    {
      "epoch": 0.7534699272967614,
      "grad_norm": 0.6903692483901978,
      "learning_rate": 4.058162590879049e-06,
      "loss": 0.2964,
      "step": 1140
    },
    {
      "epoch": 0.7541308658294779,
      "grad_norm": 0.6311887502670288,
      "learning_rate": 4.057336417713153e-06,
      "loss": 0.2925,
      "step": 1141
    },
    {
      "epoch": 0.7547918043621943,
      "grad_norm": 0.6867901682853699,
      "learning_rate": 4.056510244547257e-06,
      "loss": 0.2701,
      "step": 1142
    },
    {
      "epoch": 0.7554527428949108,
      "grad_norm": 0.6594893336296082,
      "learning_rate": 4.055684071381362e-06,
      "loss": 0.2735,
      "step": 1143
    },
    {
      "epoch": 0.7561136814276272,
      "grad_norm": 0.6231282353401184,
      "learning_rate": 4.054857898215466e-06,
      "loss": 0.2648,
      "step": 1144
    },
    {
      "epoch": 0.7567746199603437,
      "grad_norm": 0.7191299200057983,
      "learning_rate": 4.054031725049571e-06,
      "loss": 0.3235,
      "step": 1145
    },
    {
      "epoch": 0.7574355584930601,
      "grad_norm": 0.9259265065193176,
      "learning_rate": 4.053205551883675e-06,
      "loss": 0.3008,
      "step": 1146
    },
    {
      "epoch": 0.7580964970257766,
      "grad_norm": 0.6124133467674255,
      "learning_rate": 4.05237937871778e-06,
      "loss": 0.2912,
      "step": 1147
    },
    {
      "epoch": 0.7587574355584931,
      "grad_norm": 0.5982881784439087,
      "learning_rate": 4.051553205551884e-06,
      "loss": 0.2658,
      "step": 1148
    },
    {
      "epoch": 0.7594183740912095,
      "grad_norm": 0.7205435037612915,
      "learning_rate": 4.0507270323859885e-06,
      "loss": 0.2818,
      "step": 1149
    },
    {
      "epoch": 0.760079312623926,
      "grad_norm": 0.6231445074081421,
      "learning_rate": 4.049900859220093e-06,
      "loss": 0.2731,
      "step": 1150
    },
    {
      "epoch": 0.7607402511566425,
      "grad_norm": 0.6971497535705566,
      "learning_rate": 4.049074686054197e-06,
      "loss": 0.2867,
      "step": 1151
    },
    {
      "epoch": 0.7614011896893589,
      "grad_norm": 0.617091953754425,
      "learning_rate": 4.0482485128883015e-06,
      "loss": 0.2364,
      "step": 1152
    },
    {
      "epoch": 0.7620621282220753,
      "grad_norm": 0.5798447728157043,
      "learning_rate": 4.047422339722406e-06,
      "loss": 0.2669,
      "step": 1153
    },
    {
      "epoch": 0.7627230667547918,
      "grad_norm": 0.6625969409942627,
      "learning_rate": 4.046596166556511e-06,
      "loss": 0.2892,
      "step": 1154
    },
    {
      "epoch": 0.7633840052875083,
      "grad_norm": 0.6229156255722046,
      "learning_rate": 4.045769993390615e-06,
      "loss": 0.2729,
      "step": 1155
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.6334955096244812,
      "learning_rate": 4.04494382022472e-06,
      "loss": 0.2824,
      "step": 1156
    },
    {
      "epoch": 0.7647058823529411,
      "grad_norm": 0.5910221934318542,
      "learning_rate": 4.044117647058824e-06,
      "loss": 0.3026,
      "step": 1157
    },
    {
      "epoch": 0.7653668208856577,
      "grad_norm": 0.7049515843391418,
      "learning_rate": 4.043291473892928e-06,
      "loss": 0.3022,
      "step": 1158
    },
    {
      "epoch": 0.7660277594183741,
      "grad_norm": 0.6606333255767822,
      "learning_rate": 4.042465300727033e-06,
      "loss": 0.2702,
      "step": 1159
    },
    {
      "epoch": 0.7666886979510905,
      "grad_norm": 0.6777186989784241,
      "learning_rate": 4.041639127561137e-06,
      "loss": 0.2747,
      "step": 1160
    },
    {
      "epoch": 0.767349636483807,
      "grad_norm": 0.7086661458015442,
      "learning_rate": 4.040812954395242e-06,
      "loss": 0.3556,
      "step": 1161
    },
    {
      "epoch": 0.7680105750165235,
      "grad_norm": 0.6762449145317078,
      "learning_rate": 4.039986781229346e-06,
      "loss": 0.2733,
      "step": 1162
    },
    {
      "epoch": 0.7686715135492399,
      "grad_norm": 0.6209442615509033,
      "learning_rate": 4.039160608063451e-06,
      "loss": 0.2854,
      "step": 1163
    },
    {
      "epoch": 0.7693324520819563,
      "grad_norm": 0.7582787871360779,
      "learning_rate": 4.038334434897555e-06,
      "loss": 0.2841,
      "step": 1164
    },
    {
      "epoch": 0.7699933906146729,
      "grad_norm": 0.6771954298019409,
      "learning_rate": 4.037508261731659e-06,
      "loss": 0.2813,
      "step": 1165
    },
    {
      "epoch": 0.7706543291473893,
      "grad_norm": 0.6981801390647888,
      "learning_rate": 4.036682088565764e-06,
      "loss": 0.28,
      "step": 1166
    },
    {
      "epoch": 0.7713152676801057,
      "grad_norm": 0.7274665236473083,
      "learning_rate": 4.035855915399868e-06,
      "loss": 0.2763,
      "step": 1167
    },
    {
      "epoch": 0.7719762062128223,
      "grad_norm": 0.5825146436691284,
      "learning_rate": 4.0350297422339725e-06,
      "loss": 0.2763,
      "step": 1168
    },
    {
      "epoch": 0.7726371447455387,
      "grad_norm": 0.6599168181419373,
      "learning_rate": 4.034203569068077e-06,
      "loss": 0.2537,
      "step": 1169
    },
    {
      "epoch": 0.7732980832782551,
      "grad_norm": 0.6476612091064453,
      "learning_rate": 4.033377395902181e-06,
      "loss": 0.2861,
      "step": 1170
    },
    {
      "epoch": 0.7739590218109715,
      "grad_norm": 0.6492132544517517,
      "learning_rate": 4.0325512227362855e-06,
      "loss": 0.2762,
      "step": 1171
    },
    {
      "epoch": 0.7746199603436881,
      "grad_norm": 0.6226307153701782,
      "learning_rate": 4.03172504957039e-06,
      "loss": 0.2551,
      "step": 1172
    },
    {
      "epoch": 0.7752808988764045,
      "grad_norm": 0.6580615639686584,
      "learning_rate": 4.030898876404495e-06,
      "loss": 0.2757,
      "step": 1173
    },
    {
      "epoch": 0.7759418374091209,
      "grad_norm": 0.5802223086357117,
      "learning_rate": 4.0300727032385985e-06,
      "loss": 0.2492,
      "step": 1174
    },
    {
      "epoch": 0.7766027759418375,
      "grad_norm": 0.5766696333885193,
      "learning_rate": 4.029246530072704e-06,
      "loss": 0.2903,
      "step": 1175
    },
    {
      "epoch": 0.7772637144745539,
      "grad_norm": 0.6950637102127075,
      "learning_rate": 4.028420356906808e-06,
      "loss": 0.3145,
      "step": 1176
    },
    {
      "epoch": 0.7779246530072703,
      "grad_norm": 0.6388775110244751,
      "learning_rate": 4.027594183740912e-06,
      "loss": 0.2448,
      "step": 1177
    },
    {
      "epoch": 0.7785855915399867,
      "grad_norm": 0.6882670521736145,
      "learning_rate": 4.026768010575017e-06,
      "loss": 0.2839,
      "step": 1178
    },
    {
      "epoch": 0.7792465300727033,
      "grad_norm": 0.8543508052825928,
      "learning_rate": 4.025941837409121e-06,
      "loss": 0.3108,
      "step": 1179
    },
    {
      "epoch": 0.7799074686054197,
      "grad_norm": 0.639047384262085,
      "learning_rate": 4.025115664243226e-06,
      "loss": 0.2721,
      "step": 1180
    },
    {
      "epoch": 0.7805684071381361,
      "grad_norm": 0.6517179608345032,
      "learning_rate": 4.02428949107733e-06,
      "loss": 0.301,
      "step": 1181
    },
    {
      "epoch": 0.7812293456708526,
      "grad_norm": 0.6472277641296387,
      "learning_rate": 4.023463317911435e-06,
      "loss": 0.28,
      "step": 1182
    },
    {
      "epoch": 0.7818902842035691,
      "grad_norm": 0.6327829360961914,
      "learning_rate": 4.022637144745539e-06,
      "loss": 0.282,
      "step": 1183
    },
    {
      "epoch": 0.7825512227362855,
      "grad_norm": 0.5855172872543335,
      "learning_rate": 4.0218109715796436e-06,
      "loss": 0.2754,
      "step": 1184
    },
    {
      "epoch": 0.783212161269002,
      "grad_norm": 0.6049973368644714,
      "learning_rate": 4.020984798413748e-06,
      "loss": 0.3,
      "step": 1185
    },
    {
      "epoch": 0.7838730998017185,
      "grad_norm": 0.6349890828132629,
      "learning_rate": 4.020158625247852e-06,
      "loss": 0.2971,
      "step": 1186
    },
    {
      "epoch": 0.7845340383344349,
      "grad_norm": 0.6329154372215271,
      "learning_rate": 4.0193324520819566e-06,
      "loss": 0.2665,
      "step": 1187
    },
    {
      "epoch": 0.7851949768671513,
      "grad_norm": 0.6129507422447205,
      "learning_rate": 4.018506278916061e-06,
      "loss": 0.2939,
      "step": 1188
    },
    {
      "epoch": 0.7858559153998678,
      "grad_norm": 0.6264748573303223,
      "learning_rate": 4.017680105750166e-06,
      "loss": 0.2933,
      "step": 1189
    },
    {
      "epoch": 0.7865168539325843,
      "grad_norm": 0.6202442049980164,
      "learning_rate": 4.0168539325842696e-06,
      "loss": 0.2904,
      "step": 1190
    },
    {
      "epoch": 0.7871777924653007,
      "grad_norm": 0.6628229022026062,
      "learning_rate": 4.016027759418375e-06,
      "loss": 0.3177,
      "step": 1191
    },
    {
      "epoch": 0.7878387309980172,
      "grad_norm": 0.5869521498680115,
      "learning_rate": 4.015201586252479e-06,
      "loss": 0.2812,
      "step": 1192
    },
    {
      "epoch": 0.7884996695307337,
      "grad_norm": 0.6558175683021545,
      "learning_rate": 4.014375413086583e-06,
      "loss": 0.3142,
      "step": 1193
    },
    {
      "epoch": 0.7891606080634501,
      "grad_norm": 0.642217218875885,
      "learning_rate": 4.013549239920688e-06,
      "loss": 0.2704,
      "step": 1194
    },
    {
      "epoch": 0.7898215465961665,
      "grad_norm": 0.6232985854148865,
      "learning_rate": 4.012723066754792e-06,
      "loss": 0.2715,
      "step": 1195
    },
    {
      "epoch": 0.790482485128883,
      "grad_norm": 0.6370769143104553,
      "learning_rate": 4.011896893588896e-06,
      "loss": 0.2832,
      "step": 1196
    },
    {
      "epoch": 0.7911434236615995,
      "grad_norm": 0.5794206261634827,
      "learning_rate": 4.011070720423001e-06,
      "loss": 0.2763,
      "step": 1197
    },
    {
      "epoch": 0.7918043621943159,
      "grad_norm": 0.6146789193153381,
      "learning_rate": 4.010244547257106e-06,
      "loss": 0.2768,
      "step": 1198
    },
    {
      "epoch": 0.7924653007270324,
      "grad_norm": 0.6310968399047852,
      "learning_rate": 4.009418374091209e-06,
      "loss": 0.2631,
      "step": 1199
    },
    {
      "epoch": 0.7931262392597488,
      "grad_norm": 0.6681434512138367,
      "learning_rate": 4.008592200925315e-06,
      "loss": 0.3071,
      "step": 1200
    },
    {
      "epoch": 0.7937871777924653,
      "grad_norm": 0.6733741164207458,
      "learning_rate": 4.007766027759419e-06,
      "loss": 0.2852,
      "step": 1201
    },
    {
      "epoch": 0.7944481163251818,
      "grad_norm": 0.6324779391288757,
      "learning_rate": 4.006939854593523e-06,
      "loss": 0.2984,
      "step": 1202
    },
    {
      "epoch": 0.7951090548578982,
      "grad_norm": 0.648110032081604,
      "learning_rate": 4.006113681427628e-06,
      "loss": 0.2526,
      "step": 1203
    },
    {
      "epoch": 0.7957699933906147,
      "grad_norm": 0.6513838171958923,
      "learning_rate": 4.005287508261732e-06,
      "loss": 0.2762,
      "step": 1204
    },
    {
      "epoch": 0.7964309319233311,
      "grad_norm": 0.6930747032165527,
      "learning_rate": 4.004461335095837e-06,
      "loss": 0.2892,
      "step": 1205
    },
    {
      "epoch": 0.7970918704560476,
      "grad_norm": 0.5761820673942566,
      "learning_rate": 4.003635161929941e-06,
      "loss": 0.2665,
      "step": 1206
    },
    {
      "epoch": 0.797752808988764,
      "grad_norm": 0.6673570871353149,
      "learning_rate": 4.002808988764045e-06,
      "loss": 0.2804,
      "step": 1207
    },
    {
      "epoch": 0.7984137475214805,
      "grad_norm": 0.7157046794891357,
      "learning_rate": 4.00198281559815e-06,
      "loss": 0.2972,
      "step": 1208
    },
    {
      "epoch": 0.799074686054197,
      "grad_norm": 0.5550240278244019,
      "learning_rate": 4.001156642432254e-06,
      "loss": 0.2877,
      "step": 1209
    },
    {
      "epoch": 0.7997356245869134,
      "grad_norm": 0.6286203265190125,
      "learning_rate": 4.000330469266359e-06,
      "loss": 0.2655,
      "step": 1210
    },
    {
      "epoch": 0.8003965631196299,
      "grad_norm": 0.6030541062355042,
      "learning_rate": 3.999504296100463e-06,
      "loss": 0.2623,
      "step": 1211
    },
    {
      "epoch": 0.8010575016523463,
      "grad_norm": 0.6072561740875244,
      "learning_rate": 3.9986781229345674e-06,
      "loss": 0.2445,
      "step": 1212
    },
    {
      "epoch": 0.8017184401850628,
      "grad_norm": 0.5921742916107178,
      "learning_rate": 3.997851949768672e-06,
      "loss": 0.2983,
      "step": 1213
    },
    {
      "epoch": 0.8023793787177792,
      "grad_norm": 0.6385987997055054,
      "learning_rate": 3.997025776602776e-06,
      "loss": 0.2877,
      "step": 1214
    },
    {
      "epoch": 0.8030403172504957,
      "grad_norm": 0.5982218980789185,
      "learning_rate": 3.9961996034368804e-06,
      "loss": 0.2844,
      "step": 1215
    },
    {
      "epoch": 0.8037012557832122,
      "grad_norm": 0.6664754152297974,
      "learning_rate": 3.995373430270985e-06,
      "loss": 0.2572,
      "step": 1216
    },
    {
      "epoch": 0.8043621943159286,
      "grad_norm": 0.5463839769363403,
      "learning_rate": 3.99454725710509e-06,
      "loss": 0.2491,
      "step": 1217
    },
    {
      "epoch": 0.8050231328486451,
      "grad_norm": 0.6897252798080444,
      "learning_rate": 3.9937210839391934e-06,
      "loss": 0.2601,
      "step": 1218
    },
    {
      "epoch": 0.8056840713813616,
      "grad_norm": 0.5447006821632385,
      "learning_rate": 3.992894910773299e-06,
      "loss": 0.2532,
      "step": 1219
    },
    {
      "epoch": 0.806345009914078,
      "grad_norm": 0.6311291456222534,
      "learning_rate": 3.992068737607403e-06,
      "loss": 0.2699,
      "step": 1220
    },
    {
      "epoch": 0.8070059484467944,
      "grad_norm": 0.5996966361999512,
      "learning_rate": 3.991242564441507e-06,
      "loss": 0.2657,
      "step": 1221
    },
    {
      "epoch": 0.807666886979511,
      "grad_norm": 0.5665449500083923,
      "learning_rate": 3.990416391275612e-06,
      "loss": 0.2546,
      "step": 1222
    },
    {
      "epoch": 0.8083278255122274,
      "grad_norm": 0.6521081328392029,
      "learning_rate": 3.989590218109716e-06,
      "loss": 0.2907,
      "step": 1223
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.6049883365631104,
      "learning_rate": 3.98876404494382e-06,
      "loss": 0.275,
      "step": 1224
    },
    {
      "epoch": 0.8096497025776602,
      "grad_norm": 0.6251251101493835,
      "learning_rate": 3.987937871777925e-06,
      "loss": 0.2917,
      "step": 1225
    },
    {
      "epoch": 0.8103106411103768,
      "grad_norm": 0.5957170724868774,
      "learning_rate": 3.98711169861203e-06,
      "loss": 0.2793,
      "step": 1226
    },
    {
      "epoch": 0.8109715796430932,
      "grad_norm": 0.5595640540122986,
      "learning_rate": 3.986285525446133e-06,
      "loss": 0.2644,
      "step": 1227
    },
    {
      "epoch": 0.8116325181758096,
      "grad_norm": 0.6060665249824524,
      "learning_rate": 3.9854593522802385e-06,
      "loss": 0.2713,
      "step": 1228
    },
    {
      "epoch": 0.8122934567085262,
      "grad_norm": 0.6128134727478027,
      "learning_rate": 3.984633179114343e-06,
      "loss": 0.278,
      "step": 1229
    },
    {
      "epoch": 0.8129543952412426,
      "grad_norm": 0.6008544564247131,
      "learning_rate": 3.983807005948447e-06,
      "loss": 0.2471,
      "step": 1230
    },
    {
      "epoch": 0.813615333773959,
      "grad_norm": 0.6944992542266846,
      "learning_rate": 3.9829808327825515e-06,
      "loss": 0.2839,
      "step": 1231
    },
    {
      "epoch": 0.8142762723066754,
      "grad_norm": 0.6132737994194031,
      "learning_rate": 3.982154659616656e-06,
      "loss": 0.2626,
      "step": 1232
    },
    {
      "epoch": 0.814937210839392,
      "grad_norm": 0.6757850646972656,
      "learning_rate": 3.981328486450761e-06,
      "loss": 0.2837,
      "step": 1233
    },
    {
      "epoch": 0.8155981493721084,
      "grad_norm": 0.6163247227668762,
      "learning_rate": 3.9805023132848645e-06,
      "loss": 0.2558,
      "step": 1234
    },
    {
      "epoch": 0.8162590879048248,
      "grad_norm": 0.6220479011535645,
      "learning_rate": 3.97967614011897e-06,
      "loss": 0.2888,
      "step": 1235
    },
    {
      "epoch": 0.8169200264375414,
      "grad_norm": 0.6387869715690613,
      "learning_rate": 3.978849966953074e-06,
      "loss": 0.2553,
      "step": 1236
    },
    {
      "epoch": 0.8175809649702578,
      "grad_norm": 0.6163517236709595,
      "learning_rate": 3.978023793787178e-06,
      "loss": 0.2534,
      "step": 1237
    },
    {
      "epoch": 0.8182419035029742,
      "grad_norm": 0.6682347059249878,
      "learning_rate": 3.977197620621283e-06,
      "loss": 0.3101,
      "step": 1238
    },
    {
      "epoch": 0.8189028420356906,
      "grad_norm": 0.6029442548751831,
      "learning_rate": 3.976371447455387e-06,
      "loss": 0.2605,
      "step": 1239
    },
    {
      "epoch": 0.8195637805684072,
      "grad_norm": 0.6638886332511902,
      "learning_rate": 3.975545274289491e-06,
      "loss": 0.2934,
      "step": 1240
    },
    {
      "epoch": 0.8202247191011236,
      "grad_norm": 0.603148341178894,
      "learning_rate": 3.974719101123596e-06,
      "loss": 0.2345,
      "step": 1241
    },
    {
      "epoch": 0.82088565763384,
      "grad_norm": 0.644265353679657,
      "learning_rate": 3.973892927957701e-06,
      "loss": 0.2555,
      "step": 1242
    },
    {
      "epoch": 0.8215465961665566,
      "grad_norm": 0.6177694201469421,
      "learning_rate": 3.973066754791804e-06,
      "loss": 0.2719,
      "step": 1243
    },
    {
      "epoch": 0.822207534699273,
      "grad_norm": 0.660163402557373,
      "learning_rate": 3.9722405816259095e-06,
      "loss": 0.2813,
      "step": 1244
    },
    {
      "epoch": 0.8228684732319894,
      "grad_norm": 0.5484326481819153,
      "learning_rate": 3.971414408460014e-06,
      "loss": 0.2834,
      "step": 1245
    },
    {
      "epoch": 0.8235294117647058,
      "grad_norm": 0.5961437821388245,
      "learning_rate": 3.970588235294118e-06,
      "loss": 0.2456,
      "step": 1246
    },
    {
      "epoch": 0.8241903502974224,
      "grad_norm": 0.5739498138427734,
      "learning_rate": 3.9697620621282225e-06,
      "loss": 0.2525,
      "step": 1247
    },
    {
      "epoch": 0.8248512888301388,
      "grad_norm": 0.5925371646881104,
      "learning_rate": 3.968935888962327e-06,
      "loss": 0.295,
      "step": 1248
    },
    {
      "epoch": 0.8255122273628552,
      "grad_norm": 0.7723629474639893,
      "learning_rate": 3.968109715796431e-06,
      "loss": 0.2754,
      "step": 1249
    },
    {
      "epoch": 0.8261731658955717,
      "grad_norm": 0.6398627758026123,
      "learning_rate": 3.9672835426305355e-06,
      "loss": 0.2514,
      "step": 1250
    },
    {
      "epoch": 0.8268341044282882,
      "grad_norm": 0.579736590385437,
      "learning_rate": 3.96645736946464e-06,
      "loss": 0.2623,
      "step": 1251
    },
    {
      "epoch": 0.8274950429610046,
      "grad_norm": 0.5790455937385559,
      "learning_rate": 3.965631196298744e-06,
      "loss": 0.3366,
      "step": 1252
    },
    {
      "epoch": 0.828155981493721,
      "grad_norm": 1.8256489038467407,
      "learning_rate": 3.9648050231328485e-06,
      "loss": 0.3443,
      "step": 1253
    },
    {
      "epoch": 0.8288169200264376,
      "grad_norm": 1.012645959854126,
      "learning_rate": 3.963978849966954e-06,
      "loss": 0.3429,
      "step": 1254
    },
    {
      "epoch": 0.829477858559154,
      "grad_norm": 0.8500006198883057,
      "learning_rate": 3.963152676801058e-06,
      "loss": 0.3538,
      "step": 1255
    },
    {
      "epoch": 0.8301387970918704,
      "grad_norm": 0.8967239856719971,
      "learning_rate": 3.962326503635162e-06,
      "loss": 0.3718,
      "step": 1256
    },
    {
      "epoch": 0.8307997356245869,
      "grad_norm": 0.8389400839805603,
      "learning_rate": 3.961500330469267e-06,
      "loss": 0.3338,
      "step": 1257
    },
    {
      "epoch": 0.8314606741573034,
      "grad_norm": 0.9394234418869019,
      "learning_rate": 3.960674157303371e-06,
      "loss": 0.3676,
      "step": 1258
    },
    {
      "epoch": 0.8321216126900198,
      "grad_norm": 0.868820071220398,
      "learning_rate": 3.959847984137475e-06,
      "loss": 0.3613,
      "step": 1259
    },
    {
      "epoch": 0.8327825512227363,
      "grad_norm": 0.8695164322853088,
      "learning_rate": 3.95902181097158e-06,
      "loss": 0.3606,
      "step": 1260
    },
    {
      "epoch": 0.8334434897554528,
      "grad_norm": 0.8249183893203735,
      "learning_rate": 3.958195637805685e-06,
      "loss": 0.3792,
      "step": 1261
    },
    {
      "epoch": 0.8341044282881692,
      "grad_norm": 0.8726890087127686,
      "learning_rate": 3.957369464639788e-06,
      "loss": 0.3697,
      "step": 1262
    },
    {
      "epoch": 0.8347653668208856,
      "grad_norm": 0.8195677995681763,
      "learning_rate": 3.9565432914738935e-06,
      "loss": 0.3087,
      "step": 1263
    },
    {
      "epoch": 0.8354263053536021,
      "grad_norm": 1.0750885009765625,
      "learning_rate": 3.955717118307998e-06,
      "loss": 0.3649,
      "step": 1264
    },
    {
      "epoch": 0.8360872438863186,
      "grad_norm": 0.8032460808753967,
      "learning_rate": 3.954890945142102e-06,
      "loss": 0.3488,
      "step": 1265
    },
    {
      "epoch": 0.836748182419035,
      "grad_norm": 0.8510938286781311,
      "learning_rate": 3.9540647719762065e-06,
      "loss": 0.3384,
      "step": 1266
    },
    {
      "epoch": 0.8374091209517515,
      "grad_norm": 0.7681653499603271,
      "learning_rate": 3.953238598810311e-06,
      "loss": 0.401,
      "step": 1267
    },
    {
      "epoch": 0.838070059484468,
      "grad_norm": 0.8906416893005371,
      "learning_rate": 3.952412425644415e-06,
      "loss": 0.3355,
      "step": 1268
    },
    {
      "epoch": 0.8387309980171844,
      "grad_norm": 0.6772894263267517,
      "learning_rate": 3.9515862524785195e-06,
      "loss": 0.3326,
      "step": 1269
    },
    {
      "epoch": 0.8393919365499009,
      "grad_norm": 0.7962126135826111,
      "learning_rate": 3.950760079312625e-06,
      "loss": 0.3613,
      "step": 1270
    },
    {
      "epoch": 0.8400528750826173,
      "grad_norm": 0.7176060676574707,
      "learning_rate": 3.949933906146728e-06,
      "loss": 0.3337,
      "step": 1271
    },
    {
      "epoch": 0.8407138136153338,
      "grad_norm": 0.7194112539291382,
      "learning_rate": 3.949107732980833e-06,
      "loss": 0.3785,
      "step": 1272
    },
    {
      "epoch": 0.8413747521480502,
      "grad_norm": 0.7182471752166748,
      "learning_rate": 3.948281559814938e-06,
      "loss": 0.4027,
      "step": 1273
    },
    {
      "epoch": 0.8420356906807667,
      "grad_norm": 0.7007004022598267,
      "learning_rate": 3.947455386649042e-06,
      "loss": 0.3471,
      "step": 1274
    },
    {
      "epoch": 0.8426966292134831,
      "grad_norm": 0.7143979668617249,
      "learning_rate": 3.946629213483146e-06,
      "loss": 0.3167,
      "step": 1275
    },
    {
      "epoch": 0.8433575677461996,
      "grad_norm": 0.7783514857292175,
      "learning_rate": 3.945803040317251e-06,
      "loss": 0.3146,
      "step": 1276
    },
    {
      "epoch": 0.8440185062789161,
      "grad_norm": 0.8268818855285645,
      "learning_rate": 3.944976867151355e-06,
      "loss": 0.3517,
      "step": 1277
    },
    {
      "epoch": 0.8446794448116325,
      "grad_norm": 0.7170939445495605,
      "learning_rate": 3.944150693985459e-06,
      "loss": 0.3434,
      "step": 1278
    },
    {
      "epoch": 0.845340383344349,
      "grad_norm": 0.6922023296356201,
      "learning_rate": 3.9433245208195645e-06,
      "loss": 0.3276,
      "step": 1279
    },
    {
      "epoch": 0.8460013218770654,
      "grad_norm": 0.6349119544029236,
      "learning_rate": 3.942498347653669e-06,
      "loss": 0.3537,
      "step": 1280
    },
    {
      "epoch": 0.8466622604097819,
      "grad_norm": 0.6873989105224609,
      "learning_rate": 3.941672174487773e-06,
      "loss": 0.327,
      "step": 1281
    },
    {
      "epoch": 0.8473231989424983,
      "grad_norm": 0.6901788115501404,
      "learning_rate": 3.9408460013218775e-06,
      "loss": 0.3452,
      "step": 1282
    },
    {
      "epoch": 0.8479841374752148,
      "grad_norm": 0.764228343963623,
      "learning_rate": 3.940019828155982e-06,
      "loss": 0.3407,
      "step": 1283
    },
    {
      "epoch": 0.8486450760079313,
      "grad_norm": 0.6666811108589172,
      "learning_rate": 3.939193654990086e-06,
      "loss": 0.3072,
      "step": 1284
    },
    {
      "epoch": 0.8493060145406477,
      "grad_norm": 0.6696400046348572,
      "learning_rate": 3.9383674818241905e-06,
      "loss": 0.3443,
      "step": 1285
    },
    {
      "epoch": 0.8499669530733642,
      "grad_norm": 0.7131020426750183,
      "learning_rate": 3.937541308658296e-06,
      "loss": 0.3586,
      "step": 1286
    },
    {
      "epoch": 0.8506278916060807,
      "grad_norm": 0.6998680830001831,
      "learning_rate": 3.936715135492399e-06,
      "loss": 0.3293,
      "step": 1287
    },
    {
      "epoch": 0.8512888301387971,
      "grad_norm": 0.5966195464134216,
      "learning_rate": 3.935888962326504e-06,
      "loss": 0.3087,
      "step": 1288
    },
    {
      "epoch": 0.8519497686715135,
      "grad_norm": 0.6955608129501343,
      "learning_rate": 3.935062789160609e-06,
      "loss": 0.3333,
      "step": 1289
    },
    {
      "epoch": 0.85261070720423,
      "grad_norm": 0.6659545302391052,
      "learning_rate": 3.934236615994713e-06,
      "loss": 0.3665,
      "step": 1290
    },
    {
      "epoch": 0.8532716457369465,
      "grad_norm": 0.6584694981575012,
      "learning_rate": 3.933410442828817e-06,
      "loss": 0.3146,
      "step": 1291
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.694322407245636,
      "learning_rate": 3.932584269662922e-06,
      "loss": 0.2868,
      "step": 1292
    },
    {
      "epoch": 0.8545935228023793,
      "grad_norm": 0.7059181332588196,
      "learning_rate": 3.931758096497026e-06,
      "loss": 0.3738,
      "step": 1293
    },
    {
      "epoch": 0.8552544613350959,
      "grad_norm": 0.667060911655426,
      "learning_rate": 3.93093192333113e-06,
      "loss": 0.3457,
      "step": 1294
    },
    {
      "epoch": 0.8559153998678123,
      "grad_norm": 0.8178731203079224,
      "learning_rate": 3.930105750165235e-06,
      "loss": 0.2989,
      "step": 1295
    },
    {
      "epoch": 0.8565763384005287,
      "grad_norm": 0.7471630573272705,
      "learning_rate": 3.929279576999339e-06,
      "loss": 0.3578,
      "step": 1296
    },
    {
      "epoch": 0.8572372769332453,
      "grad_norm": 0.6760718822479248,
      "learning_rate": 3.928453403833443e-06,
      "loss": 0.3299,
      "step": 1297
    },
    {
      "epoch": 0.8578982154659617,
      "grad_norm": 0.6713010668754578,
      "learning_rate": 3.9276272306675486e-06,
      "loss": 0.3827,
      "step": 1298
    },
    {
      "epoch": 0.8585591539986781,
      "grad_norm": 0.6244498491287231,
      "learning_rate": 3.926801057501652e-06,
      "loss": 0.3089,
      "step": 1299
    },
    {
      "epoch": 0.8592200925313945,
      "grad_norm": 0.6440476775169373,
      "learning_rate": 3.925974884335757e-06,
      "loss": 0.3022,
      "step": 1300
    },
    {
      "epoch": 0.8598810310641111,
      "grad_norm": 1.7287200689315796,
      "learning_rate": 3.9251487111698616e-06,
      "loss": 0.319,
      "step": 1301
    },
    {
      "epoch": 0.8605419695968275,
      "grad_norm": 1.0583471059799194,
      "learning_rate": 3.924322538003966e-06,
      "loss": 0.2826,
      "step": 1302
    },
    {
      "epoch": 0.8612029081295439,
      "grad_norm": 0.6830557584762573,
      "learning_rate": 3.92349636483807e-06,
      "loss": 0.2934,
      "step": 1303
    },
    {
      "epoch": 0.8618638466622605,
      "grad_norm": 0.73353511095047,
      "learning_rate": 3.9226701916721746e-06,
      "loss": 0.3392,
      "step": 1304
    },
    {
      "epoch": 0.8625247851949769,
      "grad_norm": 0.6956177949905396,
      "learning_rate": 3.92184401850628e-06,
      "loss": 0.3648,
      "step": 1305
    },
    {
      "epoch": 0.8631857237276933,
      "grad_norm": 0.6266823410987854,
      "learning_rate": 3.921017845340383e-06,
      "loss": 0.3264,
      "step": 1306
    },
    {
      "epoch": 0.8638466622604097,
      "grad_norm": 0.651383638381958,
      "learning_rate": 3.920191672174488e-06,
      "loss": 0.3233,
      "step": 1307
    },
    {
      "epoch": 0.8645076007931263,
      "grad_norm": 0.7416667342185974,
      "learning_rate": 3.919365499008593e-06,
      "loss": 0.3431,
      "step": 1308
    },
    {
      "epoch": 0.8651685393258427,
      "grad_norm": 0.5952266454696655,
      "learning_rate": 3.918539325842697e-06,
      "loss": 0.3236,
      "step": 1309
    },
    {
      "epoch": 0.8658294778585591,
      "grad_norm": 0.6282336711883545,
      "learning_rate": 3.917713152676801e-06,
      "loss": 0.3479,
      "step": 1310
    },
    {
      "epoch": 0.8664904163912757,
      "grad_norm": 0.6541852355003357,
      "learning_rate": 3.916886979510906e-06,
      "loss": 0.3264,
      "step": 1311
    },
    {
      "epoch": 0.8671513549239921,
      "grad_norm": 0.7197279930114746,
      "learning_rate": 3.91606080634501e-06,
      "loss": 0.3226,
      "step": 1312
    },
    {
      "epoch": 0.8678122934567085,
      "grad_norm": 0.6259767413139343,
      "learning_rate": 3.915234633179114e-06,
      "loss": 0.3444,
      "step": 1313
    },
    {
      "epoch": 0.8684732319894249,
      "grad_norm": 0.6580930948257446,
      "learning_rate": 3.91440846001322e-06,
      "loss": 0.3391,
      "step": 1314
    },
    {
      "epoch": 0.8691341705221415,
      "grad_norm": 0.6094259023666382,
      "learning_rate": 3.913582286847323e-06,
      "loss": 0.3506,
      "step": 1315
    },
    {
      "epoch": 0.8697951090548579,
      "grad_norm": 0.5931289792060852,
      "learning_rate": 3.912756113681428e-06,
      "loss": 0.3495,
      "step": 1316
    },
    {
      "epoch": 0.8704560475875743,
      "grad_norm": 0.658112108707428,
      "learning_rate": 3.911929940515533e-06,
      "loss": 0.2941,
      "step": 1317
    },
    {
      "epoch": 0.8711169861202908,
      "grad_norm": 0.6822614669799805,
      "learning_rate": 3.911103767349637e-06,
      "loss": 0.3686,
      "step": 1318
    },
    {
      "epoch": 0.8717779246530073,
      "grad_norm": 0.6374245882034302,
      "learning_rate": 3.910277594183741e-06,
      "loss": 0.3206,
      "step": 1319
    },
    {
      "epoch": 0.8724388631857237,
      "grad_norm": 0.6413519382476807,
      "learning_rate": 3.909451421017846e-06,
      "loss": 0.3655,
      "step": 1320
    },
    {
      "epoch": 0.8730998017184401,
      "grad_norm": 0.5826677083969116,
      "learning_rate": 3.90862524785195e-06,
      "loss": 0.3557,
      "step": 1321
    },
    {
      "epoch": 0.8737607402511567,
      "grad_norm": 0.6294975280761719,
      "learning_rate": 3.907799074686054e-06,
      "loss": 0.3472,
      "step": 1322
    },
    {
      "epoch": 0.8744216787838731,
      "grad_norm": 0.6873359680175781,
      "learning_rate": 3.9069729015201594e-06,
      "loss": 0.3232,
      "step": 1323
    },
    {
      "epoch": 0.8750826173165895,
      "grad_norm": 0.6530473232269287,
      "learning_rate": 3.906146728354263e-06,
      "loss": 0.2926,
      "step": 1324
    },
    {
      "epoch": 0.875743555849306,
      "grad_norm": 0.6825957298278809,
      "learning_rate": 3.905320555188368e-06,
      "loss": 0.3499,
      "step": 1325
    },
    {
      "epoch": 0.8764044943820225,
      "grad_norm": 0.6920326352119446,
      "learning_rate": 3.9044943820224724e-06,
      "loss": 0.306,
      "step": 1326
    },
    {
      "epoch": 0.8770654329147389,
      "grad_norm": 0.6795158386230469,
      "learning_rate": 3.903668208856577e-06,
      "loss": 0.3441,
      "step": 1327
    },
    {
      "epoch": 0.8777263714474554,
      "grad_norm": 0.5943699479103088,
      "learning_rate": 3.902842035690681e-06,
      "loss": 0.3169,
      "step": 1328
    },
    {
      "epoch": 0.8783873099801719,
      "grad_norm": 0.6345372200012207,
      "learning_rate": 3.9020158625247854e-06,
      "loss": 0.3157,
      "step": 1329
    },
    {
      "epoch": 0.8790482485128883,
      "grad_norm": 0.6296379566192627,
      "learning_rate": 3.901189689358891e-06,
      "loss": 0.3112,
      "step": 1330
    },
    {
      "epoch": 0.8797091870456047,
      "grad_norm": 0.659595787525177,
      "learning_rate": 3.900363516192994e-06,
      "loss": 0.324,
      "step": 1331
    },
    {
      "epoch": 0.8803701255783212,
      "grad_norm": 0.6393371224403381,
      "learning_rate": 3.899537343027099e-06,
      "loss": 0.3312,
      "step": 1332
    },
    {
      "epoch": 0.8810310641110377,
      "grad_norm": 0.6638121604919434,
      "learning_rate": 3.898711169861204e-06,
      "loss": 0.3224,
      "step": 1333
    },
    {
      "epoch": 0.8816920026437541,
      "grad_norm": 0.6900203227996826,
      "learning_rate": 3.897884996695307e-06,
      "loss": 0.3162,
      "step": 1334
    },
    {
      "epoch": 0.8823529411764706,
      "grad_norm": 0.5947593450546265,
      "learning_rate": 3.897058823529412e-06,
      "loss": 0.2859,
      "step": 1335
    },
    {
      "epoch": 0.8830138797091871,
      "grad_norm": 0.6844117045402527,
      "learning_rate": 3.896232650363517e-06,
      "loss": 0.3185,
      "step": 1336
    },
    {
      "epoch": 0.8836748182419035,
      "grad_norm": 0.6693983674049377,
      "learning_rate": 3.895406477197621e-06,
      "loss": 0.3027,
      "step": 1337
    },
    {
      "epoch": 0.88433575677462,
      "grad_norm": 0.6525941491127014,
      "learning_rate": 3.894580304031725e-06,
      "loss": 0.2974,
      "step": 1338
    },
    {
      "epoch": 0.8849966953073364,
      "grad_norm": 0.6043081879615784,
      "learning_rate": 3.89375413086583e-06,
      "loss": 0.3061,
      "step": 1339
    },
    {
      "epoch": 0.8856576338400529,
      "grad_norm": 0.6737255454063416,
      "learning_rate": 3.892927957699934e-06,
      "loss": 0.3624,
      "step": 1340
    },
    {
      "epoch": 0.8863185723727693,
      "grad_norm": 0.6551905870437622,
      "learning_rate": 3.892101784534038e-06,
      "loss": 0.3324,
      "step": 1341
    },
    {
      "epoch": 0.8869795109054858,
      "grad_norm": 0.6911866068840027,
      "learning_rate": 3.8912756113681434e-06,
      "loss": 0.324,
      "step": 1342
    },
    {
      "epoch": 0.8876404494382022,
      "grad_norm": 0.6024656295776367,
      "learning_rate": 3.890449438202247e-06,
      "loss": 0.3057,
      "step": 1343
    },
    {
      "epoch": 0.8883013879709187,
      "grad_norm": 0.6494762301445007,
      "learning_rate": 3.889623265036352e-06,
      "loss": 0.2856,
      "step": 1344
    },
    {
      "epoch": 0.8889623265036352,
      "grad_norm": 0.6993942856788635,
      "learning_rate": 3.8887970918704564e-06,
      "loss": 0.3041,
      "step": 1345
    },
    {
      "epoch": 0.8896232650363516,
      "grad_norm": 0.5763841271400452,
      "learning_rate": 3.887970918704561e-06,
      "loss": 0.3346,
      "step": 1346
    },
    {
      "epoch": 0.8902842035690681,
      "grad_norm": 0.6270667314529419,
      "learning_rate": 3.887144745538665e-06,
      "loss": 0.334,
      "step": 1347
    },
    {
      "epoch": 0.8909451421017845,
      "grad_norm": 0.7467197179794312,
      "learning_rate": 3.8863185723727694e-06,
      "loss": 0.3137,
      "step": 1348
    },
    {
      "epoch": 0.891606080634501,
      "grad_norm": 0.7445353865623474,
      "learning_rate": 3.885492399206874e-06,
      "loss": 0.3918,
      "step": 1349
    },
    {
      "epoch": 0.8922670191672174,
      "grad_norm": 0.6139989495277405,
      "learning_rate": 3.884666226040978e-06,
      "loss": 0.3325,
      "step": 1350
    },
    {
      "epoch": 0.8929279576999339,
      "grad_norm": 0.7079393863677979,
      "learning_rate": 3.883840052875083e-06,
      "loss": 0.3118,
      "step": 1351
    },
    {
      "epoch": 0.8935888962326504,
      "grad_norm": 0.6203731894493103,
      "learning_rate": 3.883013879709188e-06,
      "loss": 0.295,
      "step": 1352
    },
    {
      "epoch": 0.8942498347653668,
      "grad_norm": 0.6349693536758423,
      "learning_rate": 3.882187706543292e-06,
      "loss": 0.3488,
      "step": 1353
    },
    {
      "epoch": 0.8949107732980833,
      "grad_norm": 0.7520683407783508,
      "learning_rate": 3.881361533377396e-06,
      "loss": 0.3456,
      "step": 1354
    },
    {
      "epoch": 0.8955717118307998,
      "grad_norm": 0.7435186505317688,
      "learning_rate": 3.880535360211501e-06,
      "loss": 0.3025,
      "step": 1355
    },
    {
      "epoch": 0.8962326503635162,
      "grad_norm": 0.6265279650688171,
      "learning_rate": 3.879709187045605e-06,
      "loss": 0.3434,
      "step": 1356
    },
    {
      "epoch": 0.8968935888962326,
      "grad_norm": 0.9243125915527344,
      "learning_rate": 3.878883013879709e-06,
      "loss": 0.2998,
      "step": 1357
    },
    {
      "epoch": 0.8975545274289491,
      "grad_norm": 0.6526845097541809,
      "learning_rate": 3.8780568407138145e-06,
      "loss": 0.281,
      "step": 1358
    },
    {
      "epoch": 0.8982154659616656,
      "grad_norm": 0.6254615187644958,
      "learning_rate": 3.877230667547918e-06,
      "loss": 0.2868,
      "step": 1359
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.669813334941864,
      "learning_rate": 3.876404494382023e-06,
      "loss": 0.3035,
      "step": 1360
    },
    {
      "epoch": 0.8995373430270985,
      "grad_norm": 0.6474383473396301,
      "learning_rate": 3.8755783212161275e-06,
      "loss": 0.2993,
      "step": 1361
    },
    {
      "epoch": 0.900198281559815,
      "grad_norm": 0.6259926557540894,
      "learning_rate": 3.874752148050232e-06,
      "loss": 0.3132,
      "step": 1362
    },
    {
      "epoch": 0.9008592200925314,
      "grad_norm": 0.6248362064361572,
      "learning_rate": 3.873925974884336e-06,
      "loss": 0.3253,
      "step": 1363
    },
    {
      "epoch": 0.9015201586252478,
      "grad_norm": 0.6523492336273193,
      "learning_rate": 3.8730998017184405e-06,
      "loss": 0.2973,
      "step": 1364
    },
    {
      "epoch": 0.9021810971579644,
      "grad_norm": 0.5971297025680542,
      "learning_rate": 3.872273628552545e-06,
      "loss": 0.3079,
      "step": 1365
    },
    {
      "epoch": 0.9028420356906808,
      "grad_norm": 0.6954947710037231,
      "learning_rate": 3.871447455386649e-06,
      "loss": 0.3006,
      "step": 1366
    },
    {
      "epoch": 0.9035029742233972,
      "grad_norm": 0.6005792617797852,
      "learning_rate": 3.870621282220754e-06,
      "loss": 0.2896,
      "step": 1367
    },
    {
      "epoch": 0.9041639127561136,
      "grad_norm": 0.7015804648399353,
      "learning_rate": 3.869795109054858e-06,
      "loss": 0.3246,
      "step": 1368
    },
    {
      "epoch": 0.9048248512888302,
      "grad_norm": 0.5719730257987976,
      "learning_rate": 3.868968935888963e-06,
      "loss": 0.2908,
      "step": 1369
    },
    {
      "epoch": 0.9054857898215466,
      "grad_norm": 0.6804317235946655,
      "learning_rate": 3.868142762723067e-06,
      "loss": 0.313,
      "step": 1370
    },
    {
      "epoch": 0.906146728354263,
      "grad_norm": 0.6940188407897949,
      "learning_rate": 3.867316589557172e-06,
      "loss": 0.313,
      "step": 1371
    },
    {
      "epoch": 0.9068076668869796,
      "grad_norm": 0.6645108461380005,
      "learning_rate": 3.866490416391276e-06,
      "loss": 0.3127,
      "step": 1372
    },
    {
      "epoch": 0.907468605419696,
      "grad_norm": 0.6285257935523987,
      "learning_rate": 3.86566424322538e-06,
      "loss": 0.2969,
      "step": 1373
    },
    {
      "epoch": 0.9081295439524124,
      "grad_norm": 0.7290273904800415,
      "learning_rate": 3.864838070059485e-06,
      "loss": 0.3026,
      "step": 1374
    },
    {
      "epoch": 0.9087904824851288,
      "grad_norm": 0.6492619514465332,
      "learning_rate": 3.864011896893589e-06,
      "loss": 0.3048,
      "step": 1375
    },
    {
      "epoch": 0.9094514210178454,
      "grad_norm": 0.5839377641677856,
      "learning_rate": 3.863185723727693e-06,
      "loss": 0.325,
      "step": 1376
    },
    {
      "epoch": 0.9101123595505618,
      "grad_norm": 0.6549157500267029,
      "learning_rate": 3.8623595505617985e-06,
      "loss": 0.3126,
      "step": 1377
    },
    {
      "epoch": 0.9107732980832782,
      "grad_norm": 0.6810681819915771,
      "learning_rate": 3.861533377395902e-06,
      "loss": 0.3136,
      "step": 1378
    },
    {
      "epoch": 0.9114342366159948,
      "grad_norm": 0.6616302132606506,
      "learning_rate": 3.860707204230007e-06,
      "loss": 0.3055,
      "step": 1379
    },
    {
      "epoch": 0.9120951751487112,
      "grad_norm": 0.586188554763794,
      "learning_rate": 3.8598810310641115e-06,
      "loss": 0.3413,
      "step": 1380
    },
    {
      "epoch": 0.9127561136814276,
      "grad_norm": 0.6298667788505554,
      "learning_rate": 3.859054857898216e-06,
      "loss": 0.3349,
      "step": 1381
    },
    {
      "epoch": 0.913417052214144,
      "grad_norm": 0.597804844379425,
      "learning_rate": 3.85822868473232e-06,
      "loss": 0.344,
      "step": 1382
    },
    {
      "epoch": 0.9140779907468606,
      "grad_norm": 0.6181843876838684,
      "learning_rate": 3.8574025115664245e-06,
      "loss": 0.3409,
      "step": 1383
    },
    {
      "epoch": 0.914738929279577,
      "grad_norm": 0.6321173906326294,
      "learning_rate": 3.856576338400529e-06,
      "loss": 0.3061,
      "step": 1384
    },
    {
      "epoch": 0.9153998678122934,
      "grad_norm": 0.6987273097038269,
      "learning_rate": 3.855750165234633e-06,
      "loss": 0.3345,
      "step": 1385
    },
    {
      "epoch": 0.9160608063450099,
      "grad_norm": 0.6269187331199646,
      "learning_rate": 3.854923992068738e-06,
      "loss": 0.2781,
      "step": 1386
    },
    {
      "epoch": 0.9167217448777264,
      "grad_norm": 0.639435887336731,
      "learning_rate": 3.854097818902842e-06,
      "loss": 0.3304,
      "step": 1387
    },
    {
      "epoch": 0.9173826834104428,
      "grad_norm": 0.6526311635971069,
      "learning_rate": 3.853271645736947e-06,
      "loss": 0.299,
      "step": 1388
    },
    {
      "epoch": 0.9180436219431592,
      "grad_norm": 0.6809478402137756,
      "learning_rate": 3.852445472571051e-06,
      "loss": 0.2864,
      "step": 1389
    },
    {
      "epoch": 0.9187045604758758,
      "grad_norm": 0.6417999863624573,
      "learning_rate": 3.851619299405156e-06,
      "loss": 0.3101,
      "step": 1390
    },
    {
      "epoch": 0.9193654990085922,
      "grad_norm": 0.5837154984474182,
      "learning_rate": 3.85079312623926e-06,
      "loss": 0.3383,
      "step": 1391
    },
    {
      "epoch": 0.9200264375413086,
      "grad_norm": 0.6634908318519592,
      "learning_rate": 3.849966953073364e-06,
      "loss": 0.3602,
      "step": 1392
    },
    {
      "epoch": 0.9206873760740251,
      "grad_norm": 0.755471408367157,
      "learning_rate": 3.849140779907469e-06,
      "loss": 0.3175,
      "step": 1393
    },
    {
      "epoch": 0.9213483146067416,
      "grad_norm": 0.6087191700935364,
      "learning_rate": 3.848314606741573e-06,
      "loss": 0.3408,
      "step": 1394
    },
    {
      "epoch": 0.922009253139458,
      "grad_norm": 0.6194021105766296,
      "learning_rate": 3.847488433575678e-06,
      "loss": 0.3431,
      "step": 1395
    },
    {
      "epoch": 0.9226701916721745,
      "grad_norm": 0.6523963809013367,
      "learning_rate": 3.846662260409782e-06,
      "loss": 0.3504,
      "step": 1396
    },
    {
      "epoch": 0.923331130204891,
      "grad_norm": 0.6539721488952637,
      "learning_rate": 3.845836087243887e-06,
      "loss": 0.3471,
      "step": 1397
    },
    {
      "epoch": 0.9239920687376074,
      "grad_norm": 0.5959272980690002,
      "learning_rate": 3.845009914077991e-06,
      "loss": 0.3169,
      "step": 1398
    },
    {
      "epoch": 0.9246530072703238,
      "grad_norm": 0.6481186151504517,
      "learning_rate": 3.8441837409120955e-06,
      "loss": 0.3043,
      "step": 1399
    },
    {
      "epoch": 0.9253139458030403,
      "grad_norm": 0.6901474595069885,
      "learning_rate": 3.8433575677462e-06,
      "loss": 0.3208,
      "step": 1400
    },
    {
      "epoch": 0.9259748843357568,
      "grad_norm": 0.719618558883667,
      "learning_rate": 3.842531394580304e-06,
      "loss": 0.2883,
      "step": 1401
    },
    {
      "epoch": 0.9266358228684732,
      "grad_norm": 0.6280879378318787,
      "learning_rate": 3.841705221414409e-06,
      "loss": 0.3184,
      "step": 1402
    },
    {
      "epoch": 0.9272967614011897,
      "grad_norm": 0.6168456077575684,
      "learning_rate": 3.840879048248513e-06,
      "loss": 0.3281,
      "step": 1403
    },
    {
      "epoch": 0.9279576999339062,
      "grad_norm": 0.6043931841850281,
      "learning_rate": 3.840052875082618e-06,
      "loss": 0.3213,
      "step": 1404
    },
    {
      "epoch": 0.9286186384666226,
      "grad_norm": 0.5742704272270203,
      "learning_rate": 3.839226701916722e-06,
      "loss": 0.2911,
      "step": 1405
    },
    {
      "epoch": 0.929279576999339,
      "grad_norm": 0.5859281420707703,
      "learning_rate": 3.838400528750827e-06,
      "loss": 0.3433,
      "step": 1406
    },
    {
      "epoch": 0.9299405155320555,
      "grad_norm": 0.6709789633750916,
      "learning_rate": 3.837574355584931e-06,
      "loss": 0.3446,
      "step": 1407
    },
    {
      "epoch": 0.930601454064772,
      "grad_norm": 0.6164202690124512,
      "learning_rate": 3.836748182419035e-06,
      "loss": 0.3207,
      "step": 1408
    },
    {
      "epoch": 0.9312623925974884,
      "grad_norm": 0.6169062852859497,
      "learning_rate": 3.83592200925314e-06,
      "loss": 0.3141,
      "step": 1409
    },
    {
      "epoch": 0.9319233311302049,
      "grad_norm": 0.5996342897415161,
      "learning_rate": 3.835095836087244e-06,
      "loss": 0.338,
      "step": 1410
    },
    {
      "epoch": 0.9325842696629213,
      "grad_norm": 0.6097069978713989,
      "learning_rate": 3.834269662921349e-06,
      "loss": 0.3361,
      "step": 1411
    },
    {
      "epoch": 0.9332452081956378,
      "grad_norm": 0.5856088995933533,
      "learning_rate": 3.833443489755453e-06,
      "loss": 0.3339,
      "step": 1412
    },
    {
      "epoch": 0.9339061467283543,
      "grad_norm": 0.6619246006011963,
      "learning_rate": 3.832617316589558e-06,
      "loss": 0.3078,
      "step": 1413
    },
    {
      "epoch": 0.9345670852610707,
      "grad_norm": 0.5821111798286438,
      "learning_rate": 3.831791143423662e-06,
      "loss": 0.3124,
      "step": 1414
    },
    {
      "epoch": 0.9352280237937872,
      "grad_norm": 0.6071040034294128,
      "learning_rate": 3.8309649702577665e-06,
      "loss": 0.3328,
      "step": 1415
    },
    {
      "epoch": 0.9358889623265036,
      "grad_norm": 0.6204041838645935,
      "learning_rate": 3.830138797091871e-06,
      "loss": 0.3129,
      "step": 1416
    },
    {
      "epoch": 0.9365499008592201,
      "grad_norm": 0.5678131580352783,
      "learning_rate": 3.829312623925975e-06,
      "loss": 0.2882,
      "step": 1417
    },
    {
      "epoch": 0.9372108393919365,
      "grad_norm": 0.6085466146469116,
      "learning_rate": 3.8284864507600795e-06,
      "loss": 0.2742,
      "step": 1418
    },
    {
      "epoch": 0.937871777924653,
      "grad_norm": 0.5797286629676819,
      "learning_rate": 3.827660277594184e-06,
      "loss": 0.3382,
      "step": 1419
    },
    {
      "epoch": 0.9385327164573695,
      "grad_norm": 0.6288148760795593,
      "learning_rate": 3.826834104428288e-06,
      "loss": 0.2998,
      "step": 1420
    },
    {
      "epoch": 0.9391936549900859,
      "grad_norm": 0.6408783197402954,
      "learning_rate": 3.8260079312623925e-06,
      "loss": 0.2871,
      "step": 1421
    },
    {
      "epoch": 0.9398545935228024,
      "grad_norm": 0.5585827231407166,
      "learning_rate": 3.825181758096497e-06,
      "loss": 0.3222,
      "step": 1422
    },
    {
      "epoch": 0.9405155320555189,
      "grad_norm": 0.594668984413147,
      "learning_rate": 3.824355584930602e-06,
      "loss": 0.2786,
      "step": 1423
    },
    {
      "epoch": 0.9411764705882353,
      "grad_norm": 0.6003414988517761,
      "learning_rate": 3.8235294117647055e-06,
      "loss": 0.3061,
      "step": 1424
    },
    {
      "epoch": 0.9418374091209517,
      "grad_norm": 0.6184728741645813,
      "learning_rate": 3.822703238598811e-06,
      "loss": 0.3276,
      "step": 1425
    },
    {
      "epoch": 0.9424983476536682,
      "grad_norm": 0.5977999567985535,
      "learning_rate": 3.821877065432915e-06,
      "loss": 0.2869,
      "step": 1426
    },
    {
      "epoch": 0.9431592861863847,
      "grad_norm": 0.6077285408973694,
      "learning_rate": 3.821050892267019e-06,
      "loss": 0.2891,
      "step": 1427
    },
    {
      "epoch": 0.9438202247191011,
      "grad_norm": 0.6529399156570435,
      "learning_rate": 3.820224719101124e-06,
      "loss": 0.2945,
      "step": 1428
    },
    {
      "epoch": 0.9444811632518176,
      "grad_norm": 0.6981451511383057,
      "learning_rate": 3.819398545935228e-06,
      "loss": 0.3319,
      "step": 1429
    },
    {
      "epoch": 0.9451421017845341,
      "grad_norm": 0.647243320941925,
      "learning_rate": 3.818572372769333e-06,
      "loss": 0.2948,
      "step": 1430
    },
    {
      "epoch": 0.9458030403172505,
      "grad_norm": 0.6451802849769592,
      "learning_rate": 3.817746199603437e-06,
      "loss": 0.3089,
      "step": 1431
    },
    {
      "epoch": 0.9464639788499669,
      "grad_norm": 0.6647627353668213,
      "learning_rate": 3.816920026437542e-06,
      "loss": 0.332,
      "step": 1432
    },
    {
      "epoch": 0.9471249173826835,
      "grad_norm": 0.5787500143051147,
      "learning_rate": 3.816093853271646e-06,
      "loss": 0.283,
      "step": 1433
    },
    {
      "epoch": 0.9477858559153999,
      "grad_norm": 0.534882128238678,
      "learning_rate": 3.8152676801057506e-06,
      "loss": 0.2781,
      "step": 1434
    },
    {
      "epoch": 0.9484467944481163,
      "grad_norm": 0.6267790794372559,
      "learning_rate": 3.814441506939855e-06,
      "loss": 0.294,
      "step": 1435
    },
    {
      "epoch": 0.9491077329808327,
      "grad_norm": 0.6697651147842407,
      "learning_rate": 3.8136153337739597e-06,
      "loss": 0.303,
      "step": 1436
    },
    {
      "epoch": 0.9497686715135493,
      "grad_norm": 0.6270773410797119,
      "learning_rate": 3.8127891606080636e-06,
      "loss": 0.2588,
      "step": 1437
    },
    {
      "epoch": 0.9504296100462657,
      "grad_norm": 0.5909037590026855,
      "learning_rate": 3.8119629874421683e-06,
      "loss": 0.3194,
      "step": 1438
    },
    {
      "epoch": 0.9510905485789821,
      "grad_norm": 0.5913559794425964,
      "learning_rate": 3.8111368142762727e-06,
      "loss": 0.2713,
      "step": 1439
    },
    {
      "epoch": 0.9517514871116987,
      "grad_norm": 0.6148715019226074,
      "learning_rate": 3.810310641110377e-06,
      "loss": 0.3011,
      "step": 1440
    },
    {
      "epoch": 0.9524124256444151,
      "grad_norm": 0.5851503014564514,
      "learning_rate": 3.8094844679444813e-06,
      "loss": 0.3224,
      "step": 1441
    },
    {
      "epoch": 0.9530733641771315,
      "grad_norm": 0.5744128227233887,
      "learning_rate": 3.808658294778586e-06,
      "loss": 0.3067,
      "step": 1442
    },
    {
      "epoch": 0.9537343027098479,
      "grad_norm": 0.6489179134368896,
      "learning_rate": 3.80783212161269e-06,
      "loss": 0.2716,
      "step": 1443
    },
    {
      "epoch": 0.9543952412425645,
      "grad_norm": 0.7181521058082581,
      "learning_rate": 3.8070059484467948e-06,
      "loss": 0.3102,
      "step": 1444
    },
    {
      "epoch": 0.9550561797752809,
      "grad_norm": 0.6598859429359436,
      "learning_rate": 3.8061797752808995e-06,
      "loss": 0.3034,
      "step": 1445
    },
    {
      "epoch": 0.9557171183079973,
      "grad_norm": 1.104899287223816,
      "learning_rate": 3.8053536021150034e-06,
      "loss": 0.3011,
      "step": 1446
    },
    {
      "epoch": 0.9563780568407139,
      "grad_norm": 0.5941249132156372,
      "learning_rate": 3.804527428949108e-06,
      "loss": 0.2688,
      "step": 1447
    },
    {
      "epoch": 0.9570389953734303,
      "grad_norm": 0.6297922730445862,
      "learning_rate": 3.8037012557832125e-06,
      "loss": 0.2943,
      "step": 1448
    },
    {
      "epoch": 0.9576999339061467,
      "grad_norm": 0.6340587735176086,
      "learning_rate": 3.802875082617317e-06,
      "loss": 0.2734,
      "step": 1449
    },
    {
      "epoch": 0.9583608724388631,
      "grad_norm": 0.6251026391983032,
      "learning_rate": 3.802048909451421e-06,
      "loss": 0.3089,
      "step": 1450
    },
    {
      "epoch": 0.9590218109715797,
      "grad_norm": 0.5828707218170166,
      "learning_rate": 3.801222736285526e-06,
      "loss": 0.2771,
      "step": 1451
    },
    {
      "epoch": 0.9596827495042961,
      "grad_norm": 0.6214103698730469,
      "learning_rate": 3.8003965631196303e-06,
      "loss": 0.3035,
      "step": 1452
    },
    {
      "epoch": 0.9603436880370125,
      "grad_norm": 0.6118394732475281,
      "learning_rate": 3.7995703899537346e-06,
      "loss": 0.3103,
      "step": 1453
    },
    {
      "epoch": 0.9610046265697291,
      "grad_norm": 0.6112406849861145,
      "learning_rate": 3.798744216787839e-06,
      "loss": 0.2681,
      "step": 1454
    },
    {
      "epoch": 0.9616655651024455,
      "grad_norm": 0.5607330799102783,
      "learning_rate": 3.7979180436219437e-06,
      "loss": 0.2548,
      "step": 1455
    },
    {
      "epoch": 0.9623265036351619,
      "grad_norm": 0.6398432850837708,
      "learning_rate": 3.7970918704560476e-06,
      "loss": 0.2934,
      "step": 1456
    },
    {
      "epoch": 0.9629874421678783,
      "grad_norm": 0.659392774105072,
      "learning_rate": 3.7962656972901524e-06,
      "loss": 0.2633,
      "step": 1457
    },
    {
      "epoch": 0.9636483807005949,
      "grad_norm": 0.6861571669578552,
      "learning_rate": 3.795439524124257e-06,
      "loss": 0.2763,
      "step": 1458
    },
    {
      "epoch": 0.9643093192333113,
      "grad_norm": 0.5717107057571411,
      "learning_rate": 3.794613350958361e-06,
      "loss": 0.2695,
      "step": 1459
    },
    {
      "epoch": 0.9649702577660277,
      "grad_norm": 0.58902508020401,
      "learning_rate": 3.7937871777924658e-06,
      "loss": 0.2597,
      "step": 1460
    },
    {
      "epoch": 0.9656311962987442,
      "grad_norm": 0.5989000797271729,
      "learning_rate": 3.79296100462657e-06,
      "loss": 0.3134,
      "step": 1461
    },
    {
      "epoch": 0.9662921348314607,
      "grad_norm": 0.5897753238677979,
      "learning_rate": 3.7921348314606744e-06,
      "loss": 0.3036,
      "step": 1462
    },
    {
      "epoch": 0.9669530733641771,
      "grad_norm": 0.6227952241897583,
      "learning_rate": 3.7913086582947788e-06,
      "loss": 0.2834,
      "step": 1463
    },
    {
      "epoch": 0.9676140118968936,
      "grad_norm": 0.6206607222557068,
      "learning_rate": 3.7904824851288835e-06,
      "loss": 0.2792,
      "step": 1464
    },
    {
      "epoch": 0.9682749504296101,
      "grad_norm": 0.6753286123275757,
      "learning_rate": 3.7896563119629874e-06,
      "loss": 0.2833,
      "step": 1465
    },
    {
      "epoch": 0.9689358889623265,
      "grad_norm": 0.6856574416160583,
      "learning_rate": 3.788830138797092e-06,
      "loss": 0.3127,
      "step": 1466
    },
    {
      "epoch": 0.969596827495043,
      "grad_norm": 0.6075313687324524,
      "learning_rate": 3.788003965631197e-06,
      "loss": 0.2887,
      "step": 1467
    },
    {
      "epoch": 0.9702577660277594,
      "grad_norm": 0.6884049773216248,
      "learning_rate": 3.787177792465301e-06,
      "loss": 0.303,
      "step": 1468
    },
    {
      "epoch": 0.9709187045604759,
      "grad_norm": 0.6716365218162537,
      "learning_rate": 3.7863516192994056e-06,
      "loss": 0.3136,
      "step": 1469
    },
    {
      "epoch": 0.9715796430931923,
      "grad_norm": 0.6247349977493286,
      "learning_rate": 3.78552544613351e-06,
      "loss": 0.288,
      "step": 1470
    },
    {
      "epoch": 0.9722405816259088,
      "grad_norm": 0.6397651433944702,
      "learning_rate": 3.7846992729676143e-06,
      "loss": 0.3283,
      "step": 1471
    },
    {
      "epoch": 0.9729015201586253,
      "grad_norm": 0.6736822128295898,
      "learning_rate": 3.7838730998017186e-06,
      "loss": 0.2767,
      "step": 1472
    },
    {
      "epoch": 0.9735624586913417,
      "grad_norm": 0.5725483298301697,
      "learning_rate": 3.7830469266358234e-06,
      "loss": 0.2731,
      "step": 1473
    },
    {
      "epoch": 0.9742233972240582,
      "grad_norm": 0.638840913772583,
      "learning_rate": 3.7822207534699273e-06,
      "loss": 0.2722,
      "step": 1474
    },
    {
      "epoch": 0.9748843357567746,
      "grad_norm": 0.637873113155365,
      "learning_rate": 3.781394580304032e-06,
      "loss": 0.2833,
      "step": 1475
    },
    {
      "epoch": 0.9755452742894911,
      "grad_norm": 0.6971961259841919,
      "learning_rate": 3.7805684071381364e-06,
      "loss": 0.3201,
      "step": 1476
    },
    {
      "epoch": 0.9762062128222075,
      "grad_norm": 0.6484291553497314,
      "learning_rate": 3.779742233972241e-06,
      "loss": 0.3283,
      "step": 1477
    },
    {
      "epoch": 0.976867151354924,
      "grad_norm": 0.6684535145759583,
      "learning_rate": 3.778916060806345e-06,
      "loss": 0.2624,
      "step": 1478
    },
    {
      "epoch": 0.9775280898876404,
      "grad_norm": 0.53696209192276,
      "learning_rate": 3.77808988764045e-06,
      "loss": 0.3231,
      "step": 1479
    },
    {
      "epoch": 0.9781890284203569,
      "grad_norm": 0.6516582369804382,
      "learning_rate": 3.7772637144745546e-06,
      "loss": 0.3039,
      "step": 1480
    },
    {
      "epoch": 0.9788499669530734,
      "grad_norm": 0.5999255180358887,
      "learning_rate": 3.7764375413086585e-06,
      "loss": 0.2867,
      "step": 1481
    },
    {
      "epoch": 0.9795109054857898,
      "grad_norm": 0.5685380697250366,
      "learning_rate": 3.7756113681427632e-06,
      "loss": 0.2934,
      "step": 1482
    },
    {
      "epoch": 0.9801718440185063,
      "grad_norm": 0.6393300294876099,
      "learning_rate": 3.7747851949768676e-06,
      "loss": 0.2519,
      "step": 1483
    },
    {
      "epoch": 0.9808327825512227,
      "grad_norm": 0.5563847422599792,
      "learning_rate": 3.773959021810972e-06,
      "loss": 0.2779,
      "step": 1484
    },
    {
      "epoch": 0.9814937210839392,
      "grad_norm": 0.6209897994995117,
      "learning_rate": 3.7731328486450762e-06,
      "loss": 0.3503,
      "step": 1485
    },
    {
      "epoch": 0.9821546596166556,
      "grad_norm": 0.6469204425811768,
      "learning_rate": 3.772306675479181e-06,
      "loss": 0.2994,
      "step": 1486
    },
    {
      "epoch": 0.9828155981493721,
      "grad_norm": 0.5935319662094116,
      "learning_rate": 3.771480502313285e-06,
      "loss": 0.2592,
      "step": 1487
    },
    {
      "epoch": 0.9834765366820886,
      "grad_norm": 0.6888647079467773,
      "learning_rate": 3.7706543291473896e-06,
      "loss": 0.3221,
      "step": 1488
    },
    {
      "epoch": 0.984137475214805,
      "grad_norm": 0.5832295417785645,
      "learning_rate": 3.7698281559814944e-06,
      "loss": 0.2589,
      "step": 1489
    },
    {
      "epoch": 0.9847984137475215,
      "grad_norm": 0.6232362985610962,
      "learning_rate": 3.7690019828155983e-06,
      "loss": 0.2619,
      "step": 1490
    },
    {
      "epoch": 0.985459352280238,
      "grad_norm": 0.6648767590522766,
      "learning_rate": 3.768175809649703e-06,
      "loss": 0.2768,
      "step": 1491
    },
    {
      "epoch": 0.9861202908129544,
      "grad_norm": 0.673794686794281,
      "learning_rate": 3.7673496364838074e-06,
      "loss": 0.281,
      "step": 1492
    },
    {
      "epoch": 0.9867812293456708,
      "grad_norm": 0.6455698013305664,
      "learning_rate": 3.7665234633179113e-06,
      "loss": 0.3247,
      "step": 1493
    },
    {
      "epoch": 0.9874421678783873,
      "grad_norm": 0.6759480834007263,
      "learning_rate": 3.765697290152016e-06,
      "loss": 0.2753,
      "step": 1494
    },
    {
      "epoch": 0.9881031064111038,
      "grad_norm": 0.5756412744522095,
      "learning_rate": 3.764871116986121e-06,
      "loss": 0.2605,
      "step": 1495
    },
    {
      "epoch": 0.9887640449438202,
      "grad_norm": 0.6262175440788269,
      "learning_rate": 3.7640449438202247e-06,
      "loss": 0.3122,
      "step": 1496
    },
    {
      "epoch": 0.9894249834765367,
      "grad_norm": 0.6990015506744385,
      "learning_rate": 3.7632187706543295e-06,
      "loss": 0.3265,
      "step": 1497
    },
    {
      "epoch": 0.9900859220092532,
      "grad_norm": 0.609844982624054,
      "learning_rate": 3.762392597488434e-06,
      "loss": 0.2933,
      "step": 1498
    },
    {
      "epoch": 0.9907468605419696,
      "grad_norm": 0.5252760648727417,
      "learning_rate": 3.761566424322538e-06,
      "loss": 0.2802,
      "step": 1499
    },
    {
      "epoch": 0.991407799074686,
      "grad_norm": 0.6431227922439575,
      "learning_rate": 3.7607402511566425e-06,
      "loss": 0.299,
      "step": 1500
    },
    {
      "epoch": 0.9920687376074026,
      "grad_norm": 0.584374189376831,
      "learning_rate": 3.7599140779907472e-06,
      "loss": 0.3066,
      "step": 1501
    },
    {
      "epoch": 0.992729676140119,
      "grad_norm": 0.604327917098999,
      "learning_rate": 3.759087904824852e-06,
      "loss": 0.2715,
      "step": 1502
    },
    {
      "epoch": 0.9933906146728354,
      "grad_norm": 0.6202564239501953,
      "learning_rate": 3.758261731658956e-06,
      "loss": 0.3059,
      "step": 1503
    },
    {
      "epoch": 0.9940515532055518,
      "grad_norm": 0.6427355408668518,
      "learning_rate": 3.7574355584930607e-06,
      "loss": 0.3236,
      "step": 1504
    },
    {
      "epoch": 0.9947124917382684,
      "grad_norm": 0.6392477750778198,
      "learning_rate": 3.756609385327165e-06,
      "loss": 0.2725,
      "step": 1505
    },
    {
      "epoch": 0.9953734302709848,
      "grad_norm": 0.7647290825843811,
      "learning_rate": 3.7557832121612693e-06,
      "loss": 0.3546,
      "step": 1506
    },
    {
      "epoch": 0.9960343688037012,
      "grad_norm": 0.571025013923645,
      "learning_rate": 3.7549570389953737e-06,
      "loss": 0.333,
      "step": 1507
    },
    {
      "epoch": 0.9966953073364178,
      "grad_norm": 0.6752921938896179,
      "learning_rate": 3.7541308658294784e-06,
      "loss": 0.3135,
      "step": 1508
    },
    {
      "epoch": 0.9973562458691342,
      "grad_norm": 0.6303426027297974,
      "learning_rate": 3.7533046926635823e-06,
      "loss": 0.2881,
      "step": 1509
    },
    {
      "epoch": 0.9980171844018506,
      "grad_norm": 0.651807963848114,
      "learning_rate": 3.752478519497687e-06,
      "loss": 0.3151,
      "step": 1510
    },
    {
      "epoch": 0.998678122934567,
      "grad_norm": 0.5983073711395264,
      "learning_rate": 3.751652346331792e-06,
      "loss": 0.2701,
      "step": 1511
    },
    {
      "epoch": 0.9993390614672836,
      "grad_norm": 0.5760536193847656,
      "learning_rate": 3.7508261731658958e-06,
      "loss": 0.2798,
      "step": 1512
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6244668364524841,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 0.303,
      "step": 1513
    },
    {
      "epoch": 1.0006609385327165,
      "grad_norm": 0.8164125084877014,
      "learning_rate": 3.749173826834105e-06,
      "loss": 0.245,
      "step": 1514
    },
    {
      "epoch": 1.0013218770654329,
      "grad_norm": 0.8040870428085327,
      "learning_rate": 3.7483476536682088e-06,
      "loss": 0.2161,
      "step": 1515
    },
    {
      "epoch": 1.0019828155981494,
      "grad_norm": 0.7288147211074829,
      "learning_rate": 3.7475214805023135e-06,
      "loss": 0.2318,
      "step": 1516
    },
    {
      "epoch": 1.002643754130866,
      "grad_norm": 0.9451151490211487,
      "learning_rate": 3.7466953073364183e-06,
      "loss": 0.2437,
      "step": 1517
    },
    {
      "epoch": 1.0033046926635822,
      "grad_norm": 0.930653989315033,
      "learning_rate": 3.745869134170522e-06,
      "loss": 0.2598,
      "step": 1518
    },
    {
      "epoch": 1.0039656311962988,
      "grad_norm": 0.9286924004554749,
      "learning_rate": 3.745042961004627e-06,
      "loss": 0.224,
      "step": 1519
    },
    {
      "epoch": 1.004626569729015,
      "grad_norm": 0.9587631225585938,
      "learning_rate": 3.7442167878387313e-06,
      "loss": 0.1988,
      "step": 1520
    },
    {
      "epoch": 1.0052875082617316,
      "grad_norm": 0.7850161790847778,
      "learning_rate": 3.7433906146728356e-06,
      "loss": 0.2303,
      "step": 1521
    },
    {
      "epoch": 1.0059484467944482,
      "grad_norm": 0.5943922996520996,
      "learning_rate": 3.74256444150694e-06,
      "loss": 0.1941,
      "step": 1522
    },
    {
      "epoch": 1.0066093853271645,
      "grad_norm": 0.7325173616409302,
      "learning_rate": 3.7417382683410447e-06,
      "loss": 0.2592,
      "step": 1523
    },
    {
      "epoch": 1.007270323859881,
      "grad_norm": 0.6682498455047607,
      "learning_rate": 3.7409120951751486e-06,
      "loss": 0.2311,
      "step": 1524
    },
    {
      "epoch": 1.0079312623925976,
      "grad_norm": 0.6373811960220337,
      "learning_rate": 3.7400859220092534e-06,
      "loss": 0.2223,
      "step": 1525
    },
    {
      "epoch": 1.0085922009253139,
      "grad_norm": 0.7067045569419861,
      "learning_rate": 3.739259748843358e-06,
      "loss": 0.232,
      "step": 1526
    },
    {
      "epoch": 1.0092531394580304,
      "grad_norm": 0.6307467222213745,
      "learning_rate": 3.7384335756774625e-06,
      "loss": 0.1839,
      "step": 1527
    },
    {
      "epoch": 1.009914077990747,
      "grad_norm": 0.6334698796272278,
      "learning_rate": 3.7376074025115668e-06,
      "loss": 0.215,
      "step": 1528
    },
    {
      "epoch": 1.0105750165234633,
      "grad_norm": 0.7396775484085083,
      "learning_rate": 3.736781229345671e-06,
      "loss": 0.235,
      "step": 1529
    },
    {
      "epoch": 1.0112359550561798,
      "grad_norm": 0.8389573097229004,
      "learning_rate": 3.735955056179776e-06,
      "loss": 0.2708,
      "step": 1530
    },
    {
      "epoch": 1.0118968935888963,
      "grad_norm": 0.8571174144744873,
      "learning_rate": 3.7351288830138798e-06,
      "loss": 0.2285,
      "step": 1531
    },
    {
      "epoch": 1.0125578321216127,
      "grad_norm": 0.6821151971817017,
      "learning_rate": 3.7343027098479845e-06,
      "loss": 0.2135,
      "step": 1532
    },
    {
      "epoch": 1.0132187706543292,
      "grad_norm": 0.6692001819610596,
      "learning_rate": 3.7334765366820893e-06,
      "loss": 0.2045,
      "step": 1533
    },
    {
      "epoch": 1.0138797091870455,
      "grad_norm": 0.7673258781433105,
      "learning_rate": 3.732650363516193e-06,
      "loss": 0.2248,
      "step": 1534
    },
    {
      "epoch": 1.014540647719762,
      "grad_norm": 0.5787522196769714,
      "learning_rate": 3.7318241903502975e-06,
      "loss": 0.2111,
      "step": 1535
    },
    {
      "epoch": 1.0152015862524786,
      "grad_norm": 0.6112546324729919,
      "learning_rate": 3.7309980171844023e-06,
      "loss": 0.2245,
      "step": 1536
    },
    {
      "epoch": 1.015862524785195,
      "grad_norm": 0.6473978161811829,
      "learning_rate": 3.730171844018506e-06,
      "loss": 0.2267,
      "step": 1537
    },
    {
      "epoch": 1.0165234633179114,
      "grad_norm": 0.6209287047386169,
      "learning_rate": 3.729345670852611e-06,
      "loss": 0.2297,
      "step": 1538
    },
    {
      "epoch": 1.017184401850628,
      "grad_norm": 0.6752836108207703,
      "learning_rate": 3.7285194976867157e-06,
      "loss": 0.2102,
      "step": 1539
    },
    {
      "epoch": 1.0178453403833443,
      "grad_norm": 0.6676056981086731,
      "learning_rate": 3.7276933245208196e-06,
      "loss": 0.1801,
      "step": 1540
    },
    {
      "epoch": 1.0185062789160608,
      "grad_norm": 0.7087928652763367,
      "learning_rate": 3.7268671513549244e-06,
      "loss": 0.215,
      "step": 1541
    },
    {
      "epoch": 1.0191672174487774,
      "grad_norm": 0.6065792441368103,
      "learning_rate": 3.7260409781890287e-06,
      "loss": 0.2568,
      "step": 1542
    },
    {
      "epoch": 1.0198281559814937,
      "grad_norm": 0.6330291628837585,
      "learning_rate": 3.725214805023133e-06,
      "loss": 0.2132,
      "step": 1543
    },
    {
      "epoch": 1.0204890945142102,
      "grad_norm": 0.6459116339683533,
      "learning_rate": 3.7243886318572374e-06,
      "loss": 0.2251,
      "step": 1544
    },
    {
      "epoch": 1.0211500330469265,
      "grad_norm": 0.5827454328536987,
      "learning_rate": 3.723562458691342e-06,
      "loss": 0.2137,
      "step": 1545
    },
    {
      "epoch": 1.021810971579643,
      "grad_norm": 0.6632458567619324,
      "learning_rate": 3.722736285525446e-06,
      "loss": 0.2316,
      "step": 1546
    },
    {
      "epoch": 1.0224719101123596,
      "grad_norm": 0.6037554740905762,
      "learning_rate": 3.721910112359551e-06,
      "loss": 0.2027,
      "step": 1547
    },
    {
      "epoch": 1.023132848645076,
      "grad_norm": 0.5974652767181396,
      "learning_rate": 3.7210839391936556e-06,
      "loss": 0.2255,
      "step": 1548
    },
    {
      "epoch": 1.0237937871777925,
      "grad_norm": 0.5935603976249695,
      "learning_rate": 3.7202577660277595e-06,
      "loss": 0.2427,
      "step": 1549
    },
    {
      "epoch": 1.024454725710509,
      "grad_norm": 0.6242079734802246,
      "learning_rate": 3.7194315928618642e-06,
      "loss": 0.2287,
      "step": 1550
    },
    {
      "epoch": 1.0251156642432253,
      "grad_norm": 0.5643395781517029,
      "learning_rate": 3.7186054196959686e-06,
      "loss": 0.229,
      "step": 1551
    },
    {
      "epoch": 1.0257766027759418,
      "grad_norm": 0.6474025845527649,
      "learning_rate": 3.7177792465300733e-06,
      "loss": 0.2315,
      "step": 1552
    },
    {
      "epoch": 1.0264375413086584,
      "grad_norm": 0.5833259224891663,
      "learning_rate": 3.7169530733641772e-06,
      "loss": 0.1899,
      "step": 1553
    },
    {
      "epoch": 1.0270984798413747,
      "grad_norm": 0.562185525894165,
      "learning_rate": 3.716126900198282e-06,
      "loss": 0.1941,
      "step": 1554
    },
    {
      "epoch": 1.0277594183740912,
      "grad_norm": 0.585667073726654,
      "learning_rate": 3.7153007270323863e-06,
      "loss": 0.2129,
      "step": 1555
    },
    {
      "epoch": 1.0284203569068078,
      "grad_norm": 0.6037883162498474,
      "learning_rate": 3.7144745538664907e-06,
      "loss": 0.2313,
      "step": 1556
    },
    {
      "epoch": 1.029081295439524,
      "grad_norm": 0.602729320526123,
      "learning_rate": 3.713648380700595e-06,
      "loss": 0.1826,
      "step": 1557
    },
    {
      "epoch": 1.0297422339722406,
      "grad_norm": 0.4922776222229004,
      "learning_rate": 3.7128222075346997e-06,
      "loss": 0.2002,
      "step": 1558
    },
    {
      "epoch": 1.030403172504957,
      "grad_norm": 0.6014750003814697,
      "learning_rate": 3.7119960343688037e-06,
      "loss": 0.1972,
      "step": 1559
    },
    {
      "epoch": 1.0310641110376735,
      "grad_norm": 0.6729666590690613,
      "learning_rate": 3.7111698612029084e-06,
      "loss": 0.215,
      "step": 1560
    },
    {
      "epoch": 1.03172504957039,
      "grad_norm": 0.5914225578308105,
      "learning_rate": 3.710343688037013e-06,
      "loss": 0.2208,
      "step": 1561
    },
    {
      "epoch": 1.0323859881031063,
      "grad_norm": 0.5531665682792664,
      "learning_rate": 3.709517514871117e-06,
      "loss": 0.2211,
      "step": 1562
    },
    {
      "epoch": 1.0330469266358229,
      "grad_norm": 0.5462630391120911,
      "learning_rate": 3.708691341705222e-06,
      "loss": 0.1956,
      "step": 1563
    },
    {
      "epoch": 1.0337078651685394,
      "grad_norm": 0.5553183555603027,
      "learning_rate": 3.707865168539326e-06,
      "loss": 0.2265,
      "step": 1564
    },
    {
      "epoch": 1.0343688037012557,
      "grad_norm": 0.5894500017166138,
      "learning_rate": 3.7070389953734305e-06,
      "loss": 0.2102,
      "step": 1565
    },
    {
      "epoch": 1.0350297422339723,
      "grad_norm": 0.6260697841644287,
      "learning_rate": 3.706212822207535e-06,
      "loss": 0.2123,
      "step": 1566
    },
    {
      "epoch": 1.0356906807666888,
      "grad_norm": 0.5880975723266602,
      "learning_rate": 3.7053866490416396e-06,
      "loss": 0.2378,
      "step": 1567
    },
    {
      "epoch": 1.0363516192994051,
      "grad_norm": 0.6445395350456238,
      "learning_rate": 3.7045604758757435e-06,
      "loss": 0.2161,
      "step": 1568
    },
    {
      "epoch": 1.0370125578321217,
      "grad_norm": 0.5951797962188721,
      "learning_rate": 3.7037343027098483e-06,
      "loss": 0.1788,
      "step": 1569
    },
    {
      "epoch": 1.037673496364838,
      "grad_norm": 0.5969454646110535,
      "learning_rate": 3.702908129543953e-06,
      "loss": 0.2106,
      "step": 1570
    },
    {
      "epoch": 1.0383344348975545,
      "grad_norm": 0.5682795643806458,
      "learning_rate": 3.702081956378057e-06,
      "loss": 0.1847,
      "step": 1571
    },
    {
      "epoch": 1.038995373430271,
      "grad_norm": 0.5597900152206421,
      "learning_rate": 3.7012557832121617e-06,
      "loss": 0.2101,
      "step": 1572
    },
    {
      "epoch": 1.0396563119629874,
      "grad_norm": 0.5298036336898804,
      "learning_rate": 3.700429610046266e-06,
      "loss": 0.193,
      "step": 1573
    },
    {
      "epoch": 1.040317250495704,
      "grad_norm": 0.7531526684761047,
      "learning_rate": 3.6996034368803703e-06,
      "loss": 0.2497,
      "step": 1574
    },
    {
      "epoch": 1.0409781890284204,
      "grad_norm": 0.5186863541603088,
      "learning_rate": 3.6987772637144747e-06,
      "loss": 0.2139,
      "step": 1575
    },
    {
      "epoch": 1.0416391275611367,
      "grad_norm": 0.7293111681938171,
      "learning_rate": 3.6979510905485794e-06,
      "loss": 0.2246,
      "step": 1576
    },
    {
      "epoch": 1.0423000660938533,
      "grad_norm": 0.555016040802002,
      "learning_rate": 3.6971249173826838e-06,
      "loss": 0.2032,
      "step": 1577
    },
    {
      "epoch": 1.0429610046265698,
      "grad_norm": 0.5193015336990356,
      "learning_rate": 3.696298744216788e-06,
      "loss": 0.2032,
      "step": 1578
    },
    {
      "epoch": 1.0436219431592861,
      "grad_norm": 0.5205837488174438,
      "learning_rate": 3.6954725710508924e-06,
      "loss": 0.2025,
      "step": 1579
    },
    {
      "epoch": 1.0442828816920027,
      "grad_norm": 0.6249758005142212,
      "learning_rate": 3.694646397884997e-06,
      "loss": 0.2061,
      "step": 1580
    },
    {
      "epoch": 1.0449438202247192,
      "grad_norm": 0.5928835272789001,
      "learning_rate": 3.693820224719101e-06,
      "loss": 0.1959,
      "step": 1581
    },
    {
      "epoch": 1.0456047587574355,
      "grad_norm": 0.6311817169189453,
      "learning_rate": 3.692994051553206e-06,
      "loss": 0.214,
      "step": 1582
    },
    {
      "epoch": 1.046265697290152,
      "grad_norm": 0.5548747181892395,
      "learning_rate": 3.6921678783873106e-06,
      "loss": 0.2025,
      "step": 1583
    },
    {
      "epoch": 1.0469266358228684,
      "grad_norm": 0.7089025974273682,
      "learning_rate": 3.6913417052214145e-06,
      "loss": 0.2061,
      "step": 1584
    },
    {
      "epoch": 1.047587574355585,
      "grad_norm": 0.6384260654449463,
      "learning_rate": 3.6905155320555193e-06,
      "loss": 0.2195,
      "step": 1585
    },
    {
      "epoch": 1.0482485128883015,
      "grad_norm": 0.5890927910804749,
      "learning_rate": 3.6896893588896236e-06,
      "loss": 0.228,
      "step": 1586
    },
    {
      "epoch": 1.0489094514210178,
      "grad_norm": 0.5221992135047913,
      "learning_rate": 3.688863185723728e-06,
      "loss": 0.1959,
      "step": 1587
    },
    {
      "epoch": 1.0495703899537343,
      "grad_norm": 0.5491733551025391,
      "learning_rate": 3.6880370125578323e-06,
      "loss": 0.2182,
      "step": 1588
    },
    {
      "epoch": 1.0502313284864508,
      "grad_norm": 0.6797364950180054,
      "learning_rate": 3.687210839391937e-06,
      "loss": 0.2242,
      "step": 1589
    },
    {
      "epoch": 1.0508922670191672,
      "grad_norm": 0.7477773427963257,
      "learning_rate": 3.686384666226041e-06,
      "loss": 0.2021,
      "step": 1590
    },
    {
      "epoch": 1.0515532055518837,
      "grad_norm": 0.5059320330619812,
      "learning_rate": 3.6855584930601457e-06,
      "loss": 0.2249,
      "step": 1591
    },
    {
      "epoch": 1.0522141440846002,
      "grad_norm": 0.609815776348114,
      "learning_rate": 3.6847323198942505e-06,
      "loss": 0.2145,
      "step": 1592
    },
    {
      "epoch": 1.0528750826173165,
      "grad_norm": 0.5929213762283325,
      "learning_rate": 3.6839061467283544e-06,
      "loss": 0.2315,
      "step": 1593
    },
    {
      "epoch": 1.053536021150033,
      "grad_norm": 0.5376946330070496,
      "learning_rate": 3.683079973562459e-06,
      "loss": 0.1965,
      "step": 1594
    },
    {
      "epoch": 1.0541969596827494,
      "grad_norm": 0.5154600739479065,
      "learning_rate": 3.6822538003965635e-06,
      "loss": 0.1639,
      "step": 1595
    },
    {
      "epoch": 1.054857898215466,
      "grad_norm": 0.5263983607292175,
      "learning_rate": 3.681427627230668e-06,
      "loss": 0.185,
      "step": 1596
    },
    {
      "epoch": 1.0555188367481825,
      "grad_norm": 0.6087229251861572,
      "learning_rate": 3.680601454064772e-06,
      "loss": 0.2008,
      "step": 1597
    },
    {
      "epoch": 1.0561797752808988,
      "grad_norm": 0.5605419874191284,
      "learning_rate": 3.679775280898877e-06,
      "loss": 0.2315,
      "step": 1598
    },
    {
      "epoch": 1.0568407138136153,
      "grad_norm": 0.5531878471374512,
      "learning_rate": 3.678949107732981e-06,
      "loss": 0.1971,
      "step": 1599
    },
    {
      "epoch": 1.0575016523463319,
      "grad_norm": 0.6072105169296265,
      "learning_rate": 3.6781229345670856e-06,
      "loss": 0.2078,
      "step": 1600
    },
    {
      "epoch": 1.0581625908790482,
      "grad_norm": 0.5490005612373352,
      "learning_rate": 3.67729676140119e-06,
      "loss": 0.2206,
      "step": 1601
    },
    {
      "epoch": 1.0588235294117647,
      "grad_norm": 0.4950808882713318,
      "learning_rate": 3.6764705882352946e-06,
      "loss": 0.1955,
      "step": 1602
    },
    {
      "epoch": 1.0594844679444813,
      "grad_norm": 0.5587524175643921,
      "learning_rate": 3.6756444150693986e-06,
      "loss": 0.2029,
      "step": 1603
    },
    {
      "epoch": 1.0601454064771976,
      "grad_norm": 0.5495462417602539,
      "learning_rate": 3.6748182419035033e-06,
      "loss": 0.2174,
      "step": 1604
    },
    {
      "epoch": 1.060806345009914,
      "grad_norm": 0.5615456700325012,
      "learning_rate": 3.673992068737608e-06,
      "loss": 0.2158,
      "step": 1605
    },
    {
      "epoch": 1.0614672835426306,
      "grad_norm": 0.6525754928588867,
      "learning_rate": 3.673165895571712e-06,
      "loss": 0.1853,
      "step": 1606
    },
    {
      "epoch": 1.062128222075347,
      "grad_norm": 0.5427417159080505,
      "learning_rate": 3.6723397224058167e-06,
      "loss": 0.2111,
      "step": 1607
    },
    {
      "epoch": 1.0627891606080635,
      "grad_norm": 0.5533161163330078,
      "learning_rate": 3.671513549239921e-06,
      "loss": 0.2358,
      "step": 1608
    },
    {
      "epoch": 1.0634500991407798,
      "grad_norm": 0.5899496674537659,
      "learning_rate": 3.6706873760740254e-06,
      "loss": 0.1963,
      "step": 1609
    },
    {
      "epoch": 1.0641110376734964,
      "grad_norm": 0.5315117835998535,
      "learning_rate": 3.6698612029081297e-06,
      "loss": 0.2028,
      "step": 1610
    },
    {
      "epoch": 1.064771976206213,
      "grad_norm": 0.5047438144683838,
      "learning_rate": 3.6690350297422345e-06,
      "loss": 0.2215,
      "step": 1611
    },
    {
      "epoch": 1.0654329147389292,
      "grad_norm": 0.5631422400474548,
      "learning_rate": 3.6682088565763384e-06,
      "loss": 0.2244,
      "step": 1612
    },
    {
      "epoch": 1.0660938532716457,
      "grad_norm": 0.608470618724823,
      "learning_rate": 3.667382683410443e-06,
      "loss": 0.2026,
      "step": 1613
    },
    {
      "epoch": 1.0667547918043623,
      "grad_norm": 0.4922681450843811,
      "learning_rate": 3.666556510244548e-06,
      "loss": 0.2179,
      "step": 1614
    },
    {
      "epoch": 1.0674157303370786,
      "grad_norm": 0.5035455226898193,
      "learning_rate": 3.665730337078652e-06,
      "loss": 0.1972,
      "step": 1615
    },
    {
      "epoch": 1.0680766688697951,
      "grad_norm": 0.5397522449493408,
      "learning_rate": 3.6649041639127566e-06,
      "loss": 0.1881,
      "step": 1616
    },
    {
      "epoch": 1.0687376074025117,
      "grad_norm": 0.58916836977005,
      "learning_rate": 3.664077990746861e-06,
      "loss": 0.2522,
      "step": 1617
    },
    {
      "epoch": 1.069398545935228,
      "grad_norm": 0.6084567308425903,
      "learning_rate": 3.6632518175809652e-06,
      "loss": 0.1956,
      "step": 1618
    },
    {
      "epoch": 1.0700594844679445,
      "grad_norm": 0.5084460377693176,
      "learning_rate": 3.6624256444150696e-06,
      "loss": 0.1646,
      "step": 1619
    },
    {
      "epoch": 1.0707204230006608,
      "grad_norm": 0.5646106600761414,
      "learning_rate": 3.6615994712491743e-06,
      "loss": 0.2322,
      "step": 1620
    },
    {
      "epoch": 1.0713813615333774,
      "grad_norm": 0.5573828220367432,
      "learning_rate": 3.6607732980832782e-06,
      "loss": 0.1735,
      "step": 1621
    },
    {
      "epoch": 1.072042300066094,
      "grad_norm": 0.5481899380683899,
      "learning_rate": 3.659947124917383e-06,
      "loss": 0.1966,
      "step": 1622
    },
    {
      "epoch": 1.0727032385988102,
      "grad_norm": 0.6404050588607788,
      "learning_rate": 3.6591209517514873e-06,
      "loss": 0.2355,
      "step": 1623
    },
    {
      "epoch": 1.0733641771315268,
      "grad_norm": 0.5557939410209656,
      "learning_rate": 3.6582947785855917e-06,
      "loss": 0.2328,
      "step": 1624
    },
    {
      "epoch": 1.0740251156642433,
      "grad_norm": 0.6165545582771301,
      "learning_rate": 3.657468605419696e-06,
      "loss": 0.2284,
      "step": 1625
    },
    {
      "epoch": 1.0746860541969596,
      "grad_norm": 0.5296944975852966,
      "learning_rate": 3.6566424322538008e-06,
      "loss": 0.1885,
      "step": 1626
    },
    {
      "epoch": 1.0753469927296762,
      "grad_norm": 0.47762084007263184,
      "learning_rate": 3.6558162590879055e-06,
      "loss": 0.1928,
      "step": 1627
    },
    {
      "epoch": 1.0760079312623927,
      "grad_norm": 0.605551540851593,
      "learning_rate": 3.6549900859220094e-06,
      "loss": 0.2157,
      "step": 1628
    },
    {
      "epoch": 1.076668869795109,
      "grad_norm": 0.630622923374176,
      "learning_rate": 3.654163912756114e-06,
      "loss": 0.1989,
      "step": 1629
    },
    {
      "epoch": 1.0773298083278255,
      "grad_norm": 0.5524895191192627,
      "learning_rate": 3.6533377395902185e-06,
      "loss": 0.1807,
      "step": 1630
    },
    {
      "epoch": 1.077990746860542,
      "grad_norm": 0.5486049652099609,
      "learning_rate": 3.652511566424323e-06,
      "loss": 0.1904,
      "step": 1631
    },
    {
      "epoch": 1.0786516853932584,
      "grad_norm": 0.522445559501648,
      "learning_rate": 3.651685393258427e-06,
      "loss": 0.1937,
      "step": 1632
    },
    {
      "epoch": 1.079312623925975,
      "grad_norm": 0.5284742116928101,
      "learning_rate": 3.650859220092532e-06,
      "loss": 0.211,
      "step": 1633
    },
    {
      "epoch": 1.0799735624586912,
      "grad_norm": 0.506433367729187,
      "learning_rate": 3.650033046926636e-06,
      "loss": 0.1915,
      "step": 1634
    },
    {
      "epoch": 1.0806345009914078,
      "grad_norm": 0.5679895281791687,
      "learning_rate": 3.6492068737607406e-06,
      "loss": 0.1793,
      "step": 1635
    },
    {
      "epoch": 1.0812954395241243,
      "grad_norm": 0.4918522834777832,
      "learning_rate": 3.6483807005948454e-06,
      "loss": 0.2205,
      "step": 1636
    },
    {
      "epoch": 1.0819563780568406,
      "grad_norm": 0.5987423658370972,
      "learning_rate": 3.6475545274289493e-06,
      "loss": 0.1979,
      "step": 1637
    },
    {
      "epoch": 1.0826173165895572,
      "grad_norm": 0.5345664024353027,
      "learning_rate": 3.646728354263054e-06,
      "loss": 0.1852,
      "step": 1638
    },
    {
      "epoch": 1.0832782551222737,
      "grad_norm": 0.552092432975769,
      "learning_rate": 3.6459021810971584e-06,
      "loss": 0.208,
      "step": 1639
    },
    {
      "epoch": 1.08393919365499,
      "grad_norm": 0.6421257257461548,
      "learning_rate": 3.6450760079312627e-06,
      "loss": 0.2,
      "step": 1640
    },
    {
      "epoch": 1.0846001321877066,
      "grad_norm": 0.5872278213500977,
      "learning_rate": 3.644249834765367e-06,
      "loss": 0.1722,
      "step": 1641
    },
    {
      "epoch": 1.085261070720423,
      "grad_norm": 0.5598965883255005,
      "learning_rate": 3.6434236615994718e-06,
      "loss": 0.2082,
      "step": 1642
    },
    {
      "epoch": 1.0859220092531394,
      "grad_norm": 0.607260525226593,
      "learning_rate": 3.6425974884335757e-06,
      "loss": 0.1812,
      "step": 1643
    },
    {
      "epoch": 1.086582947785856,
      "grad_norm": 0.5803655385971069,
      "learning_rate": 3.6417713152676804e-06,
      "loss": 0.2174,
      "step": 1644
    },
    {
      "epoch": 1.0872438863185723,
      "grad_norm": 0.6802850365638733,
      "learning_rate": 3.6409451421017848e-06,
      "loss": 0.2196,
      "step": 1645
    },
    {
      "epoch": 1.0879048248512888,
      "grad_norm": 0.532174289226532,
      "learning_rate": 3.640118968935889e-06,
      "loss": 0.1905,
      "step": 1646
    },
    {
      "epoch": 1.0885657633840053,
      "grad_norm": 0.5660184025764465,
      "learning_rate": 3.6392927957699934e-06,
      "loss": 0.2214,
      "step": 1647
    },
    {
      "epoch": 1.0892267019167217,
      "grad_norm": 0.5471447706222534,
      "learning_rate": 3.638466622604098e-06,
      "loss": 0.1936,
      "step": 1648
    },
    {
      "epoch": 1.0898876404494382,
      "grad_norm": 0.5467224717140198,
      "learning_rate": 3.637640449438202e-06,
      "loss": 0.2141,
      "step": 1649
    },
    {
      "epoch": 1.0905485789821547,
      "grad_norm": 0.5224032402038574,
      "learning_rate": 3.636814276272307e-06,
      "loss": 0.2184,
      "step": 1650
    },
    {
      "epoch": 1.091209517514871,
      "grad_norm": 0.7029022574424744,
      "learning_rate": 3.6359881031064116e-06,
      "loss": 0.2154,
      "step": 1651
    },
    {
      "epoch": 1.0918704560475876,
      "grad_norm": 0.584369421005249,
      "learning_rate": 3.635161929940516e-06,
      "loss": 0.1999,
      "step": 1652
    },
    {
      "epoch": 1.0925313945803041,
      "grad_norm": 0.5670763850212097,
      "learning_rate": 3.6343357567746203e-06,
      "loss": 0.1847,
      "step": 1653
    },
    {
      "epoch": 1.0931923331130204,
      "grad_norm": 0.528148353099823,
      "learning_rate": 3.6335095836087246e-06,
      "loss": 0.2045,
      "step": 1654
    },
    {
      "epoch": 1.093853271645737,
      "grad_norm": 0.6058679223060608,
      "learning_rate": 3.6326834104428294e-06,
      "loss": 0.1946,
      "step": 1655
    },
    {
      "epoch": 1.0945142101784535,
      "grad_norm": 0.634209930896759,
      "learning_rate": 3.6318572372769333e-06,
      "loss": 0.1763,
      "step": 1656
    },
    {
      "epoch": 1.0951751487111698,
      "grad_norm": 0.582754373550415,
      "learning_rate": 3.631031064111038e-06,
      "loss": 0.1645,
      "step": 1657
    },
    {
      "epoch": 1.0958360872438864,
      "grad_norm": 0.6742027401924133,
      "learning_rate": 3.630204890945143e-06,
      "loss": 0.207,
      "step": 1658
    },
    {
      "epoch": 1.0964970257766027,
      "grad_norm": 0.5547353625297546,
      "learning_rate": 3.6293787177792467e-06,
      "loss": 0.1935,
      "step": 1659
    },
    {
      "epoch": 1.0971579643093192,
      "grad_norm": 0.5458716750144958,
      "learning_rate": 3.6285525446133515e-06,
      "loss": 0.1727,
      "step": 1660
    },
    {
      "epoch": 1.0978189028420358,
      "grad_norm": 0.5858432650566101,
      "learning_rate": 3.627726371447456e-06,
      "loss": 0.179,
      "step": 1661
    },
    {
      "epoch": 1.098479841374752,
      "grad_norm": 0.5412603616714478,
      "learning_rate": 3.6269001982815597e-06,
      "loss": 0.2042,
      "step": 1662
    },
    {
      "epoch": 1.0991407799074686,
      "grad_norm": 0.5966442227363586,
      "learning_rate": 3.6260740251156645e-06,
      "loss": 0.2089,
      "step": 1663
    },
    {
      "epoch": 1.0998017184401851,
      "grad_norm": 0.5595484375953674,
      "learning_rate": 3.6252478519497692e-06,
      "loss": 0.1822,
      "step": 1664
    },
    {
      "epoch": 1.1004626569729015,
      "grad_norm": 0.6626802086830139,
      "learning_rate": 3.624421678783873e-06,
      "loss": 0.2089,
      "step": 1665
    },
    {
      "epoch": 1.101123595505618,
      "grad_norm": 0.6276155710220337,
      "learning_rate": 3.623595505617978e-06,
      "loss": 0.177,
      "step": 1666
    },
    {
      "epoch": 1.1017845340383343,
      "grad_norm": 0.6086190342903137,
      "learning_rate": 3.6227693324520822e-06,
      "loss": 0.208,
      "step": 1667
    },
    {
      "epoch": 1.1024454725710509,
      "grad_norm": 0.5895857810974121,
      "learning_rate": 3.6219431592861866e-06,
      "loss": 0.2286,
      "step": 1668
    },
    {
      "epoch": 1.1031064111037674,
      "grad_norm": 0.5719367861747742,
      "learning_rate": 3.621116986120291e-06,
      "loss": 0.2129,
      "step": 1669
    },
    {
      "epoch": 1.1037673496364837,
      "grad_norm": 0.5388264656066895,
      "learning_rate": 3.6202908129543957e-06,
      "loss": 0.1979,
      "step": 1670
    },
    {
      "epoch": 1.1044282881692002,
      "grad_norm": 0.5168271064758301,
      "learning_rate": 3.6194646397884996e-06,
      "loss": 0.1974,
      "step": 1671
    },
    {
      "epoch": 1.1050892267019168,
      "grad_norm": 0.5083076357841492,
      "learning_rate": 3.6186384666226043e-06,
      "loss": 0.1827,
      "step": 1672
    },
    {
      "epoch": 1.105750165234633,
      "grad_norm": 0.556398868560791,
      "learning_rate": 3.617812293456709e-06,
      "loss": 0.2071,
      "step": 1673
    },
    {
      "epoch": 1.1064111037673496,
      "grad_norm": 0.6457969546318054,
      "learning_rate": 3.616986120290813e-06,
      "loss": 0.2145,
      "step": 1674
    },
    {
      "epoch": 1.1070720423000662,
      "grad_norm": 0.6270151138305664,
      "learning_rate": 3.6161599471249177e-06,
      "loss": 0.1867,
      "step": 1675
    },
    {
      "epoch": 1.1077329808327825,
      "grad_norm": 0.53739333152771,
      "learning_rate": 3.615333773959022e-06,
      "loss": 0.2148,
      "step": 1676
    },
    {
      "epoch": 1.108393919365499,
      "grad_norm": 0.6274614334106445,
      "learning_rate": 3.614507600793127e-06,
      "loss": 0.1929,
      "step": 1677
    },
    {
      "epoch": 1.1090548578982156,
      "grad_norm": 0.5377930998802185,
      "learning_rate": 3.6136814276272307e-06,
      "loss": 0.1859,
      "step": 1678
    },
    {
      "epoch": 1.1097157964309319,
      "grad_norm": 0.5403597950935364,
      "learning_rate": 3.6128552544613355e-06,
      "loss": 0.2119,
      "step": 1679
    },
    {
      "epoch": 1.1103767349636484,
      "grad_norm": 0.5393192172050476,
      "learning_rate": 3.6120290812954403e-06,
      "loss": 0.2032,
      "step": 1680
    },
    {
      "epoch": 1.111037673496365,
      "grad_norm": 0.5559558868408203,
      "learning_rate": 3.611202908129544e-06,
      "loss": 0.1828,
      "step": 1681
    },
    {
      "epoch": 1.1116986120290813,
      "grad_norm": 0.5239936113357544,
      "learning_rate": 3.610376734963649e-06,
      "loss": 0.1812,
      "step": 1682
    },
    {
      "epoch": 1.1123595505617978,
      "grad_norm": 0.6667336225509644,
      "learning_rate": 3.6095505617977533e-06,
      "loss": 0.2303,
      "step": 1683
    },
    {
      "epoch": 1.1130204890945141,
      "grad_norm": 0.5394937992095947,
      "learning_rate": 3.608724388631857e-06,
      "loss": 0.1799,
      "step": 1684
    },
    {
      "epoch": 1.1136814276272307,
      "grad_norm": 0.5567249059677124,
      "learning_rate": 3.607898215465962e-06,
      "loss": 0.1944,
      "step": 1685
    },
    {
      "epoch": 1.1143423661599472,
      "grad_norm": 0.5486814379692078,
      "learning_rate": 3.6070720423000667e-06,
      "loss": 0.2078,
      "step": 1686
    },
    {
      "epoch": 1.1150033046926635,
      "grad_norm": 0.5045998096466064,
      "learning_rate": 3.6062458691341706e-06,
      "loss": 0.212,
      "step": 1687
    },
    {
      "epoch": 1.11566424322538,
      "grad_norm": 0.6244304180145264,
      "learning_rate": 3.6054196959682753e-06,
      "loss": 0.2008,
      "step": 1688
    },
    {
      "epoch": 1.1163251817580966,
      "grad_norm": 0.5391969680786133,
      "learning_rate": 3.6045935228023797e-06,
      "loss": 0.1654,
      "step": 1689
    },
    {
      "epoch": 1.116986120290813,
      "grad_norm": 0.5857031941413879,
      "learning_rate": 3.603767349636484e-06,
      "loss": 0.2048,
      "step": 1690
    },
    {
      "epoch": 1.1176470588235294,
      "grad_norm": 0.5162105560302734,
      "learning_rate": 3.6029411764705883e-06,
      "loss": 0.1603,
      "step": 1691
    },
    {
      "epoch": 1.1183079973562458,
      "grad_norm": 0.6182182431221008,
      "learning_rate": 3.602115003304693e-06,
      "loss": 0.2194,
      "step": 1692
    },
    {
      "epoch": 1.1189689358889623,
      "grad_norm": 0.5490562915802002,
      "learning_rate": 3.601288830138797e-06,
      "loss": 0.1732,
      "step": 1693
    },
    {
      "epoch": 1.1196298744216788,
      "grad_norm": 0.4922243654727936,
      "learning_rate": 3.6004626569729018e-06,
      "loss": 0.1942,
      "step": 1694
    },
    {
      "epoch": 1.1202908129543951,
      "grad_norm": 0.5166276693344116,
      "learning_rate": 3.5996364838070065e-06,
      "loss": 0.1844,
      "step": 1695
    },
    {
      "epoch": 1.1209517514871117,
      "grad_norm": 0.537379801273346,
      "learning_rate": 3.5988103106411104e-06,
      "loss": 0.169,
      "step": 1696
    },
    {
      "epoch": 1.1216126900198282,
      "grad_norm": 0.6152541637420654,
      "learning_rate": 3.597984137475215e-06,
      "loss": 0.1973,
      "step": 1697
    },
    {
      "epoch": 1.1222736285525445,
      "grad_norm": 0.5850677490234375,
      "learning_rate": 3.5971579643093195e-06,
      "loss": 0.1901,
      "step": 1698
    },
    {
      "epoch": 1.122934567085261,
      "grad_norm": 0.5321424603462219,
      "learning_rate": 3.596331791143424e-06,
      "loss": 0.2062,
      "step": 1699
    },
    {
      "epoch": 1.1235955056179776,
      "grad_norm": 0.5282427072525024,
      "learning_rate": 3.595505617977528e-06,
      "loss": 0.2019,
      "step": 1700
    },
    {
      "epoch": 1.124256444150694,
      "grad_norm": 0.5627943873405457,
      "learning_rate": 3.594679444811633e-06,
      "loss": 0.2099,
      "step": 1701
    },
    {
      "epoch": 1.1249173826834105,
      "grad_norm": 0.5857518315315247,
      "learning_rate": 3.5938532716457377e-06,
      "loss": 0.1789,
      "step": 1702
    },
    {
      "epoch": 1.125578321216127,
      "grad_norm": 0.5313911437988281,
      "learning_rate": 3.5930270984798416e-06,
      "loss": 0.1959,
      "step": 1703
    },
    {
      "epoch": 1.1262392597488433,
      "grad_norm": 0.4988425672054291,
      "learning_rate": 3.592200925313946e-06,
      "loss": 0.1834,
      "step": 1704
    },
    {
      "epoch": 1.1269001982815599,
      "grad_norm": 0.5872588157653809,
      "learning_rate": 3.5913747521480507e-06,
      "loss": 0.2112,
      "step": 1705
    },
    {
      "epoch": 1.1275611368142764,
      "grad_norm": 0.5376350283622742,
      "learning_rate": 3.5905485789821546e-06,
      "loss": 0.1683,
      "step": 1706
    },
    {
      "epoch": 1.1282220753469927,
      "grad_norm": 0.530545175075531,
      "learning_rate": 3.5897224058162594e-06,
      "loss": 0.1747,
      "step": 1707
    },
    {
      "epoch": 1.1288830138797092,
      "grad_norm": 0.6104812026023865,
      "learning_rate": 3.588896232650364e-06,
      "loss": 0.1953,
      "step": 1708
    },
    {
      "epoch": 1.1295439524124256,
      "grad_norm": 0.5538699626922607,
      "learning_rate": 3.588070059484468e-06,
      "loss": 0.1852,
      "step": 1709
    },
    {
      "epoch": 1.130204890945142,
      "grad_norm": 0.5098825097084045,
      "learning_rate": 3.587243886318573e-06,
      "loss": 0.1749,
      "step": 1710
    },
    {
      "epoch": 1.1308658294778586,
      "grad_norm": 0.5199739933013916,
      "learning_rate": 3.586417713152677e-06,
      "loss": 0.2102,
      "step": 1711
    },
    {
      "epoch": 1.131526768010575,
      "grad_norm": 0.5397120118141174,
      "learning_rate": 3.5855915399867815e-06,
      "loss": 0.2118,
      "step": 1712
    },
    {
      "epoch": 1.1321877065432915,
      "grad_norm": 0.5906826853752136,
      "learning_rate": 3.584765366820886e-06,
      "loss": 0.1818,
      "step": 1713
    },
    {
      "epoch": 1.132848645076008,
      "grad_norm": 0.5648054480552673,
      "learning_rate": 3.5839391936549905e-06,
      "loss": 0.1775,
      "step": 1714
    },
    {
      "epoch": 1.1335095836087243,
      "grad_norm": 0.6549400687217712,
      "learning_rate": 3.5831130204890945e-06,
      "loss": 0.1898,
      "step": 1715
    },
    {
      "epoch": 1.1341705221414409,
      "grad_norm": 0.49375587701797485,
      "learning_rate": 3.5822868473231992e-06,
      "loss": 0.2019,
      "step": 1716
    },
    {
      "epoch": 1.1348314606741572,
      "grad_norm": 0.5280765891075134,
      "learning_rate": 3.581460674157304e-06,
      "loss": 0.2008,
      "step": 1717
    },
    {
      "epoch": 1.1354923992068737,
      "grad_norm": 0.637541651725769,
      "learning_rate": 3.580634500991408e-06,
      "loss": 0.1532,
      "step": 1718
    },
    {
      "epoch": 1.1361533377395903,
      "grad_norm": 0.5182451605796814,
      "learning_rate": 3.5798083278255126e-06,
      "loss": 0.178,
      "step": 1719
    },
    {
      "epoch": 1.1368142762723066,
      "grad_norm": 0.5604094862937927,
      "learning_rate": 3.578982154659617e-06,
      "loss": 0.16,
      "step": 1720
    },
    {
      "epoch": 1.1374752148050231,
      "grad_norm": 0.518302857875824,
      "learning_rate": 3.5781559814937213e-06,
      "loss": 0.2087,
      "step": 1721
    },
    {
      "epoch": 1.1381361533377397,
      "grad_norm": 0.5684128403663635,
      "learning_rate": 3.5773298083278256e-06,
      "loss": 0.2013,
      "step": 1722
    },
    {
      "epoch": 1.138797091870456,
      "grad_norm": 0.5491957664489746,
      "learning_rate": 3.5765036351619304e-06,
      "loss": 0.1869,
      "step": 1723
    },
    {
      "epoch": 1.1394580304031725,
      "grad_norm": 0.5772728323936462,
      "learning_rate": 3.5756774619960347e-06,
      "loss": 0.202,
      "step": 1724
    },
    {
      "epoch": 1.140118968935889,
      "grad_norm": 0.5173880457878113,
      "learning_rate": 3.574851288830139e-06,
      "loss": 0.1631,
      "step": 1725
    },
    {
      "epoch": 1.1407799074686054,
      "grad_norm": 0.59038245677948,
      "learning_rate": 3.5740251156642434e-06,
      "loss": 0.1861,
      "step": 1726
    },
    {
      "epoch": 1.141440846001322,
      "grad_norm": 0.4510872960090637,
      "learning_rate": 3.573198942498348e-06,
      "loss": 0.1801,
      "step": 1727
    },
    {
      "epoch": 1.1421017845340384,
      "grad_norm": 0.5726613998413086,
      "learning_rate": 3.572372769332452e-06,
      "loss": 0.2071,
      "step": 1728
    },
    {
      "epoch": 1.1427627230667547,
      "grad_norm": 0.5454514622688293,
      "learning_rate": 3.571546596166557e-06,
      "loss": 0.1886,
      "step": 1729
    },
    {
      "epoch": 1.1434236615994713,
      "grad_norm": 0.5215843915939331,
      "learning_rate": 3.5707204230006616e-06,
      "loss": 0.1991,
      "step": 1730
    },
    {
      "epoch": 1.1440846001321878,
      "grad_norm": 0.4933355450630188,
      "learning_rate": 3.5698942498347655e-06,
      "loss": 0.2043,
      "step": 1731
    },
    {
      "epoch": 1.1447455386649041,
      "grad_norm": 0.5125530362129211,
      "learning_rate": 3.5690680766688702e-06,
      "loss": 0.2061,
      "step": 1732
    },
    {
      "epoch": 1.1454064771976207,
      "grad_norm": 0.6202940940856934,
      "learning_rate": 3.5682419035029746e-06,
      "loss": 0.199,
      "step": 1733
    },
    {
      "epoch": 1.146067415730337,
      "grad_norm": 0.5725690722465515,
      "learning_rate": 3.567415730337079e-06,
      "loss": 0.163,
      "step": 1734
    },
    {
      "epoch": 1.1467283542630535,
      "grad_norm": 0.5804643034934998,
      "learning_rate": 3.5665895571711832e-06,
      "loss": 0.1653,
      "step": 1735
    },
    {
      "epoch": 1.14738929279577,
      "grad_norm": 0.6033938527107239,
      "learning_rate": 3.565763384005288e-06,
      "loss": 0.1947,
      "step": 1736
    },
    {
      "epoch": 1.1480502313284864,
      "grad_norm": 0.6716550588607788,
      "learning_rate": 3.564937210839392e-06,
      "loss": 0.1821,
      "step": 1737
    },
    {
      "epoch": 1.148711169861203,
      "grad_norm": 0.5766230225563049,
      "learning_rate": 3.5641110376734967e-06,
      "loss": 0.2344,
      "step": 1738
    },
    {
      "epoch": 1.1493721083939195,
      "grad_norm": 0.5478010773658752,
      "learning_rate": 3.5632848645076014e-06,
      "loss": 0.2243,
      "step": 1739
    },
    {
      "epoch": 1.1500330469266358,
      "grad_norm": 0.5896551609039307,
      "learning_rate": 3.5624586913417053e-06,
      "loss": 0.2208,
      "step": 1740
    },
    {
      "epoch": 1.1506939854593523,
      "grad_norm": 0.5055689215660095,
      "learning_rate": 3.56163251817581e-06,
      "loss": 0.1999,
      "step": 1741
    },
    {
      "epoch": 1.1513549239920686,
      "grad_norm": 0.5929973721504211,
      "learning_rate": 3.5608063450099144e-06,
      "loss": 0.1739,
      "step": 1742
    },
    {
      "epoch": 1.1520158625247852,
      "grad_norm": 0.5672910809516907,
      "learning_rate": 3.5599801718440188e-06,
      "loss": 0.2097,
      "step": 1743
    },
    {
      "epoch": 1.1526768010575017,
      "grad_norm": 0.47975224256515503,
      "learning_rate": 3.559153998678123e-06,
      "loss": 0.1919,
      "step": 1744
    },
    {
      "epoch": 1.153337739590218,
      "grad_norm": 0.6002304553985596,
      "learning_rate": 3.558327825512228e-06,
      "loss": 0.1945,
      "step": 1745
    },
    {
      "epoch": 1.1539986781229346,
      "grad_norm": 0.6548110246658325,
      "learning_rate": 3.5575016523463318e-06,
      "loss": 0.1838,
      "step": 1746
    },
    {
      "epoch": 1.154659616655651,
      "grad_norm": 0.525297224521637,
      "learning_rate": 3.5566754791804365e-06,
      "loss": 0.1826,
      "step": 1747
    },
    {
      "epoch": 1.1553205551883674,
      "grad_norm": 0.6451539993286133,
      "learning_rate": 3.555849306014541e-06,
      "loss": 0.2077,
      "step": 1748
    },
    {
      "epoch": 1.155981493721084,
      "grad_norm": 0.6231678128242493,
      "learning_rate": 3.5550231328486456e-06,
      "loss": 0.1794,
      "step": 1749
    },
    {
      "epoch": 1.1566424322538005,
      "grad_norm": 0.553538978099823,
      "learning_rate": 3.5541969596827495e-06,
      "loss": 0.1835,
      "step": 1750
    },
    {
      "epoch": 1.1573033707865168,
      "grad_norm": 0.5132555365562439,
      "learning_rate": 3.5533707865168543e-06,
      "loss": 0.1676,
      "step": 1751
    },
    {
      "epoch": 1.1579643093192333,
      "grad_norm": 0.5605064630508423,
      "learning_rate": 3.552544613350959e-06,
      "loss": 0.1704,
      "step": 1752
    },
    {
      "epoch": 1.1586252478519499,
      "grad_norm": 0.6533176898956299,
      "learning_rate": 3.551718440185063e-06,
      "loss": 0.2292,
      "step": 1753
    },
    {
      "epoch": 1.1592861863846662,
      "grad_norm": 0.4672088623046875,
      "learning_rate": 3.5508922670191677e-06,
      "loss": 0.1796,
      "step": 1754
    },
    {
      "epoch": 1.1599471249173827,
      "grad_norm": 0.5678449869155884,
      "learning_rate": 3.550066093853272e-06,
      "loss": 0.1794,
      "step": 1755
    },
    {
      "epoch": 1.1606080634500993,
      "grad_norm": 0.4981808066368103,
      "learning_rate": 3.5492399206873764e-06,
      "loss": 0.1762,
      "step": 1756
    },
    {
      "epoch": 1.1612690019828156,
      "grad_norm": 0.6205058097839355,
      "learning_rate": 3.5484137475214807e-06,
      "loss": 0.1914,
      "step": 1757
    },
    {
      "epoch": 1.1619299405155321,
      "grad_norm": 0.623566746711731,
      "learning_rate": 3.5475875743555854e-06,
      "loss": 0.2018,
      "step": 1758
    },
    {
      "epoch": 1.1625908790482484,
      "grad_norm": 0.5490078330039978,
      "learning_rate": 3.5467614011896894e-06,
      "loss": 0.1986,
      "step": 1759
    },
    {
      "epoch": 1.163251817580965,
      "grad_norm": 0.6386075615882874,
      "learning_rate": 3.545935228023794e-06,
      "loss": 0.1926,
      "step": 1760
    },
    {
      "epoch": 1.1639127561136815,
      "grad_norm": 0.5537547469139099,
      "learning_rate": 3.545109054857899e-06,
      "loss": 0.1777,
      "step": 1761
    },
    {
      "epoch": 1.1645736946463978,
      "grad_norm": 0.6172835230827332,
      "learning_rate": 3.5442828816920028e-06,
      "loss": 0.2101,
      "step": 1762
    },
    {
      "epoch": 1.1652346331791144,
      "grad_norm": 0.5562485456466675,
      "learning_rate": 3.5434567085261075e-06,
      "loss": 0.1967,
      "step": 1763
    },
    {
      "epoch": 1.165895571711831,
      "grad_norm": 0.5447400808334351,
      "learning_rate": 3.542630535360212e-06,
      "loss": 0.1606,
      "step": 1764
    },
    {
      "epoch": 1.1665565102445472,
      "grad_norm": 0.5597674250602722,
      "learning_rate": 3.541804362194316e-06,
      "loss": 0.1815,
      "step": 1765
    },
    {
      "epoch": 1.1672174487772637,
      "grad_norm": 0.6221845149993896,
      "learning_rate": 3.5409781890284205e-06,
      "loss": 0.2212,
      "step": 1766
    },
    {
      "epoch": 1.16787838730998,
      "grad_norm": 0.48735055327415466,
      "learning_rate": 3.5401520158625253e-06,
      "loss": 0.1539,
      "step": 1767
    },
    {
      "epoch": 1.1685393258426966,
      "grad_norm": 0.4905228614807129,
      "learning_rate": 3.539325842696629e-06,
      "loss": 0.1942,
      "step": 1768
    },
    {
      "epoch": 1.1692002643754131,
      "grad_norm": 0.4653489589691162,
      "learning_rate": 3.538499669530734e-06,
      "loss": 0.1753,
      "step": 1769
    },
    {
      "epoch": 1.1698612029081294,
      "grad_norm": 0.6382767558097839,
      "learning_rate": 3.5376734963648383e-06,
      "loss": 0.2388,
      "step": 1770
    },
    {
      "epoch": 1.170522141440846,
      "grad_norm": 0.5242894887924194,
      "learning_rate": 3.5368473231989426e-06,
      "loss": 0.1617,
      "step": 1771
    },
    {
      "epoch": 1.1711830799735625,
      "grad_norm": 0.5272377729415894,
      "learning_rate": 3.536021150033047e-06,
      "loss": 0.2031,
      "step": 1772
    },
    {
      "epoch": 1.1718440185062788,
      "grad_norm": 0.6763880252838135,
      "learning_rate": 3.5351949768671517e-06,
      "loss": 0.1944,
      "step": 1773
    },
    {
      "epoch": 1.1725049570389954,
      "grad_norm": 0.4898100793361664,
      "learning_rate": 3.5343688037012565e-06,
      "loss": 0.1682,
      "step": 1774
    },
    {
      "epoch": 1.173165895571712,
      "grad_norm": 0.48850250244140625,
      "learning_rate": 3.5335426305353604e-06,
      "loss": 0.1915,
      "step": 1775
    },
    {
      "epoch": 1.1738268341044282,
      "grad_norm": 0.5846794247627258,
      "learning_rate": 3.532716457369465e-06,
      "loss": 0.2002,
      "step": 1776
    },
    {
      "epoch": 1.1744877726371448,
      "grad_norm": 0.57242351770401,
      "learning_rate": 3.5318902842035695e-06,
      "loss": 0.1996,
      "step": 1777
    },
    {
      "epoch": 1.1751487111698613,
      "grad_norm": 0.489016592502594,
      "learning_rate": 3.531064111037674e-06,
      "loss": 0.1834,
      "step": 1778
    },
    {
      "epoch": 1.1758096497025776,
      "grad_norm": 0.6033793687820435,
      "learning_rate": 3.530237937871778e-06,
      "loss": 0.1813,
      "step": 1779
    },
    {
      "epoch": 1.1764705882352942,
      "grad_norm": 0.5660027861595154,
      "learning_rate": 3.529411764705883e-06,
      "loss": 0.1947,
      "step": 1780
    },
    {
      "epoch": 1.1771315267680107,
      "grad_norm": 0.5605899095535278,
      "learning_rate": 3.528585591539987e-06,
      "loss": 0.1931,
      "step": 1781
    },
    {
      "epoch": 1.177792465300727,
      "grad_norm": 0.5851501226425171,
      "learning_rate": 3.5277594183740916e-06,
      "loss": 0.1903,
      "step": 1782
    },
    {
      "epoch": 1.1784534038334435,
      "grad_norm": 0.5349471569061279,
      "learning_rate": 3.5269332452081963e-06,
      "loss": 0.2109,
      "step": 1783
    },
    {
      "epoch": 1.1791143423661599,
      "grad_norm": 0.6171981692314148,
      "learning_rate": 3.5261070720423002e-06,
      "loss": 0.2328,
      "step": 1784
    },
    {
      "epoch": 1.1797752808988764,
      "grad_norm": 0.728152334690094,
      "learning_rate": 3.525280898876405e-06,
      "loss": 0.1687,
      "step": 1785
    },
    {
      "epoch": 1.180436219431593,
      "grad_norm": 0.5489904880523682,
      "learning_rate": 3.5244547257105093e-06,
      "loss": 0.1728,
      "step": 1786
    },
    {
      "epoch": 1.1810971579643093,
      "grad_norm": 0.5387445092201233,
      "learning_rate": 3.5236285525446136e-06,
      "loss": 0.1975,
      "step": 1787
    },
    {
      "epoch": 1.1817580964970258,
      "grad_norm": 0.5052992701530457,
      "learning_rate": 3.522802379378718e-06,
      "loss": 0.186,
      "step": 1788
    },
    {
      "epoch": 1.1824190350297423,
      "grad_norm": 0.5676986575126648,
      "learning_rate": 3.5219762062128227e-06,
      "loss": 0.1866,
      "step": 1789
    },
    {
      "epoch": 1.1830799735624586,
      "grad_norm": 0.5233434438705444,
      "learning_rate": 3.5211500330469266e-06,
      "loss": 0.1796,
      "step": 1790
    },
    {
      "epoch": 1.1837409120951752,
      "grad_norm": 0.5902861952781677,
      "learning_rate": 3.5203238598810314e-06,
      "loss": 0.1841,
      "step": 1791
    },
    {
      "epoch": 1.1844018506278915,
      "grad_norm": 0.49286800622940063,
      "learning_rate": 3.5194976867151357e-06,
      "loss": 0.1696,
      "step": 1792
    },
    {
      "epoch": 1.185062789160608,
      "grad_norm": 0.5320985913276672,
      "learning_rate": 3.51867151354924e-06,
      "loss": 0.1932,
      "step": 1793
    },
    {
      "epoch": 1.1857237276933246,
      "grad_norm": 0.5383870005607605,
      "learning_rate": 3.5178453403833444e-06,
      "loss": 0.1665,
      "step": 1794
    },
    {
      "epoch": 1.1863846662260409,
      "grad_norm": 0.6794142723083496,
      "learning_rate": 3.517019167217449e-06,
      "loss": 0.1955,
      "step": 1795
    },
    {
      "epoch": 1.1870456047587574,
      "grad_norm": 0.6098384261131287,
      "learning_rate": 3.516192994051553e-06,
      "loss": 0.1691,
      "step": 1796
    },
    {
      "epoch": 1.187706543291474,
      "grad_norm": 0.6740759611129761,
      "learning_rate": 3.515366820885658e-06,
      "loss": 0.187,
      "step": 1797
    },
    {
      "epoch": 1.1883674818241903,
      "grad_norm": 0.5834510326385498,
      "learning_rate": 3.5145406477197626e-06,
      "loss": 0.221,
      "step": 1798
    },
    {
      "epoch": 1.1890284203569068,
      "grad_norm": 0.5379936695098877,
      "learning_rate": 3.513714474553867e-06,
      "loss": 0.1926,
      "step": 1799
    },
    {
      "epoch": 1.1896893588896233,
      "grad_norm": 0.5066941976547241,
      "learning_rate": 3.5128883013879712e-06,
      "loss": 0.2007,
      "step": 1800
    },
    {
      "epoch": 1.1903502974223397,
      "grad_norm": 0.5620088577270508,
      "learning_rate": 3.5120621282220756e-06,
      "loss": 0.1758,
      "step": 1801
    },
    {
      "epoch": 1.1910112359550562,
      "grad_norm": 0.5772685408592224,
      "learning_rate": 3.5112359550561803e-06,
      "loss": 0.1763,
      "step": 1802
    },
    {
      "epoch": 1.1916721744877727,
      "grad_norm": 0.6013671159744263,
      "learning_rate": 3.5104097818902842e-06,
      "loss": 0.2091,
      "step": 1803
    },
    {
      "epoch": 1.192333113020489,
      "grad_norm": 0.672526478767395,
      "learning_rate": 3.509583608724389e-06,
      "loss": 0.1887,
      "step": 1804
    },
    {
      "epoch": 1.1929940515532056,
      "grad_norm": 0.5578392744064331,
      "learning_rate": 3.5087574355584938e-06,
      "loss": 0.1634,
      "step": 1805
    },
    {
      "epoch": 1.1936549900859221,
      "grad_norm": 0.5629353523254395,
      "learning_rate": 3.5079312623925977e-06,
      "loss": 0.175,
      "step": 1806
    },
    {
      "epoch": 1.1943159286186384,
      "grad_norm": 0.5895768404006958,
      "learning_rate": 3.5071050892267024e-06,
      "loss": 0.2198,
      "step": 1807
    },
    {
      "epoch": 1.194976867151355,
      "grad_norm": 0.5494603514671326,
      "learning_rate": 3.5062789160608068e-06,
      "loss": 0.1722,
      "step": 1808
    },
    {
      "epoch": 1.1956378056840713,
      "grad_norm": 0.6208369135856628,
      "learning_rate": 3.505452742894911e-06,
      "loss": 0.2107,
      "step": 1809
    },
    {
      "epoch": 1.1962987442167878,
      "grad_norm": 0.6702741384506226,
      "learning_rate": 3.5046265697290154e-06,
      "loss": 0.2105,
      "step": 1810
    },
    {
      "epoch": 1.1969596827495044,
      "grad_norm": 0.5579332709312439,
      "learning_rate": 3.50380039656312e-06,
      "loss": 0.1663,
      "step": 1811
    },
    {
      "epoch": 1.1976206212822207,
      "grad_norm": 0.5710148811340332,
      "learning_rate": 3.502974223397224e-06,
      "loss": 0.2147,
      "step": 1812
    },
    {
      "epoch": 1.1982815598149372,
      "grad_norm": 0.6177934408187866,
      "learning_rate": 3.502148050231329e-06,
      "loss": 0.1702,
      "step": 1813
    },
    {
      "epoch": 1.1989424983476538,
      "grad_norm": 0.515031099319458,
      "learning_rate": 3.501321877065433e-06,
      "loss": 0.2077,
      "step": 1814
    },
    {
      "epoch": 1.19960343688037,
      "grad_norm": 0.492160826921463,
      "learning_rate": 3.5004957038995375e-06,
      "loss": 0.1816,
      "step": 1815
    },
    {
      "epoch": 1.2002643754130866,
      "grad_norm": 0.4834633767604828,
      "learning_rate": 3.499669530733642e-06,
      "loss": 0.1851,
      "step": 1816
    },
    {
      "epoch": 1.200925313945803,
      "grad_norm": 0.5310917496681213,
      "learning_rate": 3.4988433575677466e-06,
      "loss": 0.1979,
      "step": 1817
    },
    {
      "epoch": 1.2015862524785195,
      "grad_norm": 0.46162843704223633,
      "learning_rate": 3.4980171844018505e-06,
      "loss": 0.1908,
      "step": 1818
    },
    {
      "epoch": 1.202247191011236,
      "grad_norm": 0.49851498007774353,
      "learning_rate": 3.4971910112359553e-06,
      "loss": 0.1722,
      "step": 1819
    },
    {
      "epoch": 1.2029081295439523,
      "grad_norm": 0.6480347514152527,
      "learning_rate": 3.49636483807006e-06,
      "loss": 0.1449,
      "step": 1820
    },
    {
      "epoch": 1.2035690680766689,
      "grad_norm": 0.5576220750808716,
      "learning_rate": 3.495538664904164e-06,
      "loss": 0.1884,
      "step": 1821
    },
    {
      "epoch": 1.2042300066093854,
      "grad_norm": 0.5662697553634644,
      "learning_rate": 3.4947124917382687e-06,
      "loss": 0.1995,
      "step": 1822
    },
    {
      "epoch": 1.2048909451421017,
      "grad_norm": 0.598405659198761,
      "learning_rate": 3.493886318572373e-06,
      "loss": 0.1831,
      "step": 1823
    },
    {
      "epoch": 1.2055518836748182,
      "grad_norm": 0.5106497406959534,
      "learning_rate": 3.4930601454064778e-06,
      "loss": 0.1757,
      "step": 1824
    },
    {
      "epoch": 1.2062128222075348,
      "grad_norm": 0.4688553810119629,
      "learning_rate": 3.4922339722405817e-06,
      "loss": 0.218,
      "step": 1825
    },
    {
      "epoch": 1.206873760740251,
      "grad_norm": 0.5284452438354492,
      "learning_rate": 3.4914077990746865e-06,
      "loss": 0.1851,
      "step": 1826
    },
    {
      "epoch": 1.2075346992729676,
      "grad_norm": 0.6288322806358337,
      "learning_rate": 3.490581625908791e-06,
      "loss": 0.1684,
      "step": 1827
    },
    {
      "epoch": 1.2081956378056842,
      "grad_norm": 0.5548178553581238,
      "learning_rate": 3.489755452742895e-06,
      "loss": 0.1681,
      "step": 1828
    },
    {
      "epoch": 1.2088565763384005,
      "grad_norm": 0.600468099117279,
      "learning_rate": 3.488929279577e-06,
      "loss": 0.1908,
      "step": 1829
    },
    {
      "epoch": 1.209517514871117,
      "grad_norm": 0.506548285484314,
      "learning_rate": 3.488103106411104e-06,
      "loss": 0.1723,
      "step": 1830
    },
    {
      "epoch": 1.2101784534038336,
      "grad_norm": 0.5455012917518616,
      "learning_rate": 3.487276933245208e-06,
      "loss": 0.1736,
      "step": 1831
    },
    {
      "epoch": 1.2108393919365499,
      "grad_norm": 0.552262544631958,
      "learning_rate": 3.486450760079313e-06,
      "loss": 0.1714,
      "step": 1832
    },
    {
      "epoch": 1.2115003304692664,
      "grad_norm": 0.4755154848098755,
      "learning_rate": 3.4856245869134176e-06,
      "loss": 0.1968,
      "step": 1833
    },
    {
      "epoch": 1.2121612690019827,
      "grad_norm": 0.5517954230308533,
      "learning_rate": 3.4847984137475215e-06,
      "loss": 0.1883,
      "step": 1834
    },
    {
      "epoch": 1.2128222075346993,
      "grad_norm": 0.6187676787376404,
      "learning_rate": 3.4839722405816263e-06,
      "loss": 0.161,
      "step": 1835
    },
    {
      "epoch": 1.2134831460674158,
      "grad_norm": 0.6287336349487305,
      "learning_rate": 3.4831460674157306e-06,
      "loss": 0.2026,
      "step": 1836
    },
    {
      "epoch": 1.2141440846001321,
      "grad_norm": 0.6077986359596252,
      "learning_rate": 3.482319894249835e-06,
      "loss": 0.1726,
      "step": 1837
    },
    {
      "epoch": 1.2148050231328487,
      "grad_norm": 0.4602915346622467,
      "learning_rate": 3.4814937210839393e-06,
      "loss": 0.1804,
      "step": 1838
    },
    {
      "epoch": 1.2154659616655652,
      "grad_norm": 0.4435085356235504,
      "learning_rate": 3.480667547918044e-06,
      "loss": 0.1644,
      "step": 1839
    },
    {
      "epoch": 1.2161269001982815,
      "grad_norm": 0.47710126638412476,
      "learning_rate": 3.479841374752148e-06,
      "loss": 0.1837,
      "step": 1840
    },
    {
      "epoch": 1.216787838730998,
      "grad_norm": 0.4758061468601227,
      "learning_rate": 3.4790152015862527e-06,
      "loss": 0.1631,
      "step": 1841
    },
    {
      "epoch": 1.2174487772637144,
      "grad_norm": 0.5325637459754944,
      "learning_rate": 3.4781890284203575e-06,
      "loss": 0.1938,
      "step": 1842
    },
    {
      "epoch": 1.218109715796431,
      "grad_norm": 0.5488409996032715,
      "learning_rate": 3.4773628552544614e-06,
      "loss": 0.1907,
      "step": 1843
    },
    {
      "epoch": 1.2187706543291474,
      "grad_norm": 0.5417855978012085,
      "learning_rate": 3.476536682088566e-06,
      "loss": 0.1947,
      "step": 1844
    },
    {
      "epoch": 1.2194315928618638,
      "grad_norm": 0.624118447303772,
      "learning_rate": 3.4757105089226705e-06,
      "loss": 0.2038,
      "step": 1845
    },
    {
      "epoch": 1.2200925313945803,
      "grad_norm": 0.5135505795478821,
      "learning_rate": 3.474884335756775e-06,
      "loss": 0.1851,
      "step": 1846
    },
    {
      "epoch": 1.2207534699272968,
      "grad_norm": 0.5252405405044556,
      "learning_rate": 3.474058162590879e-06,
      "loss": 0.1983,
      "step": 1847
    },
    {
      "epoch": 1.2214144084600131,
      "grad_norm": 0.4866640567779541,
      "learning_rate": 3.473231989424984e-06,
      "loss": 0.1507,
      "step": 1848
    },
    {
      "epoch": 1.2220753469927297,
      "grad_norm": 0.52247554063797,
      "learning_rate": 3.4724058162590887e-06,
      "loss": 0.1853,
      "step": 1849
    },
    {
      "epoch": 1.2227362855254462,
      "grad_norm": 0.46124327182769775,
      "learning_rate": 3.4715796430931926e-06,
      "loss": 0.1632,
      "step": 1850
    },
    {
      "epoch": 1.2233972240581625,
      "grad_norm": 0.49955785274505615,
      "learning_rate": 3.4707534699272973e-06,
      "loss": 0.1931,
      "step": 1851
    },
    {
      "epoch": 1.224058162590879,
      "grad_norm": 0.4879581928253174,
      "learning_rate": 3.4699272967614017e-06,
      "loss": 0.1593,
      "step": 1852
    },
    {
      "epoch": 1.2247191011235956,
      "grad_norm": 0.4997560679912567,
      "learning_rate": 3.4691011235955056e-06,
      "loss": 0.174,
      "step": 1853
    },
    {
      "epoch": 1.225380039656312,
      "grad_norm": 0.5961952209472656,
      "learning_rate": 3.4682749504296103e-06,
      "loss": 0.2364,
      "step": 1854
    },
    {
      "epoch": 1.2260409781890285,
      "grad_norm": 0.5737097859382629,
      "learning_rate": 3.467448777263715e-06,
      "loss": 0.1899,
      "step": 1855
    },
    {
      "epoch": 1.226701916721745,
      "grad_norm": 0.5599194765090942,
      "learning_rate": 3.466622604097819e-06,
      "loss": 0.1676,
      "step": 1856
    },
    {
      "epoch": 1.2273628552544613,
      "grad_norm": 0.5064544081687927,
      "learning_rate": 3.4657964309319237e-06,
      "loss": 0.1527,
      "step": 1857
    },
    {
      "epoch": 1.2280237937871779,
      "grad_norm": 0.512879490852356,
      "learning_rate": 3.464970257766028e-06,
      "loss": 0.1648,
      "step": 1858
    },
    {
      "epoch": 1.2286847323198942,
      "grad_norm": 0.5464978814125061,
      "learning_rate": 3.4641440846001324e-06,
      "loss": 0.1853,
      "step": 1859
    },
    {
      "epoch": 1.2293456708526107,
      "grad_norm": 0.6587740182876587,
      "learning_rate": 3.4633179114342367e-06,
      "loss": 0.1834,
      "step": 1860
    },
    {
      "epoch": 1.2300066093853272,
      "grad_norm": 0.5177667140960693,
      "learning_rate": 3.4624917382683415e-06,
      "loss": 0.1747,
      "step": 1861
    },
    {
      "epoch": 1.2306675479180436,
      "grad_norm": 0.5573622584342957,
      "learning_rate": 3.4616655651024454e-06,
      "loss": 0.1764,
      "step": 1862
    },
    {
      "epoch": 1.23132848645076,
      "grad_norm": 0.5584381222724915,
      "learning_rate": 3.46083939193655e-06,
      "loss": 0.1846,
      "step": 1863
    },
    {
      "epoch": 1.2319894249834766,
      "grad_norm": 0.6004858016967773,
      "learning_rate": 3.460013218770655e-06,
      "loss": 0.1983,
      "step": 1864
    },
    {
      "epoch": 1.232650363516193,
      "grad_norm": 0.5507422089576721,
      "learning_rate": 3.459187045604759e-06,
      "loss": 0.1889,
      "step": 1865
    },
    {
      "epoch": 1.2333113020489095,
      "grad_norm": 0.5062255263328552,
      "learning_rate": 3.4583608724388636e-06,
      "loss": 0.2041,
      "step": 1866
    },
    {
      "epoch": 1.2339722405816258,
      "grad_norm": 0.4439394474029541,
      "learning_rate": 3.457534699272968e-06,
      "loss": 0.1742,
      "step": 1867
    },
    {
      "epoch": 1.2346331791143423,
      "grad_norm": 0.48948371410369873,
      "learning_rate": 3.4567085261070723e-06,
      "loss": 0.1982,
      "step": 1868
    },
    {
      "epoch": 1.2352941176470589,
      "grad_norm": 0.5429707765579224,
      "learning_rate": 3.4558823529411766e-06,
      "loss": 0.2003,
      "step": 1869
    },
    {
      "epoch": 1.2359550561797752,
      "grad_norm": 0.6948951482772827,
      "learning_rate": 3.4550561797752813e-06,
      "loss": 0.2167,
      "step": 1870
    },
    {
      "epoch": 1.2366159947124917,
      "grad_norm": 0.502211332321167,
      "learning_rate": 3.4542300066093853e-06,
      "loss": 0.2143,
      "step": 1871
    },
    {
      "epoch": 1.2372769332452083,
      "grad_norm": 0.42990994453430176,
      "learning_rate": 3.45340383344349e-06,
      "loss": 0.1782,
      "step": 1872
    },
    {
      "epoch": 1.2379378717779246,
      "grad_norm": 0.5229389667510986,
      "learning_rate": 3.4525776602775943e-06,
      "loss": 0.1523,
      "step": 1873
    },
    {
      "epoch": 1.2385988103106411,
      "grad_norm": 0.6145617961883545,
      "learning_rate": 3.451751487111699e-06,
      "loss": 0.173,
      "step": 1874
    },
    {
      "epoch": 1.2392597488433577,
      "grad_norm": 0.5092983245849609,
      "learning_rate": 3.450925313945803e-06,
      "loss": 0.1511,
      "step": 1875
    },
    {
      "epoch": 1.239920687376074,
      "grad_norm": 0.5500515699386597,
      "learning_rate": 3.4500991407799078e-06,
      "loss": 0.1918,
      "step": 1876
    },
    {
      "epoch": 1.2405816259087905,
      "grad_norm": 0.49263668060302734,
      "learning_rate": 3.4492729676140125e-06,
      "loss": 0.1744,
      "step": 1877
    },
    {
      "epoch": 1.241242564441507,
      "grad_norm": 0.6621544361114502,
      "learning_rate": 3.4484467944481164e-06,
      "loss": 0.2157,
      "step": 1878
    },
    {
      "epoch": 1.2419035029742234,
      "grad_norm": 0.5380726456642151,
      "learning_rate": 3.447620621282221e-06,
      "loss": 0.1681,
      "step": 1879
    },
    {
      "epoch": 1.24256444150694,
      "grad_norm": 0.5953809022903442,
      "learning_rate": 3.4467944481163255e-06,
      "loss": 0.1617,
      "step": 1880
    },
    {
      "epoch": 1.2432253800396564,
      "grad_norm": 0.5399037003517151,
      "learning_rate": 3.44596827495043e-06,
      "loss": 0.1863,
      "step": 1881
    },
    {
      "epoch": 1.2438863185723728,
      "grad_norm": 0.6357309818267822,
      "learning_rate": 3.445142101784534e-06,
      "loss": 0.2029,
      "step": 1882
    },
    {
      "epoch": 1.2445472571050893,
      "grad_norm": 0.5325523018836975,
      "learning_rate": 3.444315928618639e-06,
      "loss": 0.1797,
      "step": 1883
    },
    {
      "epoch": 1.2452081956378056,
      "grad_norm": 0.47832024097442627,
      "learning_rate": 3.443489755452743e-06,
      "loss": 0.1671,
      "step": 1884
    },
    {
      "epoch": 1.2458691341705221,
      "grad_norm": 0.6309095025062561,
      "learning_rate": 3.4426635822868476e-06,
      "loss": 0.1604,
      "step": 1885
    },
    {
      "epoch": 1.2465300727032387,
      "grad_norm": 0.6403405666351318,
      "learning_rate": 3.4418374091209524e-06,
      "loss": 0.1791,
      "step": 1886
    },
    {
      "epoch": 1.247191011235955,
      "grad_norm": 0.5458558201789856,
      "learning_rate": 3.4410112359550563e-06,
      "loss": 0.198,
      "step": 1887
    },
    {
      "epoch": 1.2478519497686715,
      "grad_norm": 0.5940454006195068,
      "learning_rate": 3.440185062789161e-06,
      "loss": 0.1822,
      "step": 1888
    },
    {
      "epoch": 1.2485128883013878,
      "grad_norm": 0.4305691719055176,
      "learning_rate": 3.4393588896232654e-06,
      "loss": 0.1498,
      "step": 1889
    },
    {
      "epoch": 1.2491738268341044,
      "grad_norm": 0.5160983204841614,
      "learning_rate": 3.4385327164573697e-06,
      "loss": 0.1752,
      "step": 1890
    },
    {
      "epoch": 1.249834765366821,
      "grad_norm": 0.6241581439971924,
      "learning_rate": 3.437706543291474e-06,
      "loss": 0.1615,
      "step": 1891
    },
    {
      "epoch": 1.2504957038995372,
      "grad_norm": 0.5052729249000549,
      "learning_rate": 3.436880370125579e-06,
      "loss": 0.1778,
      "step": 1892
    },
    {
      "epoch": 1.2511566424322538,
      "grad_norm": 0.5723239779472351,
      "learning_rate": 3.4360541969596827e-06,
      "loss": 0.204,
      "step": 1893
    },
    {
      "epoch": 1.2518175809649703,
      "grad_norm": 0.5331043004989624,
      "learning_rate": 3.4352280237937875e-06,
      "loss": 0.1933,
      "step": 1894
    },
    {
      "epoch": 1.2524785194976866,
      "grad_norm": 0.5323217511177063,
      "learning_rate": 3.434401850627892e-06,
      "loss": 0.1981,
      "step": 1895
    },
    {
      "epoch": 1.2531394580304032,
      "grad_norm": 0.48946869373321533,
      "learning_rate": 3.433575677461996e-06,
      "loss": 0.1628,
      "step": 1896
    },
    {
      "epoch": 1.2538003965631197,
      "grad_norm": 0.48953133821487427,
      "learning_rate": 3.4327495042961005e-06,
      "loss": 0.2083,
      "step": 1897
    },
    {
      "epoch": 1.254461335095836,
      "grad_norm": 0.5150666832923889,
      "learning_rate": 3.4319233311302052e-06,
      "loss": 0.1513,
      "step": 1898
    },
    {
      "epoch": 1.2551222736285526,
      "grad_norm": 0.7859083414077759,
      "learning_rate": 3.43109715796431e-06,
      "loss": 0.2066,
      "step": 1899
    },
    {
      "epoch": 1.255783212161269,
      "grad_norm": 0.5879875421524048,
      "learning_rate": 3.430270984798414e-06,
      "loss": 0.1785,
      "step": 1900
    },
    {
      "epoch": 1.2564441506939854,
      "grad_norm": 0.5393428802490234,
      "learning_rate": 3.4294448116325186e-06,
      "loss": 0.2123,
      "step": 1901
    },
    {
      "epoch": 1.257105089226702,
      "grad_norm": 0.5426025390625,
      "learning_rate": 3.428618638466623e-06,
      "loss": 0.162,
      "step": 1902
    },
    {
      "epoch": 1.2577660277594185,
      "grad_norm": 0.5198522210121155,
      "learning_rate": 3.4277924653007273e-06,
      "loss": 0.1564,
      "step": 1903
    },
    {
      "epoch": 1.2584269662921348,
      "grad_norm": 0.5752913951873779,
      "learning_rate": 3.4269662921348316e-06,
      "loss": 0.171,
      "step": 1904
    },
    {
      "epoch": 1.2590879048248513,
      "grad_norm": 0.496182382106781,
      "learning_rate": 3.4261401189689364e-06,
      "loss": 0.1572,
      "step": 1905
    },
    {
      "epoch": 1.2597488433575679,
      "grad_norm": 0.5492073893547058,
      "learning_rate": 3.4253139458030403e-06,
      "loss": 0.1738,
      "step": 1906
    },
    {
      "epoch": 1.2604097818902842,
      "grad_norm": 0.5349553823471069,
      "learning_rate": 3.424487772637145e-06,
      "loss": 0.1855,
      "step": 1907
    },
    {
      "epoch": 1.2610707204230007,
      "grad_norm": 0.573468029499054,
      "learning_rate": 3.42366159947125e-06,
      "loss": 0.1336,
      "step": 1908
    },
    {
      "epoch": 1.261731658955717,
      "grad_norm": 0.5366078615188599,
      "learning_rate": 3.4228354263053537e-06,
      "loss": 0.206,
      "step": 1909
    },
    {
      "epoch": 1.2623925974884336,
      "grad_norm": 0.5534926056861877,
      "learning_rate": 3.4220092531394585e-06,
      "loss": 0.1713,
      "step": 1910
    },
    {
      "epoch": 1.2630535360211501,
      "grad_norm": 0.564404308795929,
      "learning_rate": 3.421183079973563e-06,
      "loss": 0.1555,
      "step": 1911
    },
    {
      "epoch": 1.2637144745538664,
      "grad_norm": 0.5126665830612183,
      "learning_rate": 3.420356906807667e-06,
      "loss": 0.1527,
      "step": 1912
    },
    {
      "epoch": 1.264375413086583,
      "grad_norm": 0.5006868839263916,
      "learning_rate": 3.4195307336417715e-06,
      "loss": 0.1594,
      "step": 1913
    },
    {
      "epoch": 1.2650363516192993,
      "grad_norm": 0.5498059391975403,
      "learning_rate": 3.4187045604758762e-06,
      "loss": 0.1859,
      "step": 1914
    },
    {
      "epoch": 1.2656972901520158,
      "grad_norm": 0.5887885093688965,
      "learning_rate": 3.41787838730998e-06,
      "loss": 0.1898,
      "step": 1915
    },
    {
      "epoch": 1.2663582286847324,
      "grad_norm": 0.5540927648544312,
      "learning_rate": 3.417052214144085e-06,
      "loss": 0.184,
      "step": 1916
    },
    {
      "epoch": 1.2670191672174487,
      "grad_norm": 0.5447502136230469,
      "learning_rate": 3.4162260409781892e-06,
      "loss": 0.1655,
      "step": 1917
    },
    {
      "epoch": 1.2676801057501652,
      "grad_norm": 0.5414884686470032,
      "learning_rate": 3.4153998678122936e-06,
      "loss": 0.1457,
      "step": 1918
    },
    {
      "epoch": 1.2683410442828817,
      "grad_norm": 0.5160375237464905,
      "learning_rate": 3.414573694646398e-06,
      "loss": 0.1667,
      "step": 1919
    },
    {
      "epoch": 1.269001982815598,
      "grad_norm": 0.4690624177455902,
      "learning_rate": 3.4137475214805027e-06,
      "loss": 0.1676,
      "step": 1920
    },
    {
      "epoch": 1.2696629213483146,
      "grad_norm": 0.4981704354286194,
      "learning_rate": 3.4129213483146066e-06,
      "loss": 0.1479,
      "step": 1921
    },
    {
      "epoch": 1.2703238598810311,
      "grad_norm": 0.4969944953918457,
      "learning_rate": 3.4120951751487113e-06,
      "loss": 0.1836,
      "step": 1922
    },
    {
      "epoch": 1.2709847984137475,
      "grad_norm": 0.5080903172492981,
      "learning_rate": 3.411269001982816e-06,
      "loss": 0.1621,
      "step": 1923
    },
    {
      "epoch": 1.271645736946464,
      "grad_norm": 0.5953225493431091,
      "learning_rate": 3.4104428288169204e-06,
      "loss": 0.1899,
      "step": 1924
    },
    {
      "epoch": 1.2723066754791805,
      "grad_norm": 0.5086445808410645,
      "learning_rate": 3.4096166556510248e-06,
      "loss": 0.1676,
      "step": 1925
    },
    {
      "epoch": 1.2729676140118968,
      "grad_norm": 0.5733029842376709,
      "learning_rate": 3.408790482485129e-06,
      "loss": 0.162,
      "step": 1926
    },
    {
      "epoch": 1.2736285525446134,
      "grad_norm": 0.5082610249519348,
      "learning_rate": 3.407964309319234e-06,
      "loss": 0.1729,
      "step": 1927
    },
    {
      "epoch": 1.27428949107733,
      "grad_norm": 0.5702828168869019,
      "learning_rate": 3.4071381361533378e-06,
      "loss": 0.189,
      "step": 1928
    },
    {
      "epoch": 1.2749504296100462,
      "grad_norm": 0.47793737053871155,
      "learning_rate": 3.4063119629874425e-06,
      "loss": 0.1824,
      "step": 1929
    },
    {
      "epoch": 1.2756113681427628,
      "grad_norm": 0.5798498392105103,
      "learning_rate": 3.4054857898215473e-06,
      "loss": 0.183,
      "step": 1930
    },
    {
      "epoch": 1.2762723066754793,
      "grad_norm": 0.6472989320755005,
      "learning_rate": 3.404659616655651e-06,
      "loss": 0.1594,
      "step": 1931
    },
    {
      "epoch": 1.2769332452081956,
      "grad_norm": 0.6754758358001709,
      "learning_rate": 3.403833443489756e-06,
      "loss": 0.1774,
      "step": 1932
    },
    {
      "epoch": 1.2775941837409122,
      "grad_norm": 0.5896613001823425,
      "learning_rate": 3.4030072703238603e-06,
      "loss": 0.1705,
      "step": 1933
    },
    {
      "epoch": 1.2782551222736285,
      "grad_norm": 0.5269129276275635,
      "learning_rate": 3.4021810971579646e-06,
      "loss": 0.1671,
      "step": 1934
    },
    {
      "epoch": 1.278916060806345,
      "grad_norm": 0.5113887786865234,
      "learning_rate": 3.401354923992069e-06,
      "loss": 0.1487,
      "step": 1935
    },
    {
      "epoch": 1.2795769993390615,
      "grad_norm": 0.49916595220565796,
      "learning_rate": 3.4005287508261737e-06,
      "loss": 0.1619,
      "step": 1936
    },
    {
      "epoch": 1.2802379378717779,
      "grad_norm": 0.4595169425010681,
      "learning_rate": 3.3997025776602776e-06,
      "loss": 0.1589,
      "step": 1937
    },
    {
      "epoch": 1.2808988764044944,
      "grad_norm": 0.481323778629303,
      "learning_rate": 3.3988764044943824e-06,
      "loss": 0.1391,
      "step": 1938
    },
    {
      "epoch": 1.2815598149372107,
      "grad_norm": 0.4734204411506653,
      "learning_rate": 3.3980502313284867e-06,
      "loss": 0.1692,
      "step": 1939
    },
    {
      "epoch": 1.2822207534699273,
      "grad_norm": 0.48946455121040344,
      "learning_rate": 3.397224058162591e-06,
      "loss": 0.1551,
      "step": 1940
    },
    {
      "epoch": 1.2828816920026438,
      "grad_norm": 0.5333616137504578,
      "learning_rate": 3.3963978849966954e-06,
      "loss": 0.1796,
      "step": 1941
    },
    {
      "epoch": 1.28354263053536,
      "grad_norm": 0.5248137712478638,
      "learning_rate": 3.3955717118308e-06,
      "loss": 0.1732,
      "step": 1942
    },
    {
      "epoch": 1.2842035690680766,
      "grad_norm": 0.6533976197242737,
      "learning_rate": 3.394745538664904e-06,
      "loss": 0.1624,
      "step": 1943
    },
    {
      "epoch": 1.2848645076007932,
      "grad_norm": 0.5870002508163452,
      "learning_rate": 3.3939193654990088e-06,
      "loss": 0.1579,
      "step": 1944
    },
    {
      "epoch": 1.2855254461335095,
      "grad_norm": 0.5610705018043518,
      "learning_rate": 3.3930931923331135e-06,
      "loss": 0.1674,
      "step": 1945
    },
    {
      "epoch": 1.286186384666226,
      "grad_norm": 0.5897876620292664,
      "learning_rate": 3.3922670191672174e-06,
      "loss": 0.1655,
      "step": 1946
    },
    {
      "epoch": 1.2868473231989426,
      "grad_norm": 0.63825923204422,
      "learning_rate": 3.391440846001322e-06,
      "loss": 0.1938,
      "step": 1947
    },
    {
      "epoch": 1.2875082617316589,
      "grad_norm": 0.5314756035804749,
      "learning_rate": 3.3906146728354265e-06,
      "loss": 0.195,
      "step": 1948
    },
    {
      "epoch": 1.2881692002643754,
      "grad_norm": 0.5615131855010986,
      "learning_rate": 3.3897884996695313e-06,
      "loss": 0.138,
      "step": 1949
    },
    {
      "epoch": 1.288830138797092,
      "grad_norm": 0.5598537921905518,
      "learning_rate": 3.388962326503635e-06,
      "loss": 0.1606,
      "step": 1950
    },
    {
      "epoch": 1.2894910773298083,
      "grad_norm": 0.545259952545166,
      "learning_rate": 3.38813615333774e-06,
      "loss": 0.1726,
      "step": 1951
    },
    {
      "epoch": 1.2901520158625248,
      "grad_norm": 0.5559274554252625,
      "learning_rate": 3.3873099801718447e-06,
      "loss": 0.1827,
      "step": 1952
    },
    {
      "epoch": 1.2908129543952414,
      "grad_norm": 0.5014683604240417,
      "learning_rate": 3.3864838070059486e-06,
      "loss": 0.1682,
      "step": 1953
    },
    {
      "epoch": 1.2914738929279577,
      "grad_norm": 0.5563486218452454,
      "learning_rate": 3.3856576338400534e-06,
      "loss": 0.2127,
      "step": 1954
    },
    {
      "epoch": 1.2921348314606742,
      "grad_norm": 0.6258285045623779,
      "learning_rate": 3.3848314606741577e-06,
      "loss": 0.1795,
      "step": 1955
    },
    {
      "epoch": 1.2927957699933907,
      "grad_norm": 0.8038773536682129,
      "learning_rate": 3.384005287508262e-06,
      "loss": 0.1771,
      "step": 1956
    },
    {
      "epoch": 1.293456708526107,
      "grad_norm": 0.48969489336013794,
      "learning_rate": 3.3831791143423664e-06,
      "loss": 0.1604,
      "step": 1957
    },
    {
      "epoch": 1.2941176470588236,
      "grad_norm": 0.6228007078170776,
      "learning_rate": 3.382352941176471e-06,
      "loss": 0.205,
      "step": 1958
    },
    {
      "epoch": 1.29477858559154,
      "grad_norm": 0.5093196034431458,
      "learning_rate": 3.381526768010575e-06,
      "loss": 0.1692,
      "step": 1959
    },
    {
      "epoch": 1.2954395241242564,
      "grad_norm": 0.5714803338050842,
      "learning_rate": 3.38070059484468e-06,
      "loss": 0.1894,
      "step": 1960
    },
    {
      "epoch": 1.296100462656973,
      "grad_norm": 0.47298941016197205,
      "learning_rate": 3.379874421678784e-06,
      "loss": 0.1685,
      "step": 1961
    },
    {
      "epoch": 1.2967614011896893,
      "grad_norm": 0.5418164730072021,
      "learning_rate": 3.3790482485128885e-06,
      "loss": 0.1612,
      "step": 1962
    },
    {
      "epoch": 1.2974223397224058,
      "grad_norm": 0.5337413549423218,
      "learning_rate": 3.378222075346993e-06,
      "loss": 0.1611,
      "step": 1963
    },
    {
      "epoch": 1.2980832782551222,
      "grad_norm": 0.4714619815349579,
      "learning_rate": 3.3773959021810976e-06,
      "loss": 0.1718,
      "step": 1964
    },
    {
      "epoch": 1.2987442167878387,
      "grad_norm": 0.5928655862808228,
      "learning_rate": 3.3765697290152015e-06,
      "loss": 0.1916,
      "step": 1965
    },
    {
      "epoch": 1.2994051553205552,
      "grad_norm": 0.49743372201919556,
      "learning_rate": 3.3757435558493062e-06,
      "loss": 0.1497,
      "step": 1966
    },
    {
      "epoch": 1.3000660938532715,
      "grad_norm": 0.5630819201469421,
      "learning_rate": 3.374917382683411e-06,
      "loss": 0.1855,
      "step": 1967
    },
    {
      "epoch": 1.300727032385988,
      "grad_norm": 0.5525755882263184,
      "learning_rate": 3.374091209517515e-06,
      "loss": 0.1917,
      "step": 1968
    },
    {
      "epoch": 1.3013879709187046,
      "grad_norm": 0.5267730951309204,
      "learning_rate": 3.3732650363516197e-06,
      "loss": 0.1915,
      "step": 1969
    },
    {
      "epoch": 1.302048909451421,
      "grad_norm": 0.4377240538597107,
      "learning_rate": 3.372438863185724e-06,
      "loss": 0.1268,
      "step": 1970
    },
    {
      "epoch": 1.3027098479841375,
      "grad_norm": 0.47238245606422424,
      "learning_rate": 3.3716126900198283e-06,
      "loss": 0.1653,
      "step": 1971
    },
    {
      "epoch": 1.303370786516854,
      "grad_norm": 0.5733052492141724,
      "learning_rate": 3.3707865168539327e-06,
      "loss": 0.1658,
      "step": 1972
    },
    {
      "epoch": 1.3040317250495703,
      "grad_norm": 0.5122539401054382,
      "learning_rate": 3.3699603436880374e-06,
      "loss": 0.145,
      "step": 1973
    },
    {
      "epoch": 1.3046926635822869,
      "grad_norm": 0.6004539132118225,
      "learning_rate": 3.369134170522142e-06,
      "loss": 0.1826,
      "step": 1974
    },
    {
      "epoch": 1.3053536021150034,
      "grad_norm": 0.5259665846824646,
      "learning_rate": 3.368307997356246e-06,
      "loss": 0.1586,
      "step": 1975
    },
    {
      "epoch": 1.3060145406477197,
      "grad_norm": 0.5306309461593628,
      "learning_rate": 3.367481824190351e-06,
      "loss": 0.1629,
      "step": 1976
    },
    {
      "epoch": 1.3066754791804363,
      "grad_norm": 0.5909976959228516,
      "learning_rate": 3.366655651024455e-06,
      "loss": 0.1928,
      "step": 1977
    },
    {
      "epoch": 1.3073364177131528,
      "grad_norm": 0.5257556438446045,
      "learning_rate": 3.3658294778585595e-06,
      "loss": 0.1526,
      "step": 1978
    },
    {
      "epoch": 1.307997356245869,
      "grad_norm": 0.46895939111709595,
      "learning_rate": 3.365003304692664e-06,
      "loss": 0.1551,
      "step": 1979
    },
    {
      "epoch": 1.3086582947785856,
      "grad_norm": 0.6101136207580566,
      "learning_rate": 3.3641771315267686e-06,
      "loss": 0.1838,
      "step": 1980
    },
    {
      "epoch": 1.3093192333113022,
      "grad_norm": 0.4433235824108124,
      "learning_rate": 3.3633509583608725e-06,
      "loss": 0.1619,
      "step": 1981
    },
    {
      "epoch": 1.3099801718440185,
      "grad_norm": 0.5255220532417297,
      "learning_rate": 3.3625247851949773e-06,
      "loss": 0.1519,
      "step": 1982
    },
    {
      "epoch": 1.310641110376735,
      "grad_norm": 0.4559022784233093,
      "learning_rate": 3.3616986120290816e-06,
      "loss": 0.1705,
      "step": 1983
    },
    {
      "epoch": 1.3113020489094513,
      "grad_norm": 0.5858221650123596,
      "learning_rate": 3.360872438863186e-06,
      "loss": 0.1938,
      "step": 1984
    },
    {
      "epoch": 1.3119629874421679,
      "grad_norm": 0.5056244134902954,
      "learning_rate": 3.3600462656972903e-06,
      "loss": 0.1528,
      "step": 1985
    },
    {
      "epoch": 1.3126239259748842,
      "grad_norm": 0.5293633341789246,
      "learning_rate": 3.359220092531395e-06,
      "loss": 0.1993,
      "step": 1986
    },
    {
      "epoch": 1.3132848645076007,
      "grad_norm": 0.4690733551979065,
      "learning_rate": 3.358393919365499e-06,
      "loss": 0.1703,
      "step": 1987
    },
    {
      "epoch": 1.3139458030403173,
      "grad_norm": 0.5675261616706848,
      "learning_rate": 3.3575677461996037e-06,
      "loss": 0.1534,
      "step": 1988
    },
    {
      "epoch": 1.3146067415730336,
      "grad_norm": 0.4758015275001526,
      "learning_rate": 3.3567415730337084e-06,
      "loss": 0.1558,
      "step": 1989
    },
    {
      "epoch": 1.3152676801057501,
      "grad_norm": 0.5366590619087219,
      "learning_rate": 3.3559153998678123e-06,
      "loss": 0.1533,
      "step": 1990
    },
    {
      "epoch": 1.3159286186384667,
      "grad_norm": 0.5577127933502197,
      "learning_rate": 3.355089226701917e-06,
      "loss": 0.1808,
      "step": 1991
    },
    {
      "epoch": 1.316589557171183,
      "grad_norm": 0.5498077869415283,
      "learning_rate": 3.3542630535360214e-06,
      "loss": 0.1359,
      "step": 1992
    },
    {
      "epoch": 1.3172504957038995,
      "grad_norm": 0.5863122344017029,
      "learning_rate": 3.3534368803701258e-06,
      "loss": 0.1939,
      "step": 1993
    },
    {
      "epoch": 1.317911434236616,
      "grad_norm": 0.5126022100448608,
      "learning_rate": 3.35261070720423e-06,
      "loss": 0.1722,
      "step": 1994
    },
    {
      "epoch": 1.3185723727693324,
      "grad_norm": 0.6526999473571777,
      "learning_rate": 3.351784534038335e-06,
      "loss": 0.1545,
      "step": 1995
    },
    {
      "epoch": 1.319233311302049,
      "grad_norm": 0.542748212814331,
      "learning_rate": 3.3509583608724388e-06,
      "loss": 0.1781,
      "step": 1996
    },
    {
      "epoch": 1.3198942498347654,
      "grad_norm": 0.5840931534767151,
      "learning_rate": 3.3501321877065435e-06,
      "loss": 0.1886,
      "step": 1997
    },
    {
      "epoch": 1.3205551883674818,
      "grad_norm": 0.5669766664505005,
      "learning_rate": 3.3493060145406483e-06,
      "loss": 0.1499,
      "step": 1998
    },
    {
      "epoch": 1.3212161269001983,
      "grad_norm": 0.5385528802871704,
      "learning_rate": 3.3484798413747526e-06,
      "loss": 0.1622,
      "step": 1999
    },
    {
      "epoch": 1.3218770654329148,
      "grad_norm": 0.7527719736099243,
      "learning_rate": 3.347653668208857e-06,
      "loss": 0.1876,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 6052,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.395535235776512e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
